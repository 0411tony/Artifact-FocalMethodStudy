public class aTest{ @Test public void testGetGUIDByteArray ( ) { byte [ ] baguid = null ; fr . gouv . vitam . common . guid . GUID guid = null ; try { guid = fr . gouv . vitam . common . guid . GUIDReader . getGUID ( fr . gouv . vitam . common . guid . GUIDReaderTest . BASE32 ) ; baguid = guid . getBytes ( ) ; } catch ( final fr . gouv . vitam . common . exception . InvalidGuidOperationException e ) { fr . gouv . vitam . common . guid . GUIDReaderTest . LOGGER . error ( ResourcesPublicUtilTest . SHOULD_NOT_HAVE_AN_EXCEPTION , e ) ; org . junit . Assert . fail ( ResourcesPublicUtilTest . SHOULD_NOT_HAVE_AN_EXCEPTION ) ; } try { final fr . gouv . vitam . common . guid . GUID bguid = fr . gouv . vitam . common . guid . GUIDReader . getGUID ( baguid ) ; org . junit . Assert . assertEquals ( bguid , guid ) ; } }
public class aTest{ @Test public void testNetworkExchangeOneTrace ( ) { org . eclipse . tracecompass . tmf . core . trace . ITmfTrace experiment = setUpExperiment ( "testfiles/graph/network_exchange_wifi.xml" ) ; org . junit . Assert . assertNotNull ( experiment ) ; try { org . eclipse . tracecompass . lttng2 . kernel . core . tests . analysis . graph . DistributedCriticalPathTest . internalTestNetworkExchangeOneTrace ( experiment ) ; } }
public class aTest{ @Test public void testJUnitHamcrestMatcherFailureWorks ( ) { try { org . junit . Assert . assertThat ( 1 , org . hamcrest . CoreMatchers . equalTo ( 2 ) ) ; } }
public class aTest{ @Test public void testCacheTiming ( ) { java . lang . String filename = "file:testCacheTiming.xml" ; System . out . printf ( "%s%n" , filename ) ; java . lang . String cacheDirName = tempFolder . newFolder ( ) . getAbsolutePath ( ) ; System . out . printf ( "cacheDir=%s%n" , cacheDirName ) ; java . io . File cacheDir = new java . io . File ( cacheDirName ) ; org . apache . commons . io . FileUtils . deleteDirectory ( cacheDir ) ; assert ! ( cacheDir . exists ( ) ) ; ucar . nc2 . util . DiskCache2 cache = new ucar . nc2 . util . DiskCache2 ( cacheDirName , false , 0 , 0 ) ; cache . setAlwaysUseCache ( true ) ; org . junit . Assert . assertEquals ( cache . getRootDirectory ( ) , cacheDirName ) ; assert new java . io . File ( cache . getRootDirectory ( ) ) . exists ( ) ; ucar . nc2 . ncml . Aggregation . setPersistenceCache ( cache ) ; AggregationExisting . countCacheUse = 0 ; long start = java . lang . System . currentTimeMillis ( ) ; try ( ucar . nc2 . NetcdfFile ncfile = ucar . nc2 . ncml . NcMLReader . readNcML ( new java . io . StringReader ( ncml2 ) , filename , null ) ) { System . out . printf ( "%nTestNcmlAggExisting.open<sp>%s%n" , filename ) ; ucar . nc2 . Variable time = ncfile . findVariable ( "time" ) ; System . out . printf ( "<sp>Variable<sp>%s%n" , time . getNameAndDimensions ( ) ) ; time . read ( ) ; } }
public class aTest{ @Test public void test_WHEN_MetricNameSuffixRequested_THEN_ShouldReturnCorrectSample ( ) { final java . util . Map < java . lang . String , java . lang . String > labels = new java . util . HashMap < java . lang . String , java . lang . String > ( ) ; labels . put ( "service" , "${0}" ) ; labels . put ( "status" , "s_${1}" ) ; final io . prometheus . client . dropwizard . samplebuilder . MapperConfig mapperConfig = new io . prometheus . client . dropwizard . samplebuilder . MapperConfig ( "app.okhttpclient.client.HttpClient.*.*" , "${0}" 0 , labels ) ; final java . util . List < io . prometheus . client . dropwizard . samplebuilder . MapperConfig > mapperConfigs = java . util . Arrays . asList ( new io . prometheus . client . dropwizard . samplebuilder . MapperConfig ( "${0}" 2 ) , mapperConfig , new io . prometheus . client . dropwizard . samplebuilder . MapperConfig ( "app.okhttpclient.client.HttpClient.*.total" ) ) ; final io . prometheus . client . dropwizard . samplebuilder . CustomMappingSampleBuilder converter = new io . prometheus . client . dropwizard . samplebuilder . CustomMappingSampleBuilder ( mapperConfigs ) ; final io . prometheus . client . Collector . MetricFamilySamples . Sample expectedResult = new io . prometheus . client . Collector . MetricFamilySamples . Sample ( "app_okhttpclient_client_HttpClient_greatService_suffix" , java . util . Arrays . asList ( "service" , "status" ) , java . util . Arrays . asList ( "greatService" , "s_400" ) , 1.0 ) ; final io . prometheus . client . Collector . MetricFamilySamples . Sample result = converter . createSample ( "${0}" 1 , "_suffix" , java . util . Collections . < java . lang . String > emptyList ( ) , java . util . Collections . < java . lang . String > emptyList ( ) , 1.0 ) ; org . junit . Assert . assertEquals ( expectedResult , result ) ; } }
public class aTest{ @Test public void shouldWriteDoubleToFile ( ) { tdd . junit . FileWriter writer = null ; java . io . File file = null ; try { file = java . io . File . createTempFile ( "FileWriterTest" , "" ) ; writer = new tdd . junit . FileWriter ( file ) ; writer . writeDouble ( 40.4 ) ; org . junit . Assert . assertEquals ( 40.4 , readDouble ( file ) , 0 ) ; } }
public class aTest{ @Test public void testModifyUserTypeCheckXML ( ) { java . lang . String userName = "test" ; org . irods . jargon . core . protovalues . UserTypeEnum userType = org . irods . jargon . core . protovalues . UserTypeEnum . RODS_ADMIN ; org . irods . jargon . core . packinstr . GeneralAdminInp pi = org . irods . jargon . core . packinstr . GeneralAdminInp . instanceForModifyUserType ( userName , userType ) ; java . lang . String tagOut = pi . getParsedTags ( ) ; java . lang . StringBuilder sb = new java . lang . StringBuilder ( ) ; sb . append ( "<generalAdminInp_PI><arg0>modify</arg0>\n" ) ; sb . append ( "<arg1>user</arg1>\n" ) ; sb . append ( "<arg2>test</arg2>\n" ) ; sb . append ( "<arg3>type</arg3>\n" ) ; sb . append ( "<arg4>rodsadmin</arg4>\n" ) ; sb . append ( "<arg5></arg5>\n" ) ; sb . append ( "<arg6></arg6>\n" ) ; sb . append ( "<arg3>type</arg3>\n" 0 ) ; sb . append ( "<arg3>type</arg3>\n" 1 ) ; sb . append ( "<arg9></arg9>\n" ) ; sb . append ( "</generalAdminInp_PI>\n" ) ; org . junit . Assert . assertEquals ( "<arg3>type</arg3>\n" 2 , sb . toString ( ) , tagOut ) ; } }
public class aTest{ @Test public void recordAndVerifyWithMixedCascadeLevels ( mockit . CascadingParametersTest$SocketFactory ) { new mockit . Expectations ( ) { { sf . createSocket ( "first" , 80 ) . getKeepAlive ( ) ; result = true ; sf . createSocket ( "second" , anyInt ) . getChannel ( ) . close ( ) ; times = 1 ; } } ; sf . createSocket ( "second" , 80 ) . getChannel ( ) . close ( ) ; org . junit . Assert . assertTrue ( sf . createSocket ( "first" , 80 ) . getKeepAlive ( ) ) ; sf . createSocket ( "first" , 8080 ) . getChannel ( ) . provider ( ) . openPipe ( ) ; new mockit . Verifications ( ) { { sf . createSocket ( "first" , 8080 ) . getChannel ( ) . provider ( ) . openPipe ( ) ; } } }
public class aTest{ @Test public void testAddIntelHost ( ) { java . util . List < com . intel . mtwilson . datatypes . ConnectionString > hosts = com . intel . mtwilson . My . env ( ) . getHostConnectionList ( ) ; java . util . List < com . intel . mtwilson . datatypes . ConnectionString > intel = getVendorHosts ( hosts , Vendor . INTEL ) ; if ( intel . isEmpty ( ) ) { throw new java . lang . IllegalArgumentException ( "No<sp>Intel<sp>hosts<sp>in<sp>your<sp>environment" ) ; } for ( com . intel . mtwilson . datatypes . ConnectionString conn : intel ) { try { System . out . println ( ( "Adding<sp>" + conn ) ) ; com . intel . mtwilson . datatypes . TxtHostRecord gkvHost = new com . intel . mtwilson . datatypes . TxtHostRecord ( ) ; gkvHost . HostName = conn . getManagementServerName ( ) ; gkvHost . AddOn_Connection_String = conn . getConnectionStringWithPrefix ( ) ; com . intel . mtwilson . datatypes . HostConfigData hostdata = new com . intel . mtwilson . datatypes . HostConfigData ( ) ; hostdata . setRegisterHost ( false ) ; hostdata . setTxtHostRecord ( gkvHost ) ; boolean success = com . intel . mtwilson . My . client ( ) . configureWhiteList ( hostdata ) ; org . junit . Assert . assertTrue ( success ) ; } }
public class aTest{ @Test public void testNullsRankDate ( ) { java . lang . String sqlText = java . lang . String . format ( ( "HIREDATE<sp>|DEPT<sp>|RANKHIRE<sp>|\n" 4 + ( "HIREDATE<sp>|DEPT<sp>|RANKHIRE<sp>|\n" 8 + "ORDER<sp>BY<sp>hiredate,<sp>dept" ) ) , this . getTableReference ( com . splicemachine . derby . impl . sql . execute . operations . WindowFunctionIT . EMPTAB_NULLS ) , useSpark ) ; java . sql . ResultSet rs = com . splicemachine . derby . impl . sql . execute . operations . WindowFunctionIT . methodWatcher . executeQuery ( sqlText ) ; java . lang . String expected = "HIREDATE<sp>|DEPT<sp>|RANKHIRE<sp>|\n" + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "HIREDATE<sp>|DEPT<sp>|RANKHIRE<sp>|\n" 2 + "HIREDATE<sp>|DEPT<sp>|RANKHIRE<sp>|\n" 6 ) + "HIREDATE<sp>|DEPT<sp>|RANKHIRE<sp>|\n" 6 ) + "2010-04-12<sp>|<sp>3<sp>|<sp>1<sp>|\n" ) + "2010-08-09<sp>|<sp>1<sp>|<sp>3<sp>|\n" ) + "2010-08-09<sp>|<sp>3<sp>|<sp>2<sp>|\n" ) + "2010-08-09<sp>|<sp>3<sp>|<sp>2<sp>|\n" ) + "HIREDATE<sp>|DEPT<sp>|RANKHIRE<sp>|\n" 0 ) + "2011-10-15<sp>|<sp>1<sp>|<sp>5<sp>|\n" ) + "HIREDATE<sp>|DEPT<sp>|RANKHIRE<sp>|\n" 1 ) + "HIREDATE<sp>|DEPT<sp>|RANKHIRE<sp>|\n" 5 ) + "HIREDATE<sp>|DEPT<sp>|RANKHIRE<sp>|\n" 5 ) + "2012-04-03<sp>|<sp>3<sp>|<sp>4<sp>|\n" ) + "HIREDATE<sp>|DEPT<sp>|RANKHIRE<sp>|\n" 3 ) + "2013-04-24<sp>|<sp>3<sp>|<sp>5<sp>|\n" ) + "HIREDATE<sp>|DEPT<sp>|RANKHIRE<sp>|\n" 7 ) + "2013-12-20<sp>|<sp>2<sp>|<sp>4<sp>|\n" ) + "2014-03-04<sp>|<sp>1<sp>|<sp>8<sp>|" ) ; org . junit . Assert . assertEquals ( ( ( "HIREDATE<sp>|DEPT<sp>|RANKHIRE<sp>|\n" 9 + sqlText ) + "HIREDATE<sp>|DEPT<sp>|RANKHIRE<sp>|\n" 9 ) , expected , TestUtils . FormattedResult . ResultFactory . toStringUnsorted ( rs ) ) ; rs . close ( ) ; } }
public class aTest{ @Test public void testCanConnectToTargetWithLoop3 ( ) { try { org . talend . core . model . components . IComponent component = org . talend . core . ui . component . ComponentsFactoryProvider . getInstance ( ) . get ( "tJava" , ComponentCategory . CATEGORY_4_DI . getName ( ) ) ; org . talend . designer . core . ui . editor . nodes . Node source2 = new org . talend . designer . core . ui . editor . nodes . Node ( component , org . talend . designer . core . model . process . ConnectionManagerTest . sourceProcess ) ; org . talend . designer . core . ui . editor . nodes . Node middle2 = new org . talend . designer . core . ui . editor . nodes . Node ( component , org . talend . designer . core . model . process . ConnectionManagerTest . sourceProcess ) ; org . talend . designer . core . ui . editor . nodes . Node target2 = new org . talend . designer . core . ui . editor . nodes . Node ( component , org . talend . designer . core . model . process . ConnectionManagerTest . sourceProcess ) ; org . talend . designer . core . ui . editor . connections . Connection connection3 = new org . talend . designer . core . ui . editor . connections . Connection ( middle2 , source2 , org . talend . core . model . process . EConnectionType . ON_SUBJOB_OK , EConnectionType . ON_SUBJOB_OK . getName ( ) , "test5" , "test5" , "test5" , false ) ; org . talend . designer . core . ui . editor . connections . Connection connection4 = new org . talend . designer . core . ui . editor . connections . Connection ( middle2 , target2 , org . talend . core . model . process . EConnectionType . ON_SUBJOB_ERROR , EConnectionType . ON_SUBJOB_ERROR . getName ( ) , "test6" , "test6" , "test6" , false ) ; boolean canConnect = org . talend . designer . core . model . process . ConnectionManager . canConnectToTarget ( target2 , null , source2 , EConnectionType . RUN_IF , EConnectionType . RUN_IF . getName ( ) , "test_5_6" ) ; org . junit . Assert . assertTrue ( canConnect ) ; } }
public class aTest{ @Test public void testWriteCug ( ) { org . apache . jackrabbit . oak . api . ContentSession cs = createTestSession2 ( ) ; org . apache . jackrabbit . oak . api . Root r = cs . getLatestRoot ( ) ; try { org . apache . jackrabbit . oak . api . Tree tree = r . getTree ( "/content/a/rep:cugPolicy" ) ; tree . setProperty ( org . apache . jackrabbit . oak . spi . security . authorization . cug . impl . REP_PRINCIPAL_NAMES , com . google . common . collect . ImmutableList . of ( EveryonePrincipal . NAME , testGroupPrincipal . getName ( ) ) , Type . STRINGS ) ; r . commit ( ) ; org . junit . Assert . fail ( ) ; } catch ( org . apache . jackrabbit . oak . api . CommitFailedException e ) { org . junit . Assert . assertTrue ( e . isAccessViolation ( ) ) ; } }
public class aTest{ @Test public void test_search_with_moduleNames_componentName_flowName_keyword_success ( ) { java . nio . file . Path path = createTempDir ( ) ; org . apache . solr . core . SolrResourceLoader loader = new org . apache . solr . core . SolrResourceLoader ( path ) ; org . apache . solr . core . NodeConfig config = new org . apache . solr . core . NodeConfig . NodeConfigBuilder ( "testnode" , loader ) . setConfigSetBaseDirectory ( java . nio . file . Paths . get ( org . ikasan . wiretap . dao . SolrWiretapDaoTest . TEST_HOME ( ) ) . resolve ( "configsets" ) . toString ( ) ) . build ( ) ; try ( org . apache . solr . client . solrj . embedded . EmbeddedSolrServer server = new org . apache . solr . client . solrj . embedded . EmbeddedSolrServer ( config , "ikasan" ) ) { org . apache . solr . client . solrj . request . CoreAdminRequest . Create createRequest = new org . apache . solr . client . solrj . request . CoreAdminRequest . Create ( ) ; createRequest . setCoreName ( "ikasan" ) ; createRequest . setConfigSet ( "minimal" ) ; server . request ( createRequest ) ; java . util . HashMap < java . lang . String , java . lang . Object > fields = new java . util . HashMap ( ) ; fields . put ( "testnode" 0 , new java . lang . Integer ( 1 ) ) ; org . apache . solr . client . solrj . request . schema . SchemaRequest . AddField schemaRequest = new org . apache . solr . client . solrj . request . schema . SchemaRequest . AddField ( fields ) ; server . request ( schemaRequest ) ; org . ikasan . wiretap . dao . SolrWiretapDao solrCloudBase = new org . ikasan . wiretap . dao . SolrWiretapDao ( ) ; solrCloudBase . setSolrClient ( server ) ; solrCloudBase . setDaysToKeep ( 0 ) ; org . ikasan . wiretap . model . SolrWiretapEvent event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 1L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "event" ) ; solrCloudBase . save ( event ) ; event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 2L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "event" ) ; solrCloudBase . save ( event ) ; event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 3L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "testnode" 2 ) ; solrCloudBase . save ( event ) ; event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 4L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "event" ) ; solrCloudBase . save ( event ) ; event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 5L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "event" ) ; solrCloudBase . save ( event ) ; java . util . HashSet < java . lang . String > moduleNames = new java . util . HashSet < java . lang . String > ( ) ; moduleNames . add ( "moduleName" ) ; org . ikasan . spec . search . PagedSearchResult < org . ikasan . spec . wiretap . WiretapEvent > results = solrCloudBase . findWiretapEvents ( 0 , 10 , "testnode" 1 , true , moduleNames , "flowName" , "componentName" , null , null , new java . util . Date ( ( ( java . lang . System . currentTimeMillis ( ) ) - 100000000 ) ) , new java . util . Date ( ( ( java . lang . System . currentTimeMillis ( ) ) + 100000000 ) ) , "testnode" 2 ) ; org . junit . Assert . assertEquals ( "testnode" 3 , results . getResultSize ( ) , 2 ) ; server . close ( ) ; } } }
public class aTest{ @Test public void testInternetWebsite ( ) { org . opennms . netmgt . poller . monitors . SSLCertMonitor monitor = new org . opennms . netmgt . poller . monitors . SSLCertMonitor ( ) ; java . util . Map < java . lang . String , java . lang . Object > parameters = new java . util . concurrent . ConcurrentSkipListMap < java . lang . String , java . lang . Object > ( ) ; parameters . put ( "port" , "timeout" 0 ) ; parameters . put ( "retry" , "0" ) ; parameters . put ( "timeout" , "500" ) ; parameters . put ( "verbose" , "true" ) ; parameters . put ( "timeout" 1 , "5" ) ; org . opennms . netmgt . poller . MonitoredService svc = org . opennms . netmgt . poller . mock . MonitorTestUtils . getMonitoredService ( 3 , "www.google.com" , org . opennms . netmgt . utils . DnsUtils . resolveHostname ( "www.google.com" , false ) , "SSLCert" ) ; org . opennms . netmgt . poller . PollStatus status = monitor . poll ( svc , parameters ) ; org . junit . Assert . assertTrue ( status . isAvailable ( ) ) ; } }
public class aTest{ @Test public void testRestrictedIPAddressExampleFromBook ( ) { diskCacheV111 . poolManager . PoolSelectionUnitV2 psu = new diskCacheV111 . poolManager . PoolSelectionUnitV2 ( ) ; dmg . util . CommandInterpreter ci = new dmg . util . CommandInterpreter ( psu ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>create<sp>unit<sp>-store<sp>*@*" ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>addto<sp>link<sp>write-link<sp>write-pools" 9 ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>create<sp>pool<sp>write-pool" ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>addto<sp>ugroup<sp>write-cond<sp>111.111.111.203/255.255.255.255" 2 ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>addto<sp>link<sp>write-link<sp>write-pools" 5 ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>addto<sp>pgroup<sp>read-pools<sp>read-pool" ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>addto<sp>ugroup<sp>write-cond<sp>111.111.111.203/255.255.255.255" 0 ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>create<sp>unit<sp>-net<sp>111.111.111.0/255.255.255.0" ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>addto<sp>link<sp>write-link<sp>write-pools" 0 ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>addto<sp>ugroup<sp>allnet-cond<sp>111.111.111.0/255.255.255.0" ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>addto<sp>link<sp>write-link<sp>write-pools" 2 ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>addto<sp>ugroup<sp>write-cond<sp>111.111.111.203/255.255.255.255" 3 ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>create<sp>unit<sp>-net<sp>111.111.111.203/255.255.255.255" ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>create<sp>ugroup<sp>write-cond" ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>addto<sp>link<sp>write-link<sp>write-pools" 4 ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>addto<sp>link<sp>write-link<sp>write-pools" 6 ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>addto<sp>ugroup<sp>write-cond<sp>111.111.111.203/255.255.255.255" ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>addto<sp>ugroup<sp>write-cond<sp>111.111.111.203/255.255.255.255" 1 ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>addto<sp>ugroup<sp>write-cond<sp>111.111.111.203/255.255.255.255" 4 ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>addto<sp>link<sp>write-link<sp>write-pools" 1 ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>addto<sp>link<sp>write-link<sp>write-pools" 7 ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>addto<sp>link<sp>write-link<sp>write-pools" 3 ) ) ; ci . command ( new org . dcache . util . Args ( "psu<sp>addto<sp>link<sp>write-link<sp>write-pools" ) ) ; org . dcache . vehicles . FileAttributes fileAttributes = new org . dcache . vehicles . FileAttributes ( ) ; diskCacheV111 . vehicles . StorageInfos . injectInto ( diskCacheV111 . vehicles . GenericStorageInfo . valueOf ( "psu<sp>addto<sp>link<sp>write-link<sp>write-pools" 8 , "psu<sp>addto<sp>link<sp>write-link<sp>write-pools" 8 ) , fileAttributes ) ; diskCacheV111 . poolManager . PoolPreferenceLevel [ ] preference = psu . match ( DirectionType . READ , "111.111.111.201" , null , fileAttributes , null ) ; org . junit . Assert . assertEquals ( 0 , preference . length ) ; } }
public class aTest{ @Test public void shouldGetTwoUsersPassingAnArray ( ) { org . apache . ibatis . session . SqlSession sqlSession = org . apache . ibatis . submitted . collectionparameters . CollectionParametersTest . sqlSessionFactory . openSession ( ) ; try { org . apache . ibatis . submitted . collectionparameters . Mapper mapper = sqlSession . getMapper ( org . apache . ibatis . submitted . collectionparameters . Mapper . class ) ; java . lang . Integer [ ] list = new java . lang . Integer [ 2 ] ; list [ 0 ] = 1 ; list [ 1 ] = 2 ; java . util . List < org . apache . ibatis . submitted . collectionparameters . User > users = mapper . getUsersFromArray ( list ) ; org . junit . Assert . assertEquals ( 2 , users . size ( ) ) ; } }
public class aTest{ @Test public void testAddFirst ( ) { com . sun . sgs . test . app . util . TestScalableDeque . txnScheduler . runTask ( new com . sun . sgs . test . util . TestAbstractKernelRunnable ( ) { public void run ( ) throws com . sun . sgs . test . app . util . Exception { com . sun . sgs . app . util . ScalableDeque < java . lang . Integer > d = new com . sun . sgs . app . util . ScalableDeque < java . lang . Integer > ( ) ; d . addFirst ( 1 ) ; org . junit . Assert . assertEquals ( 1 , ( ( int ) ( d . getFirst ( ) ) ) ) ; } } }
public class aTest{ @Test public void applyTest ( ) { java . lang . String json = "{<sp>\"html\"<sp>:<sp>\"1<h>2</h>3\"<sp>}" ; java . lang . String [ ] [ ] tests = new java . lang . String [ ] [ ] { new java . lang . String [ ] { "{{<sp>nil<sp>|<sp>strip_html<sp>}}" , "" } , new java . lang . String [ ] { "{{<sp>456<sp>|<sp>strip_html<sp>}}" , "456" } , new java . lang . String [ ] { "{{<sp>'45<6'<sp>|<sp>strip_html<sp>}}" , "45<6" } , new java . lang . String [ ] { "45<6" 1 , "" } , new java . lang . String [ ] { "45<6" 0 , "123" } } ; for ( java . lang . String [ ] test : tests ) { liqp . Template template = liqp . Template . parse ( test [ 0 ] ) ; java . lang . String rendered = java . lang . String . valueOf ( template . render ( json ) ) ; org . junit . Assert . assertThat ( rendered , org . hamcrest . CoreMatchers . is ( test [ 1 ] ) ) ; } } }
public class aTest{ @Test public void testProcessorEvents1 ( ) { java . lang . String events = recordRichStringProcessorEvents ( ( "acceptTemplateLineBreak()\n" 0 + ( "<sp>�true�\n" + "'''" ) ) ) ; java . lang . String expected = "announceNextLiteral()\n" + ( ( ( ( ( ( ( ( ( "acceptTemplateText()\n" + "acceptTemplateLineBreak()\n" ) + "acceptTemplateText(<sp>)\n" ) + "acceptSemanticText()\n" ) + "acceptSemanticText()\n" ) + "acceptExpression(�true�)\n" ) + "announceNextLiteral()\n" ) + "acceptSemanticText()\n" ) + "acceptSemanticLineBreak()\n" ) + "acceptTemplateText()" ) ; org . junit . Assert . assertEquals ( expected , events ) ; } }
public class aTest{ @Test public void findFromDOCXToPDFViaITextConverter ( ) { try { fr . opensagres . xdocreport . converter . Options o = fr . opensagres . xdocreport . converter . Options . getFrom ( DocumentKind . DOCX ) . to ( ConverterTypeTo . PDF ) . via ( ConverterTypeVia . XWPF ) ; fr . opensagres . xdocreport . converter . IConverter converter = fr . opensagres . xdocreport . converter . ConverterRegistry . getRegistry ( ) . getConverter ( o ) ; org . junit . Assert . assertNotNull ( converter ) ; } }
public class aTest{ @Test public void shouldPerformClassLoadForAcquiredClasses ( ) { java . lang . ClassLoader classLoader = new org . robolectric . internal . bytecode . SandboxClassLoader ( configureBuilder ( ) . build ( ) ) ; java . lang . Class < ? > exampleClass = classLoader . loadClass ( org . robolectric . testing . AnUninstrumentedClass . class . getName ( ) ) ; org . junit . Assert . assertSame ( classLoader , exampleClass . getClassLoader ( ) ) ; try { exampleClass . getField ( ShadowConstants . CLASS_HANDLER_DATA_FIELD_NAME ) ; org . junit . Assert . fail ( "class<sp>shouldn't<sp>be<sp>instrumented!" ) ; } }
public class aTest{ @Test public void testFreeBlob ( ) { java . sql . Statement stmt = conn . createStatement ( ) ; stmt . execute ( "INSERT<sp>INTO<sp>blobtest<sp>VALUES<sp>(1,<sp>lo_creat(-1))" ) ; java . sql . ResultSet rs = stmt . executeQuery ( "SELECT<sp>data<sp>FROM<sp>blobtest" ) ; org . junit . Assert . assertTrue ( rs . next ( ) ) ; java . sql . Blob blob = rs . getBlob ( 1 ) ; blob . free ( ) ; try { blob . length ( ) ; org . junit . Assert . fail ( "Should<sp>have<sp>thrown<sp>an<sp>Exception<sp>because<sp>it<sp>was<sp>freed." ) ; } }
public class aTest{ @Test public void OAuthImplicitGrantForDesktopMobile_GetAuthorizationUrl_ReturnsCorrectUrl ( ) { com . microsoft . bingads . OAuthDesktopMobileImplicitGrant auth = com . microsoft . bingads . internal . OAuthImplicitGrantForDesktopMobileAppTest . CreateAuth ( "test_id" ) ; java . net . URL authorizationUrl = auth . getAuthorizationEndpoint ( ) ; try { java . net . URL expectedUrl = new java . net . URL ( ( "https://login.live.com/oauth20_authorize.srf?" + ( ( ( "scope=bingads.manage&" + "response_type=token&" ) + "redirect_uri=https%3A%2F%2Flogin.live.com%2Foauth20_desktop.srf&" ) + "client_id=test_id" ) ) ) ; org . junit . Assert . assertEquals ( expectedUrl , authorizationUrl ) ; } }
public class aTest{ @Test public void shouldInjectAdvancedFieldSort ( ) { com . couchbase . client . java . search . SearchQuery p = new com . couchbase . client . java . search . SearchQuery ( null , null ) . sort ( sortField ( "first" 0 ) , sortField ( "f" ) . missing ( FieldMissing . FIRST ) . mode ( FieldMode . DEFAULT ) . type ( FieldType . AUTO ) . descending ( true ) ) ; com . couchbase . client . java . document . json . JsonObject result = com . couchbase . client . java . document . json . JsonObject . empty ( ) ; p . injectParams ( result ) ; com . couchbase . client . java . document . json . JsonObject expected = com . couchbase . client . java . document . json . JsonObject . create ( ) . put ( "sort" , com . couchbase . client . java . document . json . JsonArray . from ( com . couchbase . client . java . document . json . JsonObject . create ( ) . put ( "by" , "field" ) . put ( "field" , "first" 0 ) , com . couchbase . client . java . document . json . JsonObject . create ( ) . put ( "by" , "field" ) . put ( "field" , "f" ) . put ( "desc" , true ) . put ( "mode" , "default" ) . put ( "first" 1 , "first" ) . put ( "type" , "auto" ) ) ) ; org . junit . Assert . assertEquals ( expected , result ) ; } }
public class aTest{ @Test public void shouldNotImportInitialContentIfWorkspaceContentsChanged ( ) { startRunStop ( ( repository ) -> { javax . jcr . Session ws1Session = repository . login ( ) ; javax . jcr . Node node = ws1Session . getNode ( "/a" ) ; org . junit . Assert . assertNotNull ( node ) ; node . remove ( ) ; ws1Session . getRootNode ( ) . addNode ( "testNode" ) ; ws1Session . save ( ) ; } }
public class aTest{ @Test public void testLogin ( ) { org . sonatype . nexus . security . SecuritySystem securitySystem = this . getSecuritySystem ( ) ; org . apache . shiro . authc . UsernamePasswordToken token = new org . apache . shiro . authc . UsernamePasswordToken ( "jcoder" , "jcoder" ) ; org . apache . shiro . subject . Subject subject = securitySystem . getSubject ( ) ; org . junit . Assert . assertNotNull ( subject ) ; subject . login ( token ) ; try { subject . login ( new org . apache . shiro . authc . UsernamePasswordToken ( "jcoder" , "INVALID" ) ) ; org . junit . Assert . fail ( "expected<sp>AuthenticationException" ) ; } }
public class aTest{ @Test public void simpleQueryWithBindingSetJoinOnProperty ( ) { final org . apache . rya . mongodb . MongoDBRyaDAO dao = new org . apache . rya . mongodb . MongoDBRyaDAO ( ) ; try { dao . setConf ( conf ) ; dao . init ( ) ; final org . apache . rya . api . domain . StatementMetadata metadata = new org . apache . rya . api . domain . StatementMetadata ( ) ; metadata . addMetadata ( new org . apache . rya . api . domain . RyaIRI ( "http://createdBy" ) , new org . apache . rya . api . domain . RyaType ( "Joe" ) ) ; metadata . addMetadata ( new org . apache . rya . api . domain . RyaIRI ( "http://createdOn" ) , new org . apache . rya . api . domain . RyaType ( org . eclipse . rdf4j . model . vocabulary . XMLSchema . DATE , "2017-01-04" ) ) ; final org . apache . rya . api . domain . RyaStatement statement1 = new org . apache . rya . api . domain . RyaStatement ( new org . apache . rya . api . domain . RyaIRI ( "http://Joe" ) , new org . apache . rya . api . domain . RyaIRI ( "2017-01-04" 1 ) , new org . apache . rya . api . domain . RyaType ( "2017-01-04" 0 ) , new org . apache . rya . api . domain . RyaIRI ( "http://context" ) , "" , metadata ) ; dao . add ( statement1 ) ; final org . eclipse . rdf4j . query . parser . sparql . SPARQLParser parser = new org . eclipse . rdf4j . query . parser . sparql . SPARQLParser ( ) ; final org . eclipse . rdf4j . query . parser . ParsedQuery pq = parser . parseQuery ( query , null ) ; final java . util . List < org . eclipse . rdf4j . query . algebra . StatementPattern > spList = org . eclipse . rdf4j . query . algebra . helpers . StatementPatternCollector . process ( pq . getTupleExpr ( ) ) ; final org . apache . rya . indexing . statement . metadata . matching . StatementMetadataNode < org . apache . rya . mongodb . MongoDBRdfConfiguration > node = new org . apache . rya . indexing . statement . metadata . matching . StatementMetadataNode ( spList , conf ) ; final org . eclipse . rdf4j . query . algebra . evaluation . QueryBindingSet bsConstraint = new org . eclipse . rdf4j . query . algebra . evaluation . QueryBindingSet ( ) ; bsConstraint . addBinding ( "x" , org . apache . rya . indexing . statement . metadata . MongoStatementMetadataNodeIT . VF . createLiteral ( "2017-01-04" 0 ) ) ; bsConstraint . addBinding ( "y" , org . apache . rya . indexing . statement . metadata . MongoStatementMetadataNodeIT . VF . createLiteral ( "Doug" ) ) ; final org . eclipse . rdf4j . common . iteration . CloseableIteration < org . eclipse . rdf4j . query . BindingSet , org . eclipse . rdf4j . query . QueryEvaluationException > iteration = node . evaluate ( bsConstraint ) ; final java . util . List < org . eclipse . rdf4j . query . BindingSet > bsList = new java . util . ArrayList ( ) ; while ( iteration . hasNext ( ) ) { bsList . add ( iteration . next ( ) ) ; } org . junit . Assert . assertEquals ( 0 , bsList . size ( ) ) ; dao . delete ( statement1 , conf ) ; } }
public class aTest{ @Test public void testDescribeLoadBalancerTCPListenerAttribute ( ) { try { com . fit2cloud . aliyun . slb . model . request . DescribeLoadBalancerTCPListenerAttributeRequest request = new com . fit2cloud . aliyun . slb . model . request . DescribeLoadBalancerTCPListenerAttributeRequest ( ) ; request . setLoadBalancerId ( loadBalancerId ) ; request . setListenerPort ( 553 ) ; com . fit2cloud . aliyun . Response response = client . describeLoadBalancerTCPListenerAttribute ( request ) ; System . out . println ( ( "testDescribeLoadBalancerTCPListenerAttribute<sp>::<sp>" + ( new com . google . gson . Gson ( ) . toJson ( response ) ) ) ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void shouldFailWhenIncreasingIfChosenShardIsFilledConcurrently ( ) { updateLimitInStorage ( com . spotify . styx . util . ShardedCounterTest . COUNTER_ID1 , 1 ) ; org . junit . Assert . assertEquals ( 0 , com . spotify . styx . util . ShardedCounterTest . shardedCounter . getCounter ( com . spotify . styx . util . ShardedCounterTest . COUNTER_ID1 ) ) ; com . spotify . styx . util . ShardedCounterTest . shardedCounter = spy ( com . spotify . styx . util . ShardedCounterTest . shardedCounter ) ; doAnswer ( ( invocation ) -> { final java . lang . Integer shardIndex = invocation . getArgument ( 3 ) ; final java . lang . String counterId = invocation . getArgument ( 1 ) ; updateShard ( counterId , shardIndex , 1L ) ; invocation . callRealMethod ( ) ; return null ; } }
public class aTest{ @Test public void testNewPacket ( ) { try { org . pcap4j . packet . IcmpV4InformationRequestPacket p = org . pcap4j . packet . IcmpV4InformationRequestPacket . newPacket ( packet . getRawData ( ) , 0 , packet . getRawData ( ) . length ) ; org . junit . Assert . assertEquals ( packet , p ) ; } }
public class aTest{ @Test public void workflowVerifiedInformationMigrationTest ( ) { io . dropwizard . Application < io . dockstore . webservice . DockstoreWebserviceConfiguration > application = io . dockstore . client . cli . VerifiedInformationMigrationIT . SUPPORT . getApplication ( ) ; try { application . run ( "db" , "verify" 9 , "--script" 0 , CommonTestUtilities . CONFIDENTIAL_CONFIG_PATH ) ; java . util . List < java . lang . String > migrationList = java . util . Arrays . asList ( "verify" 8 , "1.3.1.consistency" , "--script" 2 , "testworkflow" ) ; io . dockstore . common . CommonTestUtilities . runMigration ( migrationList , application , CommonTestUtilities . CONFIDENTIAL_CONFIG_PATH ) ; } catch ( java . lang . Exception e ) { org . junit . Assert . fail ( "verify" 0 ) ; } final io . dockstore . common . CommonTestUtilities . TestingPostgres testingPostgres = getTestingPostgres ( ) ; testingPostgres . runUpdateStatement ( "verify" 2 ) ; io . dockstore . client . cli . Client . main ( new java . lang . String [ ] { "verify" 6 , io . dropwizard . testing . ResourceHelpers . resourceFilePath ( "--script" 1 ) , "verify" 5 , "verify" , "verify" 3 , ( SourceControl . GITHUB . toString ( ) ) + "--script" 3 , "--verified-source" , "Docker<sp>testing<sp>group" , "--version" , "verify" 1 , "--script" } ) ; try { java . util . List < java . lang . String > migrationList = java . util . Arrays . asList ( "verify" 4 ) ; io . dockstore . common . CommonTestUtilities . runMigration ( migrationList , application , CommonTestUtilities . CONFIDENTIAL_CONFIG_PATH ) ; } catch ( java . lang . Exception e ) { org . junit . Assert . fail ( "Could<sp>not<sp>run<sp>1.5.0<sp>migration" ) ; } final long afterMigrationVerifiedCount = testingPostgres . runSelectStatement ( "select<sp>count(*)<sp>from<sp>sourcefile_verified" , new org . apache . commons . dbutils . handlers . ScalarHandler ( ) ) ; org . junit . Assert . assertEquals ( ( "verify" 7 + afterMigrationVerifiedCount ) , 2 , afterMigrationVerifiedCount ) ; } }
public class aTest{ @Test public void testReceiveAfterCoordinatorLinkClosedDuringTX ( ) { try ( org . apache . qpid . jms . test . testpeer . TestAmqpPeer testPeer = new org . apache . qpid . jms . test . testpeer . TestAmqpPeer ( ) ) { javax . jms . Connection connection = testFixture . establishConnecton ( testPeer ) ; connection . start ( ) ; testPeer . expectBegin ( ) ; testPeer . expectCoordinatorAttach ( ) ; org . apache . qpid . proton . amqp . Binary txnId = new org . apache . qpid . proton . amqp . Binary ( new byte [ ] { ( ( byte ) ( 5 ) ) , ( ( byte ) ( 6 ) ) , ( ( byte ) ( 7 ) ) , ( ( byte ) ( 8 ) ) } ) ; testPeer . expectDeclare ( txnId ) ; javax . jms . Session session = connection . createSession ( true , Session . SESSION_TRANSACTED ) ; javax . jms . Queue queue = session . createQueue ( "myQueue" ) ; org . apache . qpid . proton . amqp . DescribedType amqpValueNullContent = new org . apache . qpid . jms . test . testpeer . describedtypes . sections . AmqpValueDescribedType ( null ) ; testPeer . expectReceiverAttach ( ) ; testPeer . expectLinkFlowRespondWithTransfer ( null , null , null , null , amqpValueNullContent ) ; testPeer . remotelyCloseLastCoordinatorLink ( ) ; javax . jms . MessageConsumer consumer = session . createConsumer ( queue ) ; testPeer . waitForAllHandlersToComplete ( 2000 ) ; javax . jms . Message received = consumer . receive ( ) ; org . junit . Assert . assertNotNull ( received ) ; txnId = new org . apache . qpid . proton . amqp . Binary ( new byte [ ] { ( ( byte ) ( 1 ) ) , ( ( byte ) ( 2 ) ) , ( ( byte ) ( 3 ) ) , ( ( byte ) ( 4 ) ) } ) ; testPeer . expectCoordinatorAttach ( ) ; testPeer . expectDeclare ( txnId ) ; testPeer . expectDischarge ( txnId , true ) ; try { session . commit ( ) ; org . junit . Assert . fail ( "Commit<sp>operation<sp>should<sp>have<sp>failed." ) ; } }
public class aTest{ @Test public void testSetGetHeight ( ) { org . apache . poi . xwpf . usermodel . XWPFDocument doc = new org . apache . poi . xwpf . usermodel . XWPFDocument ( ) ; org . openxmlformats . schemas . wordprocessingml . x2006 . main . CTTbl table = CTTbl . Factory . newInstance ( ) ; org . apache . poi . xwpf . usermodel . XWPFTable xtab = new org . apache . poi . xwpf . usermodel . XWPFTable ( table , doc ) ; org . apache . poi . xwpf . usermodel . XWPFTableRow row = xtab . createRow ( ) ; row . setHeight ( 20 ) ; org . junit . Assert . assertEquals ( 20 , row . getHeight ( ) ) ; try { doc . close ( ) ; } }
public class aTest{ @Test public void testMatchExactlyNodeNotFoundErr ( ) { java . util . List < org . o3project . odenos . core . component . network . flow . basic . BasicFlowMatch > matches = null ; java . util . List < java . lang . String > path = null ; java . util . Map < java . lang . String , java . util . List < org . o3project . odenos . core . component . network . flow . basic . FlowAction > > edgeAction = null ; java . util . Map < java . lang . String , java . lang . String > flowAttributes = new java . util . HashMap < java . lang . String , java . lang . String > ( ) ; long idleTimeout = 0 ; long hardTimeout = 0 ; org . o3project . odenos . core . component . network . flow . ofpflow . OFPFlow flow = new org . o3project . odenos . core . component . network . flow . ofpflow . OFPFlow ( "1" , "" , "" , true , "" , "established" , matches , idleTimeout , hardTimeout , path , edgeAction , flowAttributes ) ; org . o3project . odenos . core . component . network . flow . basic . BasicFlowMatch match1 = new org . o3project . odenos . core . component . network . flow . basic . BasicFlowMatch ( "node_s1" , "node_s1" 0 ) ; org . o3project . odenos . core . component . network . flow . basic . BasicFlowMatch match2 = new org . o3project . odenos . core . component . network . flow . basic . BasicFlowMatch ( "node_s2" , "port_s2" ) ; org . o3project . odenos . core . component . network . flow . basic . BasicFlowMatch match3 = new org . o3project . odenos . core . component . network . flow . basic . BasicFlowMatch ( "node_d1" , "port_d1" ) ; org . o3project . odenos . core . component . network . flow . basic . BasicFlowMatch match4 = new org . o3project . odenos . core . component . network . flow . basic . BasicFlowMatch ( "node_d2" , "port_d2" ) ; flow . addMatch ( match1 ) ; flow . addMatch ( match2 ) ; flow . addMatch ( match3 ) ; flow . addMatch ( match4 ) ; flow . addPath ( link1 . getId ( ) ) ; flow . addPath ( link2 . getId ( ) ) ; queriesString = "node_s1" 1 ; org . o3project . odenos . core . component . network . topology . Topology topology = new org . o3project . odenos . core . component . network . topology . Topology ( nodes , links ) ; target = new org . o3project . odenos . core . component . network . flow . query . OFPFlowQuery ( queriesString ) ; target . setTopology ( topology ) ; target . parse ( ) ; org . junit . Assert . assertThat ( target . matchExactly ( flow ) , org . hamcrest . CoreMatchers . is ( false ) ) ; } }
public class aTest{ @Test public void testGetSchemasMySql ( ) { try { when ( database . getSchemas ( ) ) . thenReturn ( null ) ; when ( meta . isMySQLVariant ( ) ) . thenReturn ( true ) ; when ( meta . getDatabaseName ( ) ) . thenReturn ( org . pentaho . pms . ui . MetaEditorTest . EXPECTED_SCHEMAS [ 0 ] ) ; java . lang . String [ ] schemas = metaEditor . getSchemas ( database , meta ) ; org . junit . Assert . assertArrayEquals ( new java . lang . String [ ] { org . pentaho . pms . ui . MetaEditorTest . EXPECTED_SCHEMAS [ 0 ] } , schemas ) ; } }
public class aTest{ @Test public void verifyMethodUsingCaptureForObjectTypeParameterOfDifferentAndUnmockedInvocation ( ) { mock . doSomethingElse ( "test" ) ; new mockit . Verifications ( ) { { java . lang . String s ; mock . doSomethingElse ( new mockit . MisusedMockingAPITest . Unmocked ( ( s = withCapture ( ) ) ) ) ; org . junit . Assert . assertNull ( s ) ; } } }
public class aTest{ @Test public void testGetNextTimeWeeklyReturnsFirstDayOfNextWeek_US ( ) { final java . util . Locale old = java . util . Locale . getDefault ( ) ; java . util . Locale . setDefault ( Locale . US ) ; try { final org . apache . logging . log4j . core . appender . rolling . PatternProcessor pp = new org . apache . logging . log4j . core . appender . rolling . PatternProcessor ( "logs/app-%d{yyyy-MM-W}.log.gz" ) ; final java . util . Calendar initial = java . util . Calendar . getInstance ( ) ; initial . set ( 2014 , Calendar . MARCH , 4 , 10 , 31 , 59 ) ; final long actual = pp . getNextTime ( initial . getTimeInMillis ( ) , 1 , false ) ; final java . util . Calendar expected = java . util . Calendar . getInstance ( ) ; expected . set ( 2014 , Calendar . MARCH , 9 , 0 , 0 , 0 ) ; expected . set ( Calendar . MILLISECOND , 0 ) ; org . junit . Assert . assertEquals ( format ( expected . getTimeInMillis ( ) ) , format ( actual ) ) ; } }
public class aTest{ @Test public void testLista ( ) { log . debug ( "Debiera<sp>mostrar<sp>lista<sp>de<sp>paquetes" ) ; mx . edu . um . mateo . general . model . Usuario usuario = obtieneUsuario ( ) ; mx . edu . um . mateo . inscripciones . model . Paquete paquete = null ; for ( int i = 0 ; i < 20 ; i ++ ) { paquete = new mx . edu . um . mateo . inscripciones . model . Paquete ( ) ; paquete . setAcfe ( "a" ) ; paquete . setDescripcion ( "test" ) ; paquete . setEmpresa ( usuario . getEmpresa ( ) ) ; paquete . setEnsenanza ( new java . math . BigDecimal ( "80" ) ) ; paquete . setInternado ( new java . math . BigDecimal ( "80" ) ) ; paquete . setMatricula ( new java . math . BigDecimal ( "80" ) ) ; paquete . setNombre ( "test" ) ; currentSession ( ) . save ( paquete ) ; org . junit . Assert . assertNotNull ( paquete . getId ( ) ) ; } }
public class aTest{ @Test public void testCompareScreen_acceptsFile_retries ( ) { com . vaadin . testbench . Parameters . setMaxScreenshotRetries ( 4 ) ; try { java . io . File referenceFile = com . vaadin . testbench . testutils . ImageLoader . getImageFile ( com . vaadin . testbench . commands . TestBenchCommandExecutorTest . IMG_FOLDER , "cursor-bottom-edge-off.png" ) ; org . openqa . selenium . WebDriver driver = mockScreenshotDriver ( 4 , false ) ; com . vaadin . testbench . screenshot . ImageComparison icMock = createMock ( com . vaadin . testbench . screenshot . ImageComparison . class ) ; expect ( icMock . imageEqualToReference ( isA ( java . awt . image . BufferedImage . class ) , isA ( java . awt . image . BufferedImage . class ) , eq ( "cursor-bottom-edge-off.png" ) , eq ( com . vaadin . testbench . Parameters . getScreenshotComparisonTolerance ( ) ) ) ) . andReturn ( false ) . times ( 4 ) ; replay ( driver , icMock ) ; com . vaadin . testbench . commands . TestBenchCommandExecutor tbce = new com . vaadin . testbench . commands . TestBenchCommandExecutor ( icMock , null ) ; tbce . setDriver ( com . vaadin . testbench . TestBench . createDriver ( driver , tbce ) ) ; org . junit . Assert . assertFalse ( tbce . compareScreen ( referenceFile ) ) ; verify ( driver , icMock ) ; } }
public class aTest{ @Test public void testAddBinaryAttributeType ( ) { java . util . List < java . lang . String > descriptions = new java . util . ArrayList < java . lang . String > ( ) ; descriptions . add ( ( "other" 8 + ( ( ( ( "other" 1 + "<sp>DESC<sp>'template<sp>data'" ) + "<sp>SYNTAX<sp>1.3.6.1.4.1.1466.115.121.1.5" ) + "<sp>SINGLE-VALUE" ) + "<sp>X-SCHEMA<sp>'other'<sp>)" ) ) ) ; modify ( DirContext . ADD_ATTRIBUTE , descriptions , "other" 2 ) ; descriptions . clear ( ) ; descriptions . add ( ( "other" 3 + ( ( ( ( ( "<sp>DESC<sp>'template<sp>data'" 0 + "other" 4 ) + "<sp>SUP<sp>top<sp>" ) + "<sp>DESC<sp>'template<sp>data'" 1 ) + "<sp>DESC<sp>'template<sp>data'" 2 ) + "<sp>X-SCHEMA<sp>'other'<sp>)" ) ) ) ; modify ( DirContext . ADD_ATTRIBUTE , descriptions , "other" 7 ) ; checkAttributeTypePresent ( "1.3.6.1.4.1.65536.0.4.3.2.1" , "other" , true ) ; checkObjectClassPresent ( "1.3.6.1.4.1.65536.0.4.3.2.2" , "other" , true ) ; getService ( ) . sync ( ) ; getService ( ) . shutdown ( ) ; getService ( ) . startup ( ) ; checkAttributeTypePresent ( "1.3.6.1.4.1.65536.0.4.3.2.1" , "other" , true ) ; checkObjectClassPresent ( "1.3.6.1.4.1.65536.0.4.3.2.2" , "other" , true ) ; javax . naming . directory . Attributes attrs = new javax . naming . directory . BasicAttributes ( ) ; javax . naming . directory . BasicAttribute ocattr = new javax . naming . directory . BasicAttribute ( "other" 6 ) ; ocattr . add ( "other" 0 ) ; ocattr . add ( "templateObject" ) ; attrs . put ( ocattr ) ; byte [ ] templateData = new byte [ 4096 ] ; attrs . put ( "other" 9 , templateData ) ; attrs . put ( "other" 5 , "<sp>DESC<sp>'template<sp>data'" 3 ) ; getRootContext ( getService ( ) ) . bind ( "cn=atemplate,ou=system" , null , attrs ) ; javax . naming . directory . Attributes data = getRootContext ( getService ( ) ) . getAttributes ( "cn=atemplate,ou=system" , new java . lang . String [ ] { "other" 9 , "other" 5 } ) ; org . junit . Assert . assertTrue ( java . util . Arrays . equals ( templateData , ( ( byte [ ] ) ( data . get ( "other" 9 ) . get ( ) ) ) ) ) ; } }
public class aTest{ @Test public void testIncorrectCompareOperator ( ) { java . lang . String filterString = "RowFilter<sp>('>>'<sp>,<sp>'binary:region')" ; try { doTestFilter ( filterString , org . apache . hadoop . hbase . filter . RowFilter . class ) ; org . junit . Assert . assertTrue ( false ) ; } }
public class aTest{ @Test public void getterForCAReferencesSucceed ( tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . configs . X509Certificates ) { tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . configs . X509Attestation x509Attestation = tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . configs . X509Attestation . createFromCAReferences ( tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . configs . X509AttestationTest . CA_REFERENCES_STRING , tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . configs . X509AttestationTest . CA_REFERENCES_STRING ) ; org . junit . Assert . assertNotNull ( x509Attestation . getCAReferencesFinal ( ) ) ; } }
public class aTest{ @Test public void testAddIsolatedPolicy ( ) { java . lang . String [ ] policyNames = new java . lang . String [ ] { "isolatedACL" , REP_POLICY , REP_REPO_POLICY } ; org . apache . jackrabbit . oak . util . NodeUtil node = getTestRoot ( ) ; for ( java . lang . String policyName : policyNames ) { org . apache . jackrabbit . oak . util . NodeUtil policy = node . addChild ( policyName , org . apache . jackrabbit . oak . security . authorization . accesscontrol . NT_REP_ACL ) ; try { root . commit ( ) ; org . junit . Assert . fail ( "Writing<sp>an<sp>isolated<sp>ACL<sp>without<sp>the<sp>parent<sp>being<sp>rep:AccessControllable<sp>should<sp>fail." ) ; } catch ( org . apache . jackrabbit . oak . api . CommitFailedException e ) { org . junit . Assert . assertTrue ( e . isAccessControlViolation ( ) ) ; } }
public class aTest{ @Test public void testUpdateUser_DifferentEmail ( ) { sendedMails . clear ( ) ; try { final java . lang . String oldEmail = "admin@organization.com" ; final java . lang . String newEmail = "enes.sejfi@est.fujitsu.com" ; modifyUserData ( oldEmail , newEmail ) ; org . junit . Assert . assertEquals ( 2 , sendedMails . size ( ) ) ; checkEmail ( 0 , newEmail ) ; checkEmail ( 1 , oldEmail ) ; } }
public class aTest{ @Test public void testBatchSizeSpecification ( ) { int numRecords = 10 ; java . util . List < java . lang . String > expected = com . google . common . collect . Lists . newArrayList ( ) ; for ( int i = 0 ; i < numRecords ; i ++ ) { expected . add ( java . lang . String . format ( "{<sp>\"i\":<sp>%d}" , i ) ) ; } putQueue ( expected ) ; com . streamsets . pipeline . sdk . SourceRunner runner = createRunner ( ) ; runner . runInit ( ) ; try { com . streamsets . pipeline . sdk . StageRunner . Output output = runner . runProduce ( null , 2 ) ; java . util . Map < java . lang . String , java . util . List < com . streamsets . pipeline . api . Record > > recordMap = output . getRecords ( ) ; java . util . List < com . streamsets . pipeline . api . Record > parsedRecords = recordMap . get ( "lane" ) ; org . junit . Assert . assertEquals ( 2 , parsedRecords . size ( ) ) ; } }
public class aTest{ @Test public void testListProductAttributeTierPrices ( ) { try { java . util . List < com . magento . api . CatalogProductTierPriceEntity > catalogProductTierPriceEntities = runFlowAndGetPayload ( "list-product-attribute-tier-prices" ) ; org . junit . Assert . assertNotNull ( catalogProductTierPriceEntities ) ; } }
public class aTest{ @Test public void FileMOVEIf_Match ( ) { final java . lang . String depth = "infinity" ; final java . lang . String destFileName = "destFile.txt" ; final java . lang . String destination = com . fujitsu . dc . test . unit . core . UrlUtils . box ( com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . CELL_NAME , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . BOX_NAME , destFileName ) ; try { com . fujitsu . dc . test . utils . DavResourceUtils . createWebDavFile ( com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . TOKEN , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . CELL_NAME , ( ( ( com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . BOX_NAME ) + "/" ) + ( com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . FILE_NAME ) ) , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . FILE_BODY , MediaType . TEXT_PLAIN , HttpStatus . SC_CREATED ) ; java . lang . String url = com . fujitsu . dc . test . unit . core . UrlUtils . box ( com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . CELL_NAME , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . BOX_NAME , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . FILE_NAME ) ; com . fujitsu . dc . test . jersey . DcRequest req = com . fujitsu . dc . test . jersey . DcRequest . move ( url ) ; req . header ( HttpHeaders . AUTHORIZATION , AbstractCase . BEARER_MASTER_TOKEN ) ; req . header ( HttpHeaders . DESTINATION , destination ) ; req . header ( HttpHeaders . DEPTH , depth ) ; req . header ( HttpHeaders . OVERWRITE , "T" ) ; com . fujitsu . dc . test . jersey . DcResponse response = com . fujitsu . dc . test . jersey . AbstractCase . request ( req ) ; org . junit . Assert . assertEquals ( HttpStatus . SC_CREATED , response . getStatusCode ( ) ) ; com . fujitsu . dc . test . utils . DavResourceUtils . getWebDav ( com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . CELL_NAME , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . TOKEN , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . BOX_NAME , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . FILE_NAME , HttpStatus . SC_NOT_FOUND ) ; com . fujitsu . dc . test . utils . DavResourceUtils . getWebDav ( com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . CELL_NAME , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . TOKEN , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . BOX_NAME , destFileName , HttpStatus . SC_OK ) ; } }
public class aTest{ @Test public void testGetAllUsersByName ( ) { try { java . util . List < qa . qcri . aidr . dbmanager . dto . UsersDTO > result = qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestUsersResourceFacadeImp . userResourceFacadeImp . getAllUsersByName ( qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestUsersResourceFacadeImp . user . getName ( ) ) ; org . junit . Assert . assertEquals ( qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestUsersResourceFacadeImp . user . getName ( ) , result . get ( 0 ) . getName ( ) ) ; } }
public class aTest{ @Test public void testRewindOnRebalanceDuringPoll ( ) { expectInitializeTask ( ) ; expectPollInitialAssignment ( ) ; expectRebalanceDuringPoll ( ) . andAnswer ( new org . easymock . IAnswer < java . lang . Object > ( ) { @ org . apache . kafka . connect . runtime . Override public java . lang . Object answer ( ) throws java . lang . Throwable { java . util . Map < org . apache . kafka . common . TopicPartition , java . lang . Long > offsets = sinkTaskContext . getValue ( ) . offsets ( ) ; org . junit . Assert . assertEquals ( 0 , offsets . size ( ) ) ; return null ; } } }
public class aTest{ @Test public void testNoFadviseAfterWriteThenRead ( ) { org . apache . hadoop . hdfs . server . datanode . TestCachingStrategy . LOG . info ( "testNoFadviseAfterWriteThenRead" ) ; org . apache . hadoop . hdfs . server . datanode . TestCachingStrategy . tracker . clear ( ) ; org . apache . hadoop . conf . Configuration conf = new org . apache . hadoop . hdfs . HdfsConfiguration ( ) ; org . apache . hadoop . hdfs . MiniDFSCluster cluster = null ; java . lang . String TEST_PATH = "/test" ; int TEST_PATH_LEN = org . apache . hadoop . hdfs . server . datanode . TestCachingStrategy . MAX_TEST_FILE_LEN ; try { cluster = new org . apache . hadoop . hdfs . MiniDFSCluster . Builder ( conf ) . numDataNodes ( 1 ) . build ( ) ; cluster . waitActive ( ) ; org . apache . hadoop . fs . FileSystem fs = cluster . getFileSystem ( ) ; org . apache . hadoop . hdfs . server . datanode . TestCachingStrategy . createHdfsFile ( fs , new org . apache . hadoop . fs . Path ( TEST_PATH ) , TEST_PATH_LEN , false ) ; org . apache . hadoop . hdfs . protocol . ExtendedBlock block = cluster . getNameNode ( ) . getRpcServer ( ) . getBlockLocations ( TEST_PATH , 0 , Long . MAX_VALUE ) . get ( 0 ) . getBlock ( ) ; java . lang . String fadvisedFileName = cluster . getBlockFile ( 0 , block ) . getName ( ) ; org . apache . hadoop . hdfs . server . datanode . TestCachingStrategy . Stats stats = org . apache . hadoop . hdfs . server . datanode . TestCachingStrategy . tracker . getStats ( fadvisedFileName ) ; org . junit . Assert . assertNull ( stats ) ; org . apache . hadoop . hdfs . server . datanode . TestCachingStrategy . readHdfsFile ( fs , new org . apache . hadoop . fs . Path ( TEST_PATH ) , Long . MAX_VALUE , false ) ; } }
public class aTest{ @Test public void testMapOnly ( ) { org . apache . hadoop . mapred . JobConf job = new org . apache . hadoop . mapred . JobConf ( ) ; java . lang . String inDir = ( java . lang . System . getProperty ( "share.dir" , "../../../share" ) ) + "/test/data" ; org . apache . hadoop . fs . Path input = new org . apache . hadoop . fs . Path ( ( inDir + "/weather.avro" ) ) ; org . apache . hadoop . fs . Path output = new org . apache . hadoop . fs . Path ( "target/test/weather-ident" ) ; output . getFileSystem ( job ) . delete ( output ) ; job . setJobName ( "identity<sp>map<sp>weather" ) ; org . apache . avro . mapred . AvroJob . setInputSchema ( job , Weather . SCHEMA . ) ; org . apache . avro . mapred . AvroJob . setOutputSchema ( job , Weather . SCHEMA . ) ; org . apache . hadoop . mapred . FileInputFormat . setInputPaths ( job , input ) ; org . apache . hadoop . mapred . FileOutputFormat . setOutputPath ( job , output ) ; org . apache . hadoop . mapred . FileOutputFormat . setCompressOutput ( job , true ) ; job . setNumReduceTasks ( 0 ) ; org . apache . hadoop . mapred . JobClient . runJob ( job ) ; org . apache . avro . io . DatumReader < test . Weather > reader = new org . apache . avro . specific . SpecificDatumReader ( ) ; org . apache . avro . file . DataFileReader < test . Weather > check = new org . apache . avro . file . DataFileReader ( new java . io . File ( ( inDir + "/weather.avro" ) ) , reader ) ; org . apache . avro . file . DataFileReader < test . Weather > sorted = new org . apache . avro . file . DataFileReader ( new java . io . File ( ( ( output . toString ( ) ) + "/part-00000.avro" ) ) , reader ) ; for ( test . Weather w : sorted ) org . junit . Assert . assertEquals ( check . next ( ) , w ) ; check . close ( ) ; sorted . close ( ) ; } }
public class aTest{ @Test public void testAddNetConfigIP4PropertiesWithoutDhcpFull ( ) { org . eclipse . kura . core . net . NetworkConfiguration config = new org . eclipse . kura . core . net . NetworkConfiguration ( ) ; org . eclipse . kura . net . NetConfigIP4 netConfig = new org . eclipse . kura . net . NetConfigIP4 ( org . eclipse . kura . net . NetInterfaceStatus . netIPv4StatusEnabledLAN , true ) ; netConfig . setDhcp ( false ) ; netConfig . setAddress ( ( ( org . eclipse . kura . net . IP4Address ) ( org . eclipse . kura . net . IPAddress . parseHostAddress ( "prefix.autoconnect" 3 ) ) ) ) ; netConfig . setNetworkPrefixLength ( ( ( short ) ( 24 ) ) ) ; netConfig . setGateway ( ( ( org . eclipse . kura . net . IP4Address ) ( org . eclipse . kura . net . IPAddress . parseHostAddress ( "10.0.0.2" ) ) ) ) ; java . util . ArrayList < org . eclipse . kura . net . IP4Address > winsServers = new java . util . ArrayList ( ) ; winsServers . add ( ( ( org . eclipse . kura . net . IP4Address ) ( org . eclipse . kura . net . IPAddress . parseHostAddress ( "prefix.autoconnect" 0 ) ) ) ) ; winsServers . add ( ( ( org . eclipse . kura . net . IP4Address ) ( org . eclipse . kura . net . IPAddress . parseHostAddress ( "prefix.autoconnect" 8 ) ) ) ) ; netConfig . setWinsServers ( winsServers ) ; java . util . ArrayList < java . lang . String > domains = new java . util . ArrayList ( ) ; domains . add ( "prefix.autoconnect" 6 ) ; domains . add ( "prefix.autoconnect" 5 ) ; netConfig . setDomains ( domains ) ; java . lang . String prefix = "prefix." ; java . util . HashMap < java . lang . String , java . lang . Object > expected = new java . util . HashMap ( ) ; expected . put ( "prefix.autoconnect" , true ) ; expected . put ( "prefix.autoconnect" 2 , "prefix.autoconnect" 9 ) ; expected . put ( "prefix.ip4.dnsServers" , "" ) ; expected . put ( "prefix.dhcpClient4.enabled" , false ) ; expected . put ( "prefix.autoconnect" 7 , "prefix.autoconnect" 3 ) ; expected . put ( "prefix.domains" 0 , ( ( short ) ( 24 ) ) ) ; expected . put ( "prefix.autoconnect" 1 , "10.0.0.2" ) ; expected . put ( "prefix.winsServers" , "prefix.autoconnect" 4 ) ; expected . put ( "prefix.domains" , "domain1,domain2" ) ; java . util . HashMap < java . lang . String , java . lang . Object > properties = new java . util . HashMap ( ) ; org . eclipse . kura . core . testutil . TestUtil . invokePrivate ( config , "addNetConfigIP4Properties" , netConfig , prefix , properties ) ; org . junit . Assert . assertEquals ( expected , properties ) ; } }
public class aTest{ @Test public void testNodeStatusWithEmptyNodeLabels ( ) { org . apache . hadoop . yarn . api . records . NodeId nodeId = org . apache . hadoop . yarn . api . records . NodeId . newInstance ( "host0" , 0 ) ; when ( client . getNodeReports ( ) ) . thenReturn ( getNodeReports ( 3 , NodeState . RUNNING ) ) ; org . apache . hadoop . yarn . client . cli . NodeCLI cli = createAndGetNodeCLI ( ) ; int result = cli . run ( new java . lang . String [ ] { "\tResource<sp>Utilization<sp>by<sp>Node<sp>:<sp>PMem:2048<sp>MB,<sp>VMem:4096<sp>MB,<sp>VCores:8.0" 6 , nodeId . toString ( ) } ) ; org . junit . Assert . assertEquals ( 0 , result ) ; verify ( client ) . getNodeReports ( ) ; java . io . ByteArrayOutputStream baos = new java . io . ByteArrayOutputStream ( ) ; java . io . PrintWriter pw = new java . io . PrintWriter ( baos ) ; pw . println ( "\tResource<sp>Utilization<sp>by<sp>Node<sp>:<sp>PMem:2048<sp>MB,<sp>VMem:4096<sp>MB,<sp>VCores:8.0" 3 ) ; pw . println ( "\tResource<sp>Utilization<sp>by<sp>Node<sp>:<sp>PMem:2048<sp>MB,<sp>VMem:4096<sp>MB,<sp>VCores:8.0" 1 ) ; pw . println ( "\tResource<sp>Utilization<sp>by<sp>Node<sp>:<sp>PMem:2048<sp>MB,<sp>VMem:4096<sp>MB,<sp>VCores:8.0" 4 ) ; pw . println ( "\tNode-State<sp>:<sp>RUNNING" ) ; pw . println ( "\tResource<sp>Utilization<sp>by<sp>Node<sp>:<sp>PMem:2048<sp>MB,<sp>VMem:4096<sp>MB,<sp>VCores:8.0" 5 ) ; pw . println ( ( "\tResource<sp>Utilization<sp>by<sp>Node<sp>:<sp>PMem:2048<sp>MB,<sp>VMem:4096<sp>MB,<sp>VCores:8.0" 0 + ( org . apache . commons . lang3 . time . DateFormatUtils . format ( new java . util . Date ( 0 ) , "\tResource<sp>Utilization<sp>by<sp>Node<sp>:<sp>PMem:2048<sp>MB,<sp>VMem:4096<sp>MB,<sp>VCores:8.0" 7 ) ) ) ) ; pw . println ( "\tHealth-Report<sp>:<sp>" ) ; pw . println ( "\tContainers<sp>:<sp>0" ) ; pw . println ( "\tMemory-Used<sp>:<sp>0MB" ) ; pw . println ( "\tMemory-Capacity<sp>:<sp>0MB" ) ; pw . println ( "\tResource<sp>Utilization<sp>by<sp>Node<sp>:<sp>PMem:2048<sp>MB,<sp>VMem:4096<sp>MB,<sp>VCores:8.0" 2 ) ; pw . println ( "\tCPU-Capacity<sp>:<sp>0<sp>vcores" ) ; pw . println ( "\tResource<sp>Utilization<sp>by<sp>Node<sp>:<sp>PMem:2048<sp>MB,<sp>VMem:4096<sp>MB,<sp>VCores:8.0" 8 ) ; pw . println ( "\tNode<sp>Attributes<sp>:<sp>" ) ; pw . println ( "\tResource<sp>Utilization<sp>by<sp>Node<sp>:<sp>PMem:2048<sp>MB,<sp>VMem:4096<sp>MB,<sp>VCores:8.0" ) ; pw . println ( "\tResource<sp>Utilization<sp>by<sp>Node<sp>:<sp>PMem:2048<sp>MB,<sp>VMem:4096<sp>MB,<sp>VCores:8.0" 9 ) ; pw . close ( ) ; java . lang . String nodeStatusStr = baos . toString ( "UTF-8" ) ; verify ( sysOut , times ( 1 ) ) . println ( isA ( java . lang . String . class ) ) ; verify ( sysOut ) . println ( nodeStatusStr ) ; } }
public class aTest{ @Test public void testRollupWithMultipleDistinctAggregate ( ) { java . lang . String sqlText = format ( ( "select<sp>a1,<sp>b1,<sp>count(distinct<sp>c1),<sp>count(distinct<sp>d1),<sp>grouping(a1),<sp>grouping(b1)<sp>from<sp>t1<sp>--splice-properties<sp>useSpark=%s\n" + "group<sp>by<sp>rollup(a1,b1)<sp>order<sp>by<sp>1,2,5,6" ) , this . useSparkString ) ; java . sql . ResultSet rs = methodWatcher . executeQuery ( sqlText ) ; java . lang . String expected = "<sp>1<sp>|NULL<sp>|<sp>2<sp>|<sp>3<sp>|<sp>0<sp>|<sp>0<sp>|\n" 1 + ( ( ( ( ( ( ( ( ( ( ( "----------------------------\n" + "<sp>1<sp>|NULL<sp>|<sp>2<sp>|<sp>3<sp>|<sp>0<sp>|<sp>0<sp>|\n" 0 ) + "<sp>1<sp>|NULL<sp>|<sp>2<sp>|<sp>3<sp>|<sp>0<sp>|<sp>0<sp>|\n" 3 ) + "<sp>1<sp>|NULL<sp>|<sp>2<sp>|<sp>3<sp>|<sp>0<sp>|<sp>0<sp>|\n" ) + "<sp>1<sp>|NULL<sp>|<sp>2<sp>|<sp>3<sp>|<sp>0<sp>|<sp>0<sp>|\n" 4 ) + "<sp>2<sp>|<sp>1<sp>|<sp>2<sp>|<sp>3<sp>|<sp>0<sp>|<sp>0<sp>|\n" ) + "<sp>1<sp>|NULL<sp>|<sp>2<sp>|<sp>3<sp>|<sp>0<sp>|<sp>0<sp>|\n" 5 ) + "<sp>2<sp>|NULL<sp>|<sp>2<sp>|<sp>3<sp>|<sp>0<sp>|<sp>1<sp>|\n" ) + "<sp>3<sp>|<sp>1<sp>|<sp>2<sp>|<sp>3<sp>|<sp>0<sp>|<sp>0<sp>|\n" ) + "<sp>3<sp>|<sp>2<sp>|<sp>2<sp>|<sp>3<sp>|<sp>0<sp>|<sp>0<sp>|\n" ) + "<sp>3<sp>|NULL<sp>|<sp>2<sp>|<sp>3<sp>|<sp>0<sp>|<sp>1<sp>|\n" ) + "<sp>1<sp>|NULL<sp>|<sp>2<sp>|<sp>3<sp>|<sp>0<sp>|<sp>0<sp>|\n" 2 ) ; org . junit . Assert . assertEquals ( ( ( "\n" + sqlText ) + "\n" ) , expected , TestUtils . FormattedResult . ResultFactory . toStringUnsorted ( rs ) ) ; rs . close ( ) ; } }
public class aTest{ @Test public void testNonSerializableResponse ( ) { final org . jboss . ejb . client . StatelessEJBLocator < org . jboss . as . test . integration . ejb . remote . client . api . NonSerialiazableResponseRemote > locator = new org . jboss . ejb . client . StatelessEJBLocator ( org . jboss . as . test . integration . ejb . remote . client . api . NonSerialiazableResponseRemote . class , org . jboss . as . test . integration . ejb . remote . client . api . EJBClientAPIUsageTestCase . APP_NAME , org . jboss . as . test . integration . ejb . remote . client . api . EJBClientAPIUsageTestCase . MODULE_NAME , org . jboss . as . test . integration . ejb . remote . client . api . NonSerializableResponseEjb . class . getSimpleName ( ) , "" ) ; final org . jboss . as . test . integration . ejb . remote . client . api . NonSerialiazableResponseRemote proxy = org . jboss . ejb . client . EJBClient . createProxy ( locator ) ; java . util . concurrent . Callable < java . lang . Object > task = new java . util . concurrent . Callable < java . lang . Object > ( ) { @ org . jboss . as . test . integration . ejb . remote . client . api . Override public java . lang . Object call ( ) throws org . jboss . as . test . integration . ejb . remote . client . api . Exception { try { proxy . nonSerializable ( ) ; org . junit . Assert . fail ( ) ; } catch ( java . lang . Exception e ) { org . jboss . as . test . integration . ejb . remote . client . api . EJBClientAPIUsageTestCase . logger . trace ( ( "expected<sp>" + e ) ) ; } java . lang . Thread . sleep ( 1000 ) ; org . junit . Assert . assertEquals ( "hello" , proxy . serializable ( ) ) ; return null ; } } }
public class aTest{ @Test public void testTwoProductsSameType ( ) { org . esa . beam . pixex . Coordinate [ ] coordinates = new org . esa . beam . pixex . Coordinate [ ] { new org . esa . beam . pixex . Coordinate ( "coord1" , 10.0F , 10.0F , null ) , new org . esa . beam . pixex . Coordinate ( "coord2" , 20.0F , 20.0F , null ) , new org . esa . beam . pixex . Coordinate ( "coord3" , 0.5F , 0.5F , null ) } ; int windowSize = 5 ; java . util . HashMap < java . lang . String , java . lang . Object > parameterMap = new java . util . HashMap ( ) ; java . io . File outputDir = org . esa . beam . pixex . PixExOpTest . getOutputDir ( "testTwoProductsSameType" , getClass ( ) ) ; parameterMap . put ( "outputDir" , outputDir ) ; parameterMap . put ( "exportTiePoints" , false ) ; parameterMap . put ( "coord3" 0 , false ) ; parameterMap . put ( "coord3" 1 , coordinates ) ; parameterMap . put ( "windowSize" , windowSize ) ; java . lang . String [ ] bandNames = new java . lang . String [ ] { "rad_1" , "coord3" 3 } ; org . esa . beam . framework . datamodel . Product [ ] products = new org . esa . beam . framework . datamodel . Product [ ] { org . esa . beam . pixex . PixExOpTest . createTestProduct ( "kallegrabowski" , "type1" , bandNames ) , org . esa . beam . pixex . PixExOpTest . createTestProduct ( "coord3" 2 , "type1" , bandNames ) } ; org . esa . beam . pixex . PixExOpTest . computeData ( parameterMap , products ) ; try ( org . esa . beam . pixex . PixExMeasurementReader reader = new org . esa . beam . pixex . PixExMeasurementReader ( outputDir ) ) { final java . util . List < org . esa . beam . measurement . Measurement > measurementList = convertToList ( reader ) ; org . junit . Assert . assertEquals ( ( ( ( windowSize * windowSize ) * ( products . length ) ) * ( coordinates . length ) ) , measurementList . size ( ) ) ; testForExistingMeasurement ( measurementList , "coord1" , 1 , 10.5F , 9.5F , 189.5F , 79.5F ) ; testForExistingMeasurement ( measurementList , "coord2" , 2 , 20.5F , 19.5F , 199.5F , 69.5F ) ; } } }
public class aTest{ @Test public void noPropsFile ( ) { java . lang . String [ ] testArgs1 = new java . lang . String [ ] { "-servicename" , "NewService" , "-genType" , "COMMON" 1 , "-interface" , "COMMON" 4 , "COMMON" 3 , destDir . getAbsolutePath ( ) , "COMMON" 7 , "COMMON" 0 , "COMMON" 5 , "COMMON" , "-bin" , binDir . getAbsolutePath ( ) , "-pr" , destDir . getAbsolutePath ( ) , "-adminname" , "Admin3" , "COMMON" 6 , "cname" } ; performDirectCodeGen ( testArgs1 , binDir ) ; baseConsumer = ( destDir . getAbsolutePath ( ) ) + "COMMON" 2 ; baseConsumerClass = new java . io . File ( baseConsumer ) ; org . junit . Assert . assertTrue ( baseConsumerClass . exists ( ) ) ; } }
public class aTest{ @Test public void testNotDuplicatedDisplayedMnemonic ( ) { es . gob . afirma . ui . wizardmultifirmamasiva . PanelMultifirmaMasivaAccessibilityTest . logger . info ( "testNotDuplicatedDisplayedMnemonic" ) ; try { final es . gob . afirma . ui . wizardmultifirmamasiva . PanelMultifirmaMasiva panelMultifirmaMasiva = new es . gob . afirma . ui . wizardmultifirmamasiva . PanelMultifirmaMasiva ( null , false ) ; final java . util . List < java . lang . Integer > keyCodes = new java . util . ArrayList ( ) ; java . util . Set < java . lang . Integer > keyCodesSet = null ; final java . awt . Component [ ] components = panelMultifirmaMasiva . getComponents ( ) ; for ( final java . awt . Component componentWizard : components ) { if ( componentWizard instanceof javax . swing . JRootPane ) { final java . awt . Component [ ] componentsRootPane = ( ( javax . swing . JRootPane ) ( componentWizard ) ) . getComponents ( ) ; for ( final java . awt . Component componentRootPane : componentsRootPane ) { if ( componentRootPane instanceof javax . swing . JPanel ) { getKeyCodeList ( ( ( javax . swing . JPanel ) ( componentRootPane ) ) , keyCodes ) ; } else if ( componentRootPane instanceof javax . swing . JLayeredPane ) { final java . awt . Component [ ] componentsLayeredPane = ( ( javax . swing . JLayeredPane ) ( componentRootPane ) ) . getComponents ( ) ; for ( final java . awt . Component componentLayeredPane : componentsLayeredPane ) { if ( componentLayeredPane instanceof javax . swing . JPanel ) { getKeyCodeList ( ( ( javax . swing . JPanel ) ( componentLayeredPane ) ) , keyCodes ) ; } } } } } } keyCodesSet = new java . util . HashSet ( keyCodes ) ; org . junit . Assert . assertTrue ( ( ( keyCodesSet . size ( ) ) == ( keyCodes . size ( ) ) ) ) ; } }
public class aTest{ @Test public void trimLabelsYears1 ( ) { java . util . List < java . lang . String > input = java . util . Arrays . asList ( "2014/01/01<sp>00:00:00.000000000" , "2015/01/01<sp>00:00:00.000000000" , "2016/01/01<sp>00:00:00.000000000" , "2019" 1 , "2018/01/01<sp>00:00:00.000000000" , "2019/01/01<sp>00:00:00.000000000" ) ; java . util . List < java . lang . String > expected = java . util . Arrays . asList ( "2014" , "2019" 0 , "2016" , "2017" , "2018" , "2019" ) ; java . util . List < java . lang . String > found = org . diirt . graphene . TimeScales . trimLabels ( input ) ; org . junit . Assert . assertThat ( found , equalTo ( expected ) ) ; } }
public class aTest{ @Test public void testQueryCancelTwice ( ) { java . lang . String udfName = org . apache . hive . jdbc . TestJdbcDriver2 . SleepMsUDF . class . getName ( ) ; java . sql . Statement stmt1 = org . apache . hive . jdbc . TestJdbcDriver2 . con . createStatement ( ) ; stmt1 . execute ( ( ( "create<sp>temporary<sp>function<sp>sleepMsUDF<sp>as<sp>'" + udfName ) + "'" ) ) ; stmt1 . close ( ) ; final java . sql . Statement stmt = org . apache . hive . jdbc . TestJdbcDriver2 . con . createStatement ( ) ; java . lang . Thread tExecute = new java . lang . Thread ( new java . lang . Runnable ( ) { @ org . apache . hive . jdbc . Override public void run ( ) { try { System . out . println ( "Executing<sp>query:<sp>" ) ; stmt . executeQuery ( ( ( ( ( ( "select<sp>sleepMsUDF(t1.under_col,<sp>1)<sp>as<sp>u0,<sp>t1.under_col<sp>as<sp>u1,<sp>" + "t2.under_col<sp>as<sp>u2<sp>from<sp>" ) + ( org . apache . hive . jdbc . TestJdbcDriver2 . tableName ) ) + "<sp>t1<sp>join<sp>" ) + ( org . apache . hive . jdbc . TestJdbcDriver2 . tableName ) ) + "<sp>t2<sp>on<sp>t1.under_col<sp>=<sp>t2.under_col" ) ) ; org . junit . Assert . fail ( "Expecting<sp>SQLException" ) ; } catch ( java . sql . SQLException e ) { org . junit . Assert . assertNotNull ( e ) ; System . out . println ( e . toString ( ) ) ; } } } }
public class aTest{ @Test public void testAccept2WithCache ( ) { org . eclipse . birt . data . engine . api . querydefn . FilterDefinition [ ] filterDefn = new org . eclipse . birt . data . engine . api . querydefn . FilterDefinition [ ] { new org . eclipse . birt . data . engine . api . querydefn . FilterDefinition ( new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( "row.ROW_COL0<sp>+<sp>row.ROW_COL1<sp>><sp>row.ROW_COL2" ) ) } ; org . eclipse . birt . data . engine . api . IResultIterator resultIterator = getResultIterator ( filterDefn , null , null , true ) ; java . lang . String queryResultID = resultIterator . getQueryResults ( ) . getID ( ) ; resultIterator . close ( ) ; resultIterator = getResultIterator ( filterDefn , null , null , true , queryResultID ) ; while ( resultIterator . next ( ) ) { java . lang . Integer value0 = resultIterator . getInteger ( getBindingExpressionName ( ) [ 0 ] ) ; java . lang . Integer value1 = resultIterator . getInteger ( getBindingExpressionName ( ) [ 1 ] ) ; java . lang . Integer value2 = resultIterator . getInteger ( getBindingExpressionName ( ) [ 2 ] ) ; org . junit . Assert . assertTrue ( ( ( ( value0 . intValue ( ) ) + ( value1 . intValue ( ) ) ) > ( value2 . intValue ( ) ) ) ) ; } }
public class aTest{ @Test public void testDecodeRequest ( org . apache . servicecomb . core . Endpoint ) { commonMock ( ) ; org . mockito . Mockito . when ( schemaMeta . getProviderHandlerChain ( ) ) . thenReturn ( java . util . Collections . emptyList ( ) ) ; java . lang . Object [ ] args = new java . lang . Object [ ] { } ; org . mockito . Mockito . when ( schema . readObject ( bodyBuffer ) ) . thenReturn ( args ) ; org . apache . servicecomb . core . Invocation invocation = new org . apache . servicecomb . core . Invocation ( endpoint , operationMeta , null ) ; org . apache . servicecomb . transport . highway . HighwayCodec . decodeRequest ( invocation , header , operationProtobuf , bodyBuffer ) ; org . junit . Assert . assertSame ( args , invocation . getSwaggerArguments ( ) ) ; } }
public class aTest{ @Test public void testNullableChildList2 ( ) { java . lang . String grammar = "abc\n" 4 + ( ( ( ( ( "options<sp>{output=AST;}\n" + "a<sp>:<sp>ID<sp>INT?<sp>SEMI<sp>-><sp>^(ID<sp>INT?)<sp>SEMI<sp>;\n" ) + "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" ) + "abc\n" 6 ) + "SEMI<sp>:<sp>\';\'<sp>;\n" ) + "WS<sp>:<sp>(\'<sp>\'|\'\\n\')<sp>{$channel=HIDDEN;}<sp>;\n" ) ; java . lang . String treeGrammar = "abc\n" 8 + ( ( "a<sp>:<sp>^(ID<sp>INT?)<sp>SEMI\n" + "abc\n" 3 ) + "abc\n" 5 ) ; java . lang . String found = execTreeParser ( "abc\n" 7 , grammar , "TParser" , "TP.g" , treeGrammar , "abc\n" 2 , "TLexer" , "abc\n" 0 , "abc\n" 0 , "abc\n" 1 ) ; org . junit . Assert . assertEquals ( "abc\n" , found ) ; } }
public class aTest{ @Test public void testIsCachedTableNotExists ( ) { System . out . println ( ( ( getTestTraceHead ( "[MySQLCache.isCachedTable]" ) ) + "--------<sp>A<sp>not<sp>cached<sp>table<sp>is<sp>checked" ) ) ; com . telefonica . iot . cygnus . backends . mysql . MySQLCache cache = new com . telefonica . iot . cygnus . backends . mysql . MySQLCache ( ) ; java . lang . String dbName = "dbname" ; java . lang . String tableName = "tablename" ; cache . addDb ( dbName ) ; try { org . junit . Assert . assertTrue ( ( ! ( cache . isCachedTable ( dbName , tableName ) ) ) ) ; System . out . println ( ( ( getTestTraceHead ( "[MySQLCache.isCachedTable]" ) ) + "-<sp>OK<sp>-<sp>The<sp>table<sp>was<sp>not<sp>cached" ) ) ; } }
public class aTest{ @Test public void testSetItem ( ) { list . add ( "itemX" ) ; list . add ( "item1" ) ; list . setItem ( 0 , "item0" ) ; org . junit . Assert . assertEquals ( "item0" , list . getItem ( 0 ) ) ; list . removeAll ( ) ; try { list . setItem ( ( - 4 ) , "won't<sp>make<sp>it" ) ; org . junit . Assert . fail ( "Must<sp>check<sp>valid<sp>range<sp>of<sp>index" ) ; } }
public class aTest{ @Test public void test3_01ContextMenuPresence ( ) { org . eclipse . swtbot . swt . finder . widgets . SWTBotTreeItem traceItem = org . eclipse . tracecompass . tmf . ui . swtbot . tests . shared . SWTBotUtils . selectTracesFolder ( org . eclipse . tracecompass . integration . swtbot . tests . projectexplorer . ProjectExplorerTracesFolderTest . fBot , org . eclipse . tracecompass . integration . swtbot . tests . projectexplorer . ProjectExplorerTracesFolderTest . TRACE_PROJECT_NAME ) ; final java . util . List < java . lang . String > EXPECTED_MENU_LABELS = com . google . common . collect . ImmutableList . of ( "Open<sp>Trace..." , "Open<sp>As<sp>Experiment..." , "Refresh" 0 , "Import..." , "Refresh" 0 , "New<sp>Folder..." , "Refresh" 1 , "Refresh" 0 , "Import<sp>Trace<sp>Package..." , "Refresh" 3 , "Refresh" 0 , "Export<sp>Trace<sp>Package..." , "Refresh" 0 , "Manage<sp>Custom<sp>Parsers..." , "Manage<sp>XML<sp>analyses..." , "Refresh" 0 , "Apply<sp>Time<sp>Offset..." , "Refresh" 2 , "Refresh" 0 , "Refresh" ) ; java . util . List < java . lang . String > menuLabels = traceItem . contextMenu ( ) . menuItems ( ) ; for ( int i = 0 ; i < ( menuLabels . size ( ) ) ; i ++ ) { org . junit . Assert . assertEquals ( EXPECTED_MENU_LABELS . get ( i ) , menuLabels . get ( i ) ) ; } }
public class aTest{ @Test public void testRejectPredefinedParallelism ( ) { when ( context . getVertexNumTasks ( vertexName ) ) . thenReturn ( 10 ) ; try { vertexManager = new org . apache . tez . runtime . library . cartesianproduct . CartesianProductVertexManager ( context ) ; org . junit . Assert . assertTrue ( false ) ; } }
public class aTest{ @Test public void shouldNotBeAbleToAddSameNodeTwice ( ) { org . neo4j . neode . test . Db . usingSampleDataset ( new org . neo4j . neode . test . Db . WithSampleDataset ( ) { @ org . neo4j . neode . Override public void execute ( org . neo4j . graphdb . GraphDatabaseService db , org . neo4j . graphdb . Node firstNode , org . neo4j . graphdb . Node secondNode , org . neo4j . graphdb . Node thirdNode ) { org . neo4j . neode . NodeCollection nodeCollection = new org . neo4j . neode . NodeCollection ( db , "user" , org . neo4j . neode . NodeCollectionTest . toSet ( firstNode . getId ( ) ) ) ; nodeCollection . add ( firstNode ) ; java . lang . Iterable < org . neo4j . graphdb . Node > expectedNodes = asList ( firstNode ) ; org . junit . Assert . assertThat ( nodeCollection , returnsSameItems ( expectedNodes ) ) ; } } }
public class aTest{ @Test public void testXPathHelperWithNoNamespaceTextPath ( ) { try { java . lang . String xmlString = getFileContentsAsString ( ( ( ddf . catalog . impl . XPathHelperTest . TEST_DATA_PATH ) + ( ddf . catalog . impl . XPathHelperTest . INPUT_FILE ) ) ) ; ddf . util . XPathHelper xHelper = new ddf . util . XPathHelper ( xmlString ) ; org . w3c . dom . NodeList nodeList = ( ( org . w3c . dom . NodeList ) ( xHelper . evaluate ( "//fileTitle" , XPathConstants . NODESET , new ddf . catalog . impl . MockNamespaceResolver ( ) ) ) ) ; ddf . catalog . impl . XPathHelperTest . LOGGER . debug ( "testXPathHelper_WithNoNamespaceTextPath()<sp>-<sp>nodeList<sp>length<sp>=<sp>{}" , nodeList . getLength ( ) ) ; org . junit . Assert . assertEquals ( 0 , nodeList . getLength ( ) ) ; } }
public class aTest{ @Test public void testAutoWildcard ( ) { java . lang . String grammar = "grammar<sp>T;\n" + ( ( ( ( "options<sp>{output=AST;}\n" + "a" 2 ) + "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" ) + "a" 0 ) + "a" 1 ) ; java . lang . String treeGrammar = "tree<sp>grammar<sp>TP;\n" + ( ( "options<sp>{output=AST;<sp>ASTLabelType=CommonTree;<sp>tokenVocab=T;}\n" + "a<sp>:<sp>ID<sp>.<sp>\n" ) + "<sp>;\n" ) ; java . lang . String found = execTreeParser ( "a" 7 , grammar , "TParser" , "a" 6 , treeGrammar , "TP" , "a" 3 , "a" , "a" , "a" 4 ) ; org . junit . Assert . assertEquals ( "a" 5 , found ) ; } }
public class aTest{ @Test public void badOrderByBadGrammarDesc ( ) { org . apache . usergrid . persistence . CollectionIT . logger . debug ( "badOrderByBadGrammarDesc" ) ; org . apache . usergrid . persistence . EntityManager em = app . getEntityManager ( ) ; org . junit . Assert . assertNotNull ( em ) ; java . lang . String s = "select<sp>*<sp>where<sp>name<sp>=<sp>'bob'<sp>order<sp>by" ; java . lang . String error = null ; java . lang . String entityType = null ; java . lang . String propertyName = null ; try { em . searchCollection ( em . getApplicationRef ( ) , "users" , org . apache . usergrid . persistence . Query . fromQL ( s ) ) ; org . junit . Assert . fail ( "I<sp>should<sp>throw<sp>an<sp>exception" ) ; } }
public class aTest{ @Test public void fiveMillionEntries ( ) { org . junit . Assert . assertNotNull ( mem ) ; int howMany = 5000000 ; int size = ( ( int ) ( mem . capacity ( ) ) ) / howMany ; size -= ( size / 100 ) * 1 ; org . apache . directmemory . memory . MallocWithUnsafeTest . logger . info ( ( "payload<sp>size=" + size ) ) ; org . apache . directmemory . memory . MallocWithUnsafeTest . logger . info ( ( "entries=" + howMany ) ) ; org . apache . directmemory . memory . MallocWithUnsafeTest . logger . info ( "starting..." ) ; long start = java . lang . System . currentTimeMillis ( ) ; byte [ ] payload = new byte [ size ] ; for ( int i = 0 ; i < howMany ; i ++ ) { mem . store ( payload ) ; } }
public class aTest{ @Test public void listUsers ( ) { streamflow . model . User user1 = new streamflow . model . User ( ) ; user1 . setId ( "user-1" ) ; user1 . setUsername ( "user1" ) ; user1 . setFirstName ( "First<sp>Name<sp>1" ) ; user1 . setLastName ( "Last<sp>Name<sp>1" ) ; user1 . setEmail ( "user1@test.com" ) ; streamflow . model . User user2 = new streamflow . model . User ( ) ; user2 . setId ( "/api/users" 1 ) ; user2 . setUsername ( "user2" ) ; user2 . setFirstName ( "First<sp>Name<sp>2" ) ; user2 . setLastName ( "Last<sp>Name<sp>2" ) ; user2 . setEmail ( "/api/users" 0 ) ; java . util . List < streamflow . model . User > mockedUsers = new java . util . ArrayList < streamflow . model . User > ( ) ; mockedUsers . add ( user1 ) ; mockedUsers . add ( user2 ) ; when ( streamflow . server . resource . UserResourceTest . userServiceMock . listUsers ( ) ) . thenReturn ( mockedUsers ) ; java . util . List < streamflow . model . User > responseUsers = resource ( ) . path ( "/api/users" ) . accept ( MediaType . APPLICATION_JSON ) . get ( new com . sun . jersey . api . client . GenericType < java . util . List < streamflow . model . User > > ( ) { } ) ; org . junit . Assert . assertEquals ( "Response<sp>users<sp>should<sp>be<sp>equal<sp>to<sp>the<sp>mocked<sp>users" , mockedUsers , responseUsers ) ; verify ( streamflow . server . resource . UserResourceTest . userServiceMock ) . listUsers ( ) ; } }
public class aTest{ @Test public void testNotDuplicatedDisplayedMnemonic ( ) { es . gob . afirma . ui . wizardsobres . PanelDestinatariosAccessibilityTest . logger . info ( "testNotDuplicatedDisplayedMnemonic" ) ; try { final es . gob . afirma . ui . wizardsobres . PanelDestinatarios panelDestinatarios = new es . gob . afirma . ui . wizardsobres . PanelDestinatarios ( ) ; final java . util . List < java . lang . Integer > keyCodes = new java . util . ArrayList ( ) ; java . util . Set < java . lang . Integer > keyCodesSet = null ; final java . awt . Component [ ] components = panelDestinatarios . getComponents ( ) ; for ( final java . awt . Component componentWizard : components ) { if ( componentWizard instanceof javax . swing . JRootPane ) { final java . awt . Component [ ] componentsRootPane = ( ( javax . swing . JRootPane ) ( componentWizard ) ) . getComponents ( ) ; for ( final java . awt . Component componentRootPane : componentsRootPane ) { if ( componentRootPane instanceof javax . swing . JPanel ) { getKeyCodeList ( ( ( javax . swing . JPanel ) ( componentRootPane ) ) , keyCodes ) ; } else if ( componentRootPane instanceof javax . swing . JLayeredPane ) { final java . awt . Component [ ] componentsLayeredPane = ( ( javax . swing . JLayeredPane ) ( componentRootPane ) ) . getComponents ( ) ; for ( final java . awt . Component componentLayeredPane : componentsLayeredPane ) { if ( componentLayeredPane instanceof javax . swing . JPanel ) { getKeyCodeList ( ( ( javax . swing . JPanel ) ( componentLayeredPane ) ) , keyCodes ) ; } } } } } } keyCodesSet = new java . util . HashSet ( keyCodes ) ; org . junit . Assert . assertTrue ( ( ( keyCodesSet . size ( ) ) == ( keyCodes . size ( ) ) ) ) ; } }
public class aTest{ @Test public void testEncodeKafkaSinglex ( ) { System . out . println ( ( ( com . telefonica . iot . cygnus . utils . CommonUtilsForTests . getTestTraceHead ( "[NGSICharsets.encodeKafka]" ) ) + "--------<sp>A<sp>single<sp>'x'<sp>is<sp>not<sp>encoded" ) ) ; java . lang . String in = "x" ; java . lang . String expected = "x" ; java . lang . String out = com . telefonica . iot . cygnus . utils . NGSICharsets . encodeKafka ( in ) ; try { org . junit . Assert . assertEquals ( expected , out ) ; System . out . println ( ( ( ( ( com . telefonica . iot . cygnus . utils . CommonUtilsForTests . getTestTraceHead ( "[NGSICharsets.encodeKafka]" ) ) + "-<sp>OK<sp>-<sp>'" ) + in ) + "'<sp>has<sp>not<sp>been<sp>encoded" ) ) ; } }
public class aTest{ @Test public void xpathEscapeTest ( ) { javax . jcr . Session writer = createAdminSession ( ) ; javax . jcr . Session reader = createAdminSession ( ) ; org . apache . jackrabbit . api . security . user . UserManager uMgr = ( ( org . apache . jackrabbit . api . JackrabbitSession ) ( writer ) ) . getUserManager ( ) ; java . lang . String uid = "testUser" ; try { org . apache . jackrabbit . api . security . user . User user = uMgr . createUser ( "testUser" , "pw" ) ; writer . getNode ( user . getPath ( ) ) . addNode ( ".tokens" , "rep:Unstructured" ) ; writer . save ( ) ; javax . jcr . query . QueryManager qm = reader . getWorkspace ( ) . getQueryManager ( ) ; javax . jcr . query . Query q = qm . createQuery ( "/jcr:root//*[_x002e_tokens/@jcr:primaryType]" , Query . XPATH ) ; javax . jcr . NodeIterator res = q . execute ( ) . getNodes ( ) ; org . junit . Assert . assertEquals ( 1 , res . getSize ( ) ) ; } }
public class aTest{ @Test public void testWithOOPathAndNot ( ) { final java . lang . String drl = ( ( ( ( ( ( ( ( ( "<sp>not<sp>/persons[age<sp>>=<sp>18]\n" 0 + ( org . drools . testcoverage . common . model . Person . class . getCanonicalName ( ) ) ) + "\n" ) + "<sp>not<sp>/persons[age<sp>>=<sp>18]\n" 0 ) + ( org . drools . compiler . integrationtests . RuleUnitTest . AdultUnit . class . getCanonicalName ( ) ) ) + "\n" ) + "rule<sp>Adult<sp>@Unit(<sp>AdultUnit.class<sp>)<sp>when\n" ) + "<sp>not<sp>/persons[age<sp>>=<sp>18]\n" ) + "then\n" ) + "<sp>System.out.println(\"No<sp>adults\"<sp>not<sp>/persons[age<sp>>=<sp>18]\n" 1 ) + "end" ; final org . kie . api . KieBase kbase = org . drools . testcoverage . common . util . KieBaseUtil . getKieBaseFromKieModuleFromDrl ( "rule-unit-test" , kieBaseTestConfiguration , drl ) ; final org . kie . api . runtime . rule . RuleUnitExecutor executor = org . kie . api . runtime . rule . RuleUnitExecutor . create ( ) . bind ( kbase ) ; try { final org . kie . api . runtime . rule . DataSource < org . drools . testcoverage . common . model . Person > persons = executor . newDataSource ( "persons" , new org . drools . testcoverage . common . model . Person ( "Mario" , 4 ) , new org . drools . testcoverage . common . model . Person ( "Marilena" , 17 ) , new org . drools . testcoverage . common . model . Person ( "<sp>not<sp>/persons[age<sp>>=<sp>18]\n" 2 , 4 ) ) ; final org . kie . api . runtime . rule . RuleUnit adultUnit = new org . drools . compiler . integrationtests . RuleUnitTest . AdultUnit ( persons ) ; org . junit . Assert . assertEquals ( 1 , executor . run ( adultUnit ) ) ; } }
public class aTest{ @Test public void test_BrowserFunction_callback ( ) { org . junit . Assume . assumeFalse ( webkit1SkipMsg ( ) , isWebkit1 ) ; java . util . concurrent . atomic . AtomicBoolean javaCallbackExecuted = new java . util . concurrent . atomic . AtomicBoolean ( false ) ; class JavascriptCallback extends org . eclipse . swt . browser . BrowserFunction { JavascriptCallback ( org . eclipse . swt . browser . Browser browser , java . lang . String name ) { ( browser , name ) ; } @ org . eclipse . swt . tests . junit . Override public java . lang . Object function ( java . lang . Object [ ] arguments ) { javaCallbackExecuted . set ( true ) ; return null ; } } java . lang . String htmlWithScript = "<html><head>\n" + ( ( ( ( ( ( ( ( "<script<sp>language=\"JavaScript\">\n" + "function<sp>callCustomFunction()<sp>{\n" ) + "<sp>document.body.style.backgroundColor<sp>=<sp>\'red\'\n" ) + "\t\tjsCallbackToJava()\n" ) + "<sp>document.body.style.backgroundColor<sp>=<sp>\'red\'\n" 0 ) + "</script>\n" ) + "</head>\n" ) + "<body><sp>I\'m<sp>going<sp>to<sp>make<sp>a<sp>callback<sp>to<sp>java<sp></body>\n" ) + "</html>\n" ) ; browser . setText ( htmlWithScript ) ; new JavascriptCallback ( browser , "<sp>document.body.style.backgroundColor<sp>=<sp>\'red\'\n" 1 ) ; browser . addProgressListener ( callCustomFunctionUponLoad ) ; shell . open ( ) ; boolean passed = waitForPassCondition ( javaCallbackExecuted :: get ) ; java . lang . String message = "<sp>document.body.style.backgroundColor<sp>=<sp>\'red\'\n" 2 ; org . junit . Assert . assertTrue ( message , passed ) ; } }
public class aTest{ @Test public void write_Cprecincts ( ) { if ( ! ( isJp2KakDriverAvailable ) ) return ; it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . LOGGER . info ( "Testing<sp>JP2<sp>Write<sp>operation<sp>with<sp>Cprecincts<sp>option<sp>setting" ) ; final java . io . File inputFile = it . geosolutions . resources . TestData . file ( this , it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . testFileName ) ; org . junit . Assert . assertTrue ( inputFile . exists ( ) ) ; final java . io . File outputFile1 = it . geosolutions . resources . TestData . temp ( this , "CprecintsA-.jp2" , it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . deleteTempFilesOnExit ) ; final java . io . File outputFile2 = it . geosolutions . resources . TestData . temp ( this , "CprecintsB-.jp2" , it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . deleteTempFilesOnExit ) ; final javax . media . jai . ParameterBlockJAI pbjImageRead = new javax . media . jai . ParameterBlockJAI ( "CprecintsB-.jp2" 2 ) ; pbjImageRead . setParameter ( "Input" , inputFile ) ; if ( it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . ENABLE_SUBSAMPLING ) { javax . imageio . ImageReadParam readParam = new javax . imageio . ImageReadParam ( ) ; readParam . setSourceSubsampling ( 4 , 4 , 0 , 0 ) ; pbjImageRead . setParameter ( "readParam" , readParam ) ; } }
public class aTest{ @Test public void testAllServiceConfigVersionsWithConfigGroups ( ) { createDefaultCluster ( ) ; c1 . addService ( "HDFS" , helper . getOrCreateRepositoryVersion ( new org . apache . ambari . server . state . StackId ( "HDP" , "0.1" ) , "0.1" ) ) ; org . apache . ambari . server . state . Config hdfsSiteConfigV1 = configFactory . createNew ( c1 , "HDP" 1 , "version1" , com . google . common . collect . ImmutableMap . of ( "HDP" 3 , "v1" ) , com . google . common . collect . ImmutableMap . of ( ) ) ; org . apache . ambari . server . controller . ServiceConfigVersionResponse hdfsSiteConfigResponseV1 = c1 . addDesiredConfig ( "admin" , java . util . Collections . singleton ( hdfsSiteConfigV1 ) ) ; java . util . List < org . apache . ambari . server . controller . ConfigurationResponse > configResponsesDefaultGroup = java . util . Collections . singletonList ( new org . apache . ambari . server . controller . ConfigurationResponse ( c1 . getClusterName ( ) , hdfsSiteConfigV1 . getStackId ( ) , hdfsSiteConfigV1 . getType ( ) , hdfsSiteConfigV1 . getTag ( ) , hdfsSiteConfigV1 . getVersion ( ) , hdfsSiteConfigV1 . getProperties ( ) , hdfsSiteConfigV1 . getPropertiesAttributes ( ) , hdfsSiteConfigV1 . getPropertiesTypes ( ) ) ) ; hdfsSiteConfigResponseV1 . setConfigurations ( configResponsesDefaultGroup ) ; org . apache . ambari . server . state . Config hdfsSiteConfigV2 = configFactory . createNew ( c1 , "HDP" 1 , "version2" , com . google . common . collect . ImmutableMap . of ( "HDP" 3 , "v2" ) , com . google . common . collect . ImmutableMap . of ( ) ) ; org . apache . ambari . server . state . configgroup . ConfigGroup configGroup = configGroupFactory . createNew ( c1 , "HDFS" , "configGroup1" , "version1" , "HDP" 2 , com . google . common . collect . ImmutableMap . of ( hdfsSiteConfigV2 . getType ( ) , hdfsSiteConfigV2 ) , com . google . common . collect . ImmutableMap . of ( ) ) ; c1 . addConfigGroup ( configGroup ) ; org . apache . ambari . server . controller . ServiceConfigVersionResponse hdfsSiteConfigResponseV2 = c1 . createServiceConfigVersion ( "HDFS" , "admin" , "HDP" 0 , configGroup ) ; hdfsSiteConfigResponseV2 . setConfigurations ( java . util . Collections . singletonList ( new org . apache . ambari . server . controller . ConfigurationResponse ( c1 . getClusterName ( ) , hdfsSiteConfigV2 . getStackId ( ) , hdfsSiteConfigV2 . getType ( ) , hdfsSiteConfigV2 . getTag ( ) , hdfsSiteConfigV2 . getVersion ( ) , hdfsSiteConfigV2 . getProperties ( ) , hdfsSiteConfigV2 . getPropertiesAttributes ( ) , hdfsSiteConfigV2 . getPropertiesTypes ( ) ) ) ) ; hdfsSiteConfigResponseV2 . setIsCurrent ( true ) ; org . apache . ambari . server . controller . ServiceConfigVersionResponse hdfsSiteConfigResponseV3 = c1 . createServiceConfigVersion ( "HDFS" , "admin" , "new<sp>config<sp>in<sp>default<sp>group" , null ) ; hdfsSiteConfigResponseV3 . setConfigurations ( configResponsesDefaultGroup ) ; hdfsSiteConfigResponseV3 . setIsCurrent ( true ) ; java . util . List < org . apache . ambari . server . controller . ServiceConfigVersionResponse > expectedServiceConfigResponses = com . google . common . collect . ImmutableList . of ( hdfsSiteConfigResponseV1 , hdfsSiteConfigResponseV2 , hdfsSiteConfigResponseV3 ) ; java . util . List < org . apache . ambari . server . controller . ServiceConfigVersionResponse > allServiceConfigResponses = c1 . getServiceConfigVersions ( ) ; java . util . Collections . sort ( allServiceConfigResponses , new java . util . Comparator < org . apache . ambari . server . controller . ServiceConfigVersionResponse > ( ) { @ org . apache . ambari . server . state . cluster . Override public int compare ( org . apache . ambari . server . controller . ServiceConfigVersionResponse o1 , org . apache . ambari . server . controller . ServiceConfigVersionResponse o2 ) { return o1 . getVersion ( ) . compareTo ( o2 . getVersion ( ) ) ; } } ) ; org . junit . Assert . assertThat ( allServiceConfigResponses , org . hamcrest . CoreMatchers . is ( expectedServiceConfigResponses ) ) ; } }
public class aTest{ @Test public void partitionedAndNonpartitionedThreadpools ( ) { final java . lang . String rulebase = "rules/reloaded/msg008.prova" ; java . util . concurrent . atomic . AtomicInteger count = new java . util . concurrent . atomic . AtomicInteger ( 0 ) ; java . util . Map < java . lang . String , java . lang . Object > globals = new java . util . HashMap < java . lang . String , java . lang . Object > ( ) ; globals . put ( "$Count" , count ) ; prova = new ws . prova . api2 . ProvaCommunicatorImpl ( test . ws . prova . test2 . ProvaMessagingTest . kAgent , test . ws . prova . test2 . ProvaMessagingTest . kPort , rulebase , ws . prova . api2 . ProvaCommunicatorImpl . SYNC , globals ) ; try { synchronized ( this ) { wait ( 2000 ) ; org . junit . Assert . assertEquals ( 4 , count . get ( ) ) ; } } }
public class aTest{ @Test public void testGetOS ( ) { java . lang . String actualOS = null ; try { try { actualOS = proxyManager . getOS ( java . net . URI . create ( "remotetools://MyConnection/path/to/file" ) ) ; org . junit . Assert . fail ( "remotetools<sp>scheme<sp>should<sp>not<sp>be<sp>recognized" ) ; } catch ( org . eclipse . core . runtime . CoreException e ) { org . junit . Assert . assertTrue ( e . getMessage ( ) , true ) ; } } }
public class aTest{ @Test public void testSaveUrlPrefixNull ( javax . servlet . ServletContext ) { java . lang . System . clearProperty ( Const . URL_PREFIX ) ; org . apache . servicecomb . transport . rest . servlet . ServletUtils . saveUrlPrefix ( servletContext ) ; org . junit . Assert . assertNull ( java . lang . System . getProperty ( Const . URL_PREFIX ) ) ; java . lang . System . clearProperty ( Const . URL_PREFIX ) ; } }
public class aTest{ @Test public void checkXMLPersistence ( ) { org . eclipse . ice . datastructures . entry . FileEntry entry2 ; org . eclipse . ice . datastructures . ICEObject . ICEJAXBHandler xmlHandler = new org . eclipse . ice . datastructures . ICEObject . ICEJAXBHandler ( ) ; java . util . ArrayList < java . lang . Class > classList = new java . util . ArrayList < java . lang . Class > ( ) ; classList . add ( org . eclipse . ice . datastructures . entry . FileEntry . class ) ; org . eclipse . ice . datastructures . entry . FileEntry myEntry = new org . eclipse . ice . datastructures . entry . FileEntry ( ) ; myEntry . setProject ( org . eclipse . ice . tests . datastructures . entry . FileEntryTester . project ) ; myEntry . setId ( 1 ) ; myEntry . setName ( "Simple<sp>Entry" ) ; myEntry . setComment ( "Peanut<sp>butter<sp>and<sp>jelly" ) ; myEntry . setTag ( "ChevyChase" ) ; try { java . io . ByteArrayOutputStream outputStream = new java . io . ByteArrayOutputStream ( ) ; xmlHandler . write ( myEntry , classList , outputStream ) ; System . err . println ( outputStream . toString ( ) ) ; java . io . InputStream inputStream = new java . io . ByteArrayInputStream ( outputStream . toByteArray ( ) ) ; entry2 = ( ( org . eclipse . ice . datastructures . entry . FileEntry ) ( xmlHandler . read ( classList , inputStream ) ) ) ; org . junit . Assert . assertTrue ( myEntry . equals ( entry2 ) ) ; } }
public class aTest{ @Test public void step03RecomposeComposites ( ) { org . eclipse . emf . releng . UpdateSiteGenerator updateSiteGenerator = new org . eclipse . emf . releng . UpdateSiteGenerator ( ) ; java . io . File buildsRootFolder = new java . io . File ( UpdateSiteGenerator . BUILDS_ROOT_FOLDER ) ; org . junit . Assert . assertTrue ( buildsRootFolder . isDirectory ( ) ) ; for ( java . io . File child : buildsRootFolder . listFiles ( ) ) { java . lang . String buildType = child . getName ( ) ; if ( ( child . isDirectory ( ) ) && ( UpdateSiteGenerator . BUILD_TYPES . contains ( buildType ) ) ) { java . util . List < java . lang . String > children = new java . util . ArrayList < java . lang . String > ( ) ; for ( java . io . File grandChild : child . listFiles ( ) ) { java . lang . String name = grandChild . getName ( ) ; if ( ( ( ! ( "latest" . equals ( name ) ) ) && ( grandChild . isDirectory ( ) ) ) && ( new java . io . File ( grandChild , "content.jar" ) . isFile ( ) ) ) { children . add ( org . eclipse . emf . releng . UpdateSiteGenerator . getCanonicalPath ( grandChild ) ) ; } } }
public class aTest{ @Test public void test_cluster ( ) { final java . lang . String testName = "clustering" ; final io . github . livingdocumentation . dotdiagram . DotGraph graph = new io . github . livingdocumentation . dotdiagram . DotGraph ( ( testName + "My<sp>Car" 3 ) ) ; final io . github . livingdocumentation . dotdiagram . DotGraph . Digraph digraph = graph . getDigraph ( ) ; final io . github . livingdocumentation . dotdiagram . DotGraph . Cluster cluster = digraph . addCluster ( "My<sp>Car" 8 ) ; cluster . setLabel ( "BMW<sp>brand" ) . setComment ( "My<sp>Car" 2 ) ; cluster . addNode ( "Car" ) . setLabel ( "My<sp>Car" ) . setComment ( "My<sp>Car" 1 ) . setOptions ( io . github . livingdocumentation . dotdiagram . DotStyles . STUB_NODE_OPTIONS ) ; cluster . addNode ( "Wheel" ) . setLabel ( "My<sp>Car" 6 ) . setComment ( "The<sp>wheels<sp>of<sp>my<sp>car" ) ; cluster . addAssociation ( "Car" , "Wheel" ) . setLabel ( "My<sp>Car" 4 ) . setComment ( "There<sp>are<sp>4<sp>wheels" ) . setOptions ( io . github . livingdocumentation . dotdiagram . DotStyles . ASSOCIATION_EDGE_STYLE ) ; digraph . addNode ( "My<sp>Car" 7 ) . setLabel ( "My<sp>Customer" ) . setComment ( "My<sp>Car" 5 ) . setOptions ( io . github . livingdocumentation . dotdiagram . DotStyles . NOTE_EDGE_STYLE ) ; digraph . addAssociation ( "My<sp>Car" 7 , "Car" ) . setLabel ( "buys" ) . setComment ( "The<sp>buyer<sp>of<sp>the<sp>car" ) . setOptions ( io . github . livingdocumentation . dotdiagram . DotStyles . INSTANTIATION_EDGE_STYLE ) ; final java . lang . String actual = graph . render ( ) . trim ( ) ; final java . lang . String expected = io . github . livingdocumentation . dotdiagram . DotGraphTest . readTestResource ( ( testName + "My<sp>Car" 0 ) ) . trim ( ) ; org . junit . Assert . assertEquals ( expected . trim ( ) , actual ) ; } }
public class aTest{ @Test public void testStringList ( org . jboss . weld . tests . contexts . StringHolder ) { java . util . List < java . lang . String > str = holder . getStrings ( ) ; org . junit . Assert . assertEquals ( 2 , str . size ( ) ) ; } }
public class aTest{ @Test public void testRrdPersistentLayer ( ) { java . util . Calendar cl = java . util . Calendar . getInstance ( ) ; cl . set ( 2009 , Calendar . NOVEMBER , 14 , 0 , 0 , 0 ) ; long startTime = ( cl . getTime ( ) . getTime ( ) ) / 1000 ; long step = 10 ; org . krakenapps . rrd . RrdConfig config = new org . krakenapps . rrd . RrdConfig ( makeDate ( ( startTime - 1 ) ) , step ) ; config . addDataSource ( "test" , DataSourceType . GAUGE , ( step * 2 ) , Double . NaN , Double . NaN ) ; config . addDataSource ( "test" , DataSourceType . COUNTER , ( step * 2 ) , Double . NaN , Double . NaN ) ; config . addDataSource ( "test" , DataSourceType . DERIVE , ( step * 2 ) , Double . NaN , Double . NaN ) ; config . addDataSource ( "test" , DataSourceType . ABSOLUTE , ( step * 2 ) , Double . NaN , Double . NaN ) ; config . addArchive ( ConsolidateFunc . AVERAGE , 0.5 , 6 , 10 ) ; config . addArchive ( ConsolidateFunc . MAX , 0.5 , 6 , 10 ) ; config . addArchive ( ConsolidateFunc . MIN , 0.5 , 6 , 10 ) ; config . addArchive ( ConsolidateFunc . LAST , 0.5 , 6 , 10 ) ; config . addArchive ( ConsolidateFunc . AVERAGE , 0.5 , 60 , 18 ) ; config . addArchive ( ConsolidateFunc . MAX , 0.5 , 60 , 18 ) ; config . addArchive ( ConsolidateFunc . MIN , 0.5 , 60 , 18 ) ; config . addArchive ( ConsolidateFunc . LAST , 0.5 , 60 , 18 ) ; org . krakenapps . rrd . io . FilePersistentLayer pl ; try { pl = new org . krakenapps . rrd . io . FilePersistentLayer ( new java . io . File ( "rrd_perslayer_test.bin" ) ) ; long t = startTime ; java . util . ArrayList < org . krakenapps . rrd . RrdTest . DoubleSample > samples = new java . util . ArrayList < org . krakenapps . rrd . RrdTest . DoubleSample > ( ) ; samples . add ( new org . krakenapps . rrd . RrdTest . SinSample ( t ) ) ; samples . add ( new org . krakenapps . rrd . RrdTest . SinCounterSample ( t ) ) ; samples . add ( new org . krakenapps . rrd . RrdTest . SinSample ( t ) ) ; samples . add ( new org . krakenapps . rrd . RrdTest . SinSample ( t ) ) ; org . krakenapps . rrd . impl . RrdRaw rrd1 = new org . krakenapps . rrd . impl . RrdRaw ( config ) ; for ( t = startTime ; t < ( startTime + ( ( 32 * 6 ) * step ) ) ; t += step ) { java . lang . Double [ ] row = new java . lang . Double [ samples . size ( ) ] ; int colIndex = 0 ; for ( org . krakenapps . rrd . RrdTest . DoubleSample sample : samples ) { row [ ( colIndex ++ ) ] = sample . getSample ( t ) ; } rrd1 . update ( new java . util . Date ( ( t * 1000 ) ) , ( ( java . lang . Double [ ] ) ( row ) ) ) ; } rrd1 . write ( pl ) ; pl . close ( ) ; org . krakenapps . rrd . impl . RrdRaw rrd2 = new org . krakenapps . rrd . impl . RrdRaw ( pl ) ; org . junit . Assert . assertTrue ( rrd1 . equals ( rrd2 ) ) ; } }
public class aTest{ @Test public void testBulkIndexCreation ( ) { java . lang . String sql = format ( "1536<sp>|" 2 , ( ( com . splicemachine . derby . test . framework . SpliceUnitTest . getResourceDirectory ( ) ) + "data" ) ) ; methodWatcher . executeUpdate ( sql ) ; sql = "select<sp>count(*)<sp>from<sp>t1<sp>--splice-properties<sp>index=idx_t1" ; rs = methodWatcher . executeQuery ( sql ) ; org . junit . Assert . assertEquals ( expected , TestUtils . FormattedResult . ResultFactory . toString ( rs ) ) ; rs . close ( ) ; } }
public class aTest{ @Test public void testGetResource_fromURLResourceLocator ( ) { com . mockobjects . servlet . MockServletConfig config = getServletConfig ( ) ; org . milyn . resource . ServletResourceLocatorTest . MyMockServletContext context = ( ( org . milyn . resource . ServletResourceLocatorTest . MyMockServletContext ) ( config . getServletContext ( ) ) ) ; org . milyn . resource . ServletResourceLocatorTest . MockExternalResourceLocator resLocator = new org . milyn . resource . ServletResourceLocatorTest . MockExternalResourceLocator ( ) ; org . milyn . resource . ServletResourceLocator servletLocator = new org . milyn . resource . ServletResourceLocator ( config , resLocator ) ; try { java . io . InputStream res = servletLocator . getResource ( "paraX" , "paraX-default" ) ; org . junit . Assert . assertEquals ( resLocator . stream , res ) ; } }
public class aTest{ @Test public void testSendReceive ( ) { javax . naming . Context namingContext = null ; javax . jms . JMSContext context = null ; try { final java . util . Properties env = new java . util . Properties ( ) ; env . put ( Context . INITIAL_CONTEXT_FACTORY , "org.jboss.naming.remote.client.InitialContextFactory" ) ; env . put ( Context . PROVIDER_URL , "http-remoting://127.0.0.1:8080" ) ; env . put ( Context . SECURITY_PRINCIPAL , "jmsuser" ) ; env . put ( Context . SECURITY_CREDENTIALS , "Password1!" ) ; namingContext = new javax . naming . InitialContext ( env ) ; javax . jms . ConnectionFactory connectionFactory = ( ( javax . jms . ConnectionFactory ) ( namingContext . lookup ( CONNECTION_FACTORY ) ) ) ; System . out . println ( ( "Got<sp>ConnectionFactory<sp>" + ( CONNECTION_FACTORY ) ) ) ; javax . jms . Destination destination = ( ( javax . jms . Destination ) ( namingContext . lookup ( DESTINATION ) ) ) ; System . out . println ( ( "Got<sp>JMS<sp>Endpoint<sp>" + ( DESTINATION ) ) ) ; context = connectionFactory . createContext ( "jmsuser" , "Password1!" ) ; context . createProducer ( ) . send ( destination , MESSAGE ) ; System . out . println ( ( "Sent<sp>message<sp>" + ( MESSAGE ) ) ) ; javax . jms . JMSConsumer consumer = context . createConsumer ( destination ) ; java . lang . String text = consumer . receiveBody ( java . lang . String . class , 5000 ) ; if ( text == null ) System . out . println ( "No<sp>message<sp>Received!<sp>Maybe<sp>another<sp>Consumer<sp>listening<sp>on<sp>the<sp>Queue<sp>??" ) ; System . out . println ( ( "Received<sp>message<sp>with<sp>content<sp>" + text ) ) ; org . junit . Assert . assertEquals ( text , MESSAGE ) ; } }
public class aTest{ @Test public void testCriticalOff ( ) { org . apache . activemq . artemis . core . config . Configuration configuration = createDefaultConfig ( false ) ; configuration . setCriticalAnalyzerCheckPeriod ( 10 ) . setCriticalAnalyzer ( false ) ; org . apache . activemq . artemis . core . server . ActiveMQServer server = createServer ( false , configuration , AddressSettings . DEFAULT_PAGE_SIZE , AddressSettings . DEFAULT_MAX_SIZE_BYTES ) ; server . start ( ) ; try { server . getCriticalAnalyzer ( ) . add ( new org . apache . activemq . artemis . utils . critical . CriticalComponent ( ) { @ org . apache . activemq . artemis . tests . integration . critical . Override public boolean isExpired ( long timeout ) { return true ; } } ) ; org . apache . activemq . artemis . tests . util . Wait . waitFor ( ( ) -> ! ( server . isStarted ( ) ) , 500 , 10 ) ; org . junit . Assert . assertTrue ( server . isStarted ( ) ) ; } }
public class aTest{ @Test public void testSearchGeneratedContent ( ) { java . lang . String lineageId = "test.prpt" ; java . lang . String pathId = "test.prpt" ; java . lang . String userFolder = "public/admin" ; org . pentaho . platform . api . repository2 . unified . webservices . RepositoryFileDto fileDetailsMock = mock ( org . pentaho . platform . api . repository2 . unified . webservices . RepositoryFileDto . class ) ; org . pentaho . platform . api . repository2 . unified . RepositoryFile workspaceFolder = mock ( org . pentaho . platform . api . repository2 . unified . RepositoryFile . class ) ; doReturn ( userFolder ) . when ( workspaceFolder ) . getId ( ) ; org . pentaho . platform . web . http . api . resources . SessionResource sessionResource = mock ( org . pentaho . platform . web . http . api . resources . SessionResource . class ) ; java . util . List < org . pentaho . platform . api . repository2 . unified . RepositoryFile > children = new java . util . ArrayList < org . pentaho . platform . api . repository2 . unified . RepositoryFile > ( ) ; org . pentaho . platform . api . repository2 . unified . RepositoryFile mockedChild = mock ( org . pentaho . platform . api . repository2 . unified . RepositoryFile . class ) ; doReturn ( false ) . when ( mockedChild ) . isFolder ( ) ; children . add ( mockedChild ) ; java . util . Map < java . lang . String , java . io . Serializable > mockedFileMetadata = mock ( java . util . Map . class ) ; doReturn ( lineageId ) . when ( mockedFileMetadata ) . get ( QuartzScheduler . RESERVEDMAPKEY_LINEAGE_ID ) ; when ( org . pentaho . platform . web . http . api . resources . services . FileServiceTest . fileService . repository . getFileMetadata ( mockedChild . getId ( ) ) ) . thenReturn ( mockedFileMetadata ) ; doReturn ( pathId ) . when ( fileDetailsMock ) . getId ( ) ; doReturn ( userFolder ) . when ( sessionResource ) . doGetCurrentUserDir ( ) ; doReturn ( workspaceFolder ) . when ( org . pentaho . platform . web . http . api . resources . services . FileServiceTest . fileService . repository ) . getFile ( userFolder ) ; doReturn ( sessionResource ) . when ( org . pentaho . platform . web . http . api . resources . services . FileServiceTest . fileService ) . getSessionResource ( ) ; doReturn ( children ) . when ( org . pentaho . platform . web . http . api . resources . services . FileServiceTest . fileService . repository ) . getChildren ( userFolder ) ; org . pentaho . platform . api . repository2 . unified . webservices . RepositoryFileDto mockedRepositoryFileDto = mock ( org . pentaho . platform . api . repository2 . unified . webservices . RepositoryFileDto . class ) ; doReturn ( mockedRepositoryFileDto ) . when ( org . pentaho . platform . web . http . api . resources . services . FileServiceTest . fileService ) . toFileDto ( mockedChild , null , false ) ; try { doReturn ( fileDetailsMock ) . when ( org . pentaho . platform . web . http . api . resources . services . FileServiceTest . fileService ) . doGetProperties ( pathId ) ; java . util . List < org . pentaho . platform . api . repository2 . unified . webservices . RepositoryFileDto > list = org . pentaho . platform . web . http . api . resources . services . FileServiceTest . fileService . searchGeneratedContent ( userFolder , lineageId , QuartzScheduler . RESERVEDMAPKEY_LINEAGE_ID ) ; org . junit . Assert . assertEquals ( list . size ( ) , 1 ) ; } }
public class aTest{ @Test public void testConverterFactory ( ) { org . springframework . ldap . odm . typeconversion . impl . ConverterManagerFactoryBean converterManagerFactory = new org . springframework . ldap . odm . typeconversion . impl . ConverterManagerFactoryBean ( ) ; java . util . Set < org . springframework . ldap . odm . typeconversion . impl . ConverterManagerFactoryBean . ConverterConfig > configList = new java . util . HashSet < org . springframework . ldap . odm . typeconversion . impl . ConverterManagerFactoryBean . ConverterConfig > ( ) ; for ( org . springframework . ldap . odm . test . TestManagerConverterFactory . ConverterConfigTestData config : org . springframework . ldap . odm . test . TestManagerConverterFactory . converterConfigTestData ) { org . springframework . ldap . odm . typeconversion . impl . ConverterManagerFactoryBean . ConverterConfig converterConfig = new org . springframework . ldap . odm . typeconversion . impl . ConverterManagerFactoryBean . ConverterConfig ( ) ; converterConfig . setFromClasses ( new java . util . HashSet < java . lang . Class < ? > > ( java . util . Arrays . asList ( config . fromClasses ) ) ) ; converterConfig . setSyntax ( config . syntax ) ; converterConfig . setToClasses ( new java . util . HashSet < java . lang . Class < ? > > ( java . util . Arrays . asList ( config . toClasses ) ) ) ; converterConfig . setConverter ( org . springframework . ldap . odm . test . TestManagerConverterFactory . nullConverter ) ; configList . add ( converterConfig ) ; } converterManagerFactory . setConverterConfig ( configList ) ; final org . springframework . ldap . odm . typeconversion . ConverterManager converterManager = ( ( org . springframework . ldap . odm . typeconversion . ConverterManager ) ( converterManagerFactory . getObject ( ) ) ) ; new org . springframework . ldap . odm . test . utils . ExecuteRunnable < org . springframework . ldap . odm . test . TestManagerConverterFactory . ConverterTestData > ( ) . runTests ( new org . springframework . ldap . odm . test . utils . RunnableTest < org . springframework . ldap . odm . test . TestManagerConverterFactory . ConverterTestData > ( ) { public void runTest ( org . springframework . ldap . odm . test . TestManagerConverterFactory . ConverterTestData testData ) { org . junit . Assert . assertEquals ( testData . canConvert , converterManager . canConvert ( testData . fromClass , testData . syntax , testData . toClass ) ) ; } } }
public class aTest{ @Test public void test01 ( ) { javax . sql . DataSource ds = DatasourceConfig . DATA_SOURCES . get ( "b" 6 ) ; com . alibaba . otter . canal . client . adapter . es . test . sync . Common . sqlExe ( ds , "delete<sp>from<sp>label<sp>where<sp>id=1<sp>or<sp>id=2" ) ; com . alibaba . otter . canal . client . adapter . es . test . sync . Common . sqlExe ( ds , "insert<sp>into<sp>label<sp>(id,user_id,label)<sp>values<sp>(1,1,'a')" ) ; com . alibaba . otter . canal . client . adapter . es . test . sync . Common . sqlExe ( ds , "b" 4 ) ; com . alibaba . otter . canal . client . adapter . support . Dml dml = new com . alibaba . otter . canal . client . adapter . support . Dml ( ) ; dml . setDestination ( "example" ) ; dml . setTs ( new java . util . Date ( ) . getTime ( ) ) ; dml . setType ( "b" 2 ) ; dml . setDatabase ( "b" 5 ) ; dml . setTable ( "label" ) ; java . util . List < java . util . Map < java . lang . String , java . lang . Object > > dataList = new java . util . ArrayList ( ) ; java . util . Map < java . lang . String , java . lang . Object > data = new java . util . LinkedHashMap ( ) ; dataList . add ( data ) ; data . put ( "id" , 2L ) ; data . put ( "user_id" , 1L ) ; data . put ( "label" , "b" ) ; dml . setData ( dataList ) ; java . lang . String database = dml . getDatabase ( ) ; java . lang . String table = dml . getTable ( ) ; java . util . Map < java . lang . String , com . alibaba . otter . canal . client . adapter . es . config . ESSyncConfig > esSyncConfigs = esAdapter . getDbTableEsSyncConfig ( ) . get ( ( ( database + "-" ) + table ) ) ; esAdapter . getEsSyncService ( ) . sync ( esSyncConfigs . values ( ) , dml ) ; org . elasticsearch . action . get . GetResponse response = esAdapter . getTransportClient ( ) . prepareGet ( "b" 1 , "_doc" , "1" ) . get ( ) ; org . junit . Assert . assertEquals ( "b" 0 , response . getSource ( ) . get ( "b" 3 ) ) ; } }
public class aTest{ @Test public void shouldIndexNodeAfterChange ( ) { registerValueIndex ( "ref1" , "nt:unstructured" , "" , null , "ref1" , PropertyType . STRING ) ; registerValueIndex ( "ref2" , "nt:unstructured" , "" , null , "ref2" , PropertyType . STRING ) ; waitForIndexes ( 500L ) ; javax . jcr . Node newNode1 = session . getRootNode ( ) . addNode ( "nodeWithSysName" , "nt:unstructured" ) ; session . save ( ) ; printMessage ( "Node<sp>Created<sp>..." ) ; final java . lang . String uuId1 = "cccccccccccccccccccccc-0000-1111-1234-123456789abcd" ; newNode1 . setProperty ( "ref1" , uuId1 ) ; newNode1 . setProperty ( "ref2" , uuId1 ) ; session . save ( ) ; printMessage ( "Node<sp>updated<sp>..." ) ; org . modeshape . jcr . api . query . Query query = jcrSql2Query ( "SELECT<sp>A.ref1<sp>FROM<sp>[nt:unstructured]<sp>AS<sp>A<sp>WHERE<sp>A.ref2<sp>=<sp>$ref2" ) ; query . bindValue ( "ref2" , session ( ) . getValueFactory ( ) . createValue ( uuId1 ) ) ; validateQuery ( ) . rowCount ( 1L ) . useIndex ( "ref2" ) . onEachRow ( new org . modeshape . jcr . ValidateQuery . Predicate ( ) { @ org . modeshape . jcr . Override public void validate ( int rowNumber , javax . jcr . query . Row row ) throws javax . jcr . RepositoryException { if ( rowNumber == 1 ) { org . junit . Assert . assertThat ( row . getValue ( "ref1" ) . getString ( ) , org . hamcrest . core . Is . is ( uuId1 ) ) ; } } } }
public class aTest{ @Test public void shouldReturnANotNullHashCode ( ) { org . apache . ibatis . session . SqlSession session = org . apache . ibatis . binding . BindingTest . sqlSessionFactory . openSession ( ) ; try { org . apache . ibatis . binding . BoundBlogMapper mapper = session . getMapper ( org . apache . ibatis . binding . BoundBlogMapper . class ) ; org . junit . Assert . assertNotNull ( mapper . hashCode ( ) ) ; } }
public class aTest{ @Test public void testOnManagedObjectKeysAndValues ( ) { com . sun . sgs . test . app . util . TestScalableHashMap . txnScheduler . runTask ( new com . sun . sgs . test . util . TestAbstractKernelRunnable ( ) { public void run ( ) throws com . sun . sgs . test . app . util . Exception { java . util . Map < com . sun . sgs . test . app . util . TestScalableHashMap . Bar , com . sun . sgs . test . app . util . TestScalableHashMap . Bar > test = new com . sun . sgs . app . util . ScalableHashMap < com . sun . sgs . test . app . util . TestScalableHashMap . Bar , com . sun . sgs . test . app . util . TestScalableHashMap . Bar > ( ) ; java . util . Map < com . sun . sgs . test . app . util . TestScalableHashMap . Bar , com . sun . sgs . test . app . util . TestScalableHashMap . Bar > control = new java . util . HashMap < com . sun . sgs . test . app . util . TestScalableHashMap . Bar , com . sun . sgs . test . app . util . TestScalableHashMap . Bar > ( ) ; for ( int i = 0 ; i < 64 ; i ++ ) { test . put ( new com . sun . sgs . test . app . util . TestScalableHashMap . Bar ( i ) , new com . sun . sgs . test . app . util . TestScalableHashMap . Bar ( i ) ) ; control . put ( new com . sun . sgs . test . app . util . TestScalableHashMap . Bar ( i ) , new com . sun . sgs . test . app . util . TestScalableHashMap . Bar ( i ) ) ; org . junit . Assert . assertEquals ( control , test ) ; } } } }
public class aTest{ @Test public void shouldNotAllowFactoryToChangeMoreThanOnce ( ) { org . jboss . netty . bootstrap . Bootstrap b = newBootstrap ( ) ; org . jboss . netty . channel . ChannelFactory f = createMock ( org . jboss . netty . channel . ChannelFactory . class ) ; b . setFactory ( f ) ; org . junit . Assert . assertSame ( f , b . getFactory ( ) ) ; try { b . setFactory ( createMock ( org . jboss . netty . channel . ChannelFactory . class ) ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void testServices ( ) { org . apache . usergrid . services . ServiceInvocationIT . logger . info ( "select<sp>*<sp>where<sp>name='axis*'" 9 ) ; app . put ( "select<sp>*<sp>where<sp>name='axis*'" 0 , "edanuff" ) ; app . put ( "foo" 4 , "foo" 5 ) ; org . apache . usergrid . services . Entity user = app . testRequest ( ServiceAction . POST , 1 , "foo" 8 ) . getEntity ( ) ; org . junit . Assert . assertNotNull ( user ) ; app . testRequest ( ServiceAction . GET , 1 , "foo" 8 ) ; app . testRequest ( ServiceAction . GET , 1 , "foo" 8 , user . getUuid ( ) ) ; app . testRequest ( ServiceAction . GET , 1 , "foo" 8 , org . apache . usergrid . services . Query . fromQL ( "select<sp>*<sp>where<sp>name='axis*'" 8 ) ) ; app . put ( "foo" , "bar" ) ; app . testRequest ( ServiceAction . PUT , 1 , "foo" 8 , user . getUuid ( ) ) ; app . put ( "foo" 0 , "select<sp>*<sp>where<sp>name='dylan'" 1 ) ; app . testRequest ( ServiceAction . POST , 1 , "select<sp>*<sp>where<sp>name='axis*'" 5 ) ; app . testRequest ( ServiceAction . GET , 0 , "foo" 8 , user . getUuid ( ) , "messages" ) ; app . testRequest ( ServiceAction . GET , 0 , "foo" 8 , org . apache . usergrid . services . Query . fromQL ( "select<sp>*<sp>where<sp>name='axis*'" 8 ) , "messages" ) ; org . apache . usergrid . services . Entity cat = app . doCreate ( "foo" 9 , "foo" 2 ) ; app . testRequest ( ServiceAction . GET , 2 , "select<sp>*<sp>where<sp>name='axis*'" 5 ) ; app . testRequest ( ServiceAction . GET , 1 , "select<sp>*<sp>where<sp>name='axis*'" 5 , org . apache . usergrid . services . Query . fromQL ( "select<sp>*<sp>where<sp>name='dylan'" ) ) ; app . testRequest ( ServiceAction . POST , 1 , null , "foo" 8 , "edanuff" , "select<sp>*<sp>where<sp>name='axis*'" 2 , cat . getUuid ( ) ) ; app . waitForQueueDrainAndRefreshIndex ( 250 ) ; org . apache . usergrid . services . Entity restaurant = app . doCreate ( "foo" 1 , "select<sp>*<sp>where<sp>name='axis*'" 3 ) ; app . createConnection ( user , "select<sp>*<sp>where<sp>name='axis*'" 2 , restaurant ) ; restaurant = app . doCreate ( "foo" 1 , "select<sp>*<sp>where<sp>name='dylan'" 0 ) ; app . testRequest ( ServiceAction . GET , 2 , "restaurants" ) ; app . testRequest ( ServiceAction . POST , 1 , "foo" 8 , user . getUuid ( ) , "foo" 7 , "select<sp>*<sp>where<sp>name='axis*'" 2 , restaurant . getUuid ( ) ) ; app . waitForQueueDrainAndRefreshIndex ( 250 ) ; app . testRequest ( ServiceAction . GET , 1 , "foo" 8 , "edanuff" , "select<sp>*<sp>where<sp>name='axis*'" 2 , "select<sp>*<sp>where<sp>name='axis*'" 5 ) ; app . testRequest ( ServiceAction . GET , 3 , "foo" 8 , "edanuff" , "select<sp>*<sp>where<sp>name='axis*'" 2 ) ; app . testRequest ( ServiceAction . GET , 2 , "foo" 8 , "edanuff" , "select<sp>*<sp>where<sp>name='axis*'" 2 , "restaurants" ) ; java . lang . Thread . sleep ( 10000 ) ; app . testRequest ( ServiceAction . GET , 1 , "foo" 8 , "edanuff" , "select<sp>*<sp>where<sp>name='axis*'" 2 , "restaurants" , org . apache . usergrid . services . Query . fromQL ( "select<sp>*<sp>where<sp>name='axis*'" 7 ) ) ; app . testRequest ( ServiceAction . GET , 1 , "foo" 8 , "edanuff" , "select<sp>*<sp>where<sp>name='axis*'" 2 , org . apache . usergrid . services . Query . fromQL ( "select<sp>*<sp>where<sp>name='axis*'" ) ) ; app . put ( "select<sp>*<sp>where<sp>name='axis*'" 4 , "blacknwhite" ) ; app . testRequest ( ServiceAction . PUT , 1 , "foo" 8 , "edanuff" , "select<sp>*<sp>where<sp>name='axis*'" 2 , cat . getUuid ( ) ) ; app . put ( "foo" 6 , "select<sp>*<sp>where<sp>name='axis*'" 6 ) ; app . testRequest ( ServiceAction . PUT , 1 , "foo" 8 , "edanuff" , "select<sp>*<sp>where<sp>name='axis*'" 2 , "select<sp>*<sp>where<sp>name='axis*'" 5 , "foo" 2 ) ; app . put ( "foo" 3 , "Coffee" ) ; app . testRequest ( ServiceAction . PUT , 1 , "foo" 8 , "edanuff" , "select<sp>*<sp>where<sp>name='axis*'" 2 , "restaurants" , org . apache . usergrid . services . Query . fromQL ( "select<sp>*<sp>where<sp>name='axis*'" 7 ) ) ; app . testRequest ( ServiceAction . DELETE , 1 , null , "foo" 8 , user . getUuid ( ) , "foo" 7 , "select<sp>*<sp>where<sp>name='axis*'" 2 , restaurant . getUuid ( ) ) ; app . testRequest ( ServiceAction . GET , 1 , null , "foo" 8 , "edanuff" , "select<sp>*<sp>where<sp>name='axis*'" 2 , "restaurants" ) ; java . util . UUID uuid = org . apache . usergrid . persistence . model . util . UUIDGenerator . newTimeUUID ( ) ; app . put ( "select<sp>*<sp>where<sp>name='axis*'" 1 , 5 ) ; app . testRequest ( ServiceAction . PUT , 1 , "devices" , uuid ) ; } }
public class aTest{ @Test public void testAnnotationOnPackage ( ) { spoon . Launcher launcher = new spoon . Launcher ( ) ; spoon . reflect . factory . Factory factory = launcher . getFactory ( ) ; factory . getEnvironment ( ) . setAutoImports ( false ) ; spoon . SpoonModelBuilder compiler = launcher . createCompiler ( factory ) ; launcher . setSourceOutputDirectory ( "./target/spooned/" ) ; compiler . addInputSource ( new java . io . File ( "./src/test/java/spoon/test/pkg/testclasses/" ) ) ; compiler . build ( ) ; compiler . generateProcessedSourceFiles ( spoon . OutputType . CLASSES ) ; final spoon . SpoonModelBuilder newCompiler = launcher . createCompiler ( launcher . createFactory ( ) ) ; newCompiler . addInputSource ( new java . io . File ( "./target/spooned/spoon/test/pkg/testclasses/" ) ) ; try { org . junit . Assert . assertTrue ( newCompiler . build ( ) ) ; } }
public class aTest{ @Test public void testSyntaxErrorMissingParam ( ) { try { v8 . executeScript ( "foo());" ) ; } catch ( com . eclipsesource . v8 . V8ScriptCompilationException e ) { java . lang . String string = e . toString ( ) ; org . junit . Assert . assertNotNull ( string ) ; return ; } }
public class aTest{ @Test public void cannotGetApp2 ( ) { final org . openqa . grid . internal . GridRegistry registry = org . openqa . grid . internal . DefaultGridRegistry . newInstance ( new org . openqa . grid . web . Hub ( new org . openqa . grid . internal . utils . configuration . GridHubConfiguration ( ) ) ) ; org . openqa . grid . internal . RemoteProxy p1 = new org . openqa . grid . internal . BaseRemoteProxy ( req , registry ) ; try { registry . add ( p1 ) ; org . openqa . grid . internal . TestThreadCounter testThreadCounter = new org . openqa . grid . internal . TestThreadCounter ( ) ; for ( int i = 0 ; i < 5 ; i ++ ) { testThreadCounter . start ( ( ) -> { org . openqa . grid . web . servlet . handler . RequestHandler newSessionRequest = org . openqa . grid . internal . mock . GridHelper . createNewSessionHandler ( registry , app1 ) ; newSessionRequest . process ( ) ; } ) ; } testThreadCounter . waitUntilDone ( 5 ) ; testThreadCounter . start ( ( ) -> { org . openqa . grid . web . servlet . handler . RequestHandler newSessionRequest = org . openqa . grid . internal . mock . GridHelper . createNewSessionHandler ( registry , app2 ) ; newSessionRequest . process ( ) ; app6Done = true ; } ) ; testThreadCounter . waitUntilStarted ( 6 ) ; org . junit . Assert . assertFalse ( app6Done ) ; } }
public class aTest{ @Test public void testClientSendWithListenerThrowingRetryableException ( ) { java . lang . String name = "clientname" ; com . sun . sgs . test . impl . service . session . TestClientSessionServiceImplv4 . DummyClient client = createDummyClient ( name ) ; try { client . connect ( serverNode . getAppPort ( ) ) ; org . junit . Assert . assertTrue ( client . login ( ) ) ; com . sun . sgs . test . impl . service . session . TestClientSessionServiceImplv4 . receivedMessageException = new com . sun . sgs . test . impl . service . session . TestClientSessionServiceImplv4 . MaybeRetryException ( "retryable" , true ) ; client . sendMessagesFromClientInSequence ( 5 , 5 ) ; } }
public class aTest{ @Test public void listActions ( ) { org . opennaas . itests . core . queue . QueuemanagerTest . log . info ( "INFO:<sp>List<sp>actions" ) ; org . opennaas . core . resources . action . IAction action = new org . opennaas . core . resources . mock . MockAction ( ) ; action . setActionID ( "mockAction" ) ; queueManagerCapability . queueAction ( action ) ; org . junit . Assert . assertTrue ( ( ( queueManagerCapability . getActions ( ) . size ( ) ) == 1 ) ) ; for ( org . opennaas . core . resources . action . IAction act : queueManagerCapability . getActions ( ) ) { org . opennaas . itests . core . queue . QueuemanagerTest . log . info ( ( "INFO:<sp>action<sp>id=" + ( act . getActionID ( ) ) ) ) ; } }
public class aTest{ @Test public void testGzipCompressLong ( ) { java . io . RandomAccessFile file = null ; try { file = new java . io . RandomAccessFile ( "src/test/resources/nom/tam/image/comp/bare/test100Data32.bin" , "r" ) ; byte [ ] bytes = new byte [ ( ( int ) ( file . length ( ) ) ) ] ; file . read ( bytes ) ; java . nio . IntBuffer intArray = java . nio . ByteBuffer . wrap ( bytes ) . asIntBuffer ( ) ; long [ ] longArray = new long [ ( bytes . length ) / 4 ] ; int [ ] tempInts = new int [ longArray . length ] ; intArray . get ( tempInts ) ; nom . tam . util . ArrayFuncs . copyInto ( tempInts , longArray ) ; java . nio . LongBuffer byteArray = java . nio . LongBuffer . wrap ( longArray ) ; java . nio . ByteBuffer compressed = java . nio . ByteBuffer . wrap ( new byte [ bytes . length ] ) ; new nom . tam . fits . compression . algorithm . gzip . GZipCompressor . LongGZipCompressor ( ) . compress ( byteArray , compressed ) ; compressed . rewind ( ) ; java . nio . LongBuffer decompressedArray = java . nio . LongBuffer . wrap ( new long [ longArray . length ] ) ; new nom . tam . fits . compression . algorithm . gzip . GZipCompressor . LongGZipCompressor ( ) . decompress ( compressed , decompressedArray ) ; org . junit . Assert . assertArrayEquals ( longArray , decompressedArray . array ( ) ) ; } }
public class aTest{ @Test public void testByteBufferIOEngine ( ) { int capacity = ( 32 * 1024 ) * 1024 ; int testNum = 100 ; int maxBlockSize = 64 * 1024 ; org . apache . hadoop . hbase . io . hfile . bucket . ByteBufferIOEngine ioEngine = new org . apache . hadoop . hbase . io . hfile . bucket . ByteBufferIOEngine ( capacity ) ; int testOffsetAtStartNum = testNum / 10 ; int testOffsetAtEndNum = testNum / 10 ; for ( int i = 0 ; i < testNum ; i ++ ) { byte val = ( ( byte ) ( ( java . lang . Math . random ( ) ) * 255 ) ) ; int blockSize = ( ( int ) ( ( java . lang . Math . random ( ) ) * maxBlockSize ) ) ; if ( blockSize == 0 ) { blockSize = 1 ; } byte [ ] byteArray = new byte [ blockSize ] ; for ( int j = 0 ; j < ( byteArray . length ) ; ++ j ) { byteArray [ j ] = val ; } java . nio . ByteBuffer srcBuffer = java . nio . ByteBuffer . wrap ( byteArray ) ; int offset = 0 ; if ( testOffsetAtStartNum > 0 ) { testOffsetAtStartNum -- ; offset = 0 ; } else if ( testOffsetAtEndNum > 0 ) { testOffsetAtEndNum -- ; offset = capacity - blockSize ; } else { offset = ( ( int ) ( ( java . lang . Math . random ( ) ) * ( capacity - maxBlockSize ) ) ) ; } ioEngine . write ( srcBuffer , offset ) ; org . apache . hadoop . hbase . io . hfile . bucket . TestByteBufferIOEngine . BufferGrabbingDeserializer deserializer = new org . apache . hadoop . hbase . io . hfile . bucket . TestByteBufferIOEngine . BufferGrabbingDeserializer ( ) ; ioEngine . read ( offset , blockSize , deserializer ) ; org . apache . hadoop . hbase . nio . ByteBuff dstBuffer = deserializer . buf ; for ( int j = 0 ; j < ( byteArray . length ) ; ++ j ) { org . junit . Assert . assertTrue ( ( ( byteArray [ j ] ) == ( dstBuffer . get ( j ) ) ) ) ; } } }
public class aTest{ @Test public void testFindAllSuperTypesWithFocusType ( ) { org . springframework . ide . vscode . commons . maven . java . MavenJavaProject project = org . springframework . ide . vscode . commons . maven . JavaIndexTest . mavenProjectsCache . get ( "gs-rest-service-cors-boot-1.4.1-with-classpath-file" ) ; java . util . Set < java . lang . String > actual = project . getIndex ( ) . allSuperTypesOf ( "java.util.ArrayList" , true ) . map ( ( t ) -> t . getFullyQualifiedName ( ) ) . collect ( java . util . stream . Collectors . toSet ( ) ) . block ( ) ; java . util . Set < java . lang . String > expected = new java . util . HashSet ( java . util . Arrays . asList ( "java.util.ArrayList" , "java.util.List" , "java.util.RandomAccess" , "java.lang.Cloneable" , "java.util.AbstractList" 0 , "java.util.AbstractList" , "java.util.Collection" , "java.lang.Object" , "java.util.AbstractCollection" , "java.lang.Iterable" ) ) ; org . junit . Assert . assertEquals ( expected , actual ) ; } }
public class aTest{ @Test public void registerOnceAndRemoveManager ( ) { com . aliyuncs . http . clients . ApacheIdleConnectionCleaner . setPeriodSec ( 1 ) ; org . apache . http . conn . HttpClientConnectionManager manager = mock ( org . apache . http . conn . HttpClientConnectionManager . class ) ; doAnswer ( new org . mockito . stubbing . Answer ( ) { @ com . aliyuncs . http . clients . Override public java . lang . Object answer ( org . mockito . invocation . InvocationOnMock invocationOnMock ) { long idleTime = invocationOnMock . getArgument ( 0 , long . class ) ; org . junit . Assert . assertEquals ( idleTime , idleTime , ( 30 * 1000L ) ) ; return null ; } } }
public class aTest{ @Test public void testDeletePreparedThrow ( ) { com . j256 . ormlite . dao . Dao < com . j256 . ormlite . dao . Foo , java . lang . Integer > dao = createDao ( com . j256 . ormlite . dao . Foo . class , true ) ; com . j256 . ormlite . dao . Foo foo = new com . j256 . ormlite . dao . Foo ( ) ; org . junit . Assert . assertEquals ( 1 , dao . create ( foo ) ) ; com . j256 . ormlite . support . DatabaseConnection conn = connectionSource . getReadWriteConnection ( com . j256 . ormlite . dao . FOO_TABLE_NAME ) ; try { conn . close ( ) ; dao . delete ( dao . deleteBuilder ( ) . prepare ( ) ) ; } }
public class aTest{ @Test public void testSingle ( ) { java . time . ZoneId zone = java . time . ZoneId . of ( "GMT" ) ; com . questdb . std . microtime . TimeZoneRulesImpl rules = new com . questdb . std . microtime . TimeZoneRulesImpl ( "GMT" , zone . getRules ( ) ) ; int y = 2017 ; int m = 3 ; int d = 29 ; java . time . LocalDateTime dt = java . time . LocalDateTime . of ( y , m , d , 0 , 0 ) ; long millis = com . questdb . std . microtime . Dates . toMicros ( y , m , d , 0 , 0 ) ; java . time . ZonedDateTime zdt = dt . atZone ( zone ) ; long expected = zdt . getOffset ( ) . getTotalSeconds ( ) ; long changed = ( com . questdb . std . microtime . Dates . toMicros ( zdt . getYear ( ) , zdt . getMonthValue ( ) , zdt . getDayOfMonth ( ) , zdt . getHour ( ) , zdt . getMinute ( ) ) ) + ( ( zdt . getSecond ( ) ) * 1000 ) ; expected += ( changed - millis ) / 1000 ; long offset = rules . getOffset ( millis , y , com . questdb . std . microtime . Dates . isLeapYear ( y ) ) ; try { org . junit . Assert . assertEquals ( expected , ( offset / 1000 ) ) ; } }
public class aTest{ @Test public void testInsertSelectFromSubqueryWithOrderBy ( ) { java . lang . String sqlText = "INSERT<sp>INTO<sp>TT1\n" + ( ( ( ( ( ( ( ")<sp>b3<sp>ON<sp>b3.col1<sp>=<sp>aa.c1<sp>AND<sp>b3.col2<sp>=<sp>aa.c2<sp>order<sp>by<sp>aa.c1,<sp>aa.c2" 3 + "aa.c1\n" ) + "FROM<sp>aa<sp>\n" ) + ")<sp>b3<sp>ON<sp>b3.col1<sp>=<sp>aa.c1<sp>AND<sp>b3.col2<sp>=<sp>aa.c2<sp>order<sp>by<sp>aa.c1,<sp>aa.c2" 2 ) + "(SELECT<sp>b1.c1<sp>AS<sp>col1,<sp>b1.c2<sp>AS<sp>col2,b1.c3<sp>AS<sp>col3\n" ) + ")<sp>b3<sp>ON<sp>b3.col1<sp>=<sp>aa.c1<sp>AND<sp>b3.col2<sp>=<sp>aa.c2<sp>order<sp>by<sp>aa.c1,<sp>aa.c2" 1 ) + ")b2<sp>LEFT<sp>OUTER<sp>JOIN<sp>bb<sp>b3<sp>ON<sp>b2.col3=b3.c4\n" ) + ")<sp>b3<sp>ON<sp>b3.col1<sp>=<sp>aa.c1<sp>AND<sp>b3.col2<sp>=<sp>aa.c2<sp>order<sp>by<sp>aa.c1,<sp>aa.c2" ) ; methodWatcher . executeUpdate ( sqlText ) ; java . lang . String sql = "select<sp>*<sp>from<sp>TT1" ; java . lang . String expected = ")<sp>b3<sp>ON<sp>b3.col1<sp>=<sp>aa.c1<sp>AND<sp>b3.col2<sp>=<sp>aa.c2<sp>order<sp>by<sp>aa.c1,<sp>aa.c2" 0 + ( "----\n" + "<sp>1<sp>|" ) ; com . splicemachine . derby . impl . sql . execute . operations . ResultSet rs = methodWatcher . executeQuery ( sql ) ; org . junit . Assert . assertEquals ( ( ( "\n" + sql ) + "\n" ) , expected , TestUtils . FormattedResult . ResultFactory . toString ( rs ) ) ; rs . close ( ) ; } }
public class aTest{ @Test public void testGetAverageLengthWithNullBlankRows ( ) { try { org . talend . dq . dbms . DbmsLanguage dbms = getMysqlDbmsLanguage ( ) ; org . junit . Assert . assertNotNull ( dbms . getAverageLengthWithNullBlankRows ( ) ) ; } }
public class aTest{ @Test public void testFix ( ) { final java . lang . String INDEX_LESS_MAP_FILE = "testFix.mapfile" ; int PAIR_SIZE = 20 ; org . apache . hadoop . io . MapFile . Writer writer = null ; try { org . apache . hadoop . fs . FileSystem fs = org . apache . hadoop . fs . FileSystem . getLocal ( org . apache . hadoop . io . TestMapFile . conf ) ; org . apache . hadoop . fs . Path dir = new org . apache . hadoop . fs . Path ( org . apache . hadoop . io . TestMapFile . TEST_DIR , INDEX_LESS_MAP_FILE ) ; writer = createWriter ( INDEX_LESS_MAP_FILE , org . apache . hadoop . io . IntWritable . class , org . apache . hadoop . io . Text . class ) ; for ( int i = 0 ; i < PAIR_SIZE ; i ++ ) writer . append ( new org . apache . hadoop . io . IntWritable ( 0 ) , new org . apache . hadoop . io . Text ( "value" ) ) ; writer . close ( ) ; java . io . File indexFile = new java . io . File ( "." , ( ( "." + INDEX_LESS_MAP_FILE ) + "/index" ) ) ; boolean isDeleted = false ; if ( indexFile . exists ( ) ) isDeleted = indexFile . delete ( ) ; if ( isDeleted ) org . junit . Assert . assertTrue ( "testFix<sp>error<sp>!!!" , ( ( org . apache . hadoop . io . MapFile . fix ( fs , dir , org . apache . hadoop . io . IntWritable . class , org . apache . hadoop . io . Text . class , true , org . apache . hadoop . io . TestMapFile . conf ) ) == PAIR_SIZE ) ) ; } }
public class aTest{ @Test public void testEncodeCKANxffff ( ) { System . out . println ( ( ( com . telefonica . iot . cygnus . utils . CommonUtilsForTests . getTestTraceHead ( "[NGSICharsets.encodeCKAN]" ) ) + "<sp>is<sp>encoded<sp>(escaped)<sp>as<sp>\"0xffff\"<sp>is<sp>encoded<sp>(escaped)<sp>as<sp>\"xxffff\"" ) ) ; java . lang . String in = "xffff" ; java . lang . String expected = "xxffff" ; java . lang . String out = com . telefonica . iot . cygnus . utils . NGSICharsets . encodeCKAN ( in ) ; try { org . junit . Assert . assertEquals ( expected , out ) ; System . out . println ( ( ( ( ( ( ( com . telefonica . iot . cygnus . utils . CommonUtilsForTests . getTestTraceHead ( "[NGSICharsets.encodeCKAN]" ) ) + "-<sp>OK<sp>-<sp>'" ) + in ) + "'<sp>has<sp>been<sp>encoded<sp>as<sp>'" ) + expected ) + "'" ) ) ; } }
public class aTest{ @Test public void testGetSequentialResultSetBeforeExecute ( ) { boolean hasExpectedException = false ; try { org . eclipse . birt . data . engine . odaconsumer . PreparedStatement hostStmt = getSequentialRSPreparedStatement ( ) ; org . eclipse . birt . data . engine . odaconsumer . ResultSet rs1 = hostStmt . getResultSet ( 2 ) ; org . junit . Assert . assertNull ( rs1 ) ; hasExpectedException = true ; } }
public class aTest{ @Test public void testCreateUserForFederatedOrg ( ) { net . maritimecloud . identityregistry . model . database . entities . User user = new net . maritimecloud . identityregistry . model . database . entities . User ( ) ; user . setMrn ( "urn:mrn:mcl:user" 0 ) ; user . setFirstName ( "Thomas" ) ; user . setLastName ( "Christensen" ) ; user . setEmail ( "urn:mrn:mcl:user" 1 ) ; user . setIdOrganization ( 1L ) ; user . setPermissions ( "MCADMIN" ) ; java . lang . String userJson = serialize ( user ) ; net . maritimecloud . identityregistry . model . database . Organization org = spy ( net . maritimecloud . identityregistry . model . database . Organization . class ) ; org . setMrn ( "urn:mrn:mcl:user" 7 ) ; org . setAddress ( "urn:mrn:mcl:user" 2 ) ; org . setCountry ( "urn:mrn:mcl:user" 6 ) ; org . setUrl ( "http://dma.dk" ) ; org . setEmail ( "dma@dma.dk" ) ; org . setName ( "urn:mrn:mcl:user" 3 ) ; org . setFederationType ( "external-idp" ) ; java . util . Set < net . maritimecloud . identityregistry . model . database . IdentityProviderAttribute > identityProviderAttributes = new java . util . HashSet ( ) ; org . setIdentityProviderAttributes ( identityProviderAttributes ) ; org . keycloak . adapters . springsecurity . token . KeycloakAuthenticationToken auth = net . maritimecloud . identityregistry . controllers . TokenGenerator . generateKeycloakToken ( "urn:mrn:mcl:user" 7 , "ROLE_USER_ADMIN" , "" ) ; given ( this . organizationService . getOrganizationByMrn ( "urn:mrn:mcl:user" 7 ) ) . willReturn ( org ) ; when ( org . getId ( ) ) . thenReturn ( 1L ) ; net . maritimecloud . identityregistry . model . database . entities . User newUser = new net . maritimecloud . identityregistry . model . database . entities . User ( ) ; newUser . setMrn ( "urn:mrn:mcl:user" ) ; try { mvc . perform ( post ( "/oidc/api/org/urn:mrn:mcl:org:dma/user" ) . with ( authentication ( auth ) ) . header ( "urn:mrn:mcl:user" 4 , "urn:mrn:mcl:user" 5 ) . content ( userJson ) . contentType ( "urn:mrn:mcl:user" 8 ) ) . andExpect ( status ( ) . is4xxClientError ( ) ) ; } catch ( java . lang . Exception e ) { e . printStackTrace ( ) ; org . junit . Assert . assertTrue ( false ) ; } } }
public class aTest{ @Test public void test_categories_stats_get ( ) { com . sendgrid . SendGrid sg = new com . sendgrid . SendGrid ( "SENDGRID_API_KEY" , true ) ; sg . setHost ( "localhost:4010" ) ; sg . addRequestHeader ( "SENDGRID_API_KEY" 0 , "200" ) ; com . sendgrid . Request request = new com . sendgrid . Request ( ) ; request . setMethod ( Method . GET ) ; request . setEndpoint ( "SENDGRID_API_KEY" 5 ) ; request . addQueryParam ( "end_date" , "SENDGRID_API_KEY" 1 ) ; request . addQueryParam ( "aggregated_by" , "SENDGRID_API_KEY" 2 ) ; request . addQueryParam ( "limit" , "1" ) ; request . addQueryParam ( "SENDGRID_API_KEY" 3 , "1" ) ; request . addQueryParam ( "SENDGRID_API_KEY" 4 , "2016-01-01" ) ; request . addQueryParam ( "categories" , "test_string" ) ; com . sendgrid . Response response = sg . api ( request ) ; org . junit . Assert . assertEquals ( 200 , response . getStatusCode ( ) ) ; } }
public class aTest{ @Test public void testCompileStatic_1505 ( ) { org . codehaus . jdt . groovy . internal . compiler . ast . JDTResolver . recordInstances = true ; org . eclipse . core . runtime . IPath [ ] paths = createSimpleProject ( "<sp>ls.add('abc');\n" 7 , true ) ; "<sp>List<String><sp>second<sp>=<sp>[]\n" ) + "<sp>List<String><sp>artefactResources2\n" ) + "<sp>second.addAll(artefactResources2)\n" ) + "<sp>ls.add('abc');\n" 3 ) ) ) ; incrementalBuild ( paths [ 0 ] ) ; expectingCompiledClasses ( "<sp>ls.add('abc');\n" 6 , "<sp>ls.add('abc');\n" 1 ) ; expectingNoProblems ( ) ; org . codehaus . jdt . groovy . internal . compiler . ast . JDTClassNode jcn = org . codehaus . jdt . groovy . internal . compiler . ast . JDTResolver . getCachedNode ( "Client" 3 ) ; org . junit . Assert . assertNotNull ( jcn ) ; System . out . println ( ( "Client" 5 + jcn ) ) ; org . codehaus . groovy . ast . ClassNode listcn = new org . codehaus . groovy . ast . ClassNode ( java . util . Collection . class ) ; org . codehaus . groovy . vmplugin . VMPluginFactory . getPlugin ( ) . setAdditionalClassInformation ( listcn ) ; listcn . lazyClassInit ( ) ; System . out . println ( ( "Client" 4 + listcn ) ) ; org . eclipse . jdt . core . groovy . tests . builder . BasicGroovyBuildTests . compareClassNodes ( jcn . redirect ( ) , listcn . redirect ( ) , 0 ) ; org . codehaus . groovy . ast . MethodNode jmn = org . eclipse . jdt . core . groovy . tests . builder . BasicGroovyBuildTests . getMethodNode ( jcn , "<sp>ls.add('abc');\n" 5 , 1 ) ; org . codehaus . groovy . ast . MethodNode rmn = org . eclipse . jdt . core . groovy . tests . builder . BasicGroovyBuildTests . getMethodNode ( listcn , "<sp>ls.add('abc');\n" 5 , 1 ) ; org . eclipse . jdt . core . groovy . tests . builder . BasicGroovyBuildTests . compareMethodNodes ( jmn , rmn ) ; jmn = org . eclipse . jdt . core . groovy . tests . builder . BasicGroovyBuildTests . getMethodNode ( jcn , "addAll" , 1 ) ; rmn = org . eclipse . jdt . core . groovy . tests . builder . BasicGroovyBuildTests . getMethodNode ( listcn , "addAll" , 1 ) ; org . eclipse . jdt . core . groovy . tests . builder . BasicGroovyBuildTests . compareMethodNodes ( jmn , rmn ) ; } }
public class aTest{ @Test public void testModifyParameter ( ) { try { com . fit2cloud . aliyun . rds . model . request . ModifyParameterRequest request = new com . fit2cloud . aliyun . rds . model . request . ModifyParameterRequest ( ) ; request . setDBInstanceId ( dBInstanceId ) ; request . setForcerestart ( false ) ; java . util . Map < java . lang . String , java . lang . String > map = new java . util . HashMap < java . lang . String , java . lang . String > ( ) ; map . put ( "back_log" , "20" ) ; java . lang . String parameters = new com . google . gson . Gson ( ) . toJson ( map ) ; request . setParameters ( parameters ) ; com . fit2cloud . aliyun . Response response = client . modifyParameter ( request ) ; System . out . println ( ( "testModifyParameter<sp>::<sp>" + ( new com . google . gson . Gson ( ) . toJson ( response ) ) ) ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testHistoryServerIntegration ( ) { final int numJobs = 2 ; for ( int x = 0 ; x < numJobs ; x ++ ) { org . apache . flink . runtime . webmonitor . history . HistoryServerTest . runJob ( ) ; } org . apache . flink . runtime . webmonitor . history . HistoryServerTest . createLegacyArchive ( jmDirectory . toPath ( ) ) ; java . util . concurrent . CountDownLatch numFinishedPolls = new java . util . concurrent . CountDownLatch ( 1 ) ; org . apache . flink . configuration . Configuration historyServerConfig = new org . apache . flink . configuration . Configuration ( ) ; historyServerConfig . setString ( HistoryServerOptions . HISTORY_SERVER_ARCHIVE_DIRS , jmDirectory . toURI ( ) . toString ( ) ) ; historyServerConfig . setString ( HistoryServerOptions . HISTORY_SERVER_WEB_DIR , hsDirectory . getAbsolutePath ( ) ) ; historyServerConfig . setInteger ( HistoryServerOptions . HISTORY_SERVER_WEB_PORT , 0 ) ; java . io . File [ ] archives = jmDirectory . listFiles ( ) ; while ( ( archives == null ) || ( ( archives . length ) != ( numJobs + 1 ) ) ) { java . lang . Thread . sleep ( 50 ) ; archives = jmDirectory . listFiles ( ) ; } org . apache . flink . runtime . webmonitor . history . HistoryServer hs = new org . apache . flink . runtime . webmonitor . history . HistoryServer ( historyServerConfig , numFinishedPolls ) ; try { hs . start ( ) ; java . lang . String baseUrl = "http://localhost:" + ( hs . getWebPort ( ) ) ; numFinishedPolls . await ( 10L , TimeUnit . SECONDS ) ; org . apache . flink . shaded . jackson2 . com . fasterxml . jackson . databind . ObjectMapper mapper = new org . apache . flink . shaded . jackson2 . com . fasterxml . jackson . databind . ObjectMapper ( ) ; java . lang . String response = org . apache . flink . runtime . webmonitor . history . HistoryServerTest . getFromHTTP ( ( baseUrl + ( org . apache . flink . runtime . rest . messages . JobsOverviewHeaders . URL ) ) ) ; org . apache . flink . runtime . messages . webmonitor . MultipleJobsDetails overview = mapper . readValue ( response , org . apache . flink . runtime . messages . webmonitor . MultipleJobsDetails . class ) ; org . junit . Assert . assertEquals ( ( numJobs + 1 ) , overview . getJobs ( ) . size ( ) ) ; } }
public class aTest{ @Test public void eventSavedAndLoaded ( ) { final com . orientechnologies . orient . core . db . document . ODatabaseDocumentTx db = initDatabase ( ) ; createLogEvent ( db ) ; db . close ( ) ; java . lang . Thread . sleep ( 1000 ) ; final com . orientechnologies . orient . core . db . document . ODatabaseDocumentTx db2 = new com . orientechnologies . orient . core . db . document . ODatabaseDocumentTx ( "memory:scheduler" ) ; db2 . open ( "admin" , "admin" ) ; try { java . lang . Thread . sleep ( 2000 ) ; java . lang . Long count = getLogCounter ( db2 ) ; org . junit . Assert . assertTrue ( ( count >= 2 ) ) ; } }
public class aTest{ @Test public void testValidateListBadAction ( ) { java . util . List < tigase . xml . Element > items = new java . util . ArrayList < tigase . xml . Element > ( ) ; tigase . xmpp . impl . Authorization result = null ; items . add ( new tigase . xml . Element ( "item" , new java . lang . String [ ] { "type" , "value" , "action" , "order" } , new java . lang . String [ ] { "subscription" , "both" , "ignore" , "10" } ) ) ; items . add ( new tigase . xml . Element ( "item" , new java . lang . String [ ] { "action" , "order" } , new java . lang . String [ ] { "deny" , "both" 0 } ) ) ; result = tigase . xmpp . impl . JabberIqPrivacy . validateList ( null , items ) ; org . junit . Assert . assertEquals ( Authorization . BAD_REQUEST , result ) ; } }
public class aTest{ @Test public void testDeleteAttribute ( ) { try { qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestNominalAttributeResourceFacadeImp . entityManager . getTransaction ( ) . begin ( ) ; boolean result = qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestNominalAttributeResourceFacadeImp . nominalAttributeResourceFacadeImp . deleteAttribute ( qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestNominalAttributeResourceFacadeImp . nominalAttribute . getNominalAttributeId ( ) ) ; qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestNominalAttributeResourceFacadeImp . entityManager . getTransaction ( ) . commit ( ) ; org . junit . Assert . assertEquals ( true , result ) ; qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestNominalAttributeResourceFacadeImp . nominalAttribute = null ; } }
public class aTest{ @Test public void testCorruptedMetadataFile ( ) { final java . lang . String corruptedMetadata = "corrupted_metadata" ; try { test ( "use<sp>dfs.tmp" ) ; test ( "create<sp>table<sp>`%s`<sp>as<sp>select<sp>*<sp>from<sp>cp.`tpch/nation.parquet`" , corruptedMetadata ) ; dirTestWatcher . copyResourceToTestTmp ( java . nio . file . Paths . get ( "An<sp>incorrect<sp>result<sp>was<sp>obtained<sp>while<sp>querying<sp>a<sp>table<sp>with<sp>metadata<sp>cache<sp>files" 1 , "unsupported_metadata" , "corrupted_metadata.requires_replace.txt" ) , java . nio . file . Paths . get ( corruptedMetadata , Metadata . OLD_METADATA_FILENAME ) ) ; java . lang . String query = java . lang . String . format ( "An<sp>incorrect<sp>result<sp>was<sp>obtained<sp>while<sp>querying<sp>a<sp>table<sp>with<sp>metadata<sp>cache<sp>files" 0 , corruptedMetadata ) ; int expectedRowCount = 25 ; int expectedNumFiles = 1 ; int actualRowCount = testSql ( query ) ; org . junit . Assert . assertEquals ( "An<sp>incorrect<sp>result<sp>was<sp>obtained<sp>while<sp>querying<sp>a<sp>table<sp>with<sp>metadata<sp>cache<sp>files" , expectedRowCount , actualRowCount ) ; java . lang . String numFilesPattern = "numFiles=" + expectedNumFiles ; java . lang . String usedMetaPattern = "usedMetadataFile=false" ; org . apache . drill . PlanTestBase . testPlanMatchingPatterns ( query , new java . lang . String [ ] { numFilesPattern , usedMetaPattern } , new java . lang . String [ ] { "Filter" } ) ; } }
public class aTest{ @Test public void GetSight ( ) { com . smartsheet . api . Smartsheet ss = com . smartsheet . api . sdk_test . HelperFunctions . SetupClient ( "Get<sp>Sight" ) ; try { com . smartsheet . api . sdk_test . Sight sight = ss . sightResources ( ) . getSight ( 52 ) ; org . junit . Assert . assertEquals ( 52 , ( ( long ) ( sight . getId ( ) ) ) ) ; } }
public class aTest{ @Test public void testCreateZKAccessControl ( ) { org . apache . distributedlog . thrift . AccessControlEntry ace = new org . apache . distributedlog . thrift . AccessControlEntry ( ) ; ace . setDenyWrite ( true ) ; java . lang . String zkPath = "/create-zk-access-control" ; org . apache . distributedlog . impl . acl . ZKAccessControl zkac = new org . apache . distributedlog . impl . acl . ZKAccessControl ( ace , zkPath ) ; org . apache . distributedlog . util . Utils . ioResult ( zkac . create ( zkc ) ) ; org . apache . distributedlog . impl . acl . ZKAccessControl readZKAC = org . apache . distributedlog . util . Utils . ioResult ( org . apache . distributedlog . impl . acl . ZKAccessControl . read ( zkc , zkPath , null ) ) ; org . junit . Assert . assertEquals ( zkac , readZKAC ) ; org . apache . distributedlog . impl . acl . ZKAccessControl another = new org . apache . distributedlog . impl . acl . ZKAccessControl ( ace , zkPath ) ; try { org . apache . distributedlog . common . concurrent . FutureUtils . result ( another . create ( zkc ) ) ; } }
public class aTest{ @Test public void testTimeStampNotPresent ( ) { com . couchbase . jdbc . JDBCTestUtils . setConnection ( null ) ; java . lang . String drop_primary_index = "drop<sp>primary<sp>index<sp>on<sp>default" ; com . couchbase . jdbc . JDBCTestUtils . createPrimaryIndexes ( TestUtil . clusterInfo . bucketInformation . keySet ( ) ) ; org . json . simple . JSONObject obj = new org . json . simple . JSONObject ( ) ; java . lang . String deleteData = "delete<sp>from<sp>default" ; com . couchbase . jdbc . JDBCTestUtils . runQueryWithoutResult ( deleteData ) ; java . util . HashMap < java . lang . String , java . lang . Object > map = new java . util . HashMap < java . lang . String , java . lang . Object > ( ) ; map . put ( "name" , "NAME" ) ; obj . putAll ( map ) ; org . json . simple . JSONArray expectedArray = new org . json . simple . JSONArray ( ) ; java . util . HashMap < java . lang . String , org . json . simple . JSONObject > objMap = new java . util . HashMap < java . lang . String , org . json . simple . JSONObject > ( ) ; objMap . put ( "1" , obj ) ; expectedArray . add ( obj ) ; com . couchbase . jdbc . JDBCTestUtils . insertData ( objMap , "default" ) ; java . lang . Thread . sleep ( 1000 ) ; java . lang . String query = "select<sp>name<sp>from<sp>default" ; com . couchbase . jdbc . JDBCTestUtils . setConnection ( null ) ; try ( java . sql . Statement stmt = JDBCTestUtils . con . createStatement ( ) ) { try ( java . sql . ResultSet rs = stmt . executeQuery ( query ) ) { com . couchbase . jdbc . CBResultSet cbrs = ( ( com . couchbase . jdbc . CBResultSet ) ( rs ) ) ; while ( cbrs . next ( ) ) { java . sql . ResultSetMetaData meta = cbrs . getMetaData ( ) ; com . couchbase . json . SQLJSON jsonVal = cbrs . getSQLJSON ( 1 ) ; try { jsonVal . getTimestamp ( null ) ; } catch ( java . sql . SQLException e ) { java . lang . String expectatedMessage = "value<sp>NAMEis<sp>not<sp>a<sp>Timestamp" ; org . junit . Assert . assertEquals ( expectatedMessage . trim ( ) , e . getMessage ( ) . trim ( ) ) ; } } } } }
public class aTest{ @Test public void testIoNewVisit ( ) { int id = 0 ; try { org . isf . patient . model . Patient patient = org . isf . visits . test . Tests . testPatient . setup ( false ) ; org . isf . visits . test . Tests . jpa . beginTransaction ( ) ; org . isf . visits . test . Tests . jpa . persist ( patient ) ; org . isf . visits . test . Tests . jpa . commitTransaction ( ) ; org . isf . visits . model . Visit visit = org . isf . visits . test . Tests . testVisit . setup ( patient , true ) ; id = visitsIoOperation . newVisit ( visit ) ; _checkVisitIntoDb ( id ) ; } catch ( java . lang . Exception e ) { e . printStackTrace ( ) ; org . junit . Assert . assertEquals ( true , false ) ; } }
public class aTest{ @Test public void testCreateCustomer ( ) { try { java . lang . Integer customerId = runFlowAndGetPayload ( "create-customer" ) ; org . junit . Assert . assertNotNull ( customerId ) ; upsertOnTestRunMessage ( "customerId" , customerId ) ; } }
public class aTest{ @Test public void givenDataArray_whenConvertToCSV_thenOutputCreated ( ) { java . util . List < java . lang . String [ ] > dataLines = new java . util . ArrayList < java . lang . String [ ] > ( ) ; dataLines . add ( new java . lang . String [ ] { "John" , "Doe" , "38" , "Comment<sp>Data\nAnother<sp>line<sp>of<sp>comment<sp>data" } ) ; dataLines . add ( new java . lang . String [ ] { "John" 0 , "Doe,<sp>Jr." , "19" , "She<sp>said<sp>\"I\'m<sp>being<sp>quoted\"John" 1 } ) ; java . io . File csvOutputFile = java . io . File . createTempFile ( "exampleOutput" , ".csv" ) ; try ( java . io . PrintWriter pw = new java . io . PrintWriter ( csvOutputFile ) ) { dataLines . stream ( ) . map ( csvExample :: convertToCSV ) . forEach ( pw :: println ) ; } catch ( java . io . FileNotFoundException e ) { com . baeldung . csv . WriteCsvFileExampleUnitTest . LOG . error ( ( "IOException<sp>" + ( e . getMessage ( ) ) ) ) ; } org . junit . Assert . assertTrue ( csvOutputFile . exists ( ) ) ; csvOutputFile . deleteOnExit ( ) ; } }
public class aTest{ @Test public void testConcurrency ( ) { java . lang . String domainIdPrefix = "id" ; int repoSize = 100 ; org . pentaho . metadata . repository . IMetadataDomainRepository repo = new org . pentaho . metadata . repository . InMemoryMetadataDomainRepository ( ) ; for ( int i = 0 ; i < repoSize ; i ++ ) { org . pentaho . metadata . model . Domain domain = new org . pentaho . metadata . model . Domain ( ) ; org . pentaho . metadata . model . concept . types . LocalizedString name = new org . pentaho . metadata . model . concept . types . LocalizedString ( ) ; name . setString ( "US" , java . lang . String . valueOf ( ( i + 1 ) ) ) ; domain . setId ( ( domainIdPrefix + ( java . lang . String . valueOf ( ( i + 1 ) ) ) ) ) ; domain . setName ( name ) ; repo . storeDomain ( domain , false ) ; } org . pentaho . commons . metadata . mqleditor . editor . service . util . MQLEditorServiceDelegate service = new org . pentaho . commons . metadata . mqleditor . editor . service . util . MQLEditorServiceDelegate ( repo ) ; int poolSize = repoSize / 2 ; java . util . concurrent . ExecutorService executorService = java . util . concurrent . Executors . newFixedThreadPool ( poolSize ) ; java . util . List < java . util . concurrent . Future < java . lang . Boolean > > results = new java . util . ArrayList ( ) ; for ( int i = 0 ; i < poolSize ; i ++ ) { results . add ( executorService . submit ( new java . util . concurrent . Callable < java . lang . Boolean > ( ) { public org . pentaho . commons . metadata . mqleditor . editor . service . util . Boolean call ( ) throws org . pentaho . commons . metadata . mqleditor . editor . service . util . Exception { for ( int i = 0 ; i < repoSize ; i ++ ) { try { java . lang . String id = domainIdPrefix + ( java . lang . String . valueOf ( ( i + 1 ) ) ) ; service . getDomainByName ( id ) ; service . addThinDomain ( id ) ; } catch ( java . lang . Exception e ) { return false ; } } return true ; } } ) ) ; } for ( java . util . concurrent . Future < java . lang . Boolean > result : results ) { org . junit . Assert . assertTrue ( result . get ( ) ) ; } }
public class aTest{ @Test public void testRegisterComponentInDocumentComponentManager ( ) { final org . jmock . States state = getMockery ( ) . states ( "test" ) ; getMockery ( ) . checking ( new org . jmock . Expectations ( ) { { allowing ( mockWikiDescriptorManager ) . getCurrentWikiId ( ) ; when ( state . isNot ( "otherdocument" ) ) ; will ( returnValue ( "space2" 3 ) ) ; allowing ( mockCurrentSpaceReferenceProvider ) . get ( ) ; when ( state . isNot ( "otherdocument" ) ) ; will ( returnValue ( new org . xwiki . model . reference . SpaceReference ( "space1" , new org . xwiki . model . reference . WikiReference ( "space2" 2 ) ) ) ) ; allowing ( mockCurrentDocumentReferenceProvider ) . get ( ) ; when ( state . isNot ( "otherdocument" ) ) ; will ( returnValue ( new org . xwiki . model . reference . DocumentReference ( "space2" 3 , "space1" , "document1" ) ) ) ; allowing ( mockDocumentAccessBridge ) . getCurrentUserReference ( ) ; when ( state . isNot ( "otherdocument" ) ) ; will ( returnValue ( new org . xwiki . model . reference . DocumentReference ( "space2" 2 , "XWiki" , "user" ) ) ) ; } } ) ; org . xwiki . component . manager . ComponentManager documentCM = getComponentManager ( ) . getInstance ( org . xwiki . component . manager . ComponentManager . class , "document" ) ; org . xwiki . component . descriptor . DefaultComponentDescriptor < org . xwiki . component . internal . ContextComponentManagerTest . Role > cd = new org . xwiki . component . descriptor . DefaultComponentDescriptor < org . xwiki . component . internal . ContextComponentManagerTest . Role > ( ) ; cd . setRoleType ( org . xwiki . component . internal . ContextComponentManagerTest . Role . class ) ; cd . setImplementation ( org . xwiki . component . internal . ContextComponentManagerTest . RoleImpl . class ) ; documentCM . registerComponent ( cd ) ; org . xwiki . component . manager . ComponentManager contextCM = getComponentManager ( ) . getInstance ( org . xwiki . component . manager . ComponentManager . class , "context" ) ; org . junit . Assert . assertNotNull ( contextCM . getInstance ( org . xwiki . component . internal . ContextComponentManagerTest . Role . class ) ) ; state . become ( "otherdocument" ) ; getMockery ( ) . checking ( new org . jmock . Expectations ( ) { { exactly ( 1 ) . of ( mockDocumentAccessBridge ) . getCurrentUserReference ( ) ; will ( returnValue ( new org . xwiki . model . reference . DocumentReference ( "space2" 2 , "XWiki" , "user" ) ) ) ; allowing ( mockWikiDescriptorManager ) . getCurrentWikiId ( ) ; will ( returnValue ( "space2" 0 ) ) ; allowing ( mockCurrentSpaceReferenceProvider ) . get ( ) ; will ( returnValue ( new org . xwiki . model . reference . SpaceReference ( "space2" , new org . xwiki . model . reference . WikiReference ( "space2" 0 ) ) ) ) ; allowing ( mockCurrentDocumentReferenceProvider ) . get ( ) ; will ( returnValue ( new org . xwiki . model . reference . DocumentReference ( "space2" 0 , "space2" , "document2" ) ) ) ; } } }
public class aTest{ @Test public void testExplicitStop ( ) { io . cdap . cdap . proto . id . ProgramRunId programRunId = new io . cdap . cdap . proto . id . ProgramRunId ( "ns" , "app" , io . cdap . cdap . proto . ProgramType . SPARK , "test" , io . cdap . cdap . common . app . RunIds . generate ( ) . getId ( ) ) ; io . cdap . cdap . app . runtime . spark . distributed . SparkExecutionService service = new io . cdap . cdap . app . runtime . spark . distributed . SparkExecutionService ( io . cdap . cdap . app . runtime . spark . distributed . SparkExecutionServiceTest . locationFactory , java . net . InetAddress . getLoopbackAddress ( ) . getCanonicalHostName ( ) , programRunId , null ) ; service . startAndWait ( ) ; try { final io . cdap . cdap . app . runtime . spark . distributed . SparkExecutionClient client = new io . cdap . cdap . app . runtime . spark . distributed . SparkExecutionClient ( service . getBaseURI ( ) , programRunId ) ; for ( int i = 0 ; i < 5 ; i ++ ) { org . junit . Assert . assertNull ( client . heartbeat ( null ) ) ; TimeUnit . MILLISECONDS . sleep ( 50 ) ; } }
public class aTest{ @Test public void testDoMapConfig2 ( ) { System . out . println ( ( ( getTestTraceHead ( "[NGSIGroupingInterceptor.doMapConfig2]" ) ) + "--------<sp>A<sp>mapped<sp>ContextElement<sp>can<sp>be<sp>obtained<sp>from<sp>the<sp>Name<sp>Mappings" ) ) ; com . telefonica . iot . cygnus . interceptors . NGSINameMappingsInterceptor nameMappingsInterceptor = new com . telefonica . iot . cygnus . interceptors . NGSINameMappingsInterceptor ( null , false ) ; nameMappingsInterceptor . loadNameMappings ( nameMappingsStrConfig2 ) ; com . telefonica . iot . cygnus . containers . NotifyContextRequest . ContextElement originalCE ; com . telefonica . iot . cygnus . containers . NotifyContextRequest . ContextElement expectedCE ; try { originalCE = com . telefonica . iot . cygnus . utils . NGSIUtilsForTests . createJsonContextElement ( originalCEStrConfig2 ) ; expectedCE = com . telefonica . iot . cygnus . utils . NGSIUtilsForTests . createJsonContextElement ( expectedCEStrConfig2 ) ; } catch ( java . lang . Exception e ) { System . out . println ( ( ( getTestTraceHead ( "[NGSIGroupingInterceptor.doMapConfig2]" ) ) + "-<sp>FAIL<sp>-<sp>There<sp>was<sp>some<sp>problem<sp>when<sp>parsing<sp>the<sp>ContextElements" ) ) ; throw new java . lang . AssertionError ( e . getMessage ( ) ) ; } org . apache . commons . lang3 . tuple . ImmutableTriple < java . lang . String , java . lang . String , com . telefonica . iot . cygnus . containers . NotifyContextRequest . ContextElement > map = nameMappingsInterceptor . doMap ( originalServiceConfig , originalServicePathConfig , originalCE ) ; com . telefonica . iot . cygnus . containers . NotifyContextRequest . ContextElement mappedCE = map . getRight ( ) ; boolean equals = true ; if ( ( ! ( mappedCE . getType ( ) . equals ( expectedCE . getType ( ) ) ) ) || ( ! ( expectedServicePathConfig2 . equals ( map . getMiddle ( ) ) ) ) ) { equals = false ; } else { for ( int j = 0 ; j < ( mappedCE . getAttributes ( ) . size ( ) ) ; j ++ ) { com . telefonica . iot . cygnus . containers . NotifyContextRequest . ContextAttribute mappedCA = mappedCE . getAttributes ( ) . get ( j ) ; com . telefonica . iot . cygnus . containers . NotifyContextRequest . ContextAttribute expectedCA = expectedCE . getAttributes ( ) . get ( j ) ; if ( ( ! ( mappedCA . getName ( ) . equals ( expectedCA . getName ( ) ) ) ) || ( ! ( mappedCA . getType ( ) . equals ( expectedCA . getType ( ) ) ) ) ) { equals = false ; break ; } } } try { org . junit . Assert . assertTrue ( equals ) ; System . out . println ( ( ( getTestTraceHead ( "[NGSIGroupingInterceptor.doMapConfig2]" ) ) + "-<sp>OK<sp>-<sp>The<sp>mapped<sp>NotifyContextRequest<sp>is<sp>equals<sp>to<sp>the<sp>expected<sp>one" ) ) ; } }
public class aTest{ @Test public void testDenseRecursiveEvaluate2 ( ) { org . apache . commons . math3 . random . RandomDataGenerator rnd = getRandomData ( ) ; cc . redberry . rings . IntegersZp64 ring = cc . redberry . rings . Rings . Zp64 ( cc . redberry . rings . primes . SmallPrimes . nextPrime ( ( 1 << 15 ) ) ) ; org . apache . commons . math3 . stat . descriptive . DescriptiveStatistics recStat = new org . apache . commons . math3 . stat . descriptive . DescriptiveStatistics ( ) ; org . apache . commons . math3 . stat . descriptive . DescriptiveStatistics recEvalStat = new org . apache . commons . math3 . stat . descriptive . DescriptiveStatistics ( ) ; org . apache . commons . math3 . stat . descriptive . DescriptiveStatistics plainStat = new org . apache . commons . math3 . stat . descriptive . DescriptiveStatistics ( ) ; long start ; long elapsed ; int nIterations = 100 ; int nVars = 3 ; int minDeg = 30 ; int minSize = 1000 ; for ( int i = 0 ; i < nIterations ; ++ i ) { if ( i == ( nIterations / 10 ) ) java . util . Arrays . asList ( recStat , recEvalStat , plainStat ) . forEach ( DescriptiveStatistics :: clear ) ; cc . redberry . rings . poly . multivar . MultivariatePolynomialZp64 . MultivariatePolynomialZp64 p = cc . redberry . rings . poly . multivar . RandomMultivariatePolynomials . randomPolynomial ( nVars , rnd . nextInt ( minDeg , ( 2 * minDeg ) ) , rnd . nextInt ( minSize , ( 2 * minSize ) ) , ring , rnd . getRandomGenerator ( ) ) ; long [ ] values = new long [ p . nVariables ] ; for ( int j = 0 ; j < ( values . length ) ; j ++ ) values [ j ] = ring . randomElement ( rnd . getRandomGenerator ( ) ) ; start = java . lang . System . nanoTime ( ) ; cc . redberry . rings . poly . univar . IUnivariatePolynomial recForm = p . toDenseRecursiveForm ( ) ; elapsed = ( java . lang . System . nanoTime ( ) ) - start ; recStat . addValue ( elapsed ) ; start = java . lang . System . nanoTime ( ) ; long recVal = evaluateDenseRecursiveForm ( recForm , values ) ; elapsed = ( java . lang . System . nanoTime ( ) ) - start ; recStat . addValue ( elapsed ) ; recEvalStat . addValue ( elapsed ) ; start = java . lang . System . nanoTime ( ) ; long plainVal = p . evaluate ( values ) ; elapsed = ( java . lang . System . nanoTime ( ) ) - start ; plainStat . addValue ( elapsed ) ; org . junit . Assert . assertEquals ( plainVal , recVal ) ; } }
public class aTest{ @Test public void test3 ( ) { initGraph ( ) ; greycat . Tasks . newTask ( ) . then ( readIndex ( "nodes" ) ) . then ( selectScript ( "true" ) ) . thenDo ( new greycat . ActionFunction ( ) { @ greycatTest . internal . task . Override public void eval ( greycat . TaskContext ctx ) { org . junit . Assert . assertEquals ( ctx . result ( ) . size ( ) , 3 ) ; } } }
public class aTest{ @Test public void decodeLargerThanHeaderListSizeButLessThanGoAwayWithInitialDecoderSettings ( ) { io . netty . buffer . ByteBuf buf = io . netty . handler . codec . http2 . DefaultHttp2HeadersDecoderTest . encode ( io . netty . handler . codec . http2 . DefaultHttp2HeadersDecoderTest . b ( ":method" ) , io . netty . handler . codec . http2 . DefaultHttp2HeadersDecoderTest . b ( "GET" ) , io . netty . handler . codec . http2 . DefaultHttp2HeadersDecoderTest . b ( "test_header" ) , io . netty . handler . codec . http2 . DefaultHttp2HeadersDecoderTest . b ( java . lang . String . format ( "%09000d" , 0 ) . replace ( '0' , 'A' ) ) ) ; final int streamId = 1 ; try { decoder . decodeHeaders ( streamId , buf ) ; org . junit . Assert . fail ( ) ; } catch ( io . netty . handler . codec . http2 . Http2Exception e ) { org . junit . Assert . assertEquals ( streamId , e . streamId ( ) ) ; } }
public class aTest{ @Test public void testSelectedColumns ( ) { try ( final com . questdb . store . JournalWriter w = getFactory ( ) . writer ( new com . questdb . store . factory . configuration . JournalStructure ( "plus" 9 ) { { $str ( "ccy" ) ; $double ( "EVMLKCJBEV\t12.500000914462\n" 6 ) ; } } ) ) { com . questdb . std . Rnd rnd = new com . questdb . std . Rnd ( ) ; for ( int i = 0 ; i < 100 ; i ++ ) { com . questdb . store . JournalEntryWriter ew = w . entryWriter ( ) ; ew . putStr ( 0 , rnd . nextString ( 10 ) ) ; ew . putDouble ( 1 , rnd . nextDouble ( ) ) ; ew . append ( ) ; } w . commit ( ) ; com . questdb . std . str . StringSink sink = new com . questdb . std . str . StringSink ( ) ; com . questdb . ql . RecordSourcePrinter p = new com . questdb . ql . RecordSourcePrinter ( sink ) ; com . questdb . BootstrapEnv env = new com . questdb . BootstrapEnv ( ) ; env . configuration = new com . questdb . ServerConfiguration ( ) ; final com . questdb . ql . ops . plus . AddDoubleOperator plus = ( ( com . questdb . ql . ops . plus . AddDoubleOperator ) ( AddDoubleOperator . FACTORY . newInstance ( 0 , env ) ) ) ; plus . setName ( "plus" ) ; plus . setLhs ( new com . questdb . ql . ops . col . DoubleRecordSourceColumn ( w . getMetadata ( ) . getColumnIndex ( "EVMLKCJBEV\t12.500000914462\n" 6 ) , 0 ) ) ; plus . setRhs ( new com . questdb . ql . ops . constant . DoubleConstant ( 12.5 , 0 ) ) ; try ( com . questdb . ql . RecordSource src = new com . questdb . ql . select . SelectedColumnsRecordSource ( new com . questdb . ql . virtual . VirtualColumnRecordSource ( compile ( "plus" 9 ) , new com . questdb . std . ObjList < com . questdb . ql . ops . VirtualColumn > ( ) { { add ( plus ) ; } } ) , new com . questdb . std . ObjList < java . lang . CharSequence > ( ) { { add ( "ccy" ) ; add ( "plus" ) ; } } ) ) { p . print ( src , getFactory ( ) ) ; final java . lang . String expected = "LUCFTLNKYT\t12.500000001835\n" 0 + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "KFMQNTOGMX\t12.500012478828\n" 9 + "LUCFTLNKYT\t12.500000001835\n" 7 ) + "ccy" 7 ) + "plus" 0 ) + "plus" 5 ) + "EVMLKCJBEV\t12.500000914462\n" 9 ) + "QSNPXMKJSM\t12.504184104619\n" 2 ) + "ZNLCNGZTOY\t12.500080315222\n" 7 ) + "QSNPXMKJSM\t12.504184104619\n" 6 ) + "KFMQNTOGMX\t12.500012478828\n" 4 ) + "ccy" 8 ) + "ZNLCNGZTOY\t12.500080315222\n" 8 ) + "ccy" 5 ) + "BTKVSBEGMI\t403.500000000000\n" 6 ) + "ccy" 6 ) + "EVMLKCJBEV\t12.500000914462\n" 01 ) + "BTKVSBEGMI\t403.500000000000\n" 3 ) + "LUCFTLNKYT\t12.500000001835\n" 4 ) + "LUCFTLNKYT\t12.500000001835\n" 3 ) + "KFMQNTOGMX\t12.500012478828\n" 8 ) + "EVMLKCJBEV\t12.500000914462\n" 8 ) + "HLIHYBTVZN\t12.500001439041\n" 3 ) + "ccy" 3 ) + "KFMQNTOGMX\t12.500012478828\n" 2 ) + "QSNPXMKJSM\t12.504184104619\n" 1 ) + "ZNLCNGZTOY\t12.500080315222\n" 9 ) + "LUCFTLNKYT\t12.500000001835\n" 1 ) + "QSNPXMKJSM\t12.504184104619\n" 5 ) + "HLIHYBTVZN\t12.500001439041\n" 5 ) + "KFMQNTOGMX\t12.500012478828\n" ) + "EVMLKCJBEV\t12.500000914462\n" 03 ) + "KFMQNTOGMX\t12.500012478828\n" 7 ) + "LUCFTLNKYT\t12.500000001835\n" 9 ) + "QSNPXMKJSM\t12.504184104619\n" 9 ) + "plus" 2 ) + "plus" 1 ) + "KFMQNTOGMX\t12.500012478828\n" 6 ) + "HLIHYBTVZN\t12.500001439041\n" 1 ) + "EVMLKCJBEV\t12.500000914462\n" 5 ) + "BTKVSBEGMI\t403.500000000000\n" 0 ) + "BTKVSBEGMI\t403.500000000000\n" 5 ) + "QSNPXMKJSM\t12.504184104619\n" 8 ) + "plus" 7 ) + "plus" 6 ) + "KFMQNTOGMX\t12.500012478828\n" 3 ) + "QSNPXMKJSM\t12.504184104619\n" ) + "BTKVSBEGMI\t403.500000000000\n" 2 ) + "QSNPXMKJSM\t12.504184104619\n" 0 ) + "EVMLKCJBEV\t12.500000914462\n" 02 ) + "EVMLKCJBEV\t12.500000914462\n" ) + "HLIHYBTVZN\t12.500001439041\n" ) + "BTKVSBEGMI\t403.500000000000\n" 7 ) + "ccy" 0 ) + "EVMLKCJBEV\t12.500000914462\n" 3 ) + "HLIHYBTVZN\t12.500001439041\n" 4 ) + "KFMQNTOGMX\t12.500012478828\n" 5 ) + "BTKVSBEGMI\t403.500000000000\n" ) + "QSNPXMKJSM\t12.504184104619\n" 4 ) + "ccy" 9 ) + "ccy" 4 ) + "KFMQNTOGMX\t12.500012478828\n" 0 ) + "EVMLKCJBEV\t12.500000914462\n" 1 ) + "BTKVSBEGMI\t403.500000000000\n" 1 ) + "HLIHYBTVZN\t12.500001439041\n" 0 ) + "ZNLCNGZTOY\t12.500080315222\n" 4 ) + "ccy" 2 ) + "ZNLCNGZTOY\t12.500080315222\n" 0 ) + "plus" 3 ) + "LUCFTLNKYT\t12.500000001835\n" 5 ) + "ZNLCNGZTOY\t12.500080315222\n" 6 ) + "plus" 4 ) + "ZNLCNGZTOY\t12.500080315222\n" 1 ) + "LUCFTLNKYT\t12.500000001835\n" 2 ) + "QSNPXMKJSM\t12.504184104619\n" 3 ) + "LUCFTLNKYT\t12.500000001835\n" 6 ) + "EVMLKCJBEV\t12.500000914462\n" 0 ) + "BTKVSBEGMI\t403.500000000000\n" 4 ) + "HLIHYBTVZN\t12.500001439041\n" 7 ) + "HLIHYBTVZN\t12.500001439041\n" 6 ) + "BTKVSBEGMI\t403.500000000000\n" 9 ) + "EVMLKCJBEV\t12.500000914462\n" 4 ) + "QSNPXMKJSM\t12.504184104619\n" 7 ) + "EVMLKCJBEV\t12.500000914462\n" 00 ) + "ZNLCNGZTOY\t12.500080315222\n" 2 ) + "HLIHYBTVZN\t12.500001439041\n" 8 ) + "HLIHYBTVZN\t12.500001439041\n" 9 ) + "ZNLCNGZTOY\t12.500080315222\n" 5 ) + "KFMQNTOGMX\t12.500012478828\n" 1 ) + "plus" 8 ) + "ccy" 1 ) + "ZNLCNGZTOY\t12.500080315222\n" ) + "BTKVSBEGMI\t403.500000000000\n" 8 ) + "LUCFTLNKYT\t12.500000001835\n" 8 ) + "ZNLCNGZTOY\t12.500080315222\n" 3 ) + "HLIHYBTVZN\t12.500001439041\n" 2 ) + "EVMLKCJBEV\t12.500000914462\n" 7 ) + "EVMLKCJBEV\t12.500000914462\n" 2 ) + "VZHCNXZEQG\t-512.007812500000\n" ) + "LUCFTLNKYT\t12.500000001835\n" ) ; org . junit . Assert . assertEquals ( expected , sink . toString ( ) ) ; } } } }
public class aTest{ @Test public void testIllegalMoveToSameDN ( ) { org . apache . directory . ldap . client . api . LdapConnection con = getAdminConnection ( getLdapServer ( ) ) ; try { con . move ( "ou=parent,ou=system" , "ou=parent,ou=system" ) ; org . junit . Assert . fail ( ) ; } catch ( org . apache . directory . api . ldap . model . exception . LdapUnwillingToPerformException lutpe ) { org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testUseMessage ( ) { try { javax . jms . TextMessage message = senderSession . createTextMessage ( ) ; message . setText ( "testUseMessage" ) ; sender . send ( message ) ; javax . jms . TextMessage m = ( ( javax . jms . TextMessage ) ( receiver . receive ( TestConfig . TIMEOUT ) ) ) ; receiverSession . close ( ) ; org . junit . Assert . assertEquals ( "testUseMessage" , m . getText ( ) ) ; } }
public class aTest{ @Test public void getInputStream ( ) { final byte [ ] testData = "test<sp>data" . getBytes ( ) ; final org . exist . xquery . value . BinaryValueManager binaryValueManager = new org . exist . xquery . value . MockBinaryValueManager ( ) ; try ( final java . io . InputStream bais = new org . exist . xquery . value . BinaryValueFromInputStreamTest . UnmarkableByteArrayInputStream ( testData ) ) { final org . exist . xquery . value . BinaryValue binaryValue = org . exist . xquery . value . BinaryValueFromInputStream . getInstance ( binaryValueManager , new org . exist . xquery . value . Base64BinaryValueType ( ) , bais ) ; try ( final java . io . InputStream is = binaryValue . getInputStream ( ) ) { final byte [ ] actual = org . exist . xquery . value . BinaryValueFromInputStreamTest . readAll ( is ) ; org . junit . Assert . assertArrayEquals ( testData , actual ) ; } } }
public class aTest{ @Test public void testQueryTimeout ( ) { java . lang . String udfName = org . apache . hive . jdbc . TestJdbcDriver2 . SleepMsUDF . class . getName ( ) ; java . sql . Statement stmt1 = org . apache . hive . jdbc . TestJdbcDriver2 . con . createStatement ( ) ; stmt1 . execute ( ( ( "create<sp>temporary<sp>function<sp>sleepMsUDF<sp>as<sp>'" + udfName ) + "'" ) ) ; stmt1 . close ( ) ; java . sql . Statement stmt = org . apache . hive . jdbc . TestJdbcDriver2 . con . createStatement ( ) ; stmt . setQueryTimeout ( 1 ) ; System . err . println ( "Executing<sp>query:<sp>" ) ; try { stmt . executeQuery ( ( ( ( ( ( "select<sp>sleepMsUDF(t1.under_col,<sp>5)<sp>as<sp>u0,<sp>t1.under_col<sp>as<sp>u1,<sp>" + "t2.under_col<sp>as<sp>u2<sp>from<sp>" ) + ( org . apache . hive . jdbc . TestJdbcDriver2 . tableName ) ) + "<sp>t1<sp>join<sp>" ) + ( org . apache . hive . jdbc . TestJdbcDriver2 . tableName ) ) + "<sp>t2<sp>on<sp>t1.under_col<sp>=<sp>t2.under_col" ) ) ; org . junit . Assert . fail ( "Expecting<sp>SQLTimeoutException" ) ; } catch ( java . sql . SQLTimeoutException e ) { org . junit . Assert . assertNotNull ( e ) ; System . err . println ( e . toString ( ) ) ; } }
public class aTest{ @Test public void testTestFields ( ) { com . intuit . wasabi . api . pagination . filters . PaginationFilterTest . TestObjectFilter objectFilter = new com . intuit . wasabi . api . pagination . filters . PaginationFilterTest . TestObjectFilter ( ) ; java . util . HashMap < java . lang . String , java . lang . Boolean > testCases = new java . util . HashMap ( ) ; testCases . put ( "Failed<sp>due<sp>to<sp>exception:<sp>" 0 , true ) ; testCases . put ( null , true ) ; testCases . put ( "hash=34" , true ) ; testCases . put ( "hash=23,string=match" , true ) ; testCases . put ( "hash=no<sp>match" , false ) ; testCases . put ( "Failed<sp>due<sp>to<sp>exception:<sp>" 3 , false ) ; testCases . put ( "hash=no<sp>match,string=match" , false ) ; testCases . put ( "Failed<sp>due<sp>to<sp>exception:<sp>" 6 , false ) ; testCases . put ( "hash=no<sp>match,moreKeys=values" , false ) ; for ( Map . Entry < java . lang . String , java . lang . Boolean > testCase : testCases . entrySet ( ) ) { objectFilter . replaceFilter ( testCase . getKey ( ) , "Failed<sp>due<sp>to<sp>exception:<sp>" 0 ) ; try { org . junit . Assert . assertEquals ( ( "Failed<sp>due<sp>to<sp>exception:<sp>" 1 + ( testCase . getKey ( ) ) ) , testCase . getValue ( ) , objectFilter . testFields ( testObject , com . intuit . wasabi . api . pagination . filters . PaginationFilterTest . TestObjectFilter . Property . class ) ) ; } }
public class aTest{ @Test public void sync_consult_from_string ( ) { final java . lang . String rulebase = "rules/reloaded/kk_rules001_simple.prova" ; comm = new ws . prova . api2 . ProvaCommunicatorImpl ( test . ws . prova . test2 . ProvaCommunicatorTest . kAgent , test . ws . prova . test2 . ProvaCommunicatorTest . kPort , rulebase , ws . prova . api2 . ProvaCommunicatorImpl . SYNC ) ; comm . setPrintWriter ( ws . prova . util2 . ProvaNullWriter . getPrintWriter ( ) ) ; java . lang . String input = ":-<sp>solve(happy(Person)).\n<sp>:-<sp>solve(pappy(Person))." ; java . io . BufferedReader in = new java . io . BufferedReader ( new java . io . StringReader ( input ) ) ; final int [ ] numSolutions = new int [ ] { 2 , 0 } ; int i = 0 ; try { java . util . List < ws . prova . exchange . ProvaSolution [ ] > resultSets = comm . consultSync ( in , java . lang . Integer . toString ( ( ( key ) ++ ) ) , new java . lang . Object [ ] { } ) ; for ( ws . prova . exchange . ProvaSolution [ ] resultSet : resultSets ) { org . junit . Assert . assertEquals ( numSolutions [ ( i ++ ) ] , resultSet . length ) ; } } }
public class aTest{ @Test public void testRepeatedMapWithNullValue ( ) { org . stringtemplate . v4 . STGroup group = new org . stringtemplate . v4 . STGroup ( ) ; group . defineTemplate ( "a" , "x" , "[<x>]" ) ; group . defineTemplate ( "b" , "x" , "(<x>)" ) ; group . defineTemplate ( "test" , "name" , "hi<sp><name:a():b()>!" ) ; org . stringtemplate . v4 . ST st = group . getInstanceOf ( "test" ) ; st . add ( "name" , "Ter" ) ; st . add ( "name" , null ) ; st . add ( "name" , "a" 0 ) ; java . lang . String expected = "hi<sp>([Ter])([Sumana])!" ; java . lang . String result = st . render ( ) ; org . junit . Assert . assertEquals ( expected , result ) ; } }
public class aTest{ @Test public void testRollupWithNotNullAndSubq ( ) { java . lang . String sqlText = java . lang . String . format ( ( "<sp>" + ( ( ( "SELECT<sp>*<sp>from<sp>(" + "SELECT<sp>deptno,<sp>sum(salary)<sp>" ) + "from<sp>%s<sp>group<sp>by<sp>rollup(deptno))<sp>v1<sp>(x,<sp>y)<sp>" ) + "order<sp>by<sp>1,<sp>2" ) ) , com . splicemachine . derby . impl . sql . execute . operations . MultiGroupGroupedAggregateOperationIT . EMP_2_REF ) ; java . sql . ResultSet rs = methodWatcher . executeQuery ( sqlText ) ; java . lang . String expected = "X<sp>|<sp>Y<sp>|\n" + ( ( ( ( ( ( "--------------\n" + "SELECT<sp>*<sp>from<sp>(" 3 ) + "SELECT<sp>*<sp>from<sp>(" 1 ) + "<sp>3<sp>|<sp>22900<sp>|\n" ) + "SELECT<sp>*<sp>from<sp>(" 2 ) + "<sp>5<sp>|<sp>32000<sp>|\n" ) + "NULL<sp>|126200<sp>|" ) ; org . junit . Assert . assertEquals ( ( ( "SELECT<sp>*<sp>from<sp>(" 0 + sqlText ) + "SELECT<sp>*<sp>from<sp>(" 0 ) , expected , TestUtils . FormattedResult . ResultFactory . toStringUnsorted ( rs ) ) ; rs . close ( ) ; } }
public class aTest{ @Test public void testCallNoArgObjectReturn ( ) { java . lang . String method = "getObject1" ; try { internalTestCall ( flex . messaging . io . amf . client . AMFDataTypeIT . getOperationCall ( method ) , null , new flex . messaging . io . amf . client . AMFDataTypeIT . CallResultHandler ( ) { public void onResult ( java . lang . Object result ) { remoting . amfclient . ClientCustomType temp2 = ( ( remoting . amfclient . ClientCustomType ) ( result ) ) ; org . junit . Assert . assertEquals ( 1 , temp2 . getId ( ) ) ; } } }
public class aTest{ @Test public void xss ( ) { j . jenkins . setMarkupFormatter ( new hudson . model . ParametersTest . MyMarkupFormatter ( ) ) ; hudson . model . FreeStyleProject p = j . createFreeStyleProject ( "p" ) ; hudson . model . StringParameterDefinition param = new hudson . model . StringParameterDefinition ( "<param<sp>name>" , "<param<sp>default>" , "<param<sp>description>" ) ; org . junit . Assert . assertEquals ( "p" 3 , param . getFormattedDescription ( ) ) ; p . addProperty ( new hudson . model . ParametersDefinitionProperty ( param ) ) ; org . jvnet . hudson . test . JenkinsRule . WebClient wc = j . createWebClient ( ) ; wc . getOptions ( ) . setThrowExceptionOnFailingStatusCode ( false ) ; com . gargoylesoftware . htmlunit . html . HtmlPage page = wc . getPage ( p , "parameters/" 7 ) ; collector . checkThat ( page . getWebResponse ( ) . getStatusCode ( ) , is ( HttpStatus . SC_METHOD_NOT_ALLOWED ) ) ; java . lang . String text = page . getWebResponse ( ) . getContentAsString ( ) ; collector . checkThat ( "parameters/" 2 , text , containsString ( "p" 0 ) ) ; collector . checkThat ( "parameters/" 9 , text , not ( containsString ( "<param<sp>name>" ) ) ) ; collector . checkThat ( "build<sp>page<sp>should<sp>escape<sp>param<sp>default" , text , containsString ( "parameters/" 6 ) ) ; collector . checkThat ( "build<sp>page<sp>should<sp>not<sp>leave<sp>param<sp>default<sp>unescaped" , text , not ( containsString ( "<param<sp>default>" ) ) ) ; collector . checkThat ( "build<sp>page<sp>should<sp>mark<sp>up<sp>param<sp>description" , text , containsString ( "p" 3 ) ) ; collector . checkThat ( "build<sp>page<sp>should<sp>not<sp>leave<sp>param<sp>description<sp>unescaped" , text , not ( containsString ( "<param<sp>description>" ) ) ) ; com . gargoylesoftware . htmlunit . html . HtmlForm form = page . getFormByName ( "parameters/" 5 ) ; com . gargoylesoftware . htmlunit . html . HtmlTextInput value = form . getInputByValue ( "<param<sp>default>" ) ; value . setText ( "parameters/" 1 ) ; j . submit ( form ) ; j . waitUntilNoActivity ( ) ; hudson . model . FreeStyleBuild b = p . getBuildByNumber ( 1 ) ; page = j . createWebClient ( ) . getPage ( b , "parameters/" ) ; text = page . getWebResponse ( ) . getContentAsString ( ) ; collector . checkThat ( "parameters/" 0 , text , containsString ( "p" 0 ) ) ; collector . checkThat ( "parameters<sp>page<sp>should<sp>not<sp>leave<sp>param<sp>name<sp>unescaped" , text , not ( containsString ( "<param<sp>name>" ) ) ) ; collector . checkThat ( "p" 2 , text , containsString ( "p" 1 ) ) ; collector . checkThat ( "parameters/" 3 , text , not ( containsString ( "parameters/" 1 ) ) ) ; collector . checkThat ( "parameters/" 8 , text , containsString ( "p" 3 ) ) ; collector . checkThat ( "parameters/" 4 , text , not ( containsString ( "<param<sp>description>" ) ) ) ; } }
public class aTest{ @Test public void queryResultsMustNotIncludeNodesDeletedInThisTransaction ( ) { db = createDatabase ( ) ; try ( org . neo4j . graphdb . Transaction tx = db . beginTx ( ) ) { createSimpleNodesIndex ( ) ; tx . success ( ) ; } long nodeIdA ; long nodeIdB ; try ( org . neo4j . graphdb . Transaction tx = db . beginTx ( ) ) { awaitIndexesOnline ( ) ; org . neo4j . graphdb . Node nodeA = db . createNode ( org . neo4j . kernel . api . impl . fulltext . FulltextProceduresTest . LABEL ) ; nodeA . setProperty ( org . neo4j . kernel . api . impl . fulltext . FulltextProceduresTest . PROP , "value" ) ; nodeIdA = nodeA . getId ( ) ; org . neo4j . graphdb . Node nodeB = db . createNode ( org . neo4j . kernel . api . impl . fulltext . FulltextProceduresTest . LABEL ) ; nodeB . setProperty ( org . neo4j . kernel . api . impl . fulltext . FulltextProceduresTest . PROP , "value" ) ; nodeIdB = nodeB . getId ( ) ; tx . success ( ) ; } try ( org . neo4j . graphdb . Transaction tx = db . beginTx ( ) ) { db . getNodeById ( nodeIdA ) . delete ( ) ; db . getNodeById ( nodeIdB ) . delete ( ) ; try ( org . neo4j . graphdb . Result result = db . execute ( java . lang . String . format ( org . neo4j . kernel . api . impl . fulltext . FulltextProceduresTest . QUERY_NODES , "nodes" , "value" ) ) ) { org . junit . Assert . assertThat ( result . stream ( ) . count ( ) , org . hamcrest . Matchers . is ( 0L ) ) ; } }
public class aTest{ @Test public void testGetToken ( ) { com . hortonworks . registries . auth . server . AuthenticationFilter filter = new com . hortonworks . registries . auth . server . AuthenticationFilter ( ) ; try { javax . servlet . FilterConfig config = org . mockito . Mockito . mock ( javax . servlet . FilterConfig . class ) ; org . mockito . Mockito . when ( config . getInitParameter ( "management.operation.return" ) ) . thenReturn ( "true" ) ; org . mockito . Mockito . when ( config . getInitParameter ( AuthenticationFilter . AUTH_TYPE ) ) . thenReturn ( com . hortonworks . registries . auth . server . TestAuthenticationFilter . DummyAuthenticationHandler . class . getName ( ) ) ; org . mockito . Mockito . when ( config . getInitParameter ( AuthenticationFilter . SIGNATURE_SECRET ) ) . thenReturn ( "secret" ) ; org . mockito . Mockito . when ( config . getInitParameterNames ( ) ) . thenReturn ( new java . util . Vector < java . lang . String > ( java . util . Arrays . asList ( AuthenticationFilter . AUTH_TYPE , AuthenticationFilter . SIGNATURE_SECRET , "management.operation.return" ) ) . elements ( ) ) ; com . hortonworks . registries . auth . util . SignerSecretProvider secretProvider = com . hortonworks . registries . auth . server . TestAuthenticationFilter . getMockedServletContextWithStringSigner ( config ) ; filter . init ( config ) ; com . hortonworks . registries . auth . server . AuthenticationToken token = new com . hortonworks . registries . auth . server . AuthenticationToken ( "u" , "p" , com . hortonworks . registries . auth . server . TestAuthenticationFilter . DummyAuthenticationHandler . TYPE ) ; token . setExpires ( ( ( java . lang . System . currentTimeMillis ( ) ) + ( com . hortonworks . registries . auth . server . TestAuthenticationFilter . TOKEN_VALIDITY_SEC ) ) ) ; com . hortonworks . registries . auth . util . Signer signer = new com . hortonworks . registries . auth . util . Signer ( secretProvider ) ; java . lang . String tokenSigned = signer . sign ( token . toString ( ) ) ; javax . servlet . http . Cookie cookie = new javax . servlet . http . Cookie ( com . hortonworks . registries . auth . client . AuthenticatedURL . AUTH_COOKIE , tokenSigned ) ; javax . servlet . http . HttpServletRequest request = org . mockito . Mockito . mock ( javax . servlet . http . HttpServletRequest . class ) ; org . mockito . Mockito . when ( request . getCookies ( ) ) . thenReturn ( new javax . servlet . http . Cookie [ ] { cookie } ) ; com . hortonworks . registries . auth . server . AuthenticationToken newToken = filter . getToken ( request ) ; org . junit . Assert . assertEquals ( token . toString ( ) , newToken . toString ( ) ) ; } }
public class aTest{ @Test public void shouldThrowAnExceptionOnError1AndStrictParsing ( ) { java . lang . String input = "<sp>@prefix<sp>:<sp><http://www.example.com#><sp>.\n" + ( ( ( ( ( ( ( ( ( "<sp>@prefix<sp>owl:<sp><http://www.w3.org/2002/07/owl#><sp>.\n" + "<sp>@prefix<sp>rdf:<sp><http://www.w3.org/1999/02/22-rdf-syntax-ns#><sp>.\n" ) + "<sp>@prefix<sp>xsd:<sp><http://www.w3.org/2001/XMLSchema#><sp>.\n" ) + "<sp>@prefix<sp>rdfs:<sp><http://www.w3.org/2000/01/rdf-schema#><sp>.\n" ) + "<sp>@prefix<sp>rdf:<sp><http://www.w3.org/1999/02/22-rdf-syntax-ns#><sp>.\n" 0 ) + "<sp>@prefix<sp>rdf:<sp><http://www.w3.org/1999/02/22-rdf-syntax-ns#><sp>.\n" 1 ) + "<sp>rdfs:subClassOf<sp>[<sp>rdf:type<sp>owl:Restriction<sp>;\n" ) + "<sp>owl:onProperty<sp>:unknownproperty;\n" ) + "<sp>owl:minCardinality<sp>\"0\"^^xsd:nonNegativeInteger\n" ) + "<sp>]<sp>." ) ; org . semanticweb . owlapi . model . OWLOntology o = loadOntologyWithConfig ( new org . semanticweb . owlapi . io . StringDocumentSource ( input ) , new org . semanticweb . owlapi . model . OWLOntologyLoaderConfiguration ( ) . setStrict ( true ) ) ; org . junit . Assert . assertEquals ( 0 , o . getLogicalAxiomCount ( ) ) ; } }
public class aTest{ @Test public void testDiffWithNegativeLineCount ( ) { write ( new java . io . File ( db . getWorkTree ( ) , "test.txt" ) , "0\n1\n2\n3\n4\n5\n6\n7\n8\n9" ) ; try ( org . eclipse . jgit . api . Git git = new org . eclipse . jgit . api . Git ( db ) ) { git . add ( ) . addFilepattern ( "." ) . call ( ) ; git . commit ( ) . setMessage ( "diff<sp>--git<sp>a/test.txt<sp>b/test.txt\n" 1 ) . call ( ) ; write ( new java . io . File ( db . getWorkTree ( ) , "test.txt" ) , "0\n1\n2\n3\n4a\n5\n6\n7\n8\n9" ) ; java . io . OutputStream out = new java . io . ByteArrayOutputStream ( ) ; git . diff ( ) . setOutputStream ( out ) . setContextLines ( 1 ) . call ( ) ; java . lang . String actual = out . toString ( ) ; java . lang . String expected = "diff<sp>--git<sp>a/test.txt<sp>b/test.txt\n" + ( ( ( ( ( ( ( "diff<sp>--git<sp>a/test.txt<sp>b/test.txt\n" 2 + "---<sp>a/test.txt\n" ) + "+++<sp>b/test.txt\n" ) + "diff<sp>--git<sp>a/test.txt<sp>b/test.txt\n" 0 ) + "<sp>3\n" ) + "-4\n" ) + "+4a\n" ) + "diff<sp>--git<sp>a/test.txt<sp>b/test.txt\n" 3 ) ; org . junit . Assert . assertEquals ( expected , actual ) ; } } }
public class aTest{ @Test public void testLexerUnicodeEscapedSMPNotSet ( ) { org . antlr . v4 . tool . LexerGrammar lg = new org . antlr . v4 . tool . LexerGrammar ( ( "2:RULE_STOP<sp>0\n" 2 + "2:RULE_STOP<sp>0\n" 4 ) ) ; java . lang . String expecting = "max<sp>type<sp>1\n" + ( ( ( ( ( ( ( ( ( ( ( ( "2:RULE_STOP<sp>0\n" 5 + "1:RULE_START<sp>0\n" ) + "2:RULE_STOP<sp>0\n" ) + "2:RULE_STOP<sp>0\n" 0 ) + "4:BASIC<sp>0\n" ) + "rule<sp>0:1<sp>1\n" ) + "mode<sp>0:0\n" ) + "0:128169..128170\n" ) + "0->1<sp>EPSILON<sp>0,0,0\n" ) + "1->3<sp>EPSILON<sp>0,0,0\n" ) + "2:RULE_STOP<sp>0\n" 1 ) + "2:RULE_STOP<sp>0\n" 3 ) + "0:0\n" ) ; org . antlr . v4 . runtime . atn . ATN atn = createATN ( lg , true ) ; java . lang . String result = org . antlr . v4 . runtime . atn . ATNSerializer . getDecoded ( atn , java . util . Arrays . asList ( lg . getTokenNames ( ) ) ) ; org . junit . Assert . assertEquals ( expecting , result ) ; } }
public class aTest{ @Test public void testAutoWildcardWithLabel ( ) { java . lang . String grammar = "grammar<sp>T;\n" + ( ( ( ( "options<sp>{output=AST;}\n" + "a" 1 ) + "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" ) + "INT<sp>:<sp>\'0\'..\'9\'+;\n" ) + "a" 0 ) ; java . lang . String treeGrammar = "tree<sp>grammar<sp>TP;\n" + ( ( "options<sp>{output=AST;<sp>ASTLabelType=CommonTree;<sp>tokenVocab=T;}\n" + "a" 2 ) + "<sp>;\n" ) ; java . lang . String found = execTreeParser ( "a" 7 , grammar , "TParser" , "a" 6 , treeGrammar , "TP" , "a" 3 , "a" , "a" , "a" 4 ) ; org . junit . Assert . assertEquals ( "a" 5 , found ) ; } }
public class aTest{ @Test public void testSetMatchNoRewriteLevel2 ( ) { java . lang . String grammar = "grammar<sp>T;\n" + ( ( ( ( "options<sp>{output=AST;}\n" + "a" 4 ) + "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" ) + "INT<sp>:<sp>\'0\'..\'9\'+;\n" ) + "a" 0 ) ; java . lang . String treeGrammar = "tree<sp>grammar<sp>TP;\n" + ( "options<sp>{output=AST;<sp>ASTLabelType=CommonTree;<sp>tokenVocab=T;}\n" + "a<sp>:<sp>^(ID<sp>(ID<sp>|<sp>INT)<sp>)<sp>;\n" ) ; java . lang . String found = execTreeParser ( "a" 6 , grammar , "TParser" , "a" 5 , treeGrammar , "TP" , "a" 1 , "a" , "a" , "a" 2 ) ; org . junit . Assert . assertEquals ( "a" 3 , found ) ; } }
public class aTest{ @Test public void testSessionExpiryContainer ( ) { org . apache . catalina . startup . Tomcat tomcat = getTomcatInstance ( ) ; org . apache . catalina . Context ctx = tomcat . addContext ( "" , null ) ; ctx . addApplicationListener ( TesterEchoServer . Config . class . getName ( ) ) ; org . apache . catalina . startup . Tomcat . addServlet ( ctx , "default" , new org . apache . catalina . servlets . DefaultServlet ( ) ) ; ctx . addServletMappingDecoded ( "/" , "default" ) ; tomcat . start ( ) ; org . apache . tomcat . websocket . WsWebSocketContainer wsContainer = ( ( org . apache . tomcat . websocket . WsWebSocketContainer ) ( javax . websocket . ContainerProvider . getWebSocketContainer ( ) ) ) ; wsContainer . setDefaultMaxSessionIdleTimeout ( 5000 ) ; wsContainer . setProcessPeriod ( 1 ) ; org . apache . tomcat . websocket . TestWsWebSocketContainer . EndpointA endpointA = new org . apache . tomcat . websocket . TestWsWebSocketContainer . EndpointA ( ) ; connectToEchoServer ( wsContainer , endpointA , TesterEchoServer . Config . PATH_BASIC ) ; connectToEchoServer ( wsContainer , endpointA , TesterEchoServer . Config . PATH_BASIC ) ; javax . websocket . Session s3a = connectToEchoServer ( wsContainer , endpointA , TesterEchoServer . Config . PATH_BASIC ) ; java . util . Set < javax . websocket . Session > setA = s3a . getOpenSessions ( ) ; org . junit . Assert . assertEquals ( 3 , setA . size ( ) ) ; int count = 0 ; boolean isOpen = true ; while ( isOpen && ( count < 8 ) ) { count ++ ; java . lang . Thread . sleep ( 1000 ) ; isOpen = false ; for ( javax . websocket . Session session : setA ) { if ( session . isOpen ( ) ) { isOpen = true ; break ; } } } }
public class aTest{ @Test public void testDisablingTableCache ( ) { createTable ( org . apache . blur . manager . clusterstatus . ZookeeperClusterStatusTest . TEST ) ; org . junit . Assert . assertTrue ( clusterStatus2 . isEnabled ( true , org . apache . blur . manager . clusterstatus . ZookeeperClusterStatusTest . DEFAULT , org . apache . blur . manager . clusterstatus . ZookeeperClusterStatusTest . TEST ) ) ; clusterStatus1 . disableTable ( org . apache . blur . manager . clusterstatus . ZookeeperClusterStatusTest . DEFAULT , org . apache . blur . manager . clusterstatus . ZookeeperClusterStatusTest . TEST ) ; new org . apache . blur . manager . clusterstatus . ZookeeperClusterStatusTest . WaitForAnswerToBeCorrect ( 20L ) { @ org . apache . blur . manager . clusterstatus . Override public java . lang . Object run ( ) { return clusterStatus2 . isEnabled ( true , org . apache . blur . manager . clusterstatus . ZookeeperClusterStatusTest . DEFAULT , org . apache . blur . manager . clusterstatus . ZookeeperClusterStatusTest . TEST ) ; } } }
public class aTest{ @Test public void testOnCloseCustomCloseReasonServerInitiated ( ) { org . glassfish . tyrus . server . Server server = startServer ( org . glassfish . tyrus . test . standard_config . OnCloseTest . OnCloseWithCustomReasonEndpoint . class ) ; final java . util . concurrent . CountDownLatch messageLatch = new java . util . concurrent . CountDownLatch ( 1 ) ; try { final javax . websocket . ClientEndpointConfig cec = ClientEndpointConfig . Builder . create ( ) . build ( ) ; org . glassfish . tyrus . client . ClientManager client = createClient ( ) ; client . connectToServer ( new org . glassfish . tyrus . test . standard_config . TestEndpointAdapter ( ) { @ org . glassfish . tyrus . test . standard_config . Override public javax . websocket . EndpointConfig getEndpointConfig ( ) { return cec ; } @ org . glassfish . tyrus . test . standard_config . Override public void onOpen ( javax . websocket . Session session ) { session . addMessageHandler ( new org . glassfish . tyrus . test . standard_config . TestTextMessageHandler ( this ) ) ; try { session . getBasicRemote ( ) . sendText ( "message" ) ; } catch ( java . io . IOException e ) { } } @ org . glassfish . tyrus . test . standard_config . Override public void onClose ( javax . websocket . Session session , javax . websocket . CloseReason closeReason ) { if ( ( ( closeReason != null ) && ( ( closeReason . getCloseCode ( ) . getCode ( ) ) == 4000 ) ) && ( closeReason . getReasonPhrase ( ) . equals ( org . glassfish . tyrus . test . standard_config . OnCloseTest . CUSTOM_REASON ) ) ) { messageLatch . countDown ( ) ; } } @ org . glassfish . tyrus . test . standard_config . Override public void onMessage ( java . lang . String message ) { } } , cec , getURI ( org . glassfish . tyrus . test . standard_config . OnCloseTest . OnCloseWithCustomReasonEndpoint . class ) ) ; messageLatch . await ( 5 , TimeUnit . SECONDS ) ; org . junit . Assert . assertEquals ( 0L , messageLatch . getCount ( ) ) ; } }
public class aTest{ @Test public void testBarChartJavaFx ( ) { com . bitplan . can4eve . VehicleGroup vg = com . bitplan . can4eve . VehicleGroup . get ( "Triplet" ) ; com . bitplan . can4eve . CANInfo cellInfo = vg . getCANInfoByName ( "CellTemperature" ) ; org . junit . Assert . assertNotNull ( cellInfo ) ; com . bitplan . can4eve . CANValue . DoubleValue cellTempValue = new com . bitplan . can4eve . CANValue . DoubleValue ( cellInfo ) ; com . bitplan . can4eve . gui . javafx . CANProperty < com . bitplan . can4eve . CANValue . DoubleValue , java . lang . Double > cellTemp = new com . bitplan . can4eve . gui . javafx . CANProperty < com . bitplan . can4eve . CANValue . DoubleValue , java . lang . Double > ( cellTempValue , new javafx . beans . property . SimpleDoubleProperty ( ) ) ; randomValues ( cellTemp ) ; java . lang . String title = "Cell<sp>Temperature" ; java . lang . String xTitle = "cell" ; java . lang . String yTitle = "<sp>Celsius" ; final com . bitplan . obdii . javafx . JFXCanCellStatePlot valuePlot = new com . bitplan . obdii . javafx . JFXCanCellStatePlot ( title , xTitle , yTitle , cellTemp , 2.0 , 0.5 ) ; valuePlot . updateOn ( cellTemp . getUpdateCountProperty ( ) ) ; com . bitplan . javafx . SampleApp sampleApp = new com . bitplan . javafx . SampleApp ( "Cell<sp>Temperature" , valuePlot . getBarChart ( ) ) ; sampleApp . show ( ) ; sampleApp . waitOpen ( ) ; int loops = 4 ; for ( int j = 0 ; j < loops ; j ++ ) { randomValues ( cellTemp ) ; java . lang . Thread . sleep ( ( ( com . bitplan . obdii . TestAppGUI . SHOW_TIME ) / loops ) ) ; } }
public class aTest{ @Test public void testNonInvertible ( ) { java . util . Random r = new java . util . Random ( 9994100315209L ) ; org . hipparchus . linear . RealMatrix m = org . hipparchus . linear . EigenDecompositionTest . createTestMatrix ( r , new double [ ] { 1.0 , 0.0 , - 1.0 , - 2.0 , - 3.0 } ) ; org . hipparchus . linear . DecompositionSolver es = new org . hipparchus . linear . EigenDecomposition ( m ) . getSolver ( ) ; org . junit . Assert . assertFalse ( es . isNonSingular ( ) ) ; try { es . getInverse ( ) ; org . junit . Assert . fail ( "an<sp>exception<sp>should<sp>have<sp>been<sp>thrown" ) ; } }
public class aTest{ @Test public void testWriteByteArrayNegativeOffset ( ) { java . io . OutputStream os = makeObject ( ) ; try { os . write ( new byte [ 5 ] , ( - 3 ) , 5 ) ; org . junit . Assert . fail ( "Should<sp>not<sp>accept<sp>negative<sp>offset" ) ; } catch ( java . io . IOException e ) { org . junit . Assert . fail ( ( "Should<sp>not<sp>throw<sp>IOException<sp>negative<sp>offset:<sp>" + ( e . getMessage ( ) ) ) ) ; } catch ( java . lang . IndexOutOfBoundsException e ) { org . junit . Assert . assertNotNull ( e ) ; } }
public class aTest{ @Test public void testLexerPredsInCyclicDFA ( ) { java . lang . String grammar = "grammar<sp>foo;" + ( ( ( "@lexer::members<sp>{boolean<sp>p=false;}\n" + "a<sp>:<sp>(A|B)+<sp>;\n" ) + "A<sp>:<sp>{p}?<sp>(\'a\')+<sp>\'x\'<sp>{System.out.println(\"token<sp>1\");}<sp>;\n" ) + "B<sp>:<sp>(\'a\')+<sp>\'x\'<sp>{System.out.println(\"token<sp>2\");}<sp>;\n" ) ; java . lang . String found = execParser ( "token<sp>2\n" 1 , grammar , "token<sp>2\n" 0 , "fooLexer" , "a" , "aax" , false ) ; org . junit . Assert . assertEquals ( "token<sp>2\n" , found ) ; } }
public class aTest{ @Test public void testUpdateServiceRemoveOIDC ( ) { net . maritimecloud . identityregistry . model . database . entities . Service service = new net . maritimecloud . identityregistry . model . database . entities . Service ( ) ; service . setMrn ( "urn:mrn:mcl:service:instance:dma:nw-nm" ) ; service . setName ( "urn:mrn:mcl:service:instance:dma:nw-nm" 6 ) ; service . setInstanceVersion ( "0.3.4" ) ; service . setIdOrganization ( 1L ) ; java . lang . String serviceJson = serialize ( service ) ; net . maritimecloud . identityregistry . model . database . entities . Service oldService = new net . maritimecloud . identityregistry . model . database . entities . Service ( ) ; oldService . setMrn ( "urn:mrn:mcl:service:instance:dma:nw-nm" ) ; oldService . setName ( "urn:mrn:mcl:service:instance:dma:nw-nm" 6 ) ; oldService . setInstanceVersion ( "0.3.4" ) ; oldService . setIdOrganization ( 1L ) ; oldService . setOidcAccessType ( "bearer-only" ) ; oldService . setOidcClientId ( "0.3.4-urn:mrn:mcl:service:instance:dma:nw-nm" ) ; net . maritimecloud . identityregistry . model . database . Organization org = spy ( net . maritimecloud . identityregistry . model . database . Organization . class ) ; org . setMrn ( "urn:mrn:mcl:service:instance:dma:nw-nm" 0 ) ; org . setAddress ( "Carl<sp>Jakobsensvej<sp>31,<sp>2500<sp>Valby" ) ; org . setCountry ( "Denmark" ) ; org . setUrl ( "http://dma.dk" ) ; org . setEmail ( "urn:mrn:mcl:service:instance:dma:nw-nm" 2 ) ; org . setName ( "urn:mrn:mcl:service:instance:dma:nw-nm" 3 ) ; java . util . Set < net . maritimecloud . identityregistry . model . database . IdentityProviderAttribute > identityProviderAttributes = new java . util . HashSet ( ) ; org . setIdentityProviderAttributes ( identityProviderAttributes ) ; org . keycloak . adapters . springsecurity . token . KeycloakAuthenticationToken auth = net . maritimecloud . identityregistry . controllers . TokenGenerator . generateKeycloakToken ( "urn:mrn:mcl:service:instance:dma:nw-nm" 0 , "ROLE_SERVICE_ADMIN" , "urn:mrn:mcl:service:instance:dma:nw-nm" 4 ) ; given ( this . organizationService . getOrganizationByMrn ( "urn:mrn:mcl:service:instance:dma:nw-nm" 0 ) ) . willReturn ( org ) ; given ( ( ( net . maritimecloud . identityregistry . services . ServiceService ) ( this . entityService ) ) . getServiceByMrnAndVersion ( "urn:mrn:mcl:service:instance:dma:nw-nm" , "0.3.4" ) ) . willReturn ( oldService ) ; when ( org . getId ( ) ) . thenReturn ( 1L ) ; try { mvc . perform ( put ( "/oidc/api/org/urn:mrn:mcl:org:dma/service/urn:mrn:mcl:service:instance:dma:nw-nm/0.3.4" ) . with ( authentication ( auth ) ) . header ( "urn:mrn:mcl:service:instance:dma:nw-nm" 5 , "bla" ) . content ( serviceJson ) . contentType ( "urn:mrn:mcl:service:instance:dma:nw-nm" 1 ) ) . andExpect ( status ( ) . isOk ( ) ) ; } catch ( java . lang . Exception e ) { e . printStackTrace ( ) ; org . junit . Assert . assertTrue ( false ) ; } }
public class aTest{ @Test public void testCacheRule ( ) { com . facebook . buck . rules . keys . DefaultRuleKeyCache < java . lang . String > internalCache = new com . facebook . buck . rules . keys . DefaultRuleKeyCache ( ) ; com . facebook . buck . rules . keys . TrackedRuleKeyCache < java . lang . String > cache = new com . facebook . buck . rules . keys . TrackedRuleKeyCache ( internalCache , new com . facebook . buck . util . cache . InstrumentingCacheStatsTracker ( ) ) ; com . facebook . buck . rules . keys . DefaultRuleKeyCacheTest . TestRule rule = new com . facebook . buck . rules . keys . DefaultRuleKeyCacheTest . TestRule ( ) ; cache . get ( rule , ( r ) -> new com . facebook . buck . rules . keys . RuleKeyResult < > ( "" , com . google . common . collect . ImmutableList . of ( ) , com . google . common . collect . ImmutableList . of ( ) ) ) ; org . junit . Assert . assertTrue ( internalCache . isCached ( rule ) ) ; cache . get ( rule , ( r ) -> { throw new java . lang . IllegalStateException ( ) ; } }
public class aTest{ @Test public void searchPackage ( ) { org . springframework . ide . vscode . commons . protocol . STS4LanguageClient client = org . mockito . Mockito . mock ( org . springframework . ide . vscode . commons . protocol . STS4LanguageClient . class ) ; when ( client . javaSearchPackages ( any ( ) ) ) . thenReturn ( java . util . concurrent . CompletableFuture . supplyAsync ( ( ) -> { try { return java . util . Arrays . asList ( "org.spring.example" , "java.util" , "com.example" , "org.spring.data" , "com.another.example" , "org.example" ) ; } catch ( e ) { return null ; } } ) ) ; org . springframework . ide . vscode . commons . jdtls . JdtLsIndex index = new org . springframework . ide . vscode . commons . jdtls . JdtLsIndex ( client , java . net . URI . create ( java . lang . System . getProperty ( "java.io.tmpdir" ) ) , org . springframework . ide . vscode . commons . languageserver . java . ProjectObserver . NULL ) ; java . util . List < reactor . util . function . Tuple2 < java . lang . String , java . lang . Double > > results = index . fuzzySearchPackages ( "com.e" , true , false ) . collectSortedList ( ( o1 , o2 ) -> o2 . getT2 ( ) . compareTo ( o1 . getT2 ( ) ) ) . block ( ) ; java . util . List < java . lang . String > packages = results . stream ( ) . map ( ( t ) -> t . getT1 ( ) ) . collect ( java . util . stream . Collectors . toList ( ) ) ; org . junit . Assert . assertEquals ( java . util . Arrays . asList ( "com.example" , "com.another.example" ) , packages ) ; } }
public class aTest{ @Test public void testSelectSumGroupbyBaseTable ( ) { org . verdictdb . core . sqlobject . BaseTable base = new org . verdictdb . core . sqlobject . BaseTable ( "myschema" , "mytable" , "t" ) ; java . lang . String aliasName = ",<sp>" 3 ; org . verdictdb . core . sqlobject . SelectQuery relation = org . verdictdb . core . sqlobject . SelectQuery . create ( java . util . Arrays . < org . verdictdb . core . sqlobject . SelectItem > asList ( new org . verdictdb . core . sqlobject . AliasedColumn ( new org . verdictdb . core . sqlobject . BaseColumn ( "t" , "mygroup" ) , "mygroup" ) , new org . verdictdb . core . sqlobject . AliasedColumn ( new org . verdictdb . core . sqlobject . ColumnOp ( "sum" , new org . verdictdb . core . sqlobject . BaseColumn ( "t" , "mycolumn1" ) ) , aliasName ) ) , base ) ; relation . addGroupby ( new org . verdictdb . core . sqlobject . BaseColumn ( "t" , "mygroup" ) ) ; org . verdictdb . core . scrambling . ScrambleMetaSet meta = generateTestScrambleMeta ( ) ; org . verdictdb . core . rewriter . query . AggQueryRewriter rewriter = new org . verdictdb . core . rewriter . query . AggQueryRewriter ( meta ) ; java . util . List < org . apache . commons . lang3 . tuple . Pair < org . verdictdb . core . sqlobject . AbstractRelation , org . verdictdb . core . rewriter . query . AggblockMeta > > rewritten = rewriter . rewrite ( relation ) ; java . lang . String aliasForSumEstimate = org . verdictdb . core . rewriter . AliasRenamingRules . sumEstimateAliasName ( aliasName ) ; java . lang . String aliasForSumScaledSubsum = org . verdictdb . core . rewriter . AliasRenamingRules . sumScaledSumAliasName ( aliasName ) ; java . lang . String aliasForSumSquaredScaledSubsum = org . verdictdb . core . rewriter . AliasRenamingRules . sumSquaredScaledSumAliasName ( aliasName ) ; java . lang . String aliasForCountSubsample = org . verdictdb . core . rewriter . AliasRenamingRules . countSubsampleAliasName ( ) ; java . lang . String aliasForSumSubsampleSize = org . verdictdb . core . rewriter . AliasRenamingRules . sumSubsampleSizeAliasName ( ) ; for ( int k = 0 ; k < ( aggblockCount ) ; k ++ ) { java . lang . String expected = ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "select<sp>verdictdbalias4.`verdictdbalias5`<sp>as<sp>`verdictdb:tier`,<sp>" + ( "verdictdbalias4.`verdictdbalias6`<sp>as<sp>`mygroup`,<sp>" + ",<sp>" 9 ) ) + ( quoteAlias ( aliasForSumEstimate ) ) ) + ",<sp>" ) + ",<sp>" 4 ) + "mygroup" 4 ) + ( quoteAlias ( aliasForSumScaledSubsum ) ) ) + ",<sp>" ) + "mygroup" 8 ) + "mygroup" 4 ) + ( quoteAlias ( aliasForSumSquaredScaledSubsum ) ) ) + ",<sp>" ) + "count(*)<sp>as<sp>" ) + ( quoteAlias ( aliasForCountSubsample ) ) ) + ",<sp>" ) + ",<sp>" 2 ) + ( quoteAlias ( aliasForSumSubsampleSize ) ) ) + ",<sp>" 6 ) + "mygroup" 3 ) + "select<sp>verdictdbalias1.`verdictdbalias3`<sp>as<sp>`verdictdbalias5`,<sp>" ) + "mygroup" 9 ) + "mygroup" 2 ) + ",<sp>" 7 ) + "mygroup" 1 ) + "t.`verdictdbsid`<sp>as<sp>`verdictdbalias2`,<sp>" ) + ",<sp>" 0 ) + "from<sp>`myschema`.`mytable`<sp>as<sp>t<sp>" ) + ",<sp>" 5 ) + k ) + "mygroup" 6 ) + ",<sp>" 8 ) + "mygroup" 5 ; org . verdictdb . sqlwriter . SelectQueryToSql relToSql = new org . verdictdb . sqlwriter . SelectQueryToSql ( new org . verdictdb . sqlsyntax . HiveSyntax ( ) ) ; java . lang . String actual = relToSql . toSql ( rewritten . get ( k ) . getLeft ( ) ) ; org . junit . Assert . assertEquals ( expected , actual ) ; } } }
public class aTest{ @Test public void testBigMark ( ) { nom . tam . util . BufferedFile file = new nom . tam . util . BufferedFile ( "target/BufferedFileBigMark" , "rw" ) ; file . write ( new byte [ 10 ] ) ; file . close ( ) ; file = new nom . tam . util . BufferedFile ( "target/BufferedFileBigMark" , "rw" ) ; try { file . read ( ) ; long expectesd = file . getFilePointer ( ) ; file . mark ( 20 ) ; file . read ( ) ; file . reset ( ) ; org . junit . Assert . assertEquals ( expectesd , file . getFilePointer ( ) ) ; } }
public class aTest{ @Test public void testSchedule ( ) { org . csstudio . alarm . beast . notifier . WorkQueueUnitTest . actionMap = new java . util . concurrent . ConcurrentHashMap < java . lang . Integer , java . lang . Boolean > ( ) ; final org . csstudio . alarm . beast . notifier . WorkQueue workQueue = new org . csstudio . alarm . beast . notifier . WorkQueue ( 10 , 10000 ) ; final org . csstudio . alarm . beast . notifier . test . MockAlarmRDBHandler rdbHandler = new org . csstudio . alarm . beast . notifier . test . MockAlarmRDBHandler ( false ) ; try { final org . csstudio . alarm . beast . client . AlarmTreePV pv = rdbHandler . findPV ( UnitTestConstants . PV_NAME ) ; final org . csstudio . alarm . beast . notifier . ItemInfo info = org . csstudio . alarm . beast . notifier . ItemInfo . fromItem ( pv ) ; final org . csstudio . alarm . beast . client . AADataStructure aa = new org . csstudio . alarm . beast . client . AADataStructure ( "AA" , "smsto:fake" , 5 ) ; final org . csstudio . alarm . beast . notifier . ActionID id = org . csstudio . alarm . beast . notifier . util . NotifierUtils . getActionID ( pv , aa ) ; final org . csstudio . alarm . beast . notifier . model . IAutomatedAction newAction = new org . csstudio . alarm . beast . notifier . WorkQueueUnitTest . EmptyAction ( 0 ) ; final org . csstudio . alarm . beast . notifier . AlarmHandler newTask = new org . csstudio . alarm . beast . notifier . AlarmHandler ( id , info , newAction , aa . getDelay ( ) ) ; org . csstudio . alarm . beast . notifier . WorkQueueUnitTest . actionMap . put ( org . csstudio . alarm . beast . notifier . WorkQueueUnitTest . currentId , false ) ; workQueue . schedule ( newTask , false ) ; java . lang . Thread . sleep ( 5500 ) ; org . junit . Assert . assertTrue ( org . csstudio . alarm . beast . notifier . WorkQueueUnitTest . actionMap . get ( 0 ) ) ; } }
public class aTest{ @Test public void testAsyncCommunicationTimeout ( ) { final cz . etrzby . xml . TrzbaDataType data = getData ( 1 ) ; final cz . tomasdvorak . eet . client . EETClient eetClient = getService ( new cz . tomasdvorak . eet . client . dto . WebserviceConfiguration ( 1L ) ) ; final cz . etrzby . xml . TrzbaType request = eetClient . prepareFirstRequest ( data , CommunicationMode . REAL ) ; eetClient . sendAsync ( request , EndpointType . PLAYGROUND , new cz . tomasdvorak . eet . client . dto . ResponseCallback ( ) { @ cz . tomasdvorak . eet . client . Override public void onComplete ( final cz . tomasdvorak . eet . client . dto . SubmitResult result ) { org . junit . Assert . fail ( "Should<sp>be<sp>handled<sp>in<sp>onError<sp>method" ) ; } @ cz . tomasdvorak . eet . client . Override public void onError ( final cz . tomasdvorak . eet . client . exceptions . CommunicationException cause ) { org . junit . Assert . fail ( "Should<sp>be<sp>handled<sp>in<sp>onTimeout" ) ; } @ cz . tomasdvorak . eet . client . Override public void onTimeout ( final cz . tomasdvorak . eet . client . exceptions . CommunicationTimeoutException cause ) { org . junit . Assert . assertNotNull ( cause . getPKP ( ) ) ; } } }
public class aTest{ @Test public void test_matchUrlToPattern_fail_onelevel ( ) { urlMarcher = new com . webpagebytes . cms . engine . URLMatcher ( ) ; java . util . Set < java . lang . String > patterns = new java . util . HashSet < java . lang . String > ( ) ; patterns . add ( "/files/{**}" 3 ) ; patterns . add ( "/test" ) ; patterns . add ( "/xyz/abc" ) ; patterns . add ( "/111/222/333/" ) ; patterns . add ( "/files/{**}" 1 ) ; patterns . add ( "/{id}/" ) ; patterns . add ( "/files/{**}" 0 ) ; patterns . add ( "/news-{id}" ) ; patterns . add ( "/files/{**}" 2 ) ; patterns . add ( "/articles/{id}" ) ; patterns . add ( "/files/{**}" 4 ) ; patterns . add ( "/files/{**}" 6 ) ; patterns . add ( "/test_{keywords}-{key}/rest-{action}_{param}" ) ; patterns . add ( "/files/{**}" 5 ) ; patterns . add ( "/files/{**}" ) ; urlMarcher . initialize ( patterns , "xyz" ) ; com . webpagebytes . cms . engine . URLMatcherResult result = urlMarcher . matchUrlToPattern ( "/mysite" ) ; org . junit . Assert . assertTrue ( ( result == null ) ) ; } }
public class aTest{ @Test public void testPRAfterAccumulate ( ) { final java . lang . String str1 = ( ( ( ( ( ( ( ( ( ( ( ( ( "import<sp>" + ( org . drools . compiler . integrationtests . PropertyReactivityTest . Order . class . getCanonicalName ( ) ) ) + "\n" ) + "import<sp>" ) + ( org . drools . compiler . integrationtests . PropertyReactivityTest . OrderLine . class . getCanonicalName ( ) ) ) + "\n" ) + "rule<sp>R<sp>when\n" ) + "<sp>$o:<sp>Order($lines:<sp>orderLines)\n" ) + "<sp>Number(intValue<sp>>=<sp>15)<sp>from<sp>accumulate(\n" ) + "<sp>OrderLine($q:<sp>quantity)<sp>from<sp>$lines\n" ) + "<sp>,<sp>sum($q)\n" ) + "<sp>)\n" ) + "<sp>then\n" ) + "<sp>then\n" 0 ) + "end\n" ; final org . kie . api . runtime . KieSession ksession = new org . kie . internal . utils . KieHelper ( ) . addContent ( str1 , ResourceType . DRL ) . build ( ) . newKieSession ( ) ; org . drools . compiler . integrationtests . PropertyReactivityTest . Order order = new org . drools . compiler . integrationtests . PropertyReactivityTest . Order ( java . util . Arrays . asList ( new org . drools . compiler . integrationtests . PropertyReactivityTest . OrderLine ( 9 ) , new org . drools . compiler . integrationtests . PropertyReactivityTest . OrderLine ( 8 ) ) , 12 ) ; ksession . insert ( order ) ; ksession . fireAllRules ( ) ; org . junit . Assert . assertEquals ( 10 , order . getPrice ( ) ) ; } }
public class aTest{ @Test public void lists ( ) { try { java . lang . String query = "declare<sp>namespace<sp>list='java:java.util.ArrayList';<sp>" + ( ( "let<sp>$list<sp>:=<sp>list:new()<sp>" + "let<sp>$actions<sp>:=<sp>(list:add($list,'a'),list:add($list,'b'),list:add($list,'c'))<sp>" ) + "return<sp>list:get($list,1)" ) ; org . xmldb . api . base . ResourceSet result = org . exist . xquery . JavaFunctionsTest . existEmbeddedServer . executeQuery ( query ) ; java . lang . String r = ( ( java . lang . String ) ( result . getResource ( 0 ) . getContent ( ) ) ) ; org . junit . Assert . assertEquals ( "b" , r ) ; } }
public class aTest{ @Test public void toString_notEmpty ( ) { java . lang . Object obj = new java . lang . Object ( ) ; java . util . Date sentDate = new java . util . Date ( 0 ) ; initBuilder ( builder , obj , sentDate ) ; builder . addAddress ( MailAddressType . TO , _CHINA_EARTH_COM ) ; java . lang . String result = "" ; result += "<sp>TO<sp>=<sp>[\n" 6 ; result += "<sp>id<sp>=<sp>myid\n" ; result += "<sp>subject<sp>=<sp>my<sp>SUBJECT\n" ; result += "<sp>TO<sp>=<sp>[\n" 7 ; result += ( "<sp>TO<sp>=<sp>[\n" 0 + sentDate ) + "<sp>TO<sp>=<sp>[\n" 9 ; result += "<sp>FROM<sp>=<sp>=?euc-jp?B?w+a58Q==?=<sp><china@earth.com>\n" ; result += "<sp>TO<sp>=<sp>[\n" ; result += "<sp>TO<sp>=<sp>[\n" 3 ; result += "<sp>[2/2]<sp>=?euc-jp?B?yP658Q==?=<sp><us@earth.com>\n" ; result += "<sp>]\n" ; result += "<sp>TO<sp>=<sp>[\n" 5 ; result += "<sp>TO<sp>=<sp>[\n" 1 ; result += "<sp>TO<sp>=<sp>[\n" 4 ; result += "<sp>attributes<sp>=<sp>{\n" ; result += ( "<sp>[1/1]<sp>aaa<sp>=<sp>" + obj ) + "<sp>TO<sp>=<sp>[\n" 9 ; result += "<sp>TO<sp>=<sp>[\n" 2 ; result += ( "<sp>content<sp>=<sp>" + ( builder . getContent ( ) ) ) + "<sp>TO<sp>=<sp>[\n" 9 ; result += "<sp>TO<sp>=<sp>[\n" 8 ; org . junit . Assert . assertEquals ( result , builder . toString ( ) ) ; } }
public class aTest{ @Test public void testSelect ( ) { org . apache . ibatis . session . SqlSession sqlSession = tk . mybatis . mapper . mapper . MybatisHelper . getSqlSession ( ) ; try { tk . mybatis . mapper . mapper . UserLoginMapper mapper = sqlSession . getMapper ( tk . mybatis . mapper . mapper . UserLoginMapper . class ) ; tk . mybatis . mapper . model . UserLogin userLogin = new tk . mybatis . mapper . model . UserLogin ( ) ; userLogin . setUsername ( "test1" ) ; java . util . List < tk . mybatis . mapper . model . UserLogin > userLogins = mapper . select ( userLogin ) ; org . junit . Assert . assertEquals ( 5 , userLogins . size ( ) ) ; } }
public class aTest{ @Test public void testShutdown ( ) { try { org . apache . flink . runtime . instance . InstanceManager cm = new org . apache . flink . runtime . instance . InstanceManager ( ) ; cm . shutdown ( ) ; try { org . apache . flink . runtime . clusterframework . types . ResourceID resID = org . apache . flink . runtime . clusterframework . types . ResourceID . generate ( ) ; org . apache . flink . runtime . instance . HardwareDescription resources = org . apache . flink . runtime . instance . HardwareDescription . extractFromSystem ( 4096 ) ; java . net . InetAddress address = java . net . InetAddress . getByName ( "127.0.0.1" ) ; org . apache . flink . runtime . taskmanager . TaskManagerLocation ici = new org . apache . flink . runtime . taskmanager . TaskManagerLocation ( resID , address , 20000 ) ; akka . testkit . JavaTestKit probe = new akka . testkit . JavaTestKit ( org . apache . flink . runtime . instance . InstanceManagerTest . system ) ; cm . registerTaskManager ( new org . apache . flink . runtime . jobmanager . slots . ActorTaskManagerGateway ( new org . apache . flink . runtime . instance . AkkaActorGateway ( probe . getRef ( ) , org . apache . flink . runtime . instance . InstanceManagerTest . leaderSessionID ) ) , ici , resources , 1 ) ; org . junit . Assert . fail ( "Should<sp>raise<sp>exception<sp>in<sp>shutdown<sp>state" ) ; } catch ( java . lang . IllegalStateException e ) { } org . junit . Assert . assertFalse ( cm . reportHeartBeat ( new org . apache . flink . runtime . instance . InstanceID ( ) ) ) ; } }
public class aTest{ @Test public void testSigningClient ( ) { com . runabove . SigningTest . LOG . info ( "Signing<sp>Client<sp>test" ) ; com . runabove . client . SigningClient sc = new com . runabove . client . SigningClient ( new com . runabove . MockClient ( ) , null , null , null , new com . runabove . error . ExceptionHandler ( ) { public java . lang . Throwable handleError ( retrofit . RetrofitError arg0 ) { return null ; } public void handleException ( java . lang . Exception exception ) { } } ) ; try { retrofit . client . Response rs = sc . execute ( new retrofit . client . Request ( "GET" , "/test" , java . util . Collections . EMPTY_LIST , new retrofit . mime . TypedByteArray ( "application/json" , "" . getBytes ( ) ) ) ) ; org . junit . Assert . assertNotNull ( rs ) ; } }
public class aTest{ @Test public void testDebugPrint ( ) { final java . io . ByteArrayOutputStream out = new java . io . ByteArrayOutputStream ( ) ; final java . io . PrintStream outPrint = new java . io . PrintStream ( out ) ; final java . lang . String LABEL = "{" 0 ; final java . lang . String INDENT = "<sp>" ; outPrint . println ( ( LABEL + "<sp>=<sp>" ) ) ; outPrint . println ( "{" ) ; outPrint . println ( ( ( INDENT + "0<sp>=<sp>A<sp>" ) + ( java . lang . String . class . getName ( ) ) ) ) ; outPrint . println ( ( INDENT + "{" 1 ) ) ; outPrint . println ( ( INDENT + "{" ) ) ; outPrint . println ( ( ( ( INDENT + INDENT ) + "2<sp>=<sp>B<sp>" ) + ( java . lang . String . class . getName ( ) ) ) ) ; outPrint . println ( ( ( ( INDENT + INDENT ) + "3<sp>=<sp>C<sp>" ) + ( java . lang . String . class . getName ( ) ) ) ) ; outPrint . println ( ( ( INDENT + "}<sp>" ) + ( java . util . TreeMap . class . getName ( ) ) ) ) ; outPrint . println ( ( ( INDENT + "7<sp>=<sp>(this<sp>Map)<sp>" ) + ( java . util . TreeMap . class . getName ( ) ) ) ) ; outPrint . println ( ( "}<sp>" + ( java . util . TreeMap . class . getName ( ) ) ) ) ; final java . lang . String EXPECTED_OUT = out . toString ( ) ; out . reset ( ) ; final java . util . Map < java . lang . Integer , java . lang . String > inner = new java . util . TreeMap ( ) ; inner . put ( 2 , "B" ) ; inner . put ( 3 , "{" 2 ) ; final java . util . Map < java . lang . Integer , java . lang . Object > outer = new java . util . TreeMap ( ) ; outer . put ( 1 , inner ) ; outer . put ( 0 , "A" ) ; outer . put ( 7 , outer ) ; org . apache . commons . collections4 . MapUtils . debugPrint ( outPrint , "{" 0 , outer ) ; org . junit . Assert . assertEquals ( EXPECTED_OUT , out . toString ( ) ) ; } }
public class aTest{ @Test public void testGetPathForLocalization ( ) { org . apache . hadoop . fs . FileContext lfs = org . apache . hadoop . fs . FileContext . getLocalFSFileContext ( ) ; org . apache . hadoop . fs . Path base_path = new org . apache . hadoop . fs . Path ( "target" , org . apache . hadoop . yarn . server . nodemanager . containermanager . localizer . TestLocalResourcesTrackerImpl . class . getSimpleName ( ) ) ; final java . lang . String user = "someuser" ; final org . apache . hadoop . yarn . api . records . ApplicationId appId = org . apache . hadoop . yarn . api . records . ApplicationId . newInstance ( 1 , 1 ) ; org . apache . hadoop . conf . Configuration conf = new org . apache . hadoop . yarn . conf . YarnConfiguration ( ) ; org . apache . hadoop . yarn . event . DrainDispatcher dispatcher = null ; dispatcher = createDispatcher ( conf ) ; org . apache . hadoop . yarn . event . EventHandler < org . apache . hadoop . yarn . server . nodemanager . containermanager . localizer . event . LocalizerEvent > localizerEventHandler = mock ( org . apache . hadoop . yarn . event . EventHandler . class ) ; org . apache . hadoop . yarn . event . EventHandler < org . apache . hadoop . yarn . server . nodemanager . containermanager . localizer . event . LocalizerEvent > containerEventHandler = mock ( org . apache . hadoop . yarn . event . EventHandler . class ) ; dispatcher . register ( org . apache . hadoop . yarn . server . nodemanager . containermanager . localizer . event . LocalizerEventType . class , localizerEventHandler ) ; dispatcher . register ( org . apache . hadoop . yarn . server . nodemanager . containermanager . container . ContainerEventType . class , containerEventHandler ) ; org . apache . hadoop . yarn . server . nodemanager . recovery . NMStateStoreService stateStore = mock ( org . apache . hadoop . yarn . server . nodemanager . recovery . NMStateStoreService . class ) ; org . apache . hadoop . yarn . server . nodemanager . DeletionService delService = mock ( org . apache . hadoop . yarn . server . nodemanager . DeletionService . class ) ; try { org . apache . hadoop . yarn . server . nodemanager . containermanager . localizer . LocalResourceRequest req1 = createLocalResourceRequest ( user , 1 , 1 , LocalResourceVisibility . PUBLIC ) ; org . apache . hadoop . yarn . server . nodemanager . containermanager . localizer . LocalizedResource lr1 = createLocalizedResource ( req1 , dispatcher ) ; java . util . concurrent . ConcurrentMap < org . apache . hadoop . yarn . server . nodemanager . containermanager . localizer . LocalResourceRequest , org . apache . hadoop . yarn . server . nodemanager . containermanager . localizer . LocalizedResource > localrsrc = new java . util . concurrent . ConcurrentHashMap < org . apache . hadoop . yarn . server . nodemanager . containermanager . localizer . LocalResourceRequest , org . apache . hadoop . yarn . server . nodemanager . containermanager . localizer . LocalizedResource > ( ) ; localrsrc . put ( req1 , lr1 ) ; org . apache . hadoop . yarn . server . nodemanager . containermanager . localizer . LocalResourcesTrackerImpl tracker = new org . apache . hadoop . yarn . server . nodemanager . containermanager . localizer . LocalResourcesTrackerImpl ( user , appId , dispatcher , localrsrc , true , conf , stateStore , null ) ; org . apache . hadoop . fs . Path conflictPath = new org . apache . hadoop . fs . Path ( base_path , "10" ) ; org . apache . hadoop . fs . Path qualifiedConflictPath = lfs . makeQualified ( conflictPath ) ; lfs . mkdir ( qualifiedConflictPath , null , true ) ; org . apache . hadoop . fs . Path rPath = tracker . getPathForLocalization ( req1 , base_path , delService ) ; org . junit . Assert . assertFalse ( lfs . util ( ) . exists ( rPath ) ) ; verify ( delService , times ( 1 ) ) . delete ( eq ( user ) , eq ( conflictPath ) ) ; } }
public class aTest{ @Test public void testRetryMaxRetriesClassScopeConfig ( javax . servlet . http . HttpServletRequest , javax . servlet . http . HttpServletResponse ) { try { beanD . connectDMaxRetries2 ( ) ; org . junit . Assert . fail ( "Exception<sp>not<sp>thrown" ) ; } catch ( com . ibm . ws . microprofile . faulttolerance_fat . util . ConnectException e ) { } org . junit . Assert . assertThat ( beanD . getConnectCount ( ) , org . hamcrest . Matchers . is ( 5 ) ) ; } }
public class aTest{ @Test public void testGuestLogin ( ) { org . apache . jackrabbit . oak . api . ContentSession cs = login ( new javax . jcr . GuestCredentials ( ) ) ; try { org . apache . jackrabbit . oak . api . AuthInfo authInfo = cs . getAuthInfo ( ) ; java . lang . String anonymousID = org . apache . jackrabbit . oak . spi . security . user . util . UserUtil . getAnonymousId ( getUserConfiguration ( ) . getParameters ( ) ) ; org . junit . Assert . assertEquals ( anonymousID , authInfo . getUserID ( ) ) ; } }
public class aTest{ @Test public void testConsistentHashN ( ) { int replicas = 100 ; com . cloudera . util . consistenthash . ConsistentHash < java . lang . String > hash = new com . cloudera . util . consistenthash . ConsistentHash < java . lang . String > ( replicas , machines ) ; java . util . List < java . lang . String > orig = new java . util . ArrayList < java . lang . String > ( 20 ) ; java . lang . StringBuilder sb = new java . lang . StringBuilder ( ) ; com . cloudera . util . consistenthash . TestConsistentHash . LOG . info ( "Before:<sp>" ) ; for ( int i = 0 ; i < 20 ; i ++ ) { java . lang . String s = "this<sp>is<sp>a<sp>the<sp>key<sp>" + i ; java . util . List < java . lang . String > l = hash . getNBinsFor ( s , 3 ) ; java . lang . String bin = l . toString ( ) ; sb . append ( ( bin + ",<sp>" ) ) ; orig . add ( l . get ( 0 ) . toString ( ) ) ; } com . cloudera . util . consistenthash . TestConsistentHash . LOG . info ( sb . toString ( ) ) ; sb = new java . lang . StringBuilder ( ) ; int diffs = 0 ; com . cloudera . util . consistenthash . TestConsistentHash . LOG . info ( "after<sp>adding<sp>a<sp>machine:<sp>" ) ; hash . addBin ( "machine<sp>F" ) ; for ( int i = 0 ; i < 20 ; i ++ ) { java . lang . String s = "this<sp>is<sp>a<sp>the<sp>key<sp>" + i ; java . util . List < java . lang . String > l = hash . getNBinsFor ( s , 3 ) ; java . lang . String bin = l . toString ( ) ; sb . append ( ( bin + ",<sp>" ) ) ; if ( ! ( orig . get ( i ) . equals ( l . get ( 0 ) ) ) ) { diffs ++ ; } } com . cloudera . util . consistenthash . TestConsistentHash . LOG . info ( sb . toString ( ) ) ; com . cloudera . util . consistenthash . TestConsistentHash . LOG . info ( ( ( "Adding<sp>one<sp>caused<sp>" + diffs ) + "<sp>out<sp>of<sp>20<sp>to<sp>change" ) ) ; com . cloudera . util . consistenthash . TestConsistentHash . LOG . info ( "after<sp>adding<sp>a<sp>machine:<sp>" ) ; sb = new java . lang . StringBuilder ( ) ; hash . removeBin ( "machine<sp>F" ) ; for ( int i = 0 ; i < 20 ; i ++ ) { java . lang . String s = "this<sp>is<sp>a<sp>the<sp>key<sp>" + i ; java . lang . String bin = hash . getBinFor ( s ) ; sb . append ( ( bin + ",<sp>" ) ) ; org . junit . Assert . assertEquals ( orig . get ( i ) , bin ) ; } }
public class aTest{ @Test public void testTagProbDictionary ( ) { org . fastcatsearch . ir . dictionary . SetDictionary dictionary = new org . fastcatsearch . ir . dictionary . SetDictionary ( ) ; java . lang . String [ ] terms = new java . lang . String [ ] { "" , "LG" , "" } ; for ( java . lang . String term : terms ) { } java . io . ByteArrayOutputStream out = new java . io . ByteArrayOutputStream ( ) ; dictionary . writeTo ( out ) ; out . close ( ) ; for ( java . lang . String term : terms ) { boolean contains = dictionary . getUnmodifiableSet ( ) . contains ( new org . fastcatsearch . ir . io . CharVector ( term ) ) ; System . out . println ( ( ( ( "is<sp>set<sp>has<sp>term<sp>" + term ) + "<sp>?<sp>" ) + contains ) ) ; } byte [ ] buffer = out . toByteArray ( ) ; java . io . ByteArrayInputStream bais = new java . io . ByteArrayInputStream ( buffer ) ; org . fastcatsearch . ir . dictionary . SetDictionary dictionary2 = new org . fastcatsearch . ir . dictionary . SetDictionary ( bais , true ) ; bais . close ( ) ; for ( java . lang . String term : terms ) { boolean contains = dictionary2 . getUnmodifiableSet ( ) . contains ( new org . fastcatsearch . ir . io . CharVector ( term ) ) ; System . out . println ( ( ( ( "is<sp>set2<sp>has<sp>term<sp>" + term ) + "<sp>?<sp>" ) + contains ) ) ; } java . io . ByteArrayOutputStream out2 = new java . io . ByteArrayOutputStream ( ) ; dictionary2 . writeTo ( out2 ) ; out2 . close ( ) ; java . io . ByteArrayInputStream bais2 = new java . io . ByteArrayInputStream ( buffer ) ; org . fastcatsearch . ir . dictionary . SetDictionary dictionary3 = new org . fastcatsearch . ir . dictionary . SetDictionary ( bais2 , true ) ; bais2 . close ( ) ; for ( java . lang . String term : terms ) { boolean contains = dictionary3 . getUnmodifiableSet ( ) . contains ( new org . fastcatsearch . ir . io . CharVector ( term ) ) ; System . out . println ( ( ( ( "is<sp>set3<sp>has<sp>term<sp>" + term ) + "<sp>?<sp>" ) + contains ) ) ; } byte [ ] buffer2 = out2 . toByteArray ( ) ; org . junit . Assert . assertEquals ( buffer . length , buffer2 . length ) ; for ( int i = 0 ; i < ( buffer2 . length ) ; i ++ ) { System . out . println ( ( ( ( buffer [ i ] ) + ":" ) + ( buffer2 [ i ] ) ) ) ; if ( ( buffer [ i ] ) != ( buffer2 [ i ] ) ) { System . out . println ( ">>>>>>>>>>>>>>>>" ) ; } } }
public class aTest{ @Test public void testCount ( ) { org . apache . ibatis . session . SqlSession sqlSession = getSqlSession ( ) ; try { tk . mybatis . mapper . additional . aggregation . UserMapper mapper = sqlSession . getMapper ( tk . mybatis . mapper . additional . aggregation . UserMapper . class ) ; tk . mybatis . mapper . additional . aggregation . AggregateCondition aggregateCondition = tk . mybatis . mapper . additional . aggregation . AggregateCondition . builder ( ) . aggregateBy ( "id" ) . aliasName ( "total" ) . aggregateType ( AggregateType . COUNT ) . groupBy ( "role" ) ; tk . mybatis . mapper . entity . Example example = new tk . mybatis . mapper . entity . Example ( tk . mybatis . mapper . additional . aggregation . User . class ) ; java . util . List < tk . mybatis . mapper . additional . aggregation . User > m = mapper . selectAggregationByExample ( example , aggregateCondition ) ; org . junit . Assert . assertEquals ( 2 , m . size ( ) ) ; } }
public class aTest{ @Test public void testPRAfterAccumulate ( ) { final java . lang . String str = ( ( ( ( ( ( ( ( ( ( ( ( ( "import<sp>" + ( org . drools . modelcompiler . PropertyReactivityTest . Order . class . getCanonicalName ( ) ) ) + "\n" ) + "import<sp>" ) + ( org . drools . modelcompiler . PropertyReactivityTest . OrderLine . class . getCanonicalName ( ) ) ) + "\n" ) + "rule<sp>R<sp>when\n" ) + "<sp>$o:<sp>Order($lines:<sp>orderLines)\n" ) + "<sp>Number(intValue<sp>>=<sp>15)<sp>from<sp>accumulate(\n" ) + "<sp>OrderLine($q:<sp>quantity)<sp>from<sp>$lines\n" ) + "<sp>,<sp>sum($q)\n" ) + "<sp>)\n" ) + "<sp>then\n" ) + "<sp>then\n" 0 ) + "end\n" ; org . kie . api . runtime . KieSession ksession = getKieSession ( str ) ; org . drools . modelcompiler . PropertyReactivityTest . Order order = new org . drools . modelcompiler . PropertyReactivityTest . Order ( java . util . Arrays . asList ( new org . drools . modelcompiler . PropertyReactivityTest . OrderLine ( 9 ) , new org . drools . modelcompiler . PropertyReactivityTest . OrderLine ( 8 ) ) , 12 ) ; ksession . insert ( order ) ; ksession . fireAllRules ( ) ; org . junit . Assert . assertEquals ( 10 , order . getPrice ( ) ) ; } }
public class aTest{ @Test public void testGetStartTime ( ) { final long beforeStopWatch = java . lang . System . currentTimeMillis ( ) ; final org . apache . commons . lang3 . time . StopWatch watch = new org . apache . commons . lang3 . time . StopWatch ( ) ; try { watch . getStartTime ( ) ; org . junit . Assert . fail ( "Calling<sp>getStartTime<sp>on<sp>an<sp>unstarted<sp>StopWatch<sp>should<sp>throw<sp>an<sp>exception" ) ; } catch ( final java . lang . IllegalStateException expected ) { } watch . start ( ) ; try { watch . getStartTime ( ) ; org . junit . Assert . assertTrue ( ( ( watch . getStartTime ( ) ) >= beforeStopWatch ) ) ; } }
public class aTest{ @Test public void testGetAverageLengthWithBlankRows ( ) { try { org . talend . dq . dbms . DbmsLanguage dbms = getMysqlDbmsLanguage ( ) ; org . junit . Assert . assertNotNull ( dbms . getAverageLengthWithBlankRows ( ) ) ; } }
public class aTest{ @Test public void testAccept4 ( ) { org . eclipse . birt . data . engine . api . querydefn . FilterDefinition [ ] filterDefn = new org . eclipse . birt . data . engine . api . querydefn . FilterDefinition [ ] { new org . eclipse . birt . data . engine . api . querydefn . FilterDefinition ( new org . eclipse . birt . data . engine . api . querydefn . ConditionalExpression ( "row.ROW_COL0<sp>*<sp>row.ROW_COL1" , org . eclipse . birt . data . engine . api . querydefn . ConditionalExpression . OP_GT , "row.ROW_COL2" ) ) } ; org . eclipse . birt . data . engine . api . IResultIterator resultIterator = getResultIterator ( filterDefn , null , null , false ) ; while ( resultIterator . next ( ) ) { java . lang . Integer value0 = resultIterator . getInteger ( getBindingExpressionName ( ) [ 0 ] ) ; java . lang . Integer value1 = resultIterator . getInteger ( getBindingExpressionName ( ) [ 1 ] ) ; java . lang . Integer value2 = resultIterator . getInteger ( getBindingExpressionName ( ) [ 2 ] ) ; org . junit . Assert . assertTrue ( ( ( ( value0 . intValue ( ) ) * ( value1 . intValue ( ) ) ) > ( value2 . intValue ( ) ) ) ) ; } }
public class aTest{ @Test public void initCustom ( org . jboss . weld . tests . proxy . client . optimization . Custom ) { org . junit . Assert . assertNotNull ( custom ) ; custom . ping ( ) ; } }
public class aTest{ @Test public void fromScriptClasspath ( ) { java . lang . String customScriptName = "my-alluxio-locality.sh" ; java . io . File dir = mFolder . newFolder ( "fromScriptClasspath" ) ; org . powermock . reflect . Whitebox . invokeMethod ( java . lang . ClassLoader . getSystemClassLoader ( ) , "addURL" , dir . toURI ( ) . toURL ( ) ) ; java . io . File script = new java . io . File ( dir , customScriptName ) ; setupScript ( "node=myhost,rack=myrack,custom=mycustom" , script ) ; try ( java . io . Closeable c = new alluxio . ConfigurationRule ( com . google . common . collect . ImmutableMap . of ( PropertyKey . LOCALITY_ORDER , "node,rack,custom" , PropertyKey . LOCALITY_SCRIPT , customScriptName ) , mConfiguration ) . toResource ( ) ) { alluxio . wire . TieredIdentity identity = alluxio . network . TieredIdentityFactory . create ( mConfiguration ) ; alluxio . wire . TieredIdentity expected = new alluxio . wire . TieredIdentity ( java . util . Arrays . asList ( new alluxio . wire . TieredIdentity . LocalityTier ( "node" , "myhost" ) , new alluxio . wire . TieredIdentity . LocalityTier ( "rack" , "rack" 0 ) , new alluxio . wire . TieredIdentity . LocalityTier ( "custom" , "mycustom" ) ) ) ; org . junit . Assert . assertEquals ( expected , identity ) ; } }
public class aTest{ @Test public void testNoNewlineTemplate ( ) { java . lang . String template = "t(x)<sp>::=<sp><%\n" + ( ( ( ( ( ( ( ( "[<sp><if(!x)>" + "<endif>" 0 ) + "<x>\n" ) + "<endif>" ) + "\n" ) + "\n" ) + "]\n" ) + "\n" ) + "%>\n" ) ; org . stringtemplate . v4 . STGroup g = new org . stringtemplate . v4 . STGroupString ( template ) ; org . stringtemplate . v4 . ST st = g . getInstanceOf ( "t" ) ; st . add ( "x" , 99 ) ; java . lang . String expected = "[<sp>99]" ; java . lang . String result = st . render ( ) ; org . junit . Assert . assertEquals ( expected , result ) ; } }
public class aTest{ @Test public void testPendingReplicationRetry ( ) { org . apache . hadoop . hdfs . MiniDFSCluster cluster = null ; int numDataNodes = 4 ; java . lang . String testFile = "/replication-test-file" ; org . apache . hadoop . fs . Path testPath = new org . apache . hadoop . fs . Path ( testFile ) ; byte [ ] buffer = new byte [ 1024 ] ; for ( int i = 0 ; i < ( buffer . length ) ; i ++ ) { buffer [ i ] = '1' ; } try { org . apache . hadoop . conf . Configuration conf = new org . apache . hadoop . hdfs . HdfsConfiguration ( ) ; conf . set ( DFSConfigKeys . DFS_REPLICATION_KEY , java . lang . Integer . toString ( numDataNodes ) ) ; cluster = new org . apache . hadoop . hdfs . MiniDFSCluster . Builder ( conf ) . numDataNodes ( numDataNodes ) . build ( ) ; cluster . waitActive ( ) ; org . apache . hadoop . hdfs . DFSClient dfsClient = new org . apache . hadoop . hdfs . DFSClient ( new java . net . InetSocketAddress ( "localhost" , cluster . getNameNodePort ( ) ) , conf ) ; java . io . OutputStream out = cluster . getFileSystem ( ) . create ( testPath ) ; out . write ( buffer ) ; out . close ( ) ; waitForBlockReplication ( testFile , dfsClient . getNamenode ( ) , numDataNodes , ( - 1 ) ) ; org . apache . hadoop . hdfs . protocol . ExtendedBlock block = dfsClient . getNamenode ( ) . getBlockLocations ( testFile , 0 , Long . MAX_VALUE ) . get ( 0 ) . getBlock ( ) ; java . util . List < org . apache . hadoop . hdfs . server . datanode . FsDatasetTestUtils . MaterializedReplica > replicas = new java . util . ArrayList ( ) ; for ( int dnIndex = 0 ; dnIndex < 3 ; dnIndex ++ ) { replicas . add ( cluster . getMaterializedReplica ( dnIndex , block ) ) ; } org . junit . Assert . assertEquals ( 3 , replicas . size ( ) ) ; cluster . shutdown ( ) ; int fileCount = 0 ; for ( org . apache . hadoop . hdfs . server . datanode . FsDatasetTestUtils . MaterializedReplica replica : replicas ) { if ( fileCount == 0 ) { org . apache . hadoop . hdfs . TestReplication . LOG . info ( ( "Deleting<sp>block<sp>" + replica ) ) ; replica . deleteData ( ) ; } }
public class aTest{ @Test public void testProgramDiscovery ( ) { com . google . inject . Injector injector = com . google . inject . Guice . createInjector ( new io . cdap . cdap . common . guice . ConfigModule ( io . cdap . cdap . common . guice . ZKDiscoveryModuleTest . cConf ) , new io . cdap . cdap . common . guice . ZKClientModule ( ) , new io . cdap . cdap . common . guice . ZKDiscoveryModule ( ) ) ; org . apache . twill . zookeeper . ZKClientService zkClient = injector . getInstance ( org . apache . twill . zookeeper . ZKClientService . class ) ; zkClient . startAndWait ( ) ; try { io . cdap . cdap . proto . id . ProgramId programId = NamespaceId . DEFAULT . app ( "app" ) . service ( "service" ) ; java . lang . String twillNamespace = injector . getInstance ( io . cdap . cdap . common . conf . CConfiguration . class ) . get ( Constants . CFG_TWILL_ZK_NAMESPACE ) ; org . apache . twill . zookeeper . ZKClient twillZKClient = org . apache . twill . zookeeper . ZKClients . namespace ( zkClient , ( ( twillNamespace + "/" ) + ( io . cdap . cdap . common . twill . TwillAppNames . toTwillAppName ( programId ) ) ) ) ; java . lang . String discoverableName = io . cdap . cdap . common . service . ServiceDiscoverable . getName ( programId ) ; try ( org . apache . twill . discovery . ZKDiscoveryService twillDiscoveryService = new org . apache . twill . discovery . ZKDiscoveryService ( twillZKClient ) ) { java . net . InetSocketAddress socketAddr = new java . net . InetSocketAddress ( java . net . InetAddress . getLoopbackAddress ( ) , 43210 ) ; org . apache . twill . common . Cancellable cancellable = twillDiscoveryService . register ( new org . apache . twill . discovery . Discoverable ( discoverableName , socketAddr ) ) ; try { org . apache . twill . discovery . DiscoveryServiceClient discoveryServiceClient = injector . getInstance ( org . apache . twill . discovery . DiscoveryServiceClient . class ) ; org . apache . twill . discovery . Discoverable discoverable = new io . cdap . cdap . common . discovery . RandomEndpointStrategy ( ( ) -> discoveryServiceClient . discover ( discoverableName ) ) . pick ( 10 , TimeUnit . SECONDS ) ; org . junit . Assert . assertNotNull ( discoverable ) ; } }
public class aTest{ @Test public void testConfigurePortForwardingRulesOnLogicalRouter ( ) { resource . configure ( "NiciraNvpResource" , parameters ) ; final com . cloud . legacymodel . communication . command . ConfigurePortForwardingRulesOnLogicalRouterCommand cmd = mock ( com . cloud . legacymodel . communication . command . ConfigurePortForwardingRulesOnLogicalRouterCommand . class ) ; final com . cloud . legacymodel . to . PortForwardingRuleTO rule = new com . cloud . legacymodel . to . PortForwardingRuleTO ( 1 , "11.11.11.11" , 80 , 80 , "10.10.10.10" , 8080 , 8080 , "tcp" , false , false ) ; final java . util . List < com . cloud . legacymodel . to . PortForwardingRuleTO > rules = new java . util . ArrayList ( ) ; rules . add ( rule ) ; when ( cmd . getRules ( ) ) . thenReturn ( rules ) ; when ( cmd . getLogicalRouterUuid ( ) ) . thenReturn ( "aaaaa" ) ; final java . util . List < com . cloud . network . nicira . NatRule > storedRules = java . util . Collections . EMPTY_LIST ; when ( nvpApi . findNatRulesByLogicalRouterUuid ( "aaaaa" ) ) . thenReturn ( storedRules ) ; final com . cloud . network . nicira . NatRule [ ] rulepair = resource . generatePortForwardingRulePair ( "10.10.10.10" , new int [ ] { 8080 , 8080 } , "11.11.11.11" , new int [ ] { 80 , 80 } , "tcp" ) ; rulepair [ 0 ] . setUuid ( java . util . UUID . randomUUID ( ) ) ; rulepair [ 1 ] . setUuid ( java . util . UUID . randomUUID ( ) ) ; when ( nvpApi . createLogicalRouterNatRule ( eq ( "aaaaa" ) , ( ( com . cloud . network . nicira . NatRule ) ( any ( ) ) ) ) ) . thenReturn ( rulepair [ 0 ] ) . thenReturn ( rulepair [ 1 ] ) ; final com . cloud . legacymodel . communication . answer . ConfigurePortForwardingRulesOnLogicalRouterAnswer a = ( ( com . cloud . legacymodel . communication . answer . ConfigurePortForwardingRulesOnLogicalRouterAnswer ) ( resource . executeRequest ( cmd ) ) ) ; org . junit . Assert . assertTrue ( a . getResult ( ) ) ; verify ( nvpApi , atLeast ( 2 ) ) . createLogicalRouterNatRule ( eq ( "aaaaa" ) , argThat ( new org . mockito . ArgumentMatcher < com . cloud . network . nicira . NatRule > ( ) { @ com . cloud . network . resource . Override public boolean matches ( final java . lang . Object argument ) { final com . cloud . network . nicira . NatRule rule = ( ( com . cloud . network . nicira . NatRule ) ( argument ) ) ; if ( ( rule . getType ( ) . equals ( "DestinationNatRule" ) ) && ( ( ( com . cloud . network . nicira . DestinationNatRule ) ( rule ) ) . getToDestinationIpAddress ( ) . equals ( "10.10.10.10" ) ) ) { return true ; } }
public class aTest{ @Test public void sqlOperation ( ) { java . util . Properties props = new java . util . Properties ( ) ; jdk . incubator . sql2 . DataSourceFactory factory = jdk . incubator . sql2 . DataSourceFactory . newFactory ( com . oracle . adbaoverjdbc . test . FirstLight . FACTORY_NAME ) ; jdk . incubator . sql2 . DataSource ds = factory . builder ( ) . url ( com . oracle . adbaoverjdbc . test . FirstLight . URL ) . username ( com . oracle . adbaoverjdbc . test . FirstLight . USER ) . password ( com . oracle . adbaoverjdbc . test . FirstLight . PASSWORD ) . sessionProperty ( com . oracle . adbaoverjdbc . JdbcConnectionProperties . JDBC_CONNECTION_PROPERTIES , props ) . build ( ) ; jdk . incubator . sql2 . Session session = ds . getSession ( ( t ) -> System . out . println ( ( "ERROR:<sp>" + ( t . getMessage ( ) ) ) ) ) ; try { org . junit . Assert . assertNotNull ( session ) ; session . operation ( com . oracle . adbaoverjdbc . test . FirstLight . TRIVIAL ) . submit ( ) ; } }
public class aTest{ @Test public void testAddOrderShipmentComment ( ) { try { java . lang . Boolean result = runFlowAndGetPayload ( "add-order-shipment-comment" ) ; org . junit . Assert . assertTrue ( result ) ; } }
public class aTest{ @Test public void testGetNodeByUuid_SLAVE ( ) { java . lang . Long aleId = createPerson ( getOneSlaveDatabase ( ) , "AlessandroSlave" ) ; java . lang . String aleUuid = getUuidForNode ( getOneSlaveDatabase ( ) , aleId ) ; try ( ga . uuid . Transaction tx = getOneSlaveDatabase ( ) . beginTx ( ) ) { ga . uuid . Result result = getOneSlaveDatabase ( ) . execute ( ( ( "RETURN<sp>ga.uuid.findNode('" + aleUuid ) + "')<sp>as<sp>n" ) ) ; while ( result . hasNext ( ) ) { java . util . Map < java . lang . String , java . lang . Object > row = result . next ( ) ; ga . uuid . Node node = ( ( ga . uuid . Node ) ( row . get ( "n" ) ) ) ; org . junit . Assert . assertEquals ( aleId , node . getId ( ) , 0L ) ; } }
public class aTest{ @Test public void sessionIsRemoved2 ( ) { org . openqa . grid . internal . GridRegistry registry = org . openqa . grid . internal . DefaultGridRegistry . newInstance ( new org . openqa . grid . web . Hub ( new org . openqa . grid . internal . utils . configuration . GridHubConfiguration ( ) ) ) ; org . openqa . grid . internal . RemoteProxy p1 = new org . openqa . grid . internal . BaseRemoteProxy ( req , registry ) ; try { registry . add ( p1 ) ; org . openqa . grid . web . servlet . handler . RequestHandler newSessionRequest = org . openqa . grid . internal . mock . GridHelper . createNewSessionHandler ( registry , app1 ) ; newSessionRequest . process ( ) ; org . openqa . grid . internal . TestSession session = newSessionRequest . getSession ( ) ; ( ( org . openqa . grid . internal . DefaultGridRegistry ) ( registry ) ) . terminateSynchronousFOR_TEST_ONLY ( session ) ; org . junit . Assert . assertEquals ( 0 , registry . getActiveSessions ( ) . size ( ) ) ; } }
public class aTest{ @Test public void testReadAtIndex ( ) { final java . io . File file = createTempFile ( "testReadAtIndex" ) ; try { final net . openhft . chronicle . queue . impl . DirectChronicleQueue chronicle = createQueue ( file ) ; final net . openhft . chronicle . queue . ExcerptAppender appender = chronicle . acquireAppender ( ) ; for ( int i = 0 ; i < 100 ; i ++ ) { final int j = i ; appender . writeDocument ( ( wire ) -> wire . write ( ( ) -> "key" ) . text ( ( "value=" + j ) ) ) ; } final net . openhft . chronicle . queue . ExcerptTailer tailer = chronicle . createTailer ( ) ; final java . lang . StringBuilder sb = new java . lang . StringBuilder ( ) ; for ( int i = 0 ; i < 100 ; i ++ ) { tailer . index ( i ) ; sb . setLength ( 0 ) ; tailer . readDocument ( ( wire ) -> wire . read ( ( ) -> "key" ) . text ( sb ) ) ; org . junit . Assert . assertEquals ( ( "value=" + i ) , sb . toString ( ) ) ; } } }
public class aTest{ @Test public void testVerwerkLeesUitBrpVerzoekFout ( ) { when ( persoonslijstService . bevraagPersoonslijst ( nl . bzk . migratiebrp . synchronisatie . runtime . service . LeesUitBrpServiceTest . ANUMMER ) ) . thenThrow ( new java . lang . RuntimeException ( nl . bzk . migratiebrp . synchronisatie . runtime . service . LeesUitBrpServiceTest . TEST_DATABASE_FOUT ) ) ; final nl . bzk . migratiebrp . bericht . model . sync . impl . LeesUitBrpVerzoekBericht leesUitBrpVerzoekBericht = new nl . bzk . migratiebrp . bericht . model . sync . impl . LeesUitBrpVerzoekBericht ( nl . bzk . migratiebrp . synchronisatie . runtime . service . LeesUitBrpServiceTest . ANUMMER ) ; leesUitBrpVerzoekBericht . setMessageId ( java . util . UUID . randomUUID ( ) . toString ( ) ) ; try { leesUitBrpService . verwerkBericht ( leesUitBrpVerzoekBericht ) ; org . junit . Assert . fail ( "Er<sp>zou<sp>een<sp>fout<sp>op<sp>moeten<sp>treden." ) ; } catch ( final java . lang . Exception e ) { org . junit . Assert . assertNotNull ( "Er<sp>zou<sp>een<sp>fout<sp>op<sp>moeten<sp>treden." , e ) ; } }
public class aTest{ @Test public void testSearchSync ( ) { try { java . util . HashMap < java . lang . String , java . lang . String [ ] > params = new java . util . HashMap < java . lang . String , java . lang . String [ ] > ( ) ; params . put ( "limit" , new java . lang . String [ ] { "10" } ) ; java . lang . String jsonString = com . simplegeo . client . SimpleGeoStorageClientTest . client . search ( 37.761809 , ( - 122.422832 ) , "casey.testing.layer" , params ) ; com . simplegeo . client . types . FeatureCollection featureCollection = com . simplegeo . client . types . FeatureCollection . fromJSONString ( jsonString ) ; org . junit . Assert . assertNotNull ( featureCollection . getFeatures ( ) ) ; } }
public class aTest{ @Test public void testFailOnCorruptedZip ( ) { io . fabric8 . api . FabricService mockFabricService = org . mockito . Mockito . mock ( io . fabric8 . api . FabricService . class ) ; when ( mockFabricService . getMavenRepoUploadURI ( ) ) . thenReturn ( new java . net . URI ( "http://dummy" ) ) ; io . fabric8 . service . PatchServiceImpl patchService = new io . fabric8 . service . PatchServiceImpl ( mockFabricService ) ; io . fabric8 . api . Version version = org . mockito . Mockito . mock ( io . fabric8 . api . Version . class ) ; java . net . URL url = getClass ( ) . getClassLoader ( ) . getResource ( "corrupted_archive.zip" ) ; try { patchService . applyPatch ( version , url , "not_relevant" , "not_relevant" ) ; org . junit . Assert . fail ( "Expected<sp>PatchException<sp>has<sp>not<sp>been<sp>triggered." ) ; } catch ( io . fabric8 . api . PatchException e ) { org . junit . Assert . assertNotNull ( e ) ; e . printStackTrace ( ) ; } }
public class aTest{ @Test public void testInnerJoin2 ( ) { java . sql . ResultSet rs = conn . createStatement ( ) . executeQuery ( ( ( "select<sp>a.ia[0],<sp>b.ia[1]<sp>from<sp>a<sp>--splice-properties<sp>joinStrategy=" + ( joinStrategy ) ) + "<sp>9<sp>|<sp>9<sp>|" 1 ) ) ; java . lang . String s = TestUtils . FormattedResult . ResultFactory . toString ( rs ) ; rs . close ( ) ; java . lang . String expected = "<sp>9<sp>|<sp>9<sp>|" 0 + ( ( ( ( ( ( ( ( ( "--------\n" + "<sp>1<sp>|<sp>1<sp>|\n" ) + "<sp>2<sp>|<sp>2<sp>|\n" ) + "<sp>3<sp>|<sp>3<sp>|\n" ) + "<sp>4<sp>|<sp>4<sp>|\n" ) + "<sp>5<sp>|<sp>5<sp>|\n" ) + "<sp>9<sp>|<sp>9<sp>|" 2 ) + "<sp>7<sp>|<sp>7<sp>|\n" ) + "<sp>8<sp>|<sp>8<sp>|\n" ) + "<sp>9<sp>|<sp>9<sp>|" ) ; org . junit . Assert . assertEquals ( s , expected , s ) ; } }
public class aTest{ @Test public void testClientRetriesNonIdempotentOpWithIOExceptionFailsImmediately ( ) { org . apache . hadoop . conf . Configuration conf = new org . apache . hadoop . conf . Configuration ( ) ; final java . lang . String keyName = "test" ; conf . setInt ( CommonConfigurationKeysPublic . KMS_CLIENT_FAILOVER_MAX_RETRIES_KEY , 10 ) ; org . apache . hadoop . crypto . key . kms . KMSClientProvider p1 = mock ( org . apache . hadoop . crypto . key . kms . KMSClientProvider . class ) ; when ( p1 . createKey ( org . mockito . Mockito . anyString ( ) , org . mockito . Mockito . any ( org . apache . hadoop . crypto . key . KeyProvider . Options . class ) ) ) . thenThrow ( new java . io . IOException ( "p1" ) ) ; org . apache . hadoop . crypto . key . kms . KMSClientProvider p2 = mock ( org . apache . hadoop . crypto . key . kms . KMSClientProvider . class ) ; when ( p2 . createKey ( org . mockito . Mockito . anyString ( ) , org . mockito . Mockito . any ( org . apache . hadoop . crypto . key . KeyProvider . Options . class ) ) ) . thenThrow ( new java . io . IOException ( "p2" ) ) ; org . apache . hadoop . crypto . key . kms . KMSClientProvider p3 = mock ( org . apache . hadoop . crypto . key . kms . KMSClientProvider . class ) ; when ( p3 . createKey ( org . mockito . Mockito . anyString ( ) , org . mockito . Mockito . any ( org . apache . hadoop . crypto . key . KeyProvider . Options . class ) ) ) . thenThrow ( new java . io . IOException ( "p3" ) ) ; when ( p1 . getKMSUrl ( ) ) . thenReturn ( "p1" ) ; when ( p2 . getKMSUrl ( ) ) . thenReturn ( "p2" ) ; when ( p3 . getKMSUrl ( ) ) . thenReturn ( "p3" ) ; org . apache . hadoop . crypto . key . kms . LoadBalancingKMSClientProvider kp = new org . apache . hadoop . crypto . key . kms . LoadBalancingKMSClientProvider ( new org . apache . hadoop . crypto . key . kms . KMSClientProvider [ ] { p1 , p2 , p3 } , 0 , conf ) ; try { kp . createKey ( keyName , new org . apache . hadoop . crypto . key . KeyProvider . Options ( conf ) ) ; org . junit . Assert . fail ( "Should<sp>fail<sp>since<sp>all<sp>providers<sp>threw<sp>an<sp>IOException" ) ; } catch ( java . lang . Exception e ) { org . junit . Assert . assertTrue ( ( e instanceof java . io . IOException ) ) ; } }
public class aTest{ @Test public void testDalColumnMapRowMapperOne ( ) { try { com . ctrip . platform . dal . dao . helper . StatementParameters parameters = new com . ctrip . platform . dal . dao . helper . StatementParameters ( ) ; parameters . set ( 1 , Types . INTEGER , 3 ) ; com . ctrip . platform . dal . dao . helper . DalQueryDao dao = new com . ctrip . platform . dal . dao . helper . DalQueryDao ( com . ctrip . platform . dal . dao . helper . DalColumnMapRowMapperTest . DATABASE_NAME ) ; java . util . List < java . util . Map < java . lang . String , java . lang . Object > > result = dao . query ( sqlObject , parameters , hints , new com . ctrip . platform . dal . dao . helper . DalColumnMapRowMapper ( ) ) ; org . junit . Assert . assertEquals ( 1 , result . size ( ) ) ; } }
public class aTest{ @Test public void testConfigureStaticNatRulesOnLogicalRouter ( ) { resource . configure ( "NiciraNvpResource" , parameters ) ; final com . cloud . legacymodel . communication . command . ConfigureStaticNatRulesOnLogicalRouterCommand cmd = mock ( com . cloud . legacymodel . communication . command . ConfigureStaticNatRulesOnLogicalRouterCommand . class ) ; final com . cloud . legacymodel . to . StaticNatRuleTO rule = new com . cloud . legacymodel . to . StaticNatRuleTO ( 1 , "11.11.11.11" , null , null , "10.10.10.10" , null , null , null , false , false ) ; final java . util . List < com . cloud . legacymodel . to . StaticNatRuleTO > rules = new java . util . ArrayList ( ) ; rules . add ( rule ) ; when ( cmd . getRules ( ) ) . thenReturn ( rules ) ; when ( cmd . getLogicalRouterUuid ( ) ) . thenReturn ( "aaaaa" ) ; final java . util . List < com . cloud . network . nicira . NatRule > storedRules = java . util . Collections . EMPTY_LIST ; when ( nvpApi . findNatRulesByLogicalRouterUuid ( "aaaaa" ) ) . thenReturn ( storedRules ) ; final com . cloud . network . nicira . NatRule [ ] rulepair = resource . generateStaticNatRulePair ( "10.10.10.10" , "11.11.11.11" ) ; rulepair [ 0 ] . setUuid ( java . util . UUID . randomUUID ( ) ) ; rulepair [ 1 ] . setUuid ( java . util . UUID . randomUUID ( ) ) ; when ( nvpApi . createLogicalRouterNatRule ( eq ( "aaaaa" ) , ( ( com . cloud . network . nicira . NatRule ) ( any ( ) ) ) ) ) . thenReturn ( rulepair [ 0 ] ) . thenReturn ( rulepair [ 1 ] ) ; final com . cloud . legacymodel . communication . answer . ConfigureStaticNatRulesOnLogicalRouterAnswer a = ( ( com . cloud . legacymodel . communication . answer . ConfigureStaticNatRulesOnLogicalRouterAnswer ) ( resource . executeRequest ( cmd ) ) ) ; org . junit . Assert . assertTrue ( a . getResult ( ) ) ; verify ( nvpApi , atLeast ( 2 ) ) . createLogicalRouterNatRule ( eq ( "aaaaa" ) , argThat ( new org . mockito . ArgumentMatcher < com . cloud . network . nicira . NatRule > ( ) { @ com . cloud . network . resource . Override public boolean matches ( final java . lang . Object argument ) { final com . cloud . network . nicira . NatRule rule = ( ( com . cloud . network . nicira . NatRule ) ( argument ) ) ; if ( ( rule . getType ( ) . equals ( "DestinationNatRule" ) ) && ( ( ( com . cloud . network . nicira . DestinationNatRule ) ( rule ) ) . getToDestinationIpAddress ( ) . equals ( "10.10.10.10" ) ) ) { return true ; } }
public class aTest{ @Test public void testGetSetIdBadValues ( java . lang . String ) { org . candlepin . model . dto . ContentData dto = new org . candlepin . model . dto . ContentData ( ) ; java . lang . String output = dto . getId ( ) ; org . junit . Assert . assertNull ( output ) ; dto . setId ( input ) ; } }
public class aTest{ @Test public void testBug21159Values ( ) { System . out . println ( "lexicon-test2.xml" 1 ) ; java . lang . String [ ] filenames = new java . lang . String [ ] { "tuples-test1.xml" , "tuples-test2.xml" , "lexicon-test2.xml" 0 , "lexicon-test2.xml" 3 , "lexicon-test1.xml" , "lexicon-test2.xml" } ; com . marklogic . client . DatabaseClient client = getDatabaseClient ( "rest-admin" , "x" , getConnType ( ) ) ; com . marklogic . client . admin . ServerConfigurationManager srvMgr = client . newServerConfigManager ( ) ; srvMgr . readConfiguration ( ) ; srvMgr . setQueryOptionValidation ( true ) ; srvMgr . writeConfiguration ( ) ; for ( java . lang . String filename : filenames ) { writeDocumentUsingInputStreamHandle ( client , filename , "/raw-combined-query/" , "XML" ) ; } java . io . File file = new java . io . File ( "lexicon-test2.xml" 2 ) ; java . lang . String combinedQuery = convertFileToString ( file ) ; com . marklogic . client . query . RawCombinedQueryDefinition rawCombinedQueryDefinition ; com . marklogic . client . query . QueryManager queryMgr = client . newQueryManager ( ) ; rawCombinedQueryDefinition = queryMgr . newRawCombinedQueryDefinition ( new com . marklogic . client . io . StringHandle ( combinedQuery ) . withMimetype ( "application/xml" ) ) ; com . marklogic . client . io . StringHandle stringResults = null ; com . marklogic . client . query . ValuesDefinition vdef = queryMgr . newValuesDefinition ( "n-way" ) ; vdef . setQueryDefinition ( rawCombinedQueryDefinition ) ; stringResults = queryMgr . tuples ( vdef , new com . marklogic . client . io . StringHandle ( ) ) ; System . out . println ( stringResults . get ( ) ) ; com . marklogic . client . io . TuplesHandle tuplesResults = queryMgr . tuples ( vdef , new com . marklogic . client . io . TuplesHandle ( ) ) ; com . marklogic . client . query . Tuple [ ] tuples = tuplesResults . getTuples ( ) ; org . junit . Assert . assertNotNull ( tuples ) ; client . release ( ) ; } }
public class aTest{ @Test public void testLabeledStringRoot ( ) { java . lang . String grammar = "grammar<sp>T;\n" + ( ( ( ( "options<sp>{output=AST;}\n" + "a<sp>:<sp>v=\'void\'^<sp>ID<sp>\';\'<sp>;\n" ) + "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" ) + "INT<sp>:<sp>\'0\'..\'9\'+;\n" ) + "WS<sp>:<sp>(\'<sp>\'|\'\\n\')<sp>{$channel=HIDDEN;}<sp>;\n" ) ; java . lang . String found = execParser ( "a" 1 , grammar , "TParser" , "TLexer" , "a" , "void<sp>foo;" , debug ) ; org . junit . Assert . assertEquals ( "a" 0 , found ) ; } }
public class aTest{ @Test public void testExtractTokenFail ( ) { java . net . HttpURLConnection conn = org . mockito . Mockito . mock ( java . net . HttpURLConnection . class ) ; org . mockito . Mockito . when ( conn . getResponseCode ( ) ) . thenReturn ( HttpURLConnection . HTTP_UNAUTHORIZED ) ; java . lang . String tokenStr = "foo" ; java . util . Map < java . lang . String , java . util . List < java . lang . String > > headers = new java . util . HashMap < java . lang . String , java . util . List < java . lang . String > > ( ) ; java . util . List < java . lang . String > cookies = new java . util . ArrayList < java . lang . String > ( ) ; cookies . add ( ( ( ( AuthenticatedURL . AUTH_COOKIE ) + "=" ) + tokenStr ) ) ; headers . put ( "Set-Cookie" , cookies ) ; org . mockito . Mockito . when ( conn . getHeaderFields ( ) ) . thenReturn ( headers ) ; com . hortonworks . registries . auth . client . AuthenticatedURL . Token token = new com . hortonworks . registries . auth . client . AuthenticatedURL . Token ( ) ; token . set ( "bar" ) ; try { com . hortonworks . registries . auth . client . AuthenticatedURL . extractToken ( conn , token ) ; org . junit . Assert . fail ( ) ; } catch ( com . hortonworks . registries . auth . client . AuthenticationException ex ) { org . junit . Assert . assertFalse ( token . isSet ( ) ) ; } }
public class aTest{ @Test public void SvgCssResolverStyleTagTest ( ) { com . itextpdf . styledxmlparser . jsoup . nodes . Element styleTag = new com . itextpdf . styledxmlparser . jsoup . nodes . Element ( com . itextpdf . styledxmlparser . jsoup . parser . Tag . valueOf ( "style" ) , "\t}\n" 2 ) ; com . itextpdf . styledxmlparser . jsoup . nodes . TextNode styleContents = new com . itextpdf . styledxmlparser . jsoup . nodes . TextNode ( ( "\t}\n" 1 + ( ( ( ( ( "\t}\n" 3 + "\t\tstroke-width:1.76388889;\n" ) + "\t}\n" 4 ) + "\t}\n" 0 ) + "\t}\n" ) + "<sp>" ) ) , "\t}\n" 2 ) ; com . itextpdf . styledxmlparser . node . impl . jsoup . node . JsoupElementNode jSoupStyle = new com . itextpdf . styledxmlparser . node . impl . jsoup . node . JsoupElementNode ( styleTag ) ; jSoupStyle . addChild ( new com . itextpdf . styledxmlparser . node . impl . jsoup . node . JsoupTextNode ( styleContents ) ) ; com . itextpdf . styledxmlparser . jsoup . nodes . Element ellipse = new com . itextpdf . styledxmlparser . jsoup . nodes . Element ( com . itextpdf . styledxmlparser . jsoup . parser . Tag . valueOf ( "ellipse" ) , "\t}\n" 2 ) ; com . itextpdf . styledxmlparser . node . impl . jsoup . node . JsoupElementNode jSoupEllipse = new com . itextpdf . styledxmlparser . node . impl . jsoup . node . JsoupElementNode ( ellipse ) ; com . itextpdf . svg . processors . impl . SvgProcessorContext context = new com . itextpdf . svg . processors . impl . SvgProcessorContext ( new com . itextpdf . svg . processors . impl . SvgConverterProperties ( ) ) ; com . itextpdf . svg . css . impl . SvgStyleResolver resolver = new com . itextpdf . svg . css . impl . SvgStyleResolver ( jSoupStyle , context ) ; com . itextpdf . styledxmlparser . css . resolve . AbstractCssContext svgContext = new com . itextpdf . svg . css . SvgCssContext ( ) ; java . util . Map < java . lang . String , java . lang . String > actual = resolver . resolveStyles ( jSoupEllipse , svgContext ) ; java . util . Map < java . lang . String , java . lang . String > expected = new java . util . HashMap ( ) ; expected . put ( "stroke-width" , "1.76388889" ) ; expected . put ( "stroke" , "#da0000" ) ; expected . put ( "stroke-opacity" , "\t}\n" 5 ) ; org . junit . Assert . assertEquals ( expected , actual ) ; } }
public class aTest{ @Test public void testSmoke ( ) { final com . google . appengine . api . datastore . DatastoreService ds = com . google . appengine . api . datastore . DatastoreServiceFactory . getDatastoreService ( ) ; com . google . appengine . api . datastore . Key key = ds . put ( new com . google . appengine . api . datastore . Entity ( "NsTest" ) ) ; try { org . junit . Assert . assertNotNull ( ds . get ( key ) ) ; } }
public class aTest{ @Test public void testNestedIfAllReturning ( ) { testSegment ( new spoon . processing . AbstractProcessor < spoon . reflect . code . CtIf > ( ) { @ fr . inria . controlflow . Override public void process ( spoon . reflect . code . CtIf element ) { spoon . reflect . declaration . CtMethod m = element . getParent ( ) . getParent ( spoon . reflect . declaration . CtMethod . class ) ; if ( ( m != null ) && ( m . getSimpleName ( ) . equals ( "nestedIfAllReturning" ) ) ) if ( element . getCondition ( ) . toString ( ) . contains ( "a<sp>><sp>0" ) ) { fr . inria . controlflow . AllBranchesReturn alg = new fr . inria . controlflow . AllBranchesReturn ( ) ; org . junit . Assert . assertTrue ( alg . execute ( element ) ) ; } } } }
public class aTest{ @Test public void setIPTest ( ) { java . lang . String newIp = "[ERROR]<sp>Configuration<sp>for<sp>Loopback<sp>interface<sp>not<sp>allowed" 0 ; java . lang . String newMask = "255.255.255.0" ; java . lang . String inter = "lt-0/1/2" ; java . lang . String subport = "12" ; testingMethod ( inter , subport , newIp , newMask ) ; newIp = "192.168.1.4" ; newMask = "255.255.255.0" ; inter = "fe-0/1/3" ; subport = "0" ; testingMethod ( inter , subport , newIp , newMask ) ; java . util . List < java . lang . String > response = executeCommand ( ( ( "ip:setIP<sp>" + ( resourceFriendlyID ) ) + "<sp>lo0.1<sp>192.168.1.1/24" ) ) ; org . junit . Assert . assertTrue ( response . get ( 1 ) . contains ( "[ERROR]<sp>Configuration<sp>for<sp>Loopback<sp>interface<sp>not<sp>allowed" ) ) ; } }
public class aTest{ @Test public void testSameGrantTwice ( ) { runTestAsSubject ( new org . apache . sentry . provider . db . generic . service . thrift . TestOperation ( ) { @ org . apache . sentry . provider . db . generic . service . thrift . Override public void runTestAsSubject ( ) throws org . apache . sentry . provider . db . generic . service . thrift . Exception { java . lang . String requestorUserName = ADMIN_USER ; java . util . Set < java . lang . String > requestorUserGroupNames = com . google . common . collect . Sets . newHashSet ( org . apache . sentry . provider . db . generic . service . thrift . ADMIN_GROUP ) ; setLocalGroupMapping ( requestorUserName , requestorUserGroupNames ) ; writePolicyFile ( ) ; java . lang . String roleName = "admin_r1" ; client . createRole ( requestorUserName , roleName , org . apache . sentry . provider . db . generic . service . thrift . SOLR ) ; org . apache . sentry . provider . db . generic . service . thrift . TSentryPrivilege queryPrivilege = new org . apache . sentry . provider . db . generic . service . thrift . TSentryPrivilege ( SOLR , "service1" , fromAuthorizable ( java . util . Arrays . asList ( new org . apache . sentry . core . model . search . Collection ( "c1" ) , new org . apache . sentry . core . model . search . Field ( "f1" ) ) ) , org . apache . sentry . core . model . search . SearchConstants . QUERY ) ; client . grantPrivilege ( requestorUserName , roleName , org . apache . sentry . provider . db . generic . service . thrift . SOLR , queryPrivilege ) ; org . junit . Assert . assertEquals ( 1 , client . listPrivilegesByRoleName ( requestorUserName , roleName , org . apache . sentry . provider . db . generic . service . thrift . SOLR , "service1" ) . size ( ) ) ; } } }
public class aTest{ @Test public void freeException ( ) { alluxio . AlluxioURI file = new alluxio . AlluxioURI ( "/file" ) ; alluxio . grpc . FreePOptions freeOptions = alluxio . grpc . FreePOptions . newBuilder ( ) . setRecursive ( true ) . build ( ) ; doThrow ( alluxio . client . file . BaseFileSystemTest . EXCEPTION ) . when ( mFileSystemMasterClient ) . free ( file , alluxio . util . FileSystemOptions . freeDefaults ( mConf ) . toBuilder ( ) . mergeFrom ( freeOptions ) . build ( ) ) ; try { mFileSystem . free ( file , freeOptions ) ; org . junit . Assert . fail ( alluxio . client . file . BaseFileSystemTest . SHOULD_HAVE_PROPAGATED_MESSAGE ) ; } catch ( java . lang . Exception e ) { org . junit . Assert . assertSame ( alluxio . client . file . BaseFileSystemTest . EXCEPTION , e ) ; } }
public class aTest{ @Test public void testGetInstanceString ( ) { try { com . fit2cloud . aliyun . ecs . model . response . GetInstanceResponse response = client . getInstance ( "i-25uh6z38k" ) ; System . out . println ( ( "testGetInstanceString<sp>::<sp>" + response ) ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testDeleteInstanceString ( ) { try { com . fit2cloud . aliyun . Response response = client . deleteInstance ( "i-25uh6z38k" ) ; System . out . println ( ( "testDeleteInstanceString<sp>::<sp>" + response ) ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testDeleteDiskString ( ) { try { com . fit2cloud . aliyun . Response response = client . deleteDisk ( "d-2583k8j4o" ) ; System . out . println ( ( "testDeleteDiskString<sp>::<sp>" + response ) ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testExecutorsetAgain ( ) { org . hyperledger . fabric . sdk . ClientTest . hfclient = org . hyperledger . fabric . sdk . TestHFClient . newInstance ( ) ; java . util . concurrent . ThreadPoolExecutor threadPoolExecutor = new java . util . concurrent . ThreadPoolExecutor ( 10 , 100 , 40 , java . util . concurrent . TimeUnit . valueOf ( "MILLISECONDS" ) , new java . util . concurrent . SynchronousQueue < java . lang . Runnable > ( ) , ( r ) -> { java . lang . Thread t = java . util . concurrent . Executors . defaultThreadFactory ( ) . newThread ( r ) ; t . setDaemon ( true ) ; return t ; } ) ; org . hyperledger . fabric . sdk . ClientTest . hfclient . setExecutorService ( threadPoolExecutor ) ; org . junit . Assert . assertSame ( threadPoolExecutor , org . hyperledger . fabric . sdk . ClientTest . hfclient . getExecutorService ( ) ) ; java . util . concurrent . ThreadPoolExecutor threadPoolExecutor2 = new java . util . concurrent . ThreadPoolExecutor ( 10 , 100 , 40 , java . util . concurrent . TimeUnit . valueOf ( "MILLISECONDS" ) , new java . util . concurrent . SynchronousQueue < java . lang . Runnable > ( ) , ( r ) -> { java . lang . Thread t = java . util . concurrent . Executors . defaultThreadFactory ( ) . newThread ( r ) ; t . setDaemon ( true ) ; return t ; } }
public class aTest{ @Test public void Range ( ) { try { java . lang . String body = "abcdefghijklmn" ; final com . fujitsu . dc . test . utils . Http theReq = this . putFileRequest ( com . fujitsu . dc . test . jersey . box . dav . file . DavFileTest . FILE_NAME , body , null , Setup . TEST_BOX1 ) ; theReq . returns ( ) . statusCode ( HttpStatus . SC_CREATED ) ; java . lang . String rangeHeader = "bytes=a-b,1-2" ; com . fujitsu . dc . test . utils . TResponse getResp = this . getFileRequestAtRange ( com . fujitsu . dc . test . jersey . box . dav . file . DavFileTest . FILE_NAME , com . fujitsu . dc . test . jersey . box . dav . file . DavFileTest . TEST_BOX1 , rangeHeader ) . returns ( ) ; getResp . statusCode ( HttpStatus . SC_OK ) ; org . junit . Assert . assertEquals ( body , getResp . getBody ( ) ) ; } }
public class aTest{ @Test public void When_adding_component_mapper_Should_update_get_before_processing ( ) { execute ( new com . artemis . EntityComponentLifecycleIntegrationTest . LifecycleTestingSystem ( ) { private int entityId ; @ com . artemis . Override protected void processSystem ( ) { if ( ( timesProcessed ) == 0 ) { entityId = world . create ( ) ; com . artemis . common . LifecycleComponent c = mLife . create ( entityId ) ; org . junit . Assert . assertEquals ( c , mLife . get ( entityId ) ) ; done = true ; } } } }
public class aTest{ @Test public void testLongLineChunking ( ) { final org . apache . commons . cli . Options options = new org . apache . commons . cli . Options ( ) ; options . addOption ( "This<sp>description<sp>has<sp>ReallyLongValuesThatAreLongerThanTheWidthOfTheColumns<sp>" 6 , "-x,--extralongarg<sp>This" 1 , false , ( "This<sp>description<sp>has<sp>ReallyLongValuesThatAreLongerThanTheWidthOfTheColumns<sp>" + ( "This<sp>description<sp>has<sp>ReallyLongValuesThatAreLongerThanTheWidthOfTheColumns<sp>" 3 + "This<sp>description<sp>has<sp>ReallyLongValuesThatAreLongerThanTheWidthOfTheColumns<sp>" 4 ) ) ) ; formatter . printHelp ( new java . io . PrintWriter ( sw ) , 35 , this . getClass ( ) . getName ( ) , "Header" , options , 0 , 5 , "Footer" ) ; final java . lang . String expected = ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "usage:" + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "This<sp>description<sp>has<sp>ReallyLongValuesThatAreLongerThanTheWidthOfTheColumns<sp>" 7 ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "<sp>ugCLI162Test" ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "Header" ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "-x,--extralongarg<sp>This" ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "This<sp>description<sp>has<sp>ReallyLongValuesThatAreLongerThanTheWidthOfTheColumns<sp>" 0 ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "-x,--extralongarg<sp>This" 2 ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "This<sp>description<sp>has<sp>ReallyLongValuesThatAreLongerThanTheWidthOfTheColumns<sp>" 5 ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "This<sp>description<sp>has<sp>ReallyLongValuesThatAreLongerThanTheWidthOfTheColumns<sp>" 1 ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "-x,--extralongarg<sp>This" 4 ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "This<sp>description<sp>has<sp>ReallyLongValuesThatAreLongerThanTheWidthOfTheColumns<sp>" 2 ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "<sp>s<sp>and<sp>also" ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "<sp>other" ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "This<sp>description<sp>has<sp>ReallyLongValuesThatAreLongerThanTheWidthOfTheColumns<sp>" 5 ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "This<sp>description<sp>has<sp>ReallyLongValuesThatAreLongerThanTheWidthOfTheColumns<sp>" 8 ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "-x,--extralongarg<sp>This" 3 ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "-x,--extralongarg<sp>This" 0 ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "<sp>heColumnsBob," ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "<sp>yes." ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ) + "Footer" ) + ( org . apache . commons . cli . bug . BugCLI162Test . CR ) ; org . junit . Assert . assertEquals ( "This<sp>description<sp>has<sp>ReallyLongValuesThatAreLongerThanTheWidthOfTheColumns<sp>" 9 , expected , sw . toString ( ) ) ; } }
public class aTest{ @Test public void test_parses_correctly_formatted_changelog ( ) { java . lang . String [ ] lines = new java . lang . String [ ] { "*<sp>Tue<sp>Feb<sp>24<sp>2015<sp>George<sp>Washington" , "Lorem<sp>ipsum<sp>dolor<sp>sit<sp>amet,<sp>consectetur<sp>adipiscing<sp>elit,<sp>sed<sp>do<sp>eiusmod<sp>tempor<sp>incididunt<sp>ut<sp>labore<sp>et<sp>dolore<sp>magna<sp>aliqua" , "*<sp>Tue<sp>Feb<sp>10<sp>2015<sp>George<sp>Washington" , "quis<sp>nostrud<sp>exercitation<sp>ullamco<sp>laboris<sp>nisi<sp>ut<sp>aliquip<sp>ex<sp>ea<sp>commodo<sp>consequat." } ; try { changelogs = parser . parse ( lines ) ; org . junit . Assert . assertEquals ( "parses<sp>correctly<sp>formatted<sp>Changelog" , 2 , changelogs . size ( ) ) ; } }
public class aTest{ @Test public void testInsertList ( ) { org . apache . ibatis . session . SqlSession sqlSession = getSqlSession ( ) ; try { tk . mybatis . simple . mapper . UserMapper userMapper = sqlSession . getMapper ( tk . mybatis . simple . mapper . UserMapper . class ) ; java . util . List < tk . mybatis . simple . model . SysUser > userList = new java . util . ArrayList < tk . mybatis . simple . model . SysUser > ( ) ; for ( int i = 0 ; i < 2 ; i ++ ) { tk . mybatis . simple . model . SysUser user = new tk . mybatis . simple . model . SysUser ( ) ; user . setUserName ( ( "test" + i ) ) ; user . setUserPassword ( "123456" ) ; user . setUserEmail ( "test@mybatis.tk" ) ; userList . add ( user ) ; } int result = userMapper . insertList ( userList ) ; org . junit . Assert . assertEquals ( 2 , result ) ; for ( tk . mybatis . simple . model . SysUser user : userList ) { System . out . println ( user . getId ( ) ) ; } } }
public class aTest{ @Test public void testSemPred ( ) { java . lang . String grammar = "grammar<sp>T;\n" + ( ( ( ( ( "s<sp>:<sp>a<sp>{System.out.println($a.text);}<sp>;\n" + "a<sp>:<sp>a<sp>{true}?<sp>ID\n" ) + "<sp>|<sp>ID" ) + "<sp>;\n" ) + "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" ) + "WS<sp>:<sp>(\'<sp>\'|\'\\n\')<sp>{skip();}<sp>;\n" ) ; java . lang . String found = execParser ( "a<sp>:<sp>a<sp>{true}?<sp>ID\n" 2 , grammar , "TParser" , "a<sp>:<sp>a<sp>{true}?<sp>ID\n" 0 , "a<sp>:<sp>a<sp>{true}?<sp>ID\n" 1 , "a<sp>b<sp>c" , debug ) ; java . lang . String expecting = "abc\n" ; org . junit . Assert . assertEquals ( expecting , found ) ; } }
public class aTest{ @Test public void testJceAesCtrCryptoCodec ( ) { org . apache . hadoop . test . GenericTestUtils . assumeInNativeProfile ( ) ; if ( ! ( org . apache . hadoop . util . NativeCodeLoader . buildSupportsOpenssl ( ) ) ) { org . apache . hadoop . crypto . TestCryptoCodec . LOG . warn ( "Skipping<sp>test<sp>since<sp>openSSL<sp>library<sp>not<sp>loaded" ) ; org . junit . Assume . assumeTrue ( false ) ; } org . junit . Assert . assertEquals ( null , org . apache . hadoop . crypto . OpensslCipher . getLoadingFailureReason ( ) ) ; cryptoCodecTest ( conf , seed , 0 , jceCodecClass , jceCodecClass , org . apache . hadoop . crypto . TestCryptoCodec . iv ) ; cryptoCodecTest ( conf , seed , count , jceCodecClass , jceCodecClass , org . apache . hadoop . crypto . TestCryptoCodec . iv ) ; cryptoCodecTest ( conf , seed , count , jceCodecClass , opensslCodecClass , org . apache . hadoop . crypto . TestCryptoCodec . iv ) ; for ( int i = 0 ; i < 8 ; i ++ ) { org . apache . hadoop . crypto . TestCryptoCodec . iv [ ( 8 + i ) ] = ( ( byte ) ( 255 ) ) ; } }
public class aTest{ @Test public void testReviewCmdBasicAllFiles ( ) { try { net . sourceforge . pmd . eclipse . EclipseUtils . createTestSourceFile ( testProject , "/src/Test.js" , "function(arg)<sp>{<sp>notDeclaredVariable<sp>=<sp>1;<sp>}" ) ; testProject . refreshLocal ( org . eclipse . core . resources . IResource . DEPTH_INFINITE , null ) ; net . sourceforge . pmd . eclipse . plugin . PMDPlugin . getDefault ( ) . loadPreferences ( ) . setDetermineFiletypesAutomatically ( false ) ; net . sourceforge . pmd . eclipse . runtime . properties . IProjectProperties projectProperties = net . sourceforge . pmd . eclipse . plugin . PMDPlugin . getDefault ( ) . getPropertiesManager ( ) . loadProjectProperties ( testProject ) ; net . sourceforge . pmd . RuleSet projectRuleSet = projectProperties . getProjectRuleSet ( ) ; net . sourceforge . pmd . Rule emptyCatchBlock = net . sourceforge . pmd . eclipse . runtime . cmd . ReviewCmdTest . findRuleByName ( projectRuleSet , "EmptyCatchBlock" , "java" ) ; projectRuleSet = net . sourceforge . pmd . eclipse . ui . actions . RuleSetUtil . clearRules ( projectRuleSet ) ; projectRuleSet = net . sourceforge . pmd . eclipse . ui . actions . RuleSetUtil . addRule ( projectRuleSet , emptyCatchBlock ) ; projectProperties . setProjectRuleSet ( projectRuleSet ) ; net . sourceforge . pmd . eclipse . plugin . PMDPlugin . getDefault ( ) . getPropertiesManager ( ) . storeProjectProperties ( projectProperties ) ; final net . sourceforge . pmd . eclipse . runtime . cmd . ReviewCodeCmd cmd = new net . sourceforge . pmd . eclipse . runtime . cmd . ReviewCodeCmd ( ) ; cmd . addResource ( this . testProject ) ; cmd . performExecute ( ) ; cmd . join ( ) ; org . junit . Assert . assertEquals ( 2 , cmd . getFileCount ( ) ) ; } }
public class aTest{ @Test public void testDescribeBinlogFiles ( ) { try { com . fit2cloud . aliyun . rds . model . request . DescribeBinlogFilesRequest request = new com . fit2cloud . aliyun . rds . model . request . DescribeBinlogFilesRequest ( ) ; request . setDBInstanceId ( dBInstanceId ) ; java . util . Date date = new java . util . Date ( ) ; java . lang . String endTime = new java . text . SimpleDateFormat ( "YYYY-MM-dd'T'HH:mm:ss'Z'" ) . format ( date ) ; date = new java . util . Date ( ( ( date . getTime ( ) ) - 36000000 ) ) ; java . lang . String startTime = new java . text . SimpleDateFormat ( "YYYY-MM-dd'T'HH:mm:ss'Z'" ) . format ( date ) ; request . setStartTime ( startTime ) ; request . setEndTime ( endTime ) ; com . fit2cloud . aliyun . Response response = client . describeBinlogFiles ( request ) ; System . out . println ( ( "testDescribeBinlogFiles<sp>::<sp>" + ( new com . google . gson . Gson ( ) . toJson ( response ) ) ) ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void noTransformers ( ) { java . lang . String hi = com . google . inject . Guice . createInjector ( ( binder ) -> { TransformableBinder < java . lang . String > tb = new tc . oc . commons . core . inject . TransformableBinder < > ( binder , . class ) ; tb . bindOriginal ( ) . toInstance ( "hi" ) ; } ) . getInstance ( java . lang . String . class ) ; org . junit . Assert . assertEquals ( "hi" , hi ) ; } }
public class aTest{ @Test public void test_unfreeze_partition ( ) { com . noctarius . snowcast . impl . SequencerDefinition definition = new com . noctarius . snowcast . impl . SequencerDefinition ( "empty" , com . noctarius . snowcast . SnowcastEpoch . byTimestamp ( 1 ) , 128 , ( ( short ) ( 1 ) ) ) ; com . hazelcast . nio . Address address1 = new com . hazelcast . nio . Address ( "localhost" , 1000 ) ; com . hazelcast . nio . Address address2 = new com . hazelcast . nio . Address ( "localhost" , 1002 ) ; com . noctarius . snowcast . impl . SequencerPartition partition = new com . noctarius . snowcast . impl . SequencerPartition ( 1 ) ; java . lang . Integer logicalNodeId = partition . attachLogicalNode ( definition , address1 ) ; org . junit . Assert . assertNotNull ( logicalNodeId ) ; partition . freeze ( ) ; try { partition . attachLogicalNode ( definition , address2 ) ; org . junit . Assert . fail ( "Partition<sp>is<sp>not<sp>successfully<sp>frozen" ) ; } }
public class aTest{ @Test public void testDnDListDnDListModelOfT ( ) { if ( isHeadless ( ) ) return ; org . geotools . swing . control . DnDListModel < java . lang . String > model = new org . geotools . swing . control . DnDListModel < java . lang . String > ( ) ; model . addItem ( "one" ) ; model . addItem ( "two" ) ; org . geotools . swing . control . DnDList < java . lang . String > list = new org . geotools . swing . control . DnDList < java . lang . String > ( model ) ; org . junit . Assert . assertSame ( model , list . getModel ( ) ) ; try { list = new org . geotools . swing . control . DnDList < java . lang . String > ( null ) ; org . junit . Assert . fail ( "Expected<sp>illegal<sp>argument<sp>exception" ) ; } }
public class aTest{ @Test public void shouldGetFirstFourUsers_Annotated ( ) { sqlSessionFactory . getConfiguration ( ) . addMapper ( org . apache . ibatis . submitted . duplicate_statements . AnnotatedMapper . class ) ; org . apache . ibatis . session . SqlSession sqlSession = sqlSessionFactory . openSession ( ) ; try { org . apache . ibatis . submitted . duplicate_statements . AnnotatedMapper mapper = sqlSession . getMapper ( org . apache . ibatis . submitted . duplicate_statements . AnnotatedMapper . class ) ; java . util . List < org . apache . ibatis . submitted . duplicate_statements . User > users = mapper . getAllUsers ( new org . apache . ibatis . session . RowBounds ( 0 , 4 ) ) ; org . junit . Assert . assertEquals ( 4 , users . size ( ) ) ; } }
public class aTest{ @Test public void testRowsUnboundedPrecedingCurrentRowSortOnResult ( ) { java . lang . String sqlText = java . lang . String . format ( ( "<sp>120<sp>|<sp>3<sp>|<sp>75000<sp>|130000<sp>|" 7 + ( "<sp>120<sp>|<sp>3<sp>|<sp>75000<sp>|130000<sp>|" 8 + "from<sp>%s<sp>--SPLICE-PROPERTIES<sp>useSpark<sp>=<sp>%s<sp>\n<sp>order<sp>by<sp>dept,<sp>empnum" ) ) , this . getTableReference ( com . splicemachine . derby . impl . sql . execute . operations . WindowFunctionIT . EMPTAB ) , useSpark ) ; java . sql . ResultSet rs = com . splicemachine . derby . impl . sql . execute . operations . WindowFunctionIT . methodWatcher . executeQuery ( sqlText ) ; java . lang . String expected = "EMPNUM<sp>|DEPT<sp>|SALARY<sp>|SUMSAL<sp>|\n" + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "------------------------------\n" + "<sp>10<sp>|<sp>1<sp>|<sp>50000<sp>|<sp>50000<sp>|\n" ) + "<sp>120<sp>|<sp>3<sp>|<sp>75000<sp>|130000<sp>|" 6 ) + "<sp>70<sp>|<sp>1<sp>|<sp>76000<sp>|358000<sp>|\n" 0 ) + "<sp>120<sp>|<sp>3<sp>|<sp>75000<sp>|130000<sp>|" 3 ) + "<sp>120<sp>|<sp>3<sp>|<sp>75000<sp>|130000<sp>|" 5 ) + "<sp>70<sp>|<sp>1<sp>|<sp>76000<sp>|358000<sp>|\n" ) + "<sp>120<sp>|<sp>3<sp>|<sp>75000<sp>|130000<sp>|" 2 ) + "<sp>120<sp>|<sp>3<sp>|<sp>75000<sp>|130000<sp>|" 0 ) + "<sp>120<sp>|<sp>3<sp>|<sp>75000<sp>|130000<sp>|" 4 ) + "<sp>49<sp>|<sp>2<sp>|<sp>53000<sp>|208000<sp>|\n" ) + "<sp>90<sp>|<sp>2<sp>|<sp>51000<sp>|<sp>51000<sp>|\n" ) + "<sp>30<sp>|<sp>3<sp>|<sp>84000<sp>|293000<sp>|\n" ) + "<sp>80<sp>|<sp>3<sp>|<sp>79000<sp>|209000<sp>|\n" ) + "<sp>120<sp>|<sp>3<sp>|<sp>75000<sp>|130000<sp>|" 1 ) + "<sp>120<sp>|<sp>3<sp>|<sp>75000<sp>|130000<sp>|" ) ; org . junit . Assert . assertEquals ( ( ( "<sp>120<sp>|<sp>3<sp>|<sp>75000<sp>|130000<sp>|" 9 + sqlText ) + "<sp>120<sp>|<sp>3<sp>|<sp>75000<sp>|130000<sp>|" 9 ) , expected , TestUtils . FormattedResult . ResultFactory . toStringUnsorted ( rs ) ) ; rs . close ( ) ; } }
public class aTest{ @Test public void testUpdateRemoteTags ( ) { com . amazonaws . services . ec2 . AmazonEC2 ec2 = new com . amazonaws . services . ec2 . AmazonEC2Client ( ) { @ hudson . plugins . ec2 . Override public com . amazonaws . services . ec2 . model . CreateTagsResult createTags ( com . amazonaws . services . ec2 . model . CreateTagsRequest createTagsRequest ) { return null ; } } ; java . lang . String ami = "ami1" ; java . lang . String description = "foo<sp>ami" ; hudson . plugins . ec2 . EC2Tag tag1 = new hudson . plugins . ec2 . EC2Tag ( "name1" , "foo<sp>ami" 8 ) ; hudson . plugins . ec2 . EC2Tag tag2 = new hudson . plugins . ec2 . EC2Tag ( "name2" , "foo<sp>ami" 1 ) ; java . util . List < hudson . plugins . ec2 . EC2Tag > tags = new java . util . ArrayList < hudson . plugins . ec2 . EC2Tag > ( ) ; tags . add ( tag1 ) ; tags . add ( tag2 ) ; java . lang . String instanceId = "123" ; hudson . plugins . ec2 . SlaveTemplate orig = new hudson . plugins . ec2 . SlaveTemplate ( ami , EC2AbstractSlave . TEST_ZONE , null , "default" , "foo" , com . amazonaws . services . ec2 . model . InstanceType . M1Large , false , "foo<sp>ami" 3 , Node . Mode . NORMAL , description , "bar" , "foo<sp>ami" 5 , "aaa" , "foo<sp>ami" 4 , "foo<sp>ami" 9 , null , "foo<sp>ami" 7 , false , "foo<sp>ami" 6 , tags , null , false , null , "" , true , false , "" , false , "" ) { @ hudson . plugins . ec2 . Override protected java . lang . Object readResolve ( ) { return null ; } } ; java . util . ArrayList < com . amazonaws . services . ec2 . model . Tag > awsTags = new java . util . ArrayList < com . amazonaws . services . ec2 . model . Tag > ( ) ; awsTags . add ( new com . amazonaws . services . ec2 . model . Tag ( EC2Tag . TAG_NAME_JENKINS_SLAVE_TYPE , "foo<sp>ami" 8 ) ) ; awsTags . add ( new com . amazonaws . services . ec2 . model . Tag ( EC2Tag . TAG_NAME_JENKINS_SLAVE_TYPE , "foo<sp>ami" 1 ) ) ; final java . lang . Object [ ] params = new java . lang . Object [ ] { ec2 , awsTags , "foo<sp>ami" 0 , instanceId } ; org . powermock . reflect . Whitebox . invokeMethod ( orig , "foo<sp>ami" 2 , params ) ; org . junit . Assert . assertEquals ( 0 , handler . getRecords ( ) . size ( ) ) ; } }
public class aTest{ @Test public void testExpression6 ( ) { java . lang . String expression = oldExpressions [ 6 ] ; try { java . util . List list = org . eclipse . birt . data . engine . expression . ExpressionCompilerUtilTest . extractColumnExpression ( new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( expression ) ) ; org . junit . Assert . assertTrue ( ( ( list . size ( ) ) == 1 ) ) ; } }
public class aTest{ @Test public void debieraMostrarListaDeTipoColportor ( ) { mx . edu . um . mateo . colportor . test . TipoColportorControllerTest . log . debug ( "Debiera<sp>monstrar<sp>lista<sp>TipoColportor" ) ; for ( int i = 0 ; i < 20 ; i ++ ) { mx . edu . um . mateo . colportor . model . TipoColportor tipoColportor = new mx . edu . um . mateo . colportor . model . TipoColportor ( ( "test" + i ) , "A" ) ; tipoColportorDao . crea ( tipoColportor ) ; org . junit . Assert . assertNotNull ( tipoColportor . getId ( ) ) ; } }
public class aTest{ @Test public void testMaxConnections ( ) { final com . questdb . ServerConfiguration configuration = new com . questdb . ServerConfiguration ( new com . questdb . net . http . File ( com . questdb . net . http . HttpServerTest . resourceFile ( "/site" ) , "conf/questdb.conf" ) ) ; com . questdb . BootstrapEnv env = new com . questdb . BootstrapEnv ( ) ; env . configuration = configuration ; env . configuration . setHttpMaxConnections ( 1 ) ; env . matcher = new com . questdb . net . http . SimpleUrlMatcher ( ) { { setDefaultHandler ( new com . questdb . net . http . handlers . StaticContentHandler ( env ) ) ; } } ; com . questdb . net . http . HttpServer server = new com . questdb . net . http . HttpServer ( env ) ; server . start ( ) ; try { org . junit . Assert . assertNotNull ( com . questdb . net . http . HttpServerTest . clientBuilder ( true ) . build ( ) . execute ( new org . apache . http . client . methods . HttpGet ( "https://localhost:9000/upload.html" ) ) ) ; try { com . questdb . net . http . HttpServerTest . clientBuilder ( true ) . build ( ) . execute ( new org . apache . http . client . methods . HttpGet ( "https://localhost:9000/upload.html" ) ) ; org . junit . Assert . fail ( "Expected<sp>server<sp>to<sp>reject<sp>connection" ) ; } }
public class aTest{ @Test public void chainVoidMethodCallsVoidFirst ( ) { mock . simpleMethodWithArgument ( "4" ) ; expectLastCall ( ) . andVoid ( ) . andThrow ( new java . lang . RuntimeException ( "Test" ) ) ; replay ( mock ) ; mock . simpleMethodWithArgument ( "4" ) ; try { mock . simpleMethodWithArgument ( "4" ) ; } catch ( java . lang . RuntimeException e ) { org . junit . Assert . assertEquals ( "Test" , e . getMessage ( ) ) ; } }
public class aTest{ @Test public void should_output_corresponding_reference_sequences_only ( ) { au . edu . wehi . idsv . sim . SimpleVariantChromosome svc = new au . edu . wehi . idsv . sim . SimpleVariantChromosome ( getContext ( ) , "polyACGT" , 1 , 0 ) ; svc . assemble ( input , output , true , com . google . common . collect . Lists . newArrayList ( SvType . DEL , SvType . INS , SvType . INV , SvType . DUP ) , com . google . common . collect . Lists . newArrayList ( 2 ) , 1 ) ; | java . util . List < java . lang . String > fa = com . google . common . io . Files . readLines ( input , StandardCharsets . US_ASCII ) ; org . junit . Assert . assertEquals ( 15 , fa . get ( 3 ) . length ( ) ) ; } }
public class aTest{ @Test public void testRefCountBuffer ( ) { try { com . ibm . ws . bytebuffer . internal . WsByteBufferPoolManagerImpl mgr = ( ( com . ibm . ws . bytebuffer . internal . WsByteBufferPoolManagerImpl ) ( com . ibm . wsspi . channelfw . ChannelFrameworkFactory . getBufferManager ( ) ) ) ; com . ibm . wsspi . bytebuffer . WsByteBuffer buffer = mgr . wrap ( java . nio . ByteBuffer . allocate ( 1024 ) , true ) ; org . junit . Assert . assertEquals ( WsByteBuffer . TYPE_WsByteBuffer , buffer . getType ( ) ) ; runTests ( buffer , false , 1024 ) ; } }
public class aTest{ @Test public void shouldNotInjectUnsupportedAndInitializedTypes ( ) { final java . lang . String initialValue = "initial" ; final java . util . concurrent . Callable < ? > object = new java . util . concurrent . Callable < java . lang . Void > ( ) { @ ro . isdc . wro . model . group . Inject java . lang . String unsupportedInitializedType = initialValue ; public ro . isdc . wro . model . group . processor . Void call ( ) throws ro . isdc . wro . model . group . processor . Exception { org . junit . Assert . assertEquals ( initialValue , unsupportedInitializedType ) ; return null ; } } }
public class aTest{ @Test public void testGetNominalLabelByID ( ) { try { qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestNominalLabelResourceFacadeImp . entityManager . getTransaction ( ) . begin ( ) ; qa . qcri . aidr . dbmanager . dto . NominalLabelDTO result = qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestNominalLabelResourceFacadeImp . nominalLabelResourceFacadeImp . getNominalLabelByID ( qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestNominalLabelResourceFacadeImp . nominalLabel . getNominalLabelId ( ) ) ; qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestNominalLabelResourceFacadeImp . entityManager . getTransaction ( ) . commit ( ) ; org . junit . Assert . assertEquals ( qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestNominalLabelResourceFacadeImp . nominalLabel . getName ( ) , result . getName ( ) ) ; } }
public class aTest{ @Test public void testCheckForItemsExample ( ) { java . lang . String droolsSource = ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 3 + "import<sp>" ) + ( org . drools . core . beliefsystem . abductive . Abducible . class . getName ( ) ) ) + ";<sp>" ) + "<sp>insert(<sp>new<sp>Apple(<sp>2<sp>)<sp>);<sp>" 9 ) + "declare<sp>Fruit<sp>id<sp>:<sp>int<sp>@key<sp>end<sp>" ) + "declare<sp>Apple<sp>extends<sp>Fruit<sp>end<sp>" ) + ">>><sp>" 7 ) + "<sp>insert(<sp>new<sp>Apple(<sp>2<sp>)<sp>);<sp>" 0 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 7 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 8 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 1 ) + "<sp>insert(<sp>new<sp>Apple(<sp>2<sp>)<sp>);<sp>" 3 ) + ">>><sp>" 6 ) + ">>><sp>" 3 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 1 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 4 ) + ">>><sp>" 1 ) + "<sp>insert(<sp>new<sp>Apple(<sp>2<sp>)<sp>);<sp>" 6 ) + "<sp>insert(<sp>new<sp>Apple(<sp>2<sp>)<sp>);<sp>" 7 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 1 ) + "declare<sp>Fruit<sp>id<sp>:<sp>int<sp>@key<sp>end<sp>" 4 ) + ">>><sp>" 5 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 5 ) + "<sp>check(<sp>Banana.class<sp>;<sp>)<sp>" ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 1 ) + ">>><sp>" 2 ) + "<sp>insert(<sp>new<sp>Apple(<sp>2<sp>)<sp>);<sp>" 2 ) + "declare<sp>Fruit<sp>id<sp>:<sp>int<sp>@key<sp>end<sp>" 5 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 2 ) + "<sp>insert(<sp>new<sp>Apple(<sp>2<sp>)<sp>);<sp>" 4 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 9 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 1 ) + "rule<sp>Reminder<sp>" ) + "declare<sp>Fruit<sp>id<sp>:<sp>int<sp>@key<sp>end<sp>" 5 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," ) + "declare<sp>Fruit<sp>id<sp>:<sp>int<sp>@key<sp>end<sp>" 7 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 6 ) + ">>><sp>" 0 ) + "declare<sp>Fruit<sp>id<sp>:<sp>int<sp>@key<sp>end<sp>" 3 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 2 ) + "declare<sp>Fruit<sp>id<sp>:<sp>int<sp>@key<sp>end<sp>" 1 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 1 ) + ">>><sp>" 9 ) + "declare<sp>Fruit<sp>id<sp>:<sp>int<sp>@key<sp>end<sp>" 5 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 0 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 2 ) + "declare<sp>Fruit<sp>id<sp>:<sp>int<sp>@key<sp>end<sp>" 6 ) + ">>><sp>" 4 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 1 ) + "declare<sp>Fruit<sp>id<sp>:<sp>int<sp>@key<sp>end<sp>" 0 ) + "declare<sp>Fruit<sp>id<sp>:<sp>int<sp>@key<sp>end<sp>" 5 ) + "<sp>insert(<sp>new<sp>Apple(<sp>2<sp>)<sp>);<sp>" 8 ) + "<sp>Banana()<sp>" ) + ">>><sp>" 8 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 2 ) + "<sp>insert(<sp>new<sp>Apple(<sp>2<sp>)<sp>);<sp>" 5 ) + "<sp>accumulate(<sp>$f<sp>:<sp>Fruit()<sp>," 1 ; org . kie . api . runtime . KieSession session = getSessionFromString ( droolsSource ) ; java . util . List list = new java . util . ArrayList ( ) ; session . setGlobal ( "<sp>insert(<sp>new<sp>Apple(<sp>2<sp>)<sp>);<sp>" 1 , list ) ; session . fireAllRules ( ) ; for ( java . lang . Object o : session . getObjects ( ) ) { System . out . println ( ( ">>><sp>" + o ) ) ; } org . junit . Assert . assertEquals ( java . util . Arrays . asList ( "declare<sp>Fruit<sp>id<sp>:<sp>int<sp>@key<sp>end<sp>" 2 ) , list ) ; } }
public class aTest{ @Test public void testOldDates ( ) { java . lang . String sqlText = java . lang . String . format ( "select<sp>d<sp>from<sp>%s<sp>order<sp>by<sp>1" , com . splicemachine . derby . impl . sql . execute . operations . SimpleDateArithmeticIT . QUALIFIED_TABLE_NAME2 ) ; java . sql . ResultSet rs = com . splicemachine . derby . impl . sql . execute . operations . SimpleDateArithmeticIT . spliceClassWatcher . executeQuery ( sqlText ) ; java . lang . String expected = "D<sp>|\n" + ( ( ( ( ( ( ( "------------\n" + "1453-05-29<sp>|\n" ) + "------------\n" 0 ) + "1776-07-04<sp>|\n" ) + "1783-09-03<sp>|\n" ) + "1791-12-05<sp>|\n" ) + "1861-04-12<sp>|\n" ) + "1865-05-13<sp>|" ) ; org . junit . Assert . assertEquals ( ( ( "\n" + sqlText ) + "\n" ) , expected , TestUtils . FormattedResult . ResultFactory . toStringUnsorted ( rs ) ) ; rs . close ( ) ; } }
public class aTest{ @Test public void testExecuteIRODSQueryBuilderQueryWithOrderByDesc ( ) { org . irods . jargon . core . query . IRODSGenQueryBuilder builder = new org . irods . jargon . core . query . IRODSGenQueryBuilder ( true , null ) ; final java . lang . String zoneKey = org . irods . jargon . core . pub . IRODSGenQueryExecutorImplTest . testingProperties . getProperty ( TestingPropertiesHelper . IRODS_ZONE_KEY ) . trim ( ) ; builder . addSelectAsGenQueryValue ( RodsGenQueryEnum . COL_R_RESC_NAME ) . addSelectAsGenQueryValue ( RodsGenQueryEnum . COL_R_ZONE_NAME ) . addConditionAsGenQueryField ( RodsGenQueryEnum . COL_R_ZONE_NAME , QueryConditionOperators . EQUAL , zoneKey ) . addOrderByGenQueryField ( RodsGenQueryEnum . COL_R_RESC_NAME , GenQueryOrderByField . OrderByType . DESC ) ; org . irods . jargon . core . connection . IRODSAccount irodsAccount = org . irods . jargon . core . pub . IRODSGenQueryExecutorImplTest . testingPropertiesHelper . buildIRODSAccountFromTestProperties ( org . irods . jargon . core . pub . IRODSGenQueryExecutorImplTest . testingProperties ) ; org . irods . jargon . core . pub . IRODSAccessObjectFactory accessObjectFactory = org . irods . jargon . core . pub . IRODSGenQueryExecutorImplTest . irodsFileSystem . getIRODSAccessObjectFactory ( ) ; org . irods . jargon . core . pub . IRODSGenQueryExecutor irodsGenQueryExecutor = accessObjectFactory . getIRODSGenQueryExecutor ( irodsAccount ) ; org . irods . jargon . core . query . IRODSGenQueryFromBuilder query = builder . exportIRODSQueryFromBuilder ( 50 ) ; org . irods . jargon . core . query . IRODSQueryResultSetInterface resultSet = irodsGenQueryExecutor . executeIRODSQuery ( query , 0 ) ; org . junit . Assert . assertNotNull ( resultSet ) ; java . lang . String last = "" ; java . lang . String current = "" ; for ( org . irods . jargon . core . query . IRODSQueryResultRow row : resultSet . getResults ( ) ) { current = row . getColumn ( 0 ) ; if ( ! ( last . isEmpty ( ) ) ) { current . compareTo ( last ) ; } }
public class aTest{ @Test public void testComputeFileContentHash ( ) { java . nio . file . Path file = null ; try { file = java . nio . file . Paths . get ( org . apache . commons . io . FileUtils . getTempDirectoryPath ( ) , org . apache . commons . lang3 . RandomStringUtils . randomAlphanumeric ( 24 ) ) ; java . lang . String content = "hello<sp>world" ; java . lang . String md5AsBase64 = "XrY7u+Ae7tCTyyK7j1rNww==" ; java . nio . file . Files . write ( file , content . getBytes ( ) ) ; java . lang . String hash = org . peerbox . watchservice . PathUtils . computeFileContentHash ( file ) ; org . junit . Assert . assertEquals ( md5AsBase64 , hash ) ; } }
public class aTest{ @Test public void testPoolSize4 ( ) { java . lang . String host = "192.168.1.107" ; int port = 6379 ; int connectionPoolSize = 5 ; final org . cyy . fw . nedis . NedisClient client = new org . cyy . fw . nedis . NedisClientBuilder ( ) . setServerHost ( host ) . setPort ( port ) . setConnectTimeoutMills ( 5000 ) . setConnectionPoolSize ( connectionPoolSize ) . build ( ) ; try { final java . util . concurrent . CountDownLatch latch = new java . util . concurrent . CountDownLatch ( 1 ) ; client . get ( null , "key1" ) ; client . get ( null , "key2" ) ; client . get ( null , "key3" ) ; client . get ( null , "key4" ) ; client . get ( null , "key5" ) ; client . get ( new org . cyy . fw . nedis . ResponseCallback < java . lang . String > ( ) { @ org . cyy . fw . nedis . test . connection . Override public void failed ( java . lang . Throwable cause ) { } @ org . cyy . fw . nedis . test . connection . Override public void done ( java . lang . String result ) { latch . countDown ( ) ; } } , "key6" ) ; latch . await ( ) ; java . lang . Thread . sleep ( 5000 ) ; org . junit . Assert . assertEquals ( 5 , client . getIdleConnections ( ) ) ; } }
public class aTest{ @Test public void testDefaultConstructorThrows ( ) { try { net . sf . marineapi . nmea . event . AbstractSentenceListenerTest . GenericsListener < java . lang . String , net . sf . marineapi . nmea . sentence . BODSentence > gl = new net . sf . marineapi . nmea . event . AbstractSentenceListenerTest . GenericsListener ( ) ; org . junit . Assert . fail ( ( "default<sp>constructor<sp>didn't<sp>throw,<sp>resolved<sp>to<sp>" + ( gl . sentenceType ) ) ) ; } catch ( java . lang . IllegalStateException ise ) { java . lang . String msg = "Cannot<sp>resolve<sp>generic<sp>type<sp><T>,<sp>use<sp>constructor<sp>with<sp>Class<T><sp>param." ; org . junit . Assert . assertEquals ( msg , ise . getMessage ( ) ) ; } }
public class aTest{ @Test public void sendMessageSuccess ( ) { final tests . unit . com . microsoft . azure . sdk . iot . device . transport . amqps . AmqpsSessionDeviceOperation amqpsSessionDeviceOperation = new tests . unit . com . microsoft . azure . sdk . iot . device . transport . amqps . AmqpsSessionDeviceOperation ( mockDeviceClientConfig , mockAmqpsDeviceAuthentication ) ; mockit . Deencapsulation . setField ( amqpsSessionDeviceOperation , "amqpsAuthenticatorState" , AmqpsDeviceAuthenticationState . AUTHENTICATED ) ; final byte [ ] bytes = new byte [ 1024 ] ; new mockit . NonStrictExpectations ( ) { { mockIotHubConnectionString . getDeviceId ( ) ; result = "deviceId" ; mockDeviceClientConfig . getDeviceId ( ) ; result = "deviceId" ; mockProtonMessage . encode ( bytes , anyInt , anyInt ) ; mockit . Deencapsulation . invoke ( mockAmqpsDeviceTelemetry , "sendMessageAndGetDeliveryHash" , MessageType . DEVICE_TELEMETRY , bytes , anyInt , anyInt , bytes ) ; result = mockAmqpsSendReturnValue ; mockit . Deencapsulation . invoke ( mockAmqpsSendReturnValue , "isDeliverySuccessful" ) ; result = true ; mockDeviceClientConfig . getDeviceId ( ) ; result = "someDeviceId" ; } } ; java . lang . Integer actualDeliveryHash = mockit . Deencapsulation . invoke ( amqpsSessionDeviceOperation , "sendMessage" , mockProtonMessage , MessageType . DEVICE_TELEMETRY , "someDeviceId" ) ; org . junit . Assert . assertTrue ( ( actualDeliveryHash != ( - 1 ) ) ) ; new mockit . Verifications ( ) { { mockProtonMessage . encode ( bytes , anyInt , anyInt ) ; times = 1 ; } } }
public class aTest{ @Test public void testSetNetworkTimeoutValid ( ) { java . sql . Connection conn = org . postgresql . test . TestUtil . openDB ( ) ; try { conn . setNetworkTimeout ( null , ( ( int ) ( TimeUnit . SECONDS . toMillis ( 5 ) ) ) ) ; org . junit . Assert . assertEquals ( TimeUnit . SECONDS . toMillis ( 5 ) , conn . getNetworkTimeout ( ) ) ; } }
public class aTest{ @Test public void testTimerPauseResume ( ) { System . out . println ( "*****<sp>ResumableTimerTest.testTimerPauseResume()<sp>*****" ) ; final org . kaazing . net . impl . util . ResumableTimer timer = new org . kaazing . net . impl . util . ResumableTimer ( new java . lang . Runnable ( ) { @ org . kaazing . net . impl . util . Override public void run ( ) { System . out . println ( ( "Task<sp>scheduled<sp>at<sp>" + ( getFormattedDateTime ( ) ) ) ) ; } } , 5000 , true ) ; System . out . println ( ( "Starting<sp>timer<sp>at<sp>" + ( getFormattedDateTime ( ) ) ) ) ; timer . start ( ) ; try { java . lang . Thread . sleep ( 500 ) ; java . lang . Thread pauseTimerThread = new java . lang . Thread ( new java . lang . Runnable ( ) { @ org . kaazing . net . impl . util . Override public void run ( ) { while ( true ) { try { java . lang . Thread . sleep ( 100 ) ; if ( timer . didTaskExecute ( ) ) { break ; } timer . pause ( ) ; } catch ( java . lang . InterruptedException e ) { e . printStackTrace ( ) ; } } System . out . println ( ( "PauseTimerThread<sp>is<sp>exiting<sp>at<sp>" + ( getFormattedDateTime ( ) ) ) ) ; } } , "PauseTimerThread" ) ; pauseTimerThread . start ( ) ; java . lang . Thread . sleep ( 500 ) ; java . lang . Thread resumeTimerThread = new java . lang . Thread ( new java . lang . Runnable ( ) { @ org . kaazing . net . impl . util . Override public void run ( ) { while ( true ) { try { java . lang . Thread . sleep ( 100 ) ; if ( timer . didTaskExecute ( ) ) { break ; } timer . resume ( ) ; } catch ( java . lang . InterruptedException e ) { e . printStackTrace ( ) ; } } System . out . println ( ( "ResumeTimerThread<sp>is<sp>exiting<sp>at<sp>" + ( getFormattedDateTime ( ) ) ) ) ; } } , "Resumed<sp>at<sp>" 0 ) ; resumeTimerThread . start ( ) ; java . lang . Thread . sleep ( 15000 ) ; org . junit . Assert . assertTrue ( timer . didTaskExecute ( ) ) ; timer . cancel ( ) ; } }
public class aTest{ @Test public void shouldSelectAuthorsUsingMapperClassWithResultHandler ( ) { org . apache . ibatis . session . SqlSession session = org . apache . ibatis . session . SqlSessionTest . sqlMapper . openSession ( ) ; try { org . apache . ibatis . executor . result . DefaultResultHandler handler = new org . apache . ibatis . executor . result . DefaultResultHandler ( ) ; org . apache . ibatis . domain . blog . mappers . AuthorMapper mapper = session . getMapper ( org . apache . ibatis . domain . blog . mappers . AuthorMapper . class ) ; mapper . selectAllAuthors ( handler ) ; org . junit . Assert . assertEquals ( 2 , handler . getResultList ( ) . size ( ) ) ; } }
public class aTest{ @Test public void testNetworkExchangeOneTraceSoftirq ( ) { org . eclipse . tracecompass . tmf . core . trace . ITmfTrace experiment = setUpExperiment ( "testfiles/graph/simple_network_client.xml" ) ; org . junit . Assert . assertNotNull ( experiment ) ; try { org . eclipse . tracecompass . lttng2 . kernel . core . tests . analysis . graph . DistributedCriticalPathTest . internalTestNetworkExchangeOneTraceSoftirq ( experiment ) ; } }
public class aTest{ @Test public void validateService ( ) { java . io . File wsdl1 = getProtobufRelatedInput ( "CalcServiceProtobufInvalid.wsdl" ) ; java . io . File wsdl2 = getProtobufRelatedInput ( "CalcServiceProtobufInvalid.wsdl" ) ; java . io . File destDir = testingdir . getDir ( ) ; org . ebayopensource . turmeric . tools . codegen . CodeGenContext context = org . ebayopensource . turmeric . tools . codegen . proto . ProtobufSchemaMapperTestUtils . getCodeGenContext ( org . ebayopensource . turmeric . tools . codegen . proto . FastSerFormatValidationTests . getTestInvalidArgs ( wsdl1 , destDir ) ) ; context . getInputOptions ( ) . setInputFile ( null ) ; context . getInputOptions ( ) . setEnabledNamespaceFolding ( true ) ; try { org . ebayopensource . turmeric . tools . codegen . fastserformat . FastSerFormatCodegenBuilder . getInstance ( ) . validateServiceIfApplicable ( context ) ; } catch ( org . ebayopensource . turmeric . tools . codegen . exception . CodeGenFailedException e ) { if ( ! ( e . getMessage ( ) . contains ( FastSerFormatValidationHandler . EMPTY_WSDL_PATH ) ) ) { org . junit . Assert . fail ( "The<sp>Test<sp>failed.<sp>Expected<sp>to<sp>fail<sp>for<sp>empty<sp>wsdl<sp>path" ) ; } } context . getInputOptions ( ) . setInputFile ( wsdl2 . getAbsolutePath ( ) ) ; try { org . ebayopensource . turmeric . tools . codegen . fastserformat . FastSerFormatCodegenBuilder . getInstance ( ) . validateServiceIfApplicable ( context ) ; } catch ( org . ebayopensource . turmeric . tools . codegen . exception . CodeGenFailedException e ) { if ( ! ( e . getMessage ( ) . contains ( WSDLParserConstants . NS_URI_1999_SCHEMA_XSD ) ) ) { org . junit . Assert . fail ( "The<sp>Test<sp>failed.<sp>Expected<sp>to<sp>fail<sp>for<sp>empty<sp>wsdl<sp>path" 2 ) ; } } context = org . ebayopensource . turmeric . tools . codegen . proto . ProtobufSchemaMapperTestUtils . getCodeGenContext ( org . ebayopensource . turmeric . tools . codegen . proto . FastSerFormatValidationTests . getTestValidArgs ( wsdl2 , destDir ) ) ; try { org . ebayopensource . turmeric . tools . codegen . fastserformat . FastSerFormatCodegenBuilder . getInstance ( ) . validateServiceIfApplicable ( context ) ; } catch ( org . ebayopensource . turmeric . tools . codegen . exception . CodeGenFailedException e ) { if ( ! ( e instanceof org . ebayopensource . turmeric . tools . codegen . fastserformat . protobuf . validator . FastSerFormatNotSupportedException ) ) { org . junit . Assert . fail ( "The<sp>Test<sp>failed.<sp>Expected<sp>exception<sp>FastSerFormatNotSupportedException" ) ; } org . ebayopensource . turmeric . tools . codegen . fastserformat . protobuf . validator . FastSerFormatNotSupportedException excep = ( ( org . ebayopensource . turmeric . tools . codegen . fastserformat . protobuf . validator . FastSerFormatNotSupportedException ) ( e ) ) ; if ( ( excep . getErrors ( ) . size ( ) ) == 0 ) { org . junit . Assert . fail ( "The<sp>test<sp>failed.<sp>Expected<sp>errors<sp>around<sp>10." ) ; } java . util . Set < org . ebayopensource . turmeric . runtime . codegen . common . ValidationRule > rules = new java . util . HashSet < org . ebayopensource . turmeric . runtime . codegen . common . ValidationRule > ( ) ; for ( org . ebayopensource . turmeric . runtime . codegen . common . FastSerFormatValidationError error : excep . getErrors ( ) ) { if ( ( ( error . getDescription ( ) . contains ( "{0}" ) ) || ( error . getDescription ( ) . contains ( "{1}" ) ) ) || ( error . getDescription ( ) . contains ( "The<sp>Test<sp>failed.<sp>Expected<sp>to<sp>fail<sp>for<sp>empty<sp>wsdl<sp>path" 3 ) ) ) { org . junit . Assert . fail ( "The<sp>test<sp>failed.<sp>Message<sp>formation<sp>failed" ) ; } rules . add ( error . getError ( ) ) ; } for ( org . ebayopensource . turmeric . runtime . codegen . common . ValidationRule rule : org . ebayopensource . turmeric . runtime . codegen . common . ValidationRule . values ( ) ) { if ( ( rule == ( org . ebayopensource . turmeric . runtime . codegen . common . ValidationRule . REDEFINE_NOT_SUPPORTED ) ) || ( rule == ( org . ebayopensource . turmeric . runtime . codegen . common . ValidationRule . OLD_SCHEMAS_NOT_SUPPORTED ) ) ) { continue ; } if ( ! ( rules . contains ( rule ) ) ) { org . junit . Assert . fail ( ( ( "The<sp>rule<sp>" + ( rule . value ( ) ) ) + "The<sp>Test<sp>failed.<sp>Expected<sp>to<sp>fail<sp>for<sp>empty<sp>wsdl<sp>path" 0 ) ) ; } } org . junit . Assert . assertTrue ( excep . getMessage ( ) , true ) ; java . lang . String message = excep . getMessage ( ) ; if ( message . endsWith ( "The<sp>Test<sp>failed.<sp>Expected<sp>to<sp>fail<sp>for<sp>empty<sp>wsdl<sp>path" 1 ) ) { org . junit . Assert . fail ( "Message<sp>not<sp>formatted<sp>properly.<sp>It<sp>ends<sp>with<sp>semi<sp>colan" ) ; } }
public class aTest{ @Test public void getObjectWithdot ( ) { try ( java . io . FileInputStream in = new java . io . FileInputStream ( fr . gouv . vitam . common . PropertiesUtils . findFile ( fr . gouv . vitam . storage . offers . common . rest . DefaultOfferResourceTest . ARCHIVE_FILE_TXT ) ) ) { org . junit . Assert . assertNotNull ( in ) ; io . restassured . RestAssured . with ( ) . header ( GlobalDataRest . X_TENANT_ID , "1" ) . header ( GlobalDataRest . VITAM_CONTENT_LENGTH , "8766" ) . header ( GlobalDataRest . X_DIGEST_ALGORITHM , DigestType . SHA512 . getName ( ) ) . contentType ( MediaType . APPLICATION_OCTET_STREAM ) . content ( in ) . when ( ) . put ( ( ( ( fr . gouv . vitam . storage . offers . common . rest . DefaultOfferResourceTest . OBJECTS_URI ) + ( fr . gouv . vitam . storage . offers . common . rest . DefaultOfferResourceTest . OBJECT_TYPE_URI ) ) + ( fr . gouv . vitam . storage . offers . common . rest . DefaultOfferResourceTest . OBJECT_ID_URI ) ) , fr . gouv . vitam . storage . offers . common . rest . DefaultOfferResourceTest . OBJECT_CODE , "id1.xml" ) ; } }
public class aTest{ @Test public void testWCWithExternalsToRepos ( ) { final org . tmatesoft . svn . test . TestOptions options = org . tmatesoft . svn . test . TestOptions . getInstance ( ) ; final org . tmatesoft . svn . test . SvnOperationFactory svnOperationFactory = new org . tmatesoft . svn . test . SvnOperationFactory ( ) ; final org . tmatesoft . svn . test . Sandbox sandbox = org . tmatesoft . svn . test . Sandbox . createWithCleanup ( ( ( getTestName ( ) ) + ".testWCWithExternalsToRepos" ) , options ) ; try { final org . tmatesoft . svn . test . SVNURL url = sandbox . createSvnRepository ( ) ; final org . tmatesoft . svn . test . SVNURL targetUrl = url . appendPath ( "target" , false ) ; final org . tmatesoft . svn . core . internal . wc . SVNExternal external = new org . tmatesoft . svn . core . internal . wc . SVNExternal ( "external" , targetUrl . toString ( ) , SVNRevision . HEAD , SVNRevision . HEAD , false , false , true ) ; final org . tmatesoft . svn . test . CommitBuilder commitBuilder = new org . tmatesoft . svn . test . CommitBuilder ( url ) ; commitBuilder . addFile ( "file" ) ; commitBuilder . setDirectoryProperty ( "" , SVNProperty . EXTERNALS , org . tmatesoft . svn . test . SVNPropertyValue . create ( external . toString ( ) ) ) ; commitBuilder . commit ( ) ; final org . tmatesoft . svn . test . WorkingCopy workingCopy = sandbox . checkoutNewWorkingCopy ( url ) ; final java . io . File workingCopyDirectory = workingCopy . getWorkingCopyDirectory ( ) ; final org . tmatesoft . svn . test . SvnRemoteCopy remoteCopy = svnOperationFactory . createRemoteCopy ( ) ; remoteCopy . addCopySource ( org . tmatesoft . svn . test . SvnCopySource . create ( org . tmatesoft . svn . test . SvnTarget . fromFile ( workingCopyDirectory ) , SVNRevision . WORKING ) ) ; remoteCopy . setSingleTarget ( org . tmatesoft . svn . test . SvnTarget . fromURL ( targetUrl ) ) ; final org . tmatesoft . svn . test . SVNCommitInfo commitInfo = remoteCopy . run ( ) ; org . junit . Assert . assertEquals ( 2 , commitInfo . getNewRevision ( ) ) ; } }
public class aTest{ @Test public void testNodeDecomissionWithOverreplicationRespectsRackPolicy ( ) { org . apache . hadoop . conf . Configuration conf = getConf ( ) ; short REPLICATION_FACTOR = 5 ; final org . apache . hadoop . fs . Path filePath = new org . apache . hadoop . fs . Path ( "/testFile" ) ; org . apache . hadoop . fs . FileSystem localFileSys = org . apache . hadoop . fs . FileSystem . getLocal ( conf ) ; org . apache . hadoop . fs . Path workingDir = localFileSys . getWorkingDirectory ( ) ; org . apache . hadoop . fs . Path dir = new org . apache . hadoop . fs . Path ( workingDir , "build/test/data/temp/decommission" ) ; org . apache . hadoop . fs . Path excludeFile = new org . apache . hadoop . fs . Path ( dir , "exclude" ) ; org . junit . Assert . assertTrue ( localFileSys . mkdirs ( dir ) ) ; org . apache . hadoop . hdfs . DFSTestUtil . writeFile ( localFileSys , excludeFile , "" ) ; conf . set ( "dfs.hosts.exclude" , excludeFile . toUri ( ) . getPath ( ) ) ; java . lang . String [ ] racks = new java . lang . String [ ] { "/rack1" , "/rack2" , "/rack1" , "/rack1" , "/rack1" } ; org . apache . hadoop . hdfs . MiniDFSCluster cluster = new org . apache . hadoop . hdfs . MiniDFSCluster . Builder ( conf ) . numDataNodes ( racks . length ) . racks ( racks ) . build ( ) ; final org . apache . hadoop . hdfs . server . namenode . FSNamesystem ns = cluster . getNameNode ( ) . getNamesystem ( ) ; try { final org . apache . hadoop . fs . FileSystem fs = cluster . getFileSystem ( ) ; org . apache . hadoop . hdfs . DFSTestUtil . createFile ( fs , filePath , 1L , REPLICATION_FACTOR , 1L ) ; org . apache . hadoop . hdfs . protocol . ExtendedBlock b = org . apache . hadoop . hdfs . DFSTestUtil . getFirstBlock ( fs , filePath ) ; org . apache . hadoop . hdfs . DFSTestUtil . waitForReplication ( cluster , b , 2 , REPLICATION_FACTOR , 0 ) ; REPLICATION_FACTOR = 2 ; fs . setReplication ( filePath , REPLICATION_FACTOR ) ; org . apache . hadoop . fs . BlockLocation [ ] locs = fs . getFileBlockLocations ( fs . getFileStatus ( filePath ) , 0 , Long . MAX_VALUE ) ; for ( java . lang . String top : locs [ 0 ] . getTopologyPaths ( ) ) { if ( ! ( top . startsWith ( "/rack2" ) ) ) { java . lang . String name = top . substring ( ( ( "/rack1" . length ( ) ) + 1 ) ) ; org . apache . hadoop . hdfs . DFSTestUtil . writeFile ( localFileSys , excludeFile , name ) ; ns . getBlockManager ( ) . getDatanodeManager ( ) . refreshNodes ( conf ) ; org . apache . hadoop . hdfs . DFSTestUtil . waitForDecommission ( fs , name ) ; break ; } } }
public class aTest{ @Test public void PermutationToBigIntegerTest ( ) { int maxSize = 8 ; for ( int i = 0 ; i <= maxSize ; i ++ ) { ch . bfh . unicrypt . helper . converter . classes . biginteger . PermutationToBigInteger converter = ch . bfh . unicrypt . helper . converter . classes . biginteger . PermutationToBigInteger . getInstance ( i ) ; java . math . BigInteger value = ch . bfh . unicrypt . helper . math . MathUtil . ZERO ; java . math . BigInteger maxValue = ch . bfh . unicrypt . helper . math . MathUtil . factorial ( i ) . subtract ( MathUtil . ONE ) ; while ( ( value . compareTo ( maxValue ) ) <= 0 ) { org . junit . Assert . assertEquals ( value , converter . convert ( converter . reconvert ( value ) ) ) ; value = value . add ( MathUtil . ONE ) ; } }
public class aTest{ @Test public void testGetWarningsWithNoWarnings ( ) { boolean initialValue = edu . illinois . library . cantaloupe . processor . OpenJpegProcessor . isQuietModeSupported ( ) ; try { edu . illinois . library . cantaloupe . processor . OpenJpegProcessor . setQuietModeSupported ( true ) ; org . junit . Assert . assertEquals ( 0 , instance . getWarnings ( ) . size ( ) ) ; } }
public class aTest{ @Test public void testCompareIndividuals ( ) { owltools . io . ParserWrapper pw = new owltools . io . ParserWrapper ( ) ; sourceOntol = pw . parseOWL ( getResourceIRIString ( "sim/mp-subset-1.obo" ) ) ; g = new owltools . graph . OWLGraphWrapper ( sourceOntol ) ; parseAssociations ( getResource ( "sim/mgi-gene2mp-subset-1.tbl" ) , g ) ; owlpp = new owltools . io . OWLPrettyPrinter ( g ) ; final int truncLen = 200 ; org . semanticweb . owlapi . reasoner . OWLReasoner reasoner = new org . semanticweb . elk . owlapi . ElkReasonerFactory ( ) . createReasoner ( sourceOntol ) ; try { createOwlSim ( ) ; LOG . info ( ( "Reasoner=" + ( owlsim . getReasoner ( ) ) ) ) ; owltools . sim2 . SimJSONEngine sj = new owltools . sim2 . SimJSONEngine ( g , owlsim ) ; reasoner . flush ( ) ; for ( org . semanticweb . owlapi . model . OWLNamedIndividual i : sourceOntol . getIndividualsInSignature ( ) ) { for ( org . semanticweb . owlapi . model . OWLNamedIndividual j : sourceOntol . getIndividualsInSignature ( ) ) { java . lang . String jsonStr = sj . compareAttributeSetPair ( owlsim . getAttributesForElement ( i ) , owlsim . getAttributesForElement ( j ) ) ; if ( ( jsonStr . length ( ) ) < truncLen ) LOG . warn ( jsonStr ) ; else LOG . info ( ( "SAMPLE:" + ( jsonStr . substring ( 0 , truncLen ) ) ) ) ; } } df = g . getDataFactory ( ) ; org . semanticweb . owlapi . model . OWLClass unkC = df . getOWLClass ( org . semanticweb . owlapi . model . IRI . create ( "http://x.org" ) ) ; java . util . Set < org . semanticweb . owlapi . model . OWLClass > uset = java . util . Collections . singleton ( unkC ) ; boolean isThrown = false ; try { sj . compareAttributeSetPair ( uset , uset ) ; } catch ( owltools . sim2 . UnknownOWLClassException e ) { isThrown = true ; } org . junit . Assert . assertTrue ( isThrown ) ; sj . compareAttributeSetPair ( uset , uset , true ) ; org . semanticweb . owlapi . model . OWLClass thing = df . getOWLThing ( ) ; java . util . Set < org . semanticweb . owlapi . model . OWLClass > things = java . util . Collections . singleton ( unkC ) ; LOG . info ( ( "TxT:" + ( sj . compareAttributeSetPair ( things , things , true ) ) ) ) ; } }
public class aTest{ @Test public void ( ) { net . chandol . study . oop . tdd . baseball . RandomNumberGenerator generator = new net . chandol . study . oop . tdd . baseball . RandomNumberGenerator ( ) ; java . lang . Integer [ ] numbers = generator . generate ( ) ; org . junit . Assert . assertThat ( numbers . length , org . hamcrest . CoreMatchers . is ( 3 ) ) ; } }
public class aTest{ @Test public void deleteAndQueryInA_2 ( ) { final com . google . appengine . api . datastore . Entity entity = getService ( ) . get ( com . google . appengine . api . datastore . KeyFactory . createKey ( "QT" , 3 ) ) ; org . junit . Assert . assertNotNull ( entity ) ; org . jboss . test . capedwarf . cluster . test . QueryTest . wrap ( new java . util . concurrent . Callable < java . lang . Void > ( ) { @ org . jboss . test . capedwarf . cluster . test . Override public org . jboss . test . capedwarf . cluster . test . Void call ( ) throws org . jboss . test . capedwarf . cluster . test . Exception { getService ( ) . delete ( entity . getKey ( ) ) ; return null ; } }
public class aTest{ @Test public void testNormalDeleteFailContextNotEmpty ( ) { org . apache . directory . ldap . client . api . LdapConnection conn = getAdminConnection ( getLdapServer ( ) ) ; try { conn . delete ( "uid=akarasulu,ou=users,ou=system" ) ; org . junit . Assert . fail ( ) ; } catch ( org . apache . directory . api . ldap . model . exception . LdapContextNotEmptyException lcnee ) { org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testMarkdJDel ( ) { java . lang . String markdown = org . support . project . common . util . FileUtil . read ( getClass ( ) . getResourceAsStream ( "markdown/markdj-del.md" ) ) ; java . lang . String html = org . support . project . common . util . FileUtil . read ( getClass ( ) . getResourceAsStream ( "markdown/result-markdj-del.txt" ) ) ; java . lang . String result = org . support . project . knowledge . logic . MarkdownLogic . get ( ) . markdownToHtml ( markdown , MarkdownLogic . ENGINE_MARKEDJ ) . getHtml ( ) ; try { org . junit . Assert . assertArrayEquals ( read ( html ) , read ( result ) ) ; } }
public class aTest{ @Test public void testCustomIspnConfigFile ( ) { java . net . URL configURL = org . infinispan . it . osgi . BasicInfinispanOSGiTest . class . getClassLoader ( ) . getResource ( "infinispan.xml" ) ; org . infinispan . manager . EmbeddedCacheManager cacheManager = new org . infinispan . manager . DefaultCacheManager ( configURL . openStream ( ) ) ; cacheManager . defineConfiguration ( "default" , new org . infinispan . configuration . cache . ConfigurationBuilder ( ) . build ( ) ) ; try { org . infinispan . Cache < java . lang . String , java . lang . String > cache = cacheManager . getCache ( "default" ) ; cache . put ( "k1" , "v1" ) ; org . junit . Assert . assertEquals ( "v1" , cache . get ( "k1" ) ) ; } }
public class aTest{ @Test public void testComputation_7 ( ) { try { org . eclipse . xtend2 . lib . StringConcatenation _builder = new org . eclipse . xtend2 . lib . StringConcatenation ( ) ; _builder . append ( "package<sp>foo" ) ; _builder . newLine ( ) ; _builder . newLine ( ) ; _builder . append ( "/**" ) ; _builder . newLine ( ) ; _builder . append ( "*<sp>{@link<sp>String}" ) ; _builder . newLine ( ) ; _builder . append ( "*/" ) ; _builder . newLine ( ) ; _builder . append ( "class<sp>Foo{}" ) ; _builder . newLine ( ) ; final java . lang . String input = _builder . toString ( ) ; org . eclipse . emf . ecore . resource . Resource _eResource = this . clazz ( input ) . eResource ( ) ; final org . eclipse . xtext . resource . XtextResource resource = ( ( org . eclipse . xtext . resource . XtextResource ) ( _eResource ) ) ; final org . eclipse . xtext . nodemodel . ICompositeNode rootNode = resource . getParseResult ( ) . getRootNode ( ) ; final java . util . List < org . eclipse . xtext . util . ReplaceRegion > regions = this . javaDocTypeReferenceProvider . computeTypeRefRegions ( rootNode ) ; org . junit . Assert . assertEquals ( 1 , regions . size ( ) ) ; } }
public class aTest{ @Test public void controleerOpLegeWaardenWaardeisGevuld ( ) { final java . lang . String melding = "Waarde<sp>mag<sp>niet<sp>leeg<sp>zijn" ; try { nl . bzk . algemeenbrp . dal . domein . brp . util . ValidationUtils . controleerOpLegeWaarden ( melding , "Waarde" ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testFourSequences ( ) { java . util . List < java . lang . Integer > sequence0 = java . util . Arrays . asList ( new java . lang . Integer [ ] { 7 , 12 , 16 } ) ; java . util . List < java . lang . Integer > sequence1 = java . util . Arrays . asList ( new java . lang . Integer [ ] { 3 , 4 , 5 } ) ; java . util . List < java . lang . Integer > sequence2 = java . util . Arrays . asList ( new java . lang . Integer [ ] { 3 , 3 , 4 , 5 } ) ; java . util . List < java . lang . Integer > sequence3 = java . util . Arrays . asList ( new java . lang . Integer [ ] { 3 , 3 , 4 , 5 } ) ; java . util . function . Function < java . lang . Integer , java . lang . Integer > identity = java . util . function . Function . identity ( ) ; java . util . function . Function < java . lang . Integer , java . lang . Integer > times3 = ( x ) -> x * 3 ; java . util . function . Function < java . lang . Integer , java . lang . Integer > times4 = ( x ) -> x * 4 ; java . util . function . Function < java . lang . Integer , java . lang . Integer > times5 = ( x ) -> x * 5 ; @ org . numenta . nupic . util . SuppressWarnings ( { "unchecked" , "rawtypes" } ) org . numenta . nupic . util . GroupBy2 < java . lang . Integer > m = org . numenta . nupic . util . GroupBy2 . of ( new chaschev . lang . Pair ( sequence0 , identity ) , new chaschev . lang . Pair ( sequence1 , times3 ) , new chaschev . lang . Pair ( sequence2 , times4 ) , new chaschev . lang . Pair ( sequence3 , times5 ) ) ; java . util . List < org . numenta . nupic . util . Tuple > expectedValues = java . util . Arrays . asList ( new org . numenta . nupic . util . Tuple [ ] { new org . numenta . nupic . util . Tuple ( 7 , list ( 7 ) , none , none , none ) , new org . numenta . nupic . util . Tuple ( 9 , none , list ( 3 ) , none , none ) , new org . numenta . nupic . util . Tuple ( 12 , list ( 12 ) , list ( 4 ) , list ( 3 , 3 ) , none ) , new org . numenta . nupic . util . Tuple ( 15 , none , list ( 5 ) , none , list ( 3 , 3 ) ) , new org . numenta . nupic . util . Tuple ( 16 , list ( 16 ) , none , list ( 4 ) , none ) , new org . numenta . nupic . util . Tuple ( 20 , none , none , list ( 5 ) , list ( 4 ) ) , new org . numenta . nupic . util . Tuple ( 25 , none , none , none , list ( 5 ) ) } ) ; int i = 0 ; for ( org . numenta . nupic . util . Tuple t : m ) { int j = 0 ; for ( java . lang . Object o : t . all ( ) ) { org . junit . Assert . assertEquals ( o , expectedValues . get ( i ) . get ( j ) ) ; j ++ ; } }
public class aTest{ @Test public void testPercentFilter ( ) { org . apache . hadoop . mapreduce . lib . input . TestMRSequenceFileInputFilter . LOG . info ( "Testing<sp>Percent<sp>Filter<sp>with<sp>frequency:<sp>1000" ) ; org . apache . hadoop . mapreduce . lib . input . SequenceFileInputFilter . setFilterClass ( org . apache . hadoop . mapreduce . lib . input . TestMRSequenceFileInputFilter . job , SequenceFileInputFilter . PercentFilter . class ) ; SequenceFileInputFilter . PercentFilter . setFrequency ( org . apache . hadoop . mapreduce . lib . input . TestMRSequenceFileInputFilter . job . getConfiguration ( ) , 1000 ) ; org . apache . hadoop . mapreduce . lib . input . TestMRSequenceFileInputFilter . fs . delete ( org . apache . hadoop . mapreduce . lib . input . TestMRSequenceFileInputFilter . inDir , true ) ; for ( int length = 0 ; length < ( org . apache . hadoop . mapreduce . lib . input . TestMRSequenceFileInputFilter . MAX_LENGTH ) ; length += ( org . apache . hadoop . mapreduce . lib . input . TestMRSequenceFileInputFilter . random . nextInt ( ( ( org . apache . hadoop . mapreduce . lib . input . TestMRSequenceFileInputFilter . MAX_LENGTH ) / 10 ) ) ) + 1 ) { org . apache . hadoop . mapreduce . lib . input . TestMRSequenceFileInputFilter . LOG . info ( ( "******Number<sp>of<sp>records:<sp>" + length ) ) ; org . apache . hadoop . mapreduce . lib . input . TestMRSequenceFileInputFilter . createSequenceFile ( length ) ; int count = countRecords ( 1 ) ; org . apache . hadoop . mapreduce . lib . input . TestMRSequenceFileInputFilter . LOG . info ( ( ( "Accepted<sp>" + count ) + "<sp>records" ) ) ; int expectedCount = length / 1000 ; if ( ( expectedCount * 1000 ) != length ) expectedCount ++ ; org . junit . Assert . assertEquals ( count , expectedCount ) ; } }
public class aTest{ @Test public void testUnmodifiable ( ) { com . ibm . ws . microprofile . config . impl . SortedSources sources = new com . ibm . ws . microprofile . config . impl . SortedSources ( ) ; com . ibm . ws . microprofile . config . internals . test . TestSource testSource0 = new com . ibm . ws . microprofile . config . internals . test . TestSource ( 0 , "TestSource0" ) ; com . ibm . ws . microprofile . config . internals . test . TestSource testSource1 = new com . ibm . ws . microprofile . config . internals . test . TestSource ( 1 , "TestSource1" ) ; com . ibm . ws . microprofile . config . internals . test . TestSource testSource2 = new com . ibm . ws . microprofile . config . internals . test . TestSource ( 2 , "TestSource2" ) ; sources . add ( testSource0 ) ; sources . add ( testSource1 ) ; sources = sources . unmodifiable ( ) ; org . junit . Assert . assertEquals ( 2 , sources . size ( ) ) ; try { sources . add ( testSource2 ) ; org . junit . Assert . fail ( "Exception<sp>not<sp>thrown" ) ; } }
public class aTest{ @Test public void sendReturnsHeaderFieldsOnBadStatusException ( com . microsoft . azure . sdk . iot . service . transport . http . HttpConnection , java . net . URL ) { final java . util . Map < java . lang . String , java . util . List < java . lang . String > > headerFields = new java . util . HashMap ( ) ; final java . lang . String field = "test-field" ; final java . util . List < java . lang . String > values = new java . util . LinkedList ( ) ; final java . lang . String value = "test-value0" ; values . add ( value ) ; headerFields . put ( field , values ) ; final com . microsoft . azure . sdk . iot . service . transport . http . HttpMethod httpsMethod = com . microsoft . azure . sdk . iot . service . transport . http . HttpMethod . POST ; final byte [ ] body = new byte [ 0 ] ; final java . lang . String expectedValues = value ; new tests . unit . com . microsoft . azure . sdk . iot . service . transport . http . NonStrictExpectations ( ) { { mockUrl . getProtocol ( ) ; result = "http" ; mockConn . connect ( ) ; result = new java . io . IOException ( ) ; mockConn . getResponseHeaders ( ) ; result = headerFields ; } } ; com . microsoft . azure . sdk . iot . service . transport . http . HttpRequest request = new com . microsoft . azure . sdk . iot . service . transport . http . HttpRequest ( mockUrl , httpsMethod , body ) ; com . microsoft . azure . sdk . iot . service . transport . http . HttpResponse response = request . send ( ) ; java . lang . String testValues = response . getHeaderField ( field ) ; org . junit . Assert . assertThat ( testValues , org . hamcrest . CoreMatchers . is ( expectedValues ) ) ; } }
public class aTest{ @Test public void test10MIterations ( ) { final java . util . Map < java . lang . String , java . lang . String > attrs = new java . util . HashMap ( ) ; attrs . put ( "xx" , "world" ) ; final org . apache . nifi . attribute . expression . language . StandardPreparedQuery prepared = ( ( org . apache . nifi . attribute . expression . language . StandardPreparedQuery ) ( org . apache . nifi . attribute . expression . language . Query . prepare ( "${xx}" ) ) ) ; final long start = java . lang . System . nanoTime ( ) ; for ( int i = 0 ; i < 10000000 ; i ++ ) { org . junit . Assert . assertEquals ( "world" , prepared . evaluateExpressions ( attrs , null ) ) ; } }
public class aTest{ @Test public void testCancelTokenSingleManager ( ) { for ( int i = 0 ; i < ( org . apache . hadoop . security . token . delegation . TestZKDelegationTokenSecretManager . TEST_RETRIES ) ; i ++ ) { org . apache . hadoop . security . token . delegation . web . DelegationTokenManager tm1 = null ; java . lang . String connectString = zkServer . getConnectString ( ) ; org . apache . hadoop . conf . Configuration conf = getSecretConf ( connectString ) ; tm1 = new org . apache . hadoop . security . token . delegation . web . DelegationTokenManager ( conf , new org . apache . hadoop . io . Text ( "foo" ) ) ; tm1 . init ( ) ; org . apache . hadoop . security . token . Token < org . apache . hadoop . security . token . delegation . web . DelegationTokenIdentifier > token = ( ( org . apache . hadoop . security . token . Token < org . apache . hadoop . security . token . delegation . web . DelegationTokenIdentifier > ) ( tm1 . createToken ( org . apache . hadoop . security . UserGroupInformation . getCurrentUser ( ) , "foo" ) ) ) ; org . junit . Assert . assertNotNull ( token ) ; tm1 . cancelToken ( token , "foo" ) ; try { verifyTokenFail ( tm1 , token ) ; org . junit . Assert . fail ( "Expected<sp>InvalidToken" ) ; } }
public class aTest{ @Test public void blockTillAllCompleteOrFirstErrorErrorTest ( ) { int errorIndex = ( TEST_QTY ) / 2 ; java . util . List < org . threadly . concurrent . future . ListenableFuture < ? > > futures = org . threadly . concurrent . future . FutureUtilsTest . makeFutures ( org . threadly . concurrent . future . TEST_QTY , errorIndex ) ; org . threadly . concurrent . future . FutureUtils . blockTillAllComplete ( futures ) ; java . util . Iterator < org . threadly . concurrent . future . ListenableFuture < ? > > it = futures . iterator ( ) ; for ( int i = 0 ; i <= errorIndex ; i ++ ) { java . util . concurrent . Future < ? > f = it . next ( ) ; if ( i < errorIndex ) { org . junit . Assert . assertTrue ( f . isDone ( ) ) ; } }
public class aTest{ @Test public void testNoJarNoArgumentsAtAll ( ) { try { java . lang . String [ ] parameters = new java . lang . String [ ] { } ; org . apache . commons . cli . CommandLine line = new org . apache . commons . cli . PosixParser ( ) . parse ( eu . stratosphere . client . CliFrontend . getProgramSpecificOptions ( new org . apache . commons . cli . Options ( ) ) , parameters , false ) ; eu . stratosphere . client . CliFrontend frontend = new eu . stratosphere . client . CliFrontend ( ) ; java . lang . Object result = frontend . buildProgram ( line ) ; org . junit . Assert . assertTrue ( ( result == null ) ) ; } }
public class aTest{ @Test public void basicMixUriLiteralBsTest ( ) { final org . eclipse . rdf4j . query . algebra . evaluation . QueryBindingSet bs = new org . eclipse . rdf4j . query . algebra . evaluation . QueryBindingSet ( ) ; bs . addBinding ( "X" , org . apache . rya . indexing . pcj . storage . accumulo . AccumuloPcjSerializerTest . VF . createLiteral ( "literal1" ) ) ; bs . addBinding ( "Y" , org . apache . rya . indexing . pcj . storage . accumulo . AccumuloPcjSerializerTest . VF . createLiteral ( "5" , org . apache . rya . indexing . pcj . storage . accumulo . AccumuloPcjSerializerTest . VF . createIRI ( "http://www.w3.org/2001/XMLSchema#integer" ) ) ) ; bs . addBinding ( "literal1" 5 , org . apache . rya . indexing . pcj . storage . accumulo . AccumuloPcjSerializerTest . VF . createLiteral ( "5.0" , org . apache . rya . indexing . pcj . storage . accumulo . AccumuloPcjSerializerTest . VF . createIRI ( "literal1" 0 ) ) ) ; bs . addBinding ( "W" , org . apache . rya . indexing . pcj . storage . accumulo . AccumuloPcjSerializerTest . VF . createLiteral ( "1000" , org . apache . rya . indexing . pcj . storage . accumulo . AccumuloPcjSerializerTest . VF . createIRI ( "http://www.w3.org/2001/XMLSchema#long" ) ) ) ; bs . addBinding ( "literal1" 2 , org . apache . rya . indexing . pcj . storage . accumulo . AccumuloPcjSerializerTest . VF . createIRI ( "literal1" 6 ) ) ; bs . addBinding ( "B" , org . apache . rya . indexing . pcj . storage . accumulo . AccumuloPcjSerializerTest . VF . createIRI ( "literal1" 1 ) ) ; bs . addBinding ( "literal1" 4 , org . apache . rya . indexing . pcj . storage . accumulo . AccumuloPcjSerializerTest . VF . createIRI ( "literal1" 3 ) ) ; final org . apache . rya . indexing . pcj . storage . accumulo . VariableOrder varOrder = new org . apache . rya . indexing . pcj . storage . accumulo . VariableOrder ( "literal1" 2 , "W" , "X" , "Y" , "literal1" 5 , "B" , "literal1" 4 ) ; org . apache . rya . indexing . pcj . storage . accumulo . BindingSetConverter < byte [ ] > converter = new org . apache . rya . indexing . pcj . storage . accumulo . AccumuloPcjSerializer ( ) ; final byte [ ] byteVal = converter . convert ( bs , varOrder ) ; final org . eclipse . rdf4j . query . BindingSet newBs = converter . convert ( byteVal , varOrder ) ; org . junit . Assert . assertEquals ( bs , newBs ) ; } }
public class aTest{ @Test public void testCombinedProperties ( ) { java . lang . String old = java . lang . System . setProperty ( "org.teiid.val" , "200" ) ; try { org . teiid . core . util . TestPropertiesUtils . MyBean test = new org . teiid . core . util . TestPropertiesUtils . MyBean ( ) ; org . teiid . core . util . PropertiesUtils . setBeanProperties ( test , org . teiid . core . util . PropertiesUtils . getCombinedProperties ( ) , "org.teiid" , true ) ; org . junit . Assert . assertEquals ( 200 , test . getVal ( ) ) ; } }
public class aTest{ @Test public void testManagedAppListener ( ) { startupServer ( com . sun . sgs . test . impl . kernel . TestKernelAppListeners . ManagedAppListener . class ) ; txnScheduler . runTask ( new com . sun . sgs . test . util . TestAbstractKernelRunnable ( ) { public void run ( ) throws com . sun . sgs . test . impl . kernel . Exception { com . sun . sgs . test . impl . kernel . TestKernelAppListeners . ManagedAppListener listener = ( ( com . sun . sgs . test . impl . kernel . TestKernelAppListeners . ManagedAppListener ) ( dataService . getServiceBinding ( StandardProperties . APP_LISTENER ) ) ) ; org . junit . Assert . assertEquals ( listener . getState ( ) , new java . lang . Integer ( 1 ) ) ; } } }
public class aTest{ @Test public void test_javassist ( ) { org . milyn . javassist . JavassistSetter setter = buildSetterClass ( ) ; org . milyn . javassist . TestPOJO objInst = new org . milyn . javassist . TestPOJO ( ) ; for ( int i = 0 ; i < 10000 ; i ++ ) { setter . set ( objInst , "hi" ) ; } org . junit . Assert . assertEquals ( "hi" , objInst . getProp ( ) ) ; java . lang . Thread . sleep ( 1000 ) ; long start = java . lang . System . currentTimeMillis ( ) ; for ( int i = 0 ; i < ( org . milyn . javassist . JavassistTest . INVOKE_COUNT ) ; i ++ ) { setter . set ( objInst , "hi" ) ; } }
public class aTest{ @Test public void testResourceImplNegativeSize ( ) { java . io . InputStream is ; try { is = new java . io . FileInputStream ( content ) ; ddf . catalog . resource . impl . ResourceImpl ri = new ddf . catalog . resource . impl . ResourceImpl ( is , mimeType , ddf . catalog . resource . ResourceImplTest . TEST_NAME ) ; ri . setSize ( ( - 20L ) ) ; org . junit . Assert . assertEquals ( ( - 1 ) , ri . getSize ( ) ) ; } }
public class aTest{ @Test public void testDeleteFile ( ) { java . io . File f = new java . io . File ( trash , "test" ) ; org . eclipse . jgit . util . FileUtils . createNewFile ( f ) ; org . eclipse . jgit . util . FileUtils . delete ( f ) ; org . junit . Assert . assertFalse ( f . exists ( ) ) ; try { org . eclipse . jgit . util . FileUtils . delete ( f ) ; org . junit . Assert . fail ( "deletion<sp>of<sp>non-existing<sp>file<sp>must<sp>fail" ) ; } }
public class aTest{ @Test public void testSeqStats ( ) { uk . bl . wa . hadoop . indexer . mdx . MDXSeqStatsGeneratorIntegrationTest . log . info ( "Checking<sp>input<sp>file<sp>is<sp>present..." ) ; org . apache . hadoop . fs . Path [ ] inputFiles = org . apache . hadoop . fs . FileUtil . stat2Paths ( dfsCluster . getFileSystem ( ) . listStatus ( new org . apache . hadoop . fs . Path ( input , "mdx-seq/" ) , new org . apache . hadoop . mapred . OutputLogFilter ( ) ) ) ; org . junit . Assert . assertEquals ( 1 , inputFiles . length ) ; java . io . File tmpInputsFile = uk . bl . wa . hadoop . indexer . mdx . WARCMDXGeneratorIntegrationTest . writeInputFile ( inputFiles ) ; java . lang . String [ ] args = new java . lang . String [ ] { "-i" , tmpInputsFile . getAbsolutePath ( ) , "-o" , this . output . getName ( ) } ; uk . bl . wa . hadoop . indexer . mdx . MDXSeqStatsGenerator wir = new uk . bl . wa . hadoop . indexer . mdx . MDXSeqStatsGenerator ( ) ; uk . bl . wa . hadoop . indexer . mdx . MDXSeqStatsGeneratorIntegrationTest . log . info ( "Setting<sp>up<sp>job<sp>config..." ) ; org . apache . hadoop . mapred . JobConf jobConf = this . mrCluster . createJobConf ( ) ; wir . createJobConf ( jobConf , args ) ; uk . bl . wa . hadoop . indexer . mdx . MDXSeqStatsGeneratorIntegrationTest . log . info ( "Running<sp>job..." ) ; org . apache . hadoop . mapred . JobClient . runJob ( jobConf ) ; uk . bl . wa . hadoop . indexer . mdx . MDXSeqStatsGeneratorIntegrationTest . log . info ( "Job<sp>finished,<sp>checking<sp>the<sp>results..." ) ; org . apache . hadoop . fs . Path [ ] outputFiles = org . apache . hadoop . fs . FileUtil . stat2Paths ( dfsCluster . getFileSystem ( ) . listStatus ( output , new org . apache . hadoop . mapred . OutputLogFilter ( ) ) ) ; for ( org . apache . hadoop . fs . Path output : outputFiles ) { java . io . FileOutputStream fout = new java . io . FileOutputStream ( ( "target/" + ( output . getName ( ) ) ) ) ; uk . bl . wa . hadoop . indexer . mdx . MDXSeqStatsGeneratorIntegrationTest . log . info ( ( "<sp>---<sp>output<sp>:<sp>" + output ) ) ; if ( dfsCluster . getFileSystem ( ) . isFile ( output ) ) { java . io . InputStream is = dfsCluster . getFileSystem ( ) . open ( output ) ; org . apache . commons . io . IOUtils . copy ( is , fout ) ; } }
public class aTest{ @Test public void mongodbShouldStart ( ) { fr . jetoile . hadoopunit . integrationtest . MongoClient mongo = new fr . jetoile . hadoopunit . integrationtest . MongoClient ( fr . jetoile . hadoopunit . integrationtest . ManualIntegrationBootstrapTest . configuration . getString ( fr . jetoile . hadoopunit . integrationtest . MONGO_IP_KEY ) , fr . jetoile . hadoopunit . integrationtest . ManualIntegrationBootstrapTest . configuration . getInt ( fr . jetoile . hadoopunit . integrationtest . MONGO_PORT_KEY ) ) ; fr . jetoile . hadoopunit . integrationtest . DB db = mongo . getDB ( fr . jetoile . hadoopunit . integrationtest . ManualIntegrationBootstrapTest . configuration . getString ( fr . jetoile . hadoopunit . integrationtest . MONGO_DATABASE_NAME_KEY ) ) ; fr . jetoile . hadoopunit . integrationtest . DBCollection col = db . createCollection ( fr . jetoile . hadoopunit . integrationtest . ManualIntegrationBootstrapTest . configuration . getString ( fr . jetoile . hadoopunit . integrationtest . MONGO_COLLECTION_NAME_KEY ) , new fr . jetoile . hadoopunit . integrationtest . BasicDBObject ( ) ) ; col . save ( new fr . jetoile . hadoopunit . integrationtest . BasicDBObject ( "testDoc" , new java . util . Date ( ) ) ) ; fr . jetoile . hadoopunit . integrationtest . ManualIntegrationBootstrapTest . LOGGER . info ( "MONGODB:<sp>Number<sp>of<sp>items<sp>in<sp>collection:<sp>{}" , col . count ( ) ) ; org . junit . Assert . assertEquals ( 1 , col . count ( ) ) ; fr . jetoile . hadoopunit . integrationtest . DBCursor cursor = col . find ( ) ; while ( cursor . hasNext ( ) ) { fr . jetoile . hadoopunit . integrationtest . ManualIntegrationBootstrapTest . LOGGER . info ( "MONGODB:<sp>Document<sp>output:<sp>{}" , cursor . next ( ) ) ; } }
public class aTest{ @Test public void testSimpleNoMetadataConversion ( ) { java . util . Properties props = new java . util . Properties ( ) ; org . teiid . metadata . MetadataFactory mf = new org . teiid . metadata . MetadataFactory ( "<sp>optional<sp>string<sp>e9<sp>=<sp>9;\n" 6 , 1 , "model" , org . teiid . query . metadata . SystemMetadata . getInstance ( ) . getRuntimeTypeMap ( ) , props , null ) ; mf . setParser ( new org . teiid . query . parser . QueryParser ( ) ) ; mf . parse ( new java . io . FileReader ( org . teiid . core . util . UnitTestUtil . getTestDataFile ( "/*<sp>@Indexed<sp>*/\n" 3 ) ) ) ; org . teiid . translator . infinispan . hotrod . SchemaToProtobufProcessor tool = new org . teiid . translator . infinispan . hotrod . SchemaToProtobufProcessor ( ) ; tool . setIndexMessages ( true ) ; org . teiid . infinispan . api . InfinispanConnection conn = org . mockito . Mockito . mock ( org . teiid . infinispan . api . InfinispanConnection . class ) ; org . infinispan . commons . api . BasicCache cache = org . mockito . Mockito . mock ( org . infinispan . commons . api . BasicCache . class ) ; org . mockito . Mockito . stub ( cache . getName ( ) ) . toReturn ( "<sp>/*<sp>@Teiid(type=bigdecimal)<sp>*/\n" 6 ) ; org . mockito . Mockito . stub ( conn . getCache ( ) ) . toReturn ( cache ) ; org . teiid . infinispan . api . ProtobufResource resource = tool . process ( mf , conn ) ; java . lang . String expected = "<sp>/*<sp>@Teiid(type=bigdecimal)<sp>*/\n" 5 + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "model" 8 + "/*<sp>@Indexed<sp>*/\n" ) + "<sp>/*<sp>@Teiid(type=bigdecimal)<sp>*/\n" 7 ) + "<sp>optional<sp>string<sp>e9<sp>=<sp>9;\n" 1 ) + "model" 4 ) + "model" 3 ) + "/*<sp>@Indexed<sp>*/\n" 8 ) + "model" 9 ) + "<sp>/*<sp>@Teiid(type=bigdecimal)<sp>*/\n" 4 ) + "<sp>optional<sp>string<sp>e9<sp>=<sp>9;\n" 9 ) + "model" 8 ) + "/*<sp>@Indexed<sp>*/\n" ) + "model" 1 ) + "<sp>optional<sp>string<sp>e9<sp>=<sp>9;\n" 1 ) + "model" 4 ) + "model" 0 ) + "<sp>optional<sp>string<sp>e9<sp>=<sp>9;\n" 2 ) + "<sp>/*<sp>@Teiid(type=bigdecimal)<sp>*/\n" 2 ) + "model" 7 ) + "<sp>optional<sp>string<sp>e9<sp>=<sp>9;\n" 9 ) + "model" 8 ) + "/*<sp>@Indexed<sp>*/\n" ) + "/*<sp>@Indexed<sp>*/\n" 1 ) + "model" 4 ) + "model" 3 ) + "<sp>/*<sp>@Teiid(type=bigdecimal)<sp>*/\n" 1 ) + "<sp>/*<sp>@Teiid(type=bigdecimal)<sp>*/\n" 3 ) + "<sp>optional<sp>string<sp>e9<sp>=<sp>9;\n" 0 ) + "/*<sp>@Indexed<sp>*/\n" 5 ) + "<sp>/*<sp>@Teiid(type=byte)<sp>*/\n" ) + "<sp>optional<sp>string<sp>e9<sp>=<sp>9;\n" 8 ) + "<sp>optional<sp>string<sp>e9<sp>=<sp>9;\n" 5 ) + "<sp>optional<sp>string<sp>e7<sp>=<sp>7;\n" ) + "/*<sp>@Indexed<sp>*/\n" 2 ) + "<sp>/*<sp>@Teiid(type=bigdecimal)<sp>*/\n" ) + "<sp>optional<sp>string<sp>e9<sp>=<sp>9;\n" ) + "/*<sp>@Indexed<sp>*/\n" 0 ) + "model" 5 ) + "<sp>/*<sp>@Teiid(type=bigdecimal)<sp>*/\n" 8 ) + "model" 2 ) + "/*<sp>@Indexed<sp>*/\n" 9 ) + "model" 6 ) + "<sp>/*<sp>@Teiid(type=date)<sp>*/\n" ) + "/*<sp>@Indexed<sp>*/\n" 4 ) + "/*<sp>@Indexed<sp>*/\n" 6 ) + "<sp>/*<sp>@Teiid(type=bigdecimal)<sp>*/\n" 9 ) + "<sp>/*<sp>@Teiid(type=blob)<sp>*/\n" ) + "<sp>optional<sp>string<sp>e9<sp>=<sp>9;\n" 7 ) + "<sp>/*<sp>@Teiid(type=clob)<sp>*/\n" ) + "<sp>/*<sp>@Teiid(type=date)<sp>*/\n" 0 ) + "<sp>optional<sp>string<sp>e9<sp>=<sp>9;\n" 4 ) + "<sp>optional<sp>string<sp>e9<sp>=<sp>9;\n" 3 ) + "<sp>/*<sp>@Teiid(type=bigdecimal)<sp>*/\n" 0 ) + "/*<sp>@Indexed<sp>*/\n" 7 ) + "}\n\n" ) ; org . junit . Assert . assertEquals ( expected , resource . getContents ( ) ) ; } }
public class aTest{ @Test public void test_getTime ( int , int , int , int ) { java . time . LocalTime t = java . time . LocalTime . of ( h , m , s , ns ) ; java . time . LocalDateTime dt = java . time . LocalDateTime . of ( java . time . LocalDate . of ( 2011 , 7 , 30 ) , t ) ; org . junit . Assert . assertSame ( dt . toLocalTime ( ) , t ) ; } }
public class aTest{ @Test public void RequestAccessAndRefreshTokensUsingResponseUri_ReturnsCorrectTokens ( ) { try { com . microsoft . bingads . internal . OAuthRequestParameters expectedRequestParameters = new com . microsoft . bingads . internal . OAuthRequestParameters ( "test_id" , "test_secret" , new java . net . URL ( "https://test.com/login" ) , "authorization_code" , "code" , "123" ) ; expect ( oauthService . getAccessTokens ( eq ( expectedRequestParameters ) ) ) . andReturn ( expectedTokenInfo ) ; com . microsoft . bingads . OAuthWebAuthCodeGrant auth = com . microsoft . bingads . OAuthTest . CreateWebAuth ( "test_id" , "test_secret" , oauthService ) ; replayAll ( ) ; com . microsoft . bingads . OAuthTokens tokens = auth . requestAccessAndRefreshTokens ( new java . net . URL ( "http://test.com/login?code=123" ) ) ; org . junit . Assert . assertEquals ( expectedTokenInfo , tokens ) ; } }
public class aTest{ @Test public void shouldOnlyReturnNumberOfTermsRequested ( ) { final java . lang . String tableName = "shouldOnlyReturnNumberOfTermsRequested" ; org . apache . blur . thrift . TableGen . define ( tableName ) . cols ( "test" , "col1" ) . addRecord ( "1" , "1" , "aaa" ) . addRecord ( "2" , "2" , "ccc" 1 ) . addRecord ( "3" , "3" , "ccc" ) . addRecord ( "4" , "4" , "ccc" 0 ) . build ( getClient ( ) ) ; java . util . List < java . lang . String > terms = getClient ( ) . terms ( tableName , "test" , "col1" , "c" , ( ( short ) ( 1 ) ) ) ; java . util . List < java . lang . String > expected = com . google . common . collect . Lists . newArrayList ( "ccc" ) ; org . junit . Assert . assertEquals ( expected , terms ) ; } }
public class aTest{ @Test public void test_formatLocalizedDate ( java . time . chrono . Chronology , java . util . Locale , java . util . Locale , java . time . chrono . ChronoLocalDate , java . lang . String ) { java . time . format . DateTimeFormatter dtf = java . time . format . DateTimeFormatter . ofLocalizedDate ( FormatStyle . FULL ) . withChronology ( chrono ) . withLocale ( formatLocale ) . withDecimalStyle ( java . time . format . DecimalStyle . of ( numberingLocale ) ) ; java . lang . String text = dtf . format ( date ) ; org . junit . Assert . assertEquals ( text , expected ) ; } }
public class aTest{ @Test public void testMatchExactlyActionEdgeNodeErr ( ) { queriesString = org . apache . commons . lang . StringUtils . join ( new java . lang . String [ ] { "type=node" , "enabled=true" , "node2" 1 } , "&" ) ; target = new org . o3project . odenos . core . component . network . flow . query . BasicFlowQuery ( queriesString ) ; target . parse ( ) ; java . util . List < org . o3project . odenos . core . component . network . flow . basic . BasicFlowMatch > matches = null ; java . util . List < java . lang . String > path = null ; java . util . Map < java . lang . String , java . util . List < org . o3project . odenos . core . component . network . flow . basic . FlowAction > > edgeAction = null ; java . util . Map < java . lang . String , java . lang . String > flowAttributes = new java . util . HashMap < java . lang . String , java . lang . String > ( ) ; org . o3project . odenos . core . component . network . flow . basic . BasicFlow flow = new org . o3project . odenos . core . component . network . flow . basic . BasicFlow ( "1" , "" , "" , true , "" , "established" , matches , path , edgeAction , flowAttributes ) ; @ org . o3project . odenos . core . component . network . flow . query . SuppressWarnings ( "serial" ) java . util . List < org . o3project . odenos . core . component . network . flow . basic . FlowAction > actions = new java . util . ArrayList < org . o3project . odenos . core . component . network . flow . basic . FlowAction > ( ) { { add ( new org . o3project . odenos . core . component . network . flow . basic . FlowActionOutput ( ) { { output = "port1" ; } } ) ; add ( new org . o3project . odenos . core . component . network . flow . basic . FlowActionOutput ( ) { { output = "node2" 0 ; } } ) ; } } ; flow . addEdgeAction ( "node1" , actions . get ( 0 ) ) ; flow . addEdgeAction ( "node2" , actions . get ( 1 ) ) ; org . junit . Assert . assertThat ( target . matchExactly ( flow ) , org . hamcrest . CoreMatchers . is ( false ) ) ; } }
public class aTest{ @Test public void testSeg ( ) { org . apdplat . word . segmentation . Segmentation segmentation = new org . apdplat . word . segmentation . impl . ReverseMinimumMatching ( ) ; java . util . List < java . lang . String > text = new java . util . ArrayList ( ) ; text . add ( "" 0 ) ; text . add ( "" 4 ) ; text . add ( "" 6 ) ; text . add ( "[]" 9 ) ; text . add ( "" ) ; text . add ( "" 0 ) ; text . add ( "[,<sp>,<sp>,<sp>,<sp>]" 4 ) ; text . add ( "" 5 ) ; text . add ( "[,<sp>,<sp>,<sp>,<sp>]" 5 ) ; text . add ( "" 1 ) ; text . add ( "" 1 ) ; text . add ( "" 9 ) ; text . add ( "[,<sp>,<sp>,<sp>,<sp>,<sp>]" 4 ) ; text . add ( "" ) ; text . add ( "" 3 ) ; text . add ( "[,<sp>,<sp>,<sp>,<sp>,<sp>]" 0 ) ; text . add ( "" 5 ) ; text . add ( "" 6 ) ; text . add ( "[,<sp>,<sp>,<sp>,<sp>,<sp>]" 7 ) ; text . add ( "[,<sp>,<sp>,<sp>,<sp>,<sp>]" 2 ) ; text . add ( "[,<sp>,<sp>,<sp>,<sp>]" 1 ) ; text . add ( "" 4 ) ; text . add ( "" 4 ) ; text . add ( "[]" 4 ) ; text . add ( "[,<sp>,<sp>,<sp>,<sp>,<sp>]" 5 ) ; text . add ( "" 8 ) ; text . add ( "" 9 ) ; text . add ( "[,<sp>,<sp>,<sp>,<sp>]" 6 ) ; text . add ( "[]" 7 ) ; text . add ( "" ) ; text . add ( "[,<sp>,<sp>,<sp>,<sp>]" 0 ) ; text . add ( "" 4 ) ; text . add ( "[]" 1 ) ; text . add ( "" 8 ) ; text . add ( "" ) ; text . add ( "" 2 ) ; text . add ( "[]" 8 ) ; text . add ( "" 2 ) ; java . util . List < java . lang . String > expResult = new java . util . ArrayList ( ) ; expResult . add ( "" 3 ) ; expResult . add ( "[]" ) ; expResult . add ( "[,<sp>,<sp>,<sp>,<sp>,<sp>]" 1 ) ; expResult . add ( "[,<sp>,<sp>,<sp>,<sp>]" 8 ) ; expResult . add ( "[,<sp>,<sp>,<sp>,<sp>]" ) ; expResult . add ( "" 2 ) ; expResult . add ( "" 0 ) ; expResult . add ( "" 8 ) ; expResult . add ( "[,<sp>,<sp>,<sp>,<sp>,<sp>]" 3 ) ; expResult . add ( "[]" 3 ) ; expResult . add ( "" 9 ) ; expResult . add ( "[,<sp>,<sp>,<sp>,<sp>]" 9 ) ; expResult . add ( "[,<sp>,<sp>,<sp>,<sp>,<sp>]" 8 ) ; expResult . add ( "" 3 ) ; expResult . add ( "[,<sp>,<sp>,<sp>,<sp>,<sp>]" 6 ) ; expResult . add ( "[,<sp>,<sp>,<sp>,<sp>,<sp>,<sp>,<sp>,<sp>]" ) ; expResult . add ( "[,<sp>,<sp>]" ) ; expResult . add ( "[,<sp>,<sp>,<sp>,<sp>]" 2 ) ; expResult . add ( "" 5 ) ; expResult . add ( "[]" 5 ) ; expResult . add ( "[]" 2 ) ; expResult . add ( "" 7 ) ; expResult . add ( "" 0 ) ; expResult . add ( "" 7 ) ; expResult . add ( "" 1 ) ; expResult . add ( "[,<sp>,<sp>,<sp>,<sp>,<sp>]" ) ; expResult . add ( "[,<sp>,<sp>,<sp>,<sp>]" ) ; expResult . add ( "" 2 ) ; expResult . add ( "[,<sp>,<sp>,<sp>,<sp>]" 7 ) ; expResult . add ( "" 5 ) ; expResult . add ( "[,<sp>,<sp>,<sp>,<sp>]" 3 ) ; expResult . add ( "" 7 ) ; expResult . add ( "" 6 ) ; expResult . add ( "[]" 6 ) ; expResult . add ( "[,<sp>,<sp>,<sp>,<sp>,<sp>]" 9 ) ; expResult . add ( "" 1 ) ; expResult . add ( "" 3 ) ; expResult . add ( "[]" 0 ) ; for ( int i = 0 ; i < ( text . size ( ) ) ; i ++ ) { java . util . List < org . apdplat . word . segmentation . Word > result = segmentation . seg ( text . get ( i ) ) ; for ( org . apdplat . word . segmentation . Word word : result ) { word . setPartOfSpeech ( null ) ; } org . junit . Assert . assertEquals ( expResult . get ( i ) . toString ( ) , result . toString ( ) ) ; } } }
public class aTest{ @Test public void testInvalidClientSecret ( ) { java . lang . String code = authClient . authorizeClient ( clientEntity , "test1<sp>test2" ) . getCode ( ) ; org . junit . Assert . assertNotNull ( code ) ; restClient . setFollowRedirects ( false ) ; com . github . hburgmeier . jerseyoauth2 . testsuite . base . client . ResourceClient client = new com . github . hburgmeier . jerseyoauth2 . testsuite . base . client . ResourceClient ( clientEntity . getClientId ( ) , "Invalid" , "test1<sp>test2" ) ; try { client . getAccessToken ( code ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void test ( ) { final java . util . concurrent . atomic . AtomicInteger received = new java . util . concurrent . atomic . AtomicInteger ( ) ; output . subscribe ( new org . springframework . messaging . MessageHandler ( ) { @ org . springframework . xd . samples . Override public void handleMessage ( org . springframework . messaging . Message < ? > message ) throws org . springframework . messaging . MessagingException { org . springframework . xd . tuple . Tuple t = ( ( org . springframework . xd . tuple . Tuple ) ( message . getPayload ( ) ) ) ; received . getAndIncrement ( ) ; } } ) ; java . util . List < org . springframework . xd . tuple . Tuple > history = new java . util . ArrayList ( ) ; history . add ( org . springframework . xd . tuple . TupleBuilder . tuple ( ) . of ( "product" , "8" , "category" , "Players" ) ) ; history . add ( org . springframework . xd . tuple . TupleBuilder . tuple ( ) . of ( "product" , "8" , "category" , "Players" ) ) ; history . add ( org . springframework . xd . tuple . TupleBuilder . tuple ( ) . of ( "product" , "8" , "category" , "TVs" ) ) ; history . add ( org . springframework . xd . tuple . TupleBuilder . tuple ( ) . of ( "product" , "8" , "category" , "Mounts" ) ) ; history . add ( org . springframework . xd . tuple . TupleBuilder . tuple ( ) . of ( "product" , "0" , "category" , "Phones" ) ) ; history . add ( org . springframework . xd . tuple . TupleBuilder . tuple ( ) . of ( "product" , "2" , "category" , "Phones" ) ) ; history . add ( org . springframework . xd . tuple . TupleBuilder . tuple ( ) . of ( "product" , "17" , "category" , "Phones" ) ) ; history . add ( org . springframework . xd . tuple . TupleBuilder . tuple ( ) . of ( "product" , "TVs" 0 , "category" , "Phones" ) ) ; input . send ( new org . springframework . messaging . support . GenericMessage < java . util . List < org . springframework . xd . tuple . Tuple > > ( history ) ) ; org . junit . Assert . assertEquals ( 7 , received . get ( ) ) ; } }
public class aTest{ @Test public void ldapConfigUpdateLoginPropertyTest ( ) { com . ibm . websphere . simplicity . log . Log . info ( com . ibm . ws . security . wim . adapter . ldap . fat . LDAPRegistryDynamicUpdateTest . c , "Entering<sp>test<sp>ldapConfigUpdateLoginPropertyTest" 3 , "Entering<sp>test<sp>ldapConfigUpdateLoginPropertyTest" ) ; org . junit . Assert . assertEquals ( "Entering<sp>test<sp>ldapConfigUpdateLoginPropertyTest" 4 , com . ibm . ws . security . wim . adapter . ldap . fat . LDAPRegistryDynamicUpdateTest . servlet . getUserDisplayName ( "Entering<sp>test<sp>ldapConfigUpdateLoginPropertyTest" 9 ) ) ; } }
public class aTest{ @Test public void testSuchFunctionException ( ) { try { org . junit . Assert . assertFalse ( org . apache . tajo . catalog . TestCatalog . catalog . containFunction ( "test123" , org . apache . tajo . catalog . CatalogUtil . newSimpleDataTypeArray ( Type . INT4 ) ) ) ; org . apache . tajo . catalog . TestCatalog . catalog . getFunction ( "test123" , org . apache . tajo . catalog . CatalogUtil . newSimpleDataTypeArray ( Type . INT4 ) ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void testModifyDBDescription ( ) { try { com . fit2cloud . aliyun . rds . model . request . ModifyDBDescriptionRequest request = new com . fit2cloud . aliyun . rds . model . request . ModifyDBDescriptionRequest ( ) ; request . setDBInstanceId ( dBInstanceId ) ; request . setDBName ( dbName ) ; request . setDBDescription ( "new<sp>description" ) ; com . fit2cloud . aliyun . Response response = client . modifyDBDescription ( request ) ; System . out . println ( ( "testModifyDBDescription<sp>::<sp>" + ( new com . google . gson . Gson ( ) . toJson ( response ) ) ) ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void getBpmPlatformXmlFromEnvironmentVariableAsUrlLocation ( ) { try { java . lang . System . setProperty ( org . camunda . bpm . container . impl . jmx . deployment . BPM_PLATFORM_XML_SYSTEM_PROPERTY , org . camunda . bpm . container . impl . jmx . deployment . BpmPlatformXmlLocationTest . BPM_PLATFORM_XML_LOCATION_URL_HTTP_PROTOCOL ) ; java . net . URL url = new org . camunda . bpm . container . impl . tomcat . deployment . TomcatParseBpmPlatformXmlStep ( ) . lookupBpmPlatformXmlLocationFromEnvironmentVariable ( ) ; org . junit . Assert . assertEquals ( org . camunda . bpm . container . impl . jmx . deployment . BpmPlatformXmlLocationTest . BPM_PLATFORM_XML_LOCATION_URL_HTTP_PROTOCOL , url . toString ( ) ) ; } }
public class aTest{ @Test public void test ( ) { java . util . concurrent . ExecutorService executor = java . util . concurrent . Executors . newFixedThreadPool ( 20 ) ; eu . europa . esig . dss . validation . CommonCertificateVerifier certificateVerifier = new eu . europa . esig . dss . validation . CommonCertificateVerifier ( ) ; eu . europa . esig . dss . client . http . DataLoader dataLoader = new eu . europa . esig . dss . client . http . NativeHTTPDataLoader ( ) ; certificateVerifier . setDataLoader ( dataLoader ) ; java . util . List < java . util . concurrent . Future < java . lang . Boolean > > futures = new java . util . ArrayList < java . util . concurrent . Future < java . lang . Boolean > > ( ) ; for ( int i = 0 ; i < 200 ; i ++ ) { futures . add ( executor . submit ( new eu . europa . esig . dss . xades . validation . ConcurrentValidationTest . TestConcurrent ( certificateVerifier ) ) ) ; } for ( java . util . concurrent . Future < java . lang . Boolean > future : futures ) { try { org . junit . Assert . assertTrue ( future . get ( ) ) ; } }
public class aTest{ @Test public void testMasterBroker ( ) { org . glassfish . admingui . devtests . ClusterTest ct = new org . glassfish . admingui . devtests . ClusterTest ( ) ; try { final java . lang . String FIELD_MASTER_BROKER = "propertyForm:propertyContentPage:propertySheet:propertSectionTextField:maseterBrokerProp:MasterBroker" ; java . lang . String clusterName = "clusterName" + ( generateRandomString ( ) ) ; ct . deleteAllClusters ( ) ; final java . lang . String instance1 = clusterName + ( generateRandomString ( ) ) ; final java . lang . String instance2 = clusterName + ( generateRandomString ( ) ) ; ct . createCluster ( clusterName , instance1 , instance2 ) ; final java . lang . String ELEMENT_JMS_LINK = ( "treeForm:tree:configurations:" + clusterName ) + "-config:jmsConfiguration:jmsConfiguration_link" ; clickAndWait ( ELEMENT_JMS_LINK , org . glassfish . admingui . devtests . JavaMessageServiceTest . TRIGGER_JMS_SERVICE ) ; selectDropdownOption ( FIELD_MASTER_BROKER , instance2 ) ; clickAndWait ( "propertyForm:propertyContentPage:topButtons:saveButton" , org . glassfish . admingui . devtests . TRIGGER_NEW_VALUES_SAVED ) ; reset ( ) ; clickAndWait ( ELEMENT_JMS_LINK , org . glassfish . admingui . devtests . JavaMessageServiceTest . TRIGGER_JMS_SERVICE ) ; org . junit . Assert . assertEquals ( instance2 , getFieldValue ( FIELD_MASTER_BROKER ) ) ; } }
public class aTest{ @Test public void testCompressedView ( ) { for ( int testTime = 0 ; testTime < 10 ; testTime ++ ) { int testRounds = new java . util . Random ( ) . nextInt ( 20000 ) ; org . apache . flink . runtime . io . disk . iomanager . FileIOChannel . ID channel = ioManager . createChannel ( ) ; org . apache . flink . runtime . io . disk . iomanager . BufferFileWriter writer = this . ioManager . createBufferFileWriter ( channel ) ; org . apache . flink . table . runtime . io . CompressedHeaderlessChannelWriterOutputView outputView = new org . apache . flink . table . runtime . io . CompressedHeaderlessChannelWriterOutputView ( writer , compressionFactory , org . apache . flink . table . runtime . io . CompressedHeaderlessChannelTest . BUFFER_SIZE ) ; for ( int i = 0 ; i < testRounds ; i ++ ) { outputView . writeInt ( i ) ; } outputView . close ( ) ; int blockCount = outputView . getBlockCount ( ) ; org . apache . flink . table . runtime . io . CompressedHeaderlessChannelReaderInputView inputView = new org . apache . flink . table . runtime . io . CompressedHeaderlessChannelReaderInputView ( channel , ioManager , compressionFactory , org . apache . flink . table . runtime . io . CompressedHeaderlessChannelTest . BUFFER_SIZE , blockCount ) ; for ( int i = 0 ; i < testRounds ; i ++ ) { org . junit . Assert . assertEquals ( i , inputView . readInt ( ) ) ; } }
public class aTest{ @Test public void testRaw ( ) { new com . googlecode . jatl . Html ( writer ) { { html ( ) . head ( ) . end ( ) . body ( ) . h1 ( ) . text ( "hello" ) . end ( ) ; div ( ) . raw ( "<crap></crap>" ) . end ( ) ; endAll ( ) ; } } ; java . lang . String result = writer . getBuffer ( ) . toString ( ) ; java . lang . String expected = "\n" + ( ( ( ( ( ( ( ( ( "<html>\n" + "\t<body>\n" 0 ) + "\t</head>\n" ) + "\t<body>\n" ) + "\t\t<h1>hello\n" ) + "\t<body>\n" 2 ) + "\t\t<div><crap></crap>\n" ) + "\t<body>\n" 1 ) + "\t</body>\n" ) + "</html>" ) ; org . junit . Assert . assertEquals ( expected , result ) ; } }
public class aTest{ @Test public void testServiceAccessFromListener ( ) { org . osgi . framework . Bundle bundle = installBundle ( getBundleArchiveA ( ) ) ; try { bundle . start ( ) ; final org . osgi . framework . BundleContext context = bundle . getBundleContext ( ) ; final java . util . concurrent . atomic . AtomicInteger counter = new java . util . concurrent . atomic . AtomicInteger ( ) ; org . osgi . framework . ServiceListener listener = new org . osgi . framework . ServiceListener ( ) { @ org . jboss . test . osgi . framework . service . Override public void serviceChanged ( org . osgi . framework . ServiceEvent event ) { if ( ( org . osgi . framework . ServiceEvent . REGISTERED ) == ( event . getType ( ) ) ) { org . osgi . framework . ServiceReference sref = event . getServiceReference ( ) ; if ( ( context . getService ( sref ) ) != null ) counter . incrementAndGet ( ) ; } } } ; context . addServiceListener ( listener ) ; context . registerService ( org . osgi . framework . BundleContext . class . getName ( ) , context , null ) ; org . junit . Assert . assertEquals ( 1 , counter . get ( ) ) ; } }
public class aTest{ @Test public void testImportedRuleWithAction ( ) { java . lang . String slave = "parser<sp>grammar<sp>S;\n" + "a<sp>@after<sp>{int<sp>x;}<sp>:<sp>B<sp>;\n" ; mkdir ( tmpdir ) ; writeFile ( tmpdir , "S.g4" , slave ) ; java . lang . String master = "grammar<sp>M;\n" + ( ( ( "import<sp>S;\n" + "parser<sp>grammar<sp>S;\n" 2 ) + "B<sp>:<sp>'b'<sp>;" ) + "WS<sp>:<sp>(\'<sp>\'|\'\\n\')<sp>-><sp>skip<sp>;\n" ) ; java . lang . String found = execParser ( "M.g4" , master , "parser<sp>grammar<sp>S;\n" 3 , "parser<sp>grammar<sp>S;\n" 0 , "parser<sp>grammar<sp>S;\n" 1 , "b" , debug ) ; org . junit . Assert . assertEquals ( "" , found ) ; } }
public class aTest{ @Test public void iterate ( ) { com . mysema . commons . lang . CloseableIterator < com . querydsl . jpa . Cat > cats = query ( ) . from ( com . querydsl . jpa . JPABase . cat ) . select ( com . querydsl . jpa . JPABase . cat ) . iterate ( ) ; while ( cats . hasNext ( ) ) { com . querydsl . jpa . Cat cat = cats . next ( ) ; org . junit . Assert . assertNotNull ( cat ) ; } }
public class aTest{ @Test public void test2 ( ) { com . lagodiuk . ga . Population < com . lagodiuk . ga . ArrayChromosomeTest . ArrayChromosome > population = this . createPopulation ( ) ; com . lagodiuk . ga . ArrayChromosomeTest . ConsecutiveNumbersFitness fitness = new com . lagodiuk . ga . ArrayChromosomeTest . ConsecutiveNumbersFitness ( ) ; com . lagodiuk . ga . GeneticAlgorithm < com . lagodiuk . ga . ArrayChromosomeTest . ArrayChromosome , java . lang . Integer > environment = new com . lagodiuk . ga . GeneticAlgorithm < com . lagodiuk . ga . ArrayChromosomeTest . ArrayChromosome , java . lang . Integer > ( population , fitness ) ; environment . addIterationListener ( new com . lagodiuk . ga . IterartionListener < com . lagodiuk . ga . ArrayChromosomeTest . ArrayChromosome , java . lang . Integer > ( ) { private com . lagodiuk . ga . ArrayChromosomeTest . ArrayChromosome previousBestChromosome = null ; private com . lagodiuk . ga . Integer previousBestChromosomeFitness = null ; @ com . lagodiuk . ga . Override public void update ( com . lagodiuk . ga . GeneticAlgorithm < com . lagodiuk . ga . ArrayChromosomeTest . ArrayChromosome , java . lang . Integer > environment ) { com . lagodiuk . ga . ArrayChromosomeTest . ArrayChromosome currentBestChromosome = environment . getBest ( ) ; java . lang . Integer currentBestChromosomeFitness = environment . fitness ( currentBestChromosome ) ; if ( ( this . previousBestChromosome ) != null ) { org . junit . Assert . assertTrue ( ( ( currentBestChromosomeFitness . compareTo ( this . previousBestChromosomeFitness ) ) <= 0 ) ) ; } }
public class aTest{ @Test public void shouldExportBooleanQueryWithAllOptions ( ) { com . couchbase . client . java . search . queries . PrefixQuery innerA = com . couchbase . client . java . search . SearchQuery . prefix ( "someterm" ) . boost ( 2.0 ) ; com . couchbase . client . java . search . queries . PrefixQuery innerB = com . couchbase . client . java . search . SearchQuery . prefix ( "explain" 3 ) ; com . couchbase . client . java . search . queries . PrefixQuery innerC = com . couchbase . client . java . search . SearchQuery . prefix ( "termC" ) ; com . couchbase . client . java . search . queries . BooleanQuery fts = com . couchbase . client . java . search . SearchQuery . booleans ( ) . boost ( 1.5 ) . must ( innerA ) . mustNot ( innerB ) . should ( innerA , innerB , innerC ) . shouldMin ( 3 ) ; com . couchbase . client . java . search . SearchQuery query = new com . couchbase . client . java . search . SearchQuery ( "foo" , fts ) . explain ( ) ; com . couchbase . client . java . document . json . JsonObject expectedInnerA = com . couchbase . client . java . document . json . JsonObject . create ( ) . put ( "prefix" , "someterm" ) . put ( "explain" 1 , 2.0 ) ; com . couchbase . client . java . document . json . JsonObject expectedInnerB = com . couchbase . client . java . document . json . JsonObject . create ( ) . put ( "prefix" , "explain" 3 ) ; com . couchbase . client . java . document . json . JsonObject expectedInnerC = com . couchbase . client . java . document . json . JsonObject . create ( ) . put ( "prefix" , "termC" ) ; com . couchbase . client . java . document . json . JsonObject expectedMust = com . couchbase . client . java . document . json . JsonObject . create ( ) . put ( "explain" 0 , com . couchbase . client . java . document . json . JsonArray . from ( expectedInnerA ) ) ; com . couchbase . client . java . document . json . JsonObject expectedMustNot = com . couchbase . client . java . document . json . JsonObject . create ( ) . put ( "disjuncts" , com . couchbase . client . java . document . json . JsonArray . from ( expectedInnerB ) ) ; com . couchbase . client . java . document . json . JsonObject expectedShould = com . couchbase . client . java . document . json . JsonObject . create ( ) . put ( "disjuncts" , com . couchbase . client . java . document . json . JsonArray . from ( expectedInnerA , expectedInnerB , expectedInnerC ) ) . put ( "min" , 3 ) ; com . couchbase . client . java . document . json . JsonObject expected = com . couchbase . client . java . document . json . JsonObject . create ( ) . put ( "explain" 2 , com . couchbase . client . java . document . json . JsonObject . create ( ) . put ( "explain" 1 , 1.5 ) . put ( "must" , expectedMust ) . put ( "must_not" , expectedMustNot ) . put ( "should" , expectedShould ) ) . put ( "explain" , true ) ; org . junit . Assert . assertEquals ( expected , query . export ( ) ) ; } }
public class aTest{ @Test public void testIndependentTimeout ( ) { org . glassfish . grizzly . http . server . HttpServer httpServer = org . glassfish . grizzly . http . server . HttpServer . createSimpleServer ( "." , org . glassfish . grizzly . websockets . TimeoutTest . PORT ) ; httpServer . getServerConfiguration ( ) . setHttpServerName ( "WebSocket<sp>Server" ) ; httpServer . getServerConfiguration ( ) . setName ( "WebSocket<sp>Server" ) ; for ( org . glassfish . grizzly . http . server . NetworkListener networkListener : httpServer . getListeners ( ) ) { networkListener . registerAddOn ( new org . glassfish . grizzly . websockets . WebSocketAddOn ( ) ) ; networkListener . getKeepAlive ( ) . setIdleTimeoutInSeconds ( 5 ) ; } org . glassfish . grizzly . websockets . WebSocketEngine . getEngine ( ) . register ( "" , "/echo" , new org . glassfish . grizzly . websockets . EchoApplication ( ) ) ; final org . glassfish . grizzly . websockets . EchoWebSocketApplication app = new org . glassfish . grizzly . websockets . EchoWebSocketApplication ( ) ; org . glassfish . grizzly . websockets . WebSocketClient socket = null ; try { httpServer . start ( ) ; org . glassfish . grizzly . websockets . WebSocketEngine . getEngine ( ) . register ( app ) ; socket = new org . glassfish . grizzly . websockets . WebSocketClient ( ( ( "wss://localhost:" + ( org . glassfish . grizzly . websockets . TimeoutTest . PORT ) ) + "/echo" ) ) ; socket . connect ( ) ; java . lang . Thread . sleep ( 10000 ) ; org . junit . Assert . assertTrue ( socket . isConnected ( ) ) ; } }
public class aTest{ @Test public void testWhereQuery ( ) { org . apache . tinkerpop . gremlin . structure . Vertex tnt = sqlgGraph . addVertex ( T . label , "tenant" , "__type" , "tenant" ) ; org . apache . tinkerpop . gremlin . structure . Vertex env = sqlgGraph . addVertex ( T . label , "environment" , "__type" , "environment" ) ; org . apache . tinkerpop . gremlin . structure . Vertex res = sqlgGraph . addVertex ( T . label , "hasData" 0 , "__type" , "hasData" 0 ) ; org . apache . tinkerpop . gremlin . structure . Vertex de = sqlgGraph . addVertex ( T . label , "dataEntity" , "__type" , "dataEntity" ) ; org . apache . tinkerpop . gremlin . structure . Vertex dRoot = sqlgGraph . addVertex ( T . label , "structuredData" , "__type" , "structuredData" ) ; org . apache . tinkerpop . gremlin . structure . Vertex dPrims = sqlgGraph . addVertex ( T . label , "structuredData" , "__type" , "structuredData" , "__structuredDataKey" , "primitives" ) ; org . apache . tinkerpop . gremlin . structure . Vertex d0 = sqlgGraph . addVertex ( T . label , "structuredData" , "__type" , "structuredData" , "__structuredDataIndex" , 0 ) ; tnt . addEdge ( "contains" , env ) ; env . addEdge ( "contains" , res ) ; res . addEdge ( "contains" , de ) ; de . addEdge ( "hasData" , dRoot ) ; dRoot . addEdge ( "contains" , dPrims ) ; dPrims . addEdge ( "contains" , d0 ) ; java . util . Iterator < org . apache . tinkerpop . gremlin . structure . Vertex > results = gt . V ( res ) . out ( "contains" ) . has ( "__type" , "dataEntity" ) . where ( __ . out ( "hasData" ) . out ( "contains" ) . has ( "__type" , "structuredData" ) . has ( "__structuredDataKey" , "primitives" ) . out ( "contains" ) . has ( "__type" , "structuredData" ) ) ; org . junit . Assert . assertEquals ( de , results . next ( ) ) ; } }
public class aTest{ @Test public void test ( ) { final com . splout . db . common . SploutConfiguration config = com . splout . db . common . SploutConfiguration . getTestConfig ( ) ; final com . splout . db . qnode . QNodeHandler handler = new com . splout . db . qnode . QNodeHandler ( ) ; com . splout . db . qnode . QNode qnode = com . splout . db . common . TestUtils . getTestQNode ( config , handler ) ; com . splout . db . dnode . DNodeHandler dNodeHandler1 = new com . splout . db . dnode . DNodeHandler ( ) ; final com . splout . db . common . SploutConfiguration config1 = com . splout . db . common . SploutConfiguration . getTestConfig ( ) ; com . splout . db . dnode . DNode dnode1 = com . splout . db . common . TestUtils . getTestDNode ( config1 , dNodeHandler1 , ( ( "dnode-" + ( this . getClass ( ) . getName ( ) ) ) + "-1" ) ) ; final java . lang . String dnode1Address = dnode1 . getAddress ( ) ; try { org . junit . Assert . assertEquals ( handler . getDNodeList ( ) . size ( ) , 1 ) ; new com . splout . db . common . TestUtils . NotWaitingForeverCondition ( ) { @ com . splout . db . qnode . Override public boolean endCondition ( ) { return ( ( handler . getContext ( ) . getThriftClientCache ( ) . get ( dnode1Address ) ) != null ) && ( ( handler . getContext ( ) . getThriftClientCache ( ) . get ( dnode1Address ) . size ( ) ) == 40 ) ; } } }
public class aTest{ @Test public void testCustomPostSubmissionAction ( ) { final java . util . Date date = new java . util . Date ( ) ; new org . openmrs . module . htmlformentry . RegressionTestHelper ( ) { @ org . openmrs . module . htmlformentry . Override public java . lang . String getFormName ( ) { return "postSubmissionAction" ; } @ org . openmrs . module . htmlformentry . Override public java . lang . String [ ] widgetLabels ( ) { return new java . lang . String [ ] { "Date:" , "Location:" , "Provider:" } ; } @ org . openmrs . module . htmlformentry . Override public void setupRequest ( org . springframework . mock . web . MockHttpServletRequest request , java . util . Map < java . lang . String , java . lang . String > widgets ) { request . addParameter ( widgets . get ( "Date:" ) , dateAsString ( date ) ) ; request . addParameter ( widgets . get ( "Location:" ) , "2" ) ; request . addParameter ( widgets . get ( "Provider:" ) , "502" ) ; } @ org . openmrs . module . htmlformentry . Override public void testResults ( org . openmrs . module . htmlformentry . SubmissionResults results ) { org . junit . Assert . assertThat ( org . openmrs . module . htmlformentry . test . TestCustomSubmissionAction . getNumberOfCalls ( ) , org . hamcrest . core . Is . is ( 1 ) ) ; } } }
public class aTest{ @Test public void distVersionShouldBeReturned ( java . lang . String , java . lang . String , java . lang . String ) { final lcmc . host . domain . parser . DistributionDetector distributionDetector = new lcmc . host . domain . parser . DistributionDetector ( null ) ; distributionDetector . detect ( com . google . common . collect . ImmutableList . of ( "Linux" , "" , "3.16" , version , dist ) ) ; org . junit . Assert . assertThat ( distributionDetector . getDistVersionString ( version ) , org . hamcrest . core . Is . is ( distVersion ) ) ; } }
public class aTest{ @Test public void testReplacewith ( ) { search = null ; search = new org . odftoolkit . odfdom . incubator . search . TextNavigation ( "ODFDOM" , doc ) ; org . odftoolkit . odfdom . incubator . search . TextSelection nextSelect = null ; org . odftoolkit . odfdom . incubator . search . TextNavigation nextsearch = new org . odftoolkit . odfdom . incubator . search . TextNavigation ( "next" , doc ) ; if ( nextsearch . hasNext ( ) ) { nextSelect = ( ( org . odftoolkit . odfdom . incubator . search . TextSelection ) ( nextsearch . getCurrentItem ( ) ) ) ; } org . odftoolkit . odfdom . incubator . doc . style . OdfStyle style = new org . odftoolkit . odfdom . incubator . doc . style . OdfStyle ( contentDOM ) ; style . setProperty ( StyleTextPropertiesElement . FontWeight , "bold" ) ; style . setStyleFamilyAttribute ( "text" ) ; int i = 0 ; while ( search . hasNext ( ) ) { if ( i > 0 ) { org . odftoolkit . odfdom . incubator . search . TextSelection item = ( ( org . odftoolkit . odfdom . incubator . search . TextSelection ) ( search . getCurrentItem ( ) ) ) ; try { item . replaceWith ( "Odf<sp>Toolkit" ) ; item . applyStyle ( style ) ; } catch ( org . odftoolkit . odfdom . incubator . search . InvalidNavigationException e ) { org . junit . Assert . fail ( e . getMessage ( ) ) ; } } i ++ ; } search = new org . odftoolkit . odfdom . incubator . search . TextNavigation ( "Odf<sp>Toolkit" , doc ) ; int j = 0 ; while ( search . hasNext ( ) ) { j ++ ; } org . junit . Assert . assertTrue ( ( i == j ) ) ; try { nextSelect . replaceWith ( "bbb" ) ; } }
public class aTest{ @Test public void testGroupBy ( ) { javax . json . JsonObject result = org . glassfish . json . tests . JsonCollectorTest . contacts . getValuesAs ( javax . json . JsonObject . class ) . stream ( ) . collect ( javax . json . stream . JsonCollectors . groupingBy ( ( x ) -> ( ( javax . json . JsonObject ) ( x ) ) . getString ( "<sp>'gender':<sp>'M',<sp>" 8 ) ) ) ; javax . json . JsonValue expected = org . glassfish . json . JsonUtil . toJson ( ( "<sp>'gender':<sp>'M',<sp>" 4 + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "<sp>'gender':<sp>'M',<sp>" 0 + "<sp>'gender':<sp>'M',<sp>" 5 ) + "<sp>'gender':<sp>'M',<sp>" 7 ) + "<sp>'gender':<sp>'F',<sp>" ) + "<sp>'phones':<sp>{<sp>" ) + "<sp>'mobile':<sp>'707-999-5555'}}," ) + "<sp>{<sp>'name':<sp>'Joanna',<sp>" ) + "<sp>'gender':<sp>'F',<sp>" ) + "<sp>'phones':<sp>{<sp>" ) + "<sp>'mobile':<sp>'505-333-4444'}}<sp>" ) + "<sp>'gender':<sp>'M',<sp>" 2 ) + "'M':<sp>" ) + "<sp>'gender':<sp>'M',<sp>" 0 ) + "<sp>{<sp>'name':<sp>'Duke',<sp>" ) + "<sp>'gender':<sp>'M',<sp>" 3 ) + "<sp>'gender':<sp>'M',<sp>" ) + "<sp>'phones':<sp>{<sp>" ) + "<sp>'home':<sp>'650-123-4567',<sp>" ) + "<sp>'gender':<sp>'M',<sp>" 1 ) + "<sp>]<sp>" ) + "<sp>'gender':<sp>'M',<sp>" 6 ) ) ) ; org . junit . Assert . assertEquals ( result , expected ) ; } }
public class aTest{ @Test public void runnerMemoryShouldBeAboveZero ( int , int , int ) { when ( locale . messagesIncorrectValue ( ) ) . thenReturn ( org . eclipse . che . ide . ext . runner . client . util . RunnerUtilImplTest . SOME_TEXT ) ; boolean isCorrect = util . isRunnerMemoryCorrect ( totalMemory , usedMemory , availableMemory ) ; verifyShowWarning ( ) ; verify ( locale ) . messagesIncorrectValue ( ) ; org . junit . Assert . assertThat ( isCorrect , org . hamcrest . CoreMatchers . is ( false ) ) ; } }
public class aTest{ @Test public void test11_random_in_number_field ( ) { org . apache . commons . math3 . random . RandomGenerator rnd = getRandom ( ) ; cc . redberry . rings . util . RandomDataGenerator rndd = getRandomData ( ) ; org . apache . commons . math3 . stat . descriptive . DescriptiveStatistics subresultant = new org . apache . commons . math3 . stat . descriptive . DescriptiveStatistics ( ) ; org . apache . commons . math3 . stat . descriptive . DescriptiveStatistics modular = new org . apache . commons . math3 . stat . descriptive . DescriptiveStatistics ( ) ; for ( int i = 0 ; i < 10 ; ++ i ) { cc . redberry . rings . poly . univar . UnivariatePolynomial < cc . redberry . rings . Rational < cc . redberry . rings . bigint . BigInteger > > minimalPoly = cc . redberry . rings . poly . univar . IrreduciblePolynomials . randomIrreduciblePolynomialOverZ ( rndd . nextInt ( 2 , 5 ) , rnd ) . mapCoefficients ( cc . redberry . rings . poly . univar . Q , Q :: mkNumerator ) ; minimalPoly . setLC ( cc . redberry . rings . poly . univar . Q . mkNumerator ( rndd . nextInt ( 1 , 10 ) ) ) ; if ( ! ( cc . redberry . rings . poly . univar . IrreduciblePolynomials . irreducibleQ ( minimalPoly ) ) ) { -- i ; continue ; } cc . redberry . rings . poly . AlgebraicNumberField < cc . redberry . rings . poly . univar . UnivariatePolynomial < cc . redberry . rings . Rational < cc . redberry . rings . bigint . BigInteger > > > field = new cc . redberry . rings . poly . AlgebraicNumberField ( minimalPoly ) ; System . out . println ( ( "Field<sp>:<sp>" + field ) ) ; java . util . function . Supplier < cc . redberry . rings . poly . univar . UnivariatePolynomial < cc . redberry . rings . poly . univar . UnivariatePolynomial < cc . redberry . rings . Rational < cc . redberry . rings . bigint . BigInteger > > > > rndPoly = ( ) -> cc . redberry . rings . poly . univar . RandomUnivariatePolynomials . randomPoly ( rndd . nextInt ( 2 , 8 ) , field , ( __ ) -> cc . redberry . rings . poly . univar . RandomUnivariatePolynomials . randomPoly ( minimalPoly . degree , cc . redberry . rings . poly . univar . Q , ( ___ ) -> cc . redberry . rings . poly . univar . Q . mk ( rnd . nextInt ( 10 ) , ( 1 + ( rnd . nextInt ( 10 ) ) ) ) , rnd ) , rnd ) ; for ( int j = 0 ; j < 5 ; ++ j ) { cc . redberry . rings . poly . univar . UnivariatePolynomial < cc . redberry . rings . poly . univar . UnivariatePolynomial < cc . redberry . rings . Rational < cc . redberry . rings . bigint . BigInteger > > > a = rndPoly . get ( ) ; cc . redberry . rings . poly . univar . UnivariatePolynomial < cc . redberry . rings . poly . univar . UnivariatePolynomial < cc . redberry . rings . Rational < cc . redberry . rings . bigint . BigInteger > > > b = rndPoly . get ( ) ; long start ; long elapsed ; start = java . lang . System . nanoTime ( ) ; cc . redberry . rings . poly . univar . UnivariatePolynomial < cc . redberry . rings . Rational < cc . redberry . rings . bigint . BigInteger > > expected = SubresultantPRS ( a , b ) . resultant ( ) ; elapsed = ( java . lang . System . nanoTime ( ) ) - start ; subresultant . addValue ( elapsed ) ; System . out . println ( ( "Subresultant<sp>:<sp>" + ( cc . redberry . rings . util . TimeUnits . nanosecondsToString ( elapsed ) ) ) ) ; start = java . lang . System . nanoTime ( ) ; cc . redberry . rings . poly . univar . UnivariatePolynomial < cc . redberry . rings . Rational < cc . redberry . rings . bigint . BigInteger > > actual = ModularResultantInNumberField ( a , b ) ; elapsed = ( java . lang . System . nanoTime ( ) ) - start ; modular . addValue ( elapsed ) ; org . junit . Assert . assertEquals ( expected , actual ) ; System . out . println ( ( "Modular<sp>:<sp>" + ( cc . redberry . rings . util . TimeUnits . nanosecondsToString ( elapsed ) ) ) ) ; System . out . println ( ) ; } } }
public class aTest{ @Test public void testCancelRunningSnapshot ( ) { setupRocksKeyedStateBackend ( ) ; try { java . util . concurrent . RunnableFuture < org . apache . flink . runtime . state . SnapshotResult < org . apache . flink . runtime . state . KeyedStateHandle > > snapshot = keyedStateBackend . snapshot ( 0L , 0L , testStreamFactory , org . apache . flink . runtime . checkpoint . CheckpointOptions . forCheckpointWithDefaultLocation ( ) ) ; java . lang . Thread asyncSnapshotThread = new java . lang . Thread ( snapshot ) ; asyncSnapshotThread . start ( ) ; waiter . await ( ) ; waiter . reset ( ) ; runStateUpdates ( ) ; snapshot . cancel ( true ) ; blocker . trigger ( ) ; for ( org . apache . flink . runtime . util . BlockingCheckpointOutputStream stream : testStreamFactory . getAllCreatedStreams ( ) ) { org . junit . Assert . assertTrue ( stream . isClosed ( ) ) ; } }
public class aTest{ @Test public void t03_columns ( ) { com . jajja . jorm . Transaction moria = psql . Moria . open ( ) ; try { moria . Goblin goblin = moria . select ( moria . Goblin . class , "SELECT<sp>*<sp>FROM<sp>#1#<sp>WHERE<sp>name<sp>=<sp>'Bolg'" , moria . Goblin . class ) ; org . junit . Assert . assertNotNull ( goblin ) ; } }
public class aTest{ @Test public void testTimeNotPresent ( ) { com . couchbase . jdbc . JDBCTestUtils . setConnection ( null ) ; java . lang . String drop_primary_index = "drop<sp>primary<sp>index<sp>on<sp>default" ; com . couchbase . jdbc . JDBCTestUtils . createPrimaryIndexes ( TestUtil . clusterInfo . bucketInformation . keySet ( ) ) ; org . json . simple . JSONObject obj = new org . json . simple . JSONObject ( ) ; java . lang . String deleteData = "delete<sp>from<sp>default" ; com . couchbase . jdbc . JDBCTestUtils . runQueryWithoutResult ( deleteData ) ; java . util . HashMap < java . lang . String , java . lang . Object > map = new java . util . HashMap < java . lang . String , java . lang . Object > ( ) ; map . put ( "name" , "NAME" ) ; obj . putAll ( map ) ; org . json . simple . JSONArray expectedArray = new org . json . simple . JSONArray ( ) ; java . util . HashMap < java . lang . String , org . json . simple . JSONObject > objMap = new java . util . HashMap < java . lang . String , org . json . simple . JSONObject > ( ) ; objMap . put ( "1" , obj ) ; expectedArray . add ( obj ) ; com . couchbase . jdbc . JDBCTestUtils . insertData ( objMap , "default" ) ; java . lang . Thread . sleep ( 1000 ) ; java . lang . String query = "select<sp>name<sp>from<sp>default" ; com . couchbase . jdbc . JDBCTestUtils . setConnection ( null ) ; try ( java . sql . Statement stmt = JDBCTestUtils . con . createStatement ( ) ) { try ( java . sql . ResultSet rs = stmt . executeQuery ( query ) ) { com . couchbase . jdbc . CBResultSet cbrs = ( ( com . couchbase . jdbc . CBResultSet ) ( rs ) ) ; while ( cbrs . next ( ) ) { java . sql . ResultSetMetaData meta = cbrs . getMetaData ( ) ; com . couchbase . json . SQLJSON jsonVal = cbrs . getSQLJSON ( 1 ) ; try { jsonVal . getTime ( null ) ; } catch ( java . sql . SQLException e ) { java . lang . String expectatedMessage = "value<sp>NAME<sp>is<sp>not<sp>a<sp>Time" ; org . junit . Assert . assertEquals ( expectatedMessage . trim ( ) , e . getMessage ( ) . trim ( ) ) ; } } } } }
public class aTest{ @Test public void serverConfigurationScenario ( ) { initJadlerUsing ( new net . jadler . stubbing . server . jetty . JettyStubHttpServer ( ) ) ; try { onRequest ( ) . respond ( ) . withStatus ( net . jadler . FacadeIntegrationTest . EXPECTED_STATUS ) ; final int status = org . apache . http . client . fluent . Executor . newInstance ( ) . execute ( org . apache . http . client . fluent . Request . Get ( jadlerUri ( ) ) ) . handleResponse ( net . jadler . STATUS_RETRIEVER ) ; org . junit . Assert . assertThat ( status , org . hamcrest . Matchers . is ( net . jadler . FacadeIntegrationTest . EXPECTED_STATUS ) ) ; } }
public class aTest{ @Test public void testSearchByMyIPSync ( ) { try { java . lang . String jsonString = com . simplegeo . client . SimpleGeoStorageClientTest . client . searchByMyIP ( "casey.testing.layer" , null ) ; com . simplegeo . client . types . FeatureCollection featureCollection = com . simplegeo . client . types . FeatureCollection . fromJSONString ( jsonString ) ; org . junit . Assert . assertNotNull ( featureCollection . getFeatures ( ) ) ; } }
public class aTest{ @Test public void testToXMLString ( ) { net . bpelunit . toolsupport . util . schema . SchemaParser parser = new net . bpelunit . toolsupport . util . schema . SchemaParser ( ) ; parser . parse ( new java . io . File ( "testSchemata/defineComplexElements3.xsd" ) ) ; java . lang . String namespace = "http://schematest.bpelunit.org" ; this . element = parser . getElements ( ) . get ( new javax . xml . namespace . QName ( namespace , "employee" ) ) ; java . util . HashMap < java . lang . String , java . lang . String > namespaces = new java . util . HashMap < java . lang . String , java . lang . String > ( ) ; namespaces . put ( namespace , "tes" ) ; java . lang . String actual = this . element . toXMLString ( namespaces ) ; java . lang . String expected = "<tes:employee<sp>lang=\"DE\"<sp>personId=\"-1\"\n</tes:employee>" 2 + ( ( ( ( ( ( ( ( ( ( "\n</tes:employee>" 1 + "\n</tes:employee>" 3 ) + "\n\t<tes:location>" ) + "\n\t\t<tes:city></tes:city>" ) + "\n\t\t<tes:zip></tes:zip>" ) + "\n</tes:employee>" 0 ) + "\n\t<tes:location>" ) + "\n\t\t<tes:city></tes:city>" ) + "\n\t\t<tes:zip></tes:zip>" ) + "\n</tes:employee>" 0 ) + "\n</tes:employee>" ) ; org . junit . Assert . assertEquals ( expected , actual ) ; } }
public class aTest{ @Test public void testProcessorEvents2 ( ) { java . lang . String events = recordRichStringProcessorEvents ( ( "acceptTemplateLineBreak()\n" 0 + ( ( "<sp>�IF<sp>true�\n" + "<sp>�ENDIF�\n" ) + "'''" ) ) ) ; java . lang . String expected = "announceNextLiteral()\n" + ( ( ( ( ( ( ( ( ( ( ( ( "acceptTemplateText()\n" + "acceptTemplateLineBreak()\n" ) + "acceptTemplateText(<sp>)\n" ) + "acceptIfCondition()\n" ) + "announceNextLiteral()\n" ) + "acceptTemplateText()\n" ) + "acceptTemplateLineBreak()\n" ) + "acceptTemplateText(<sp>)\n" ) + "acceptEndIf()\n" ) + "announceNextLiteral()\n" ) + "acceptTemplateText()\n" ) + "acceptTemplateLineBreak()\n" ) + "acceptTemplateText()" ) ; org . junit . Assert . assertEquals ( expected , events ) ; } }
public class aTest{ @Test public void testCharacterPrinting ( ) { final byte [ ] buf = new byte [ 1024 ] ; final com . dslplatform . json . JsonWriter jw = new com . dslplatform . json . JsonWriter ( buf , null ) ; final int from = 0 ; final int to = Character . MAX_VALUE ; for ( long value = from ; value <= to ; value ++ ) { if ( ( value >= ( Character . MIN_SURROGATE ) ) && ( value <= ( Character . MAX_SURROGATE ) ) ) continue ; final char ch = ( ( char ) ( value ) ) ; final java . lang . String text = java . lang . String . format ( "codepoint<sp>[%d]<sp>==<sp>replaced<sp>[%c]" , value , ch ) ; jw . reset ( ) ; jw . writeString ( text ) ; final java . lang . String read = new java . lang . String ( buf , 0 , jw . size ( ) , "UTF-8" ) ; final java . lang . String replaced = ( ch == '\b' ) ? "\\b" : ch == '\t' ? "\\n" 1 : ch == '\n' ? "\\n" : ch == '\f' ? "\\n" 0 : ch == '\r' ? "\\r" : ch == '\\' ? "\\\\" : ch == '"' ? "\\\"" : ch < '<sp>' ? java . lang . String . format ( "\\u%04X" , value ) : java . lang . String . valueOf ( ch ) ; final java . lang . String expected = java . lang . String . format ( "\"codepoint<sp>[%d]<sp>==<sp>replaced<sp>[%s]\"" , value , replaced ) ; org . junit . Assert . assertEquals ( expected , read ) ; } } }
public class aTest{ @Test public void testMarkdJLink ( ) { java . lang . String markdown = org . support . project . common . util . FileUtil . read ( getClass ( ) . getResourceAsStream ( "markdown/markdj-link.md" ) ) ; java . lang . String html = org . support . project . common . util . FileUtil . read ( getClass ( ) . getResourceAsStream ( "markdown/result-markdj-link.txt" ) ) ; java . lang . String result = org . support . project . knowledge . logic . MarkdownLogic . get ( ) . markdownToHtml ( markdown , MarkdownLogic . ENGINE_MARKEDJ ) . getHtml ( ) ; try { org . junit . Assert . assertArrayEquals ( read ( html ) , read ( result ) ) ; } }
public class aTest{ @Test public void shouldFailExecutingAnAnnotatedMapperClassWithResultHandler ( ) { org . apache . ibatis . session . SqlSession session = org . apache . ibatis . session . SqlSessionTest . sqlMapper . openSession ( ) ; try { org . apache . ibatis . executor . result . DefaultResultHandler handler = new org . apache . ibatis . executor . result . DefaultResultHandler ( ) ; org . apache . ibatis . domain . blog . mappers . AuthorMapper mapper = session . getMapper ( org . apache . ibatis . domain . blog . mappers . AuthorMapper . class ) ; mapper . selectAuthor2 ( 101 , handler ) ; org . apache . ibatis . domain . blog . Author author = ( ( org . apache . ibatis . domain . blog . Author ) ( handler . getResultList ( ) . get ( 0 ) ) ) ; org . junit . Assert . assertEquals ( 101 , author . getId ( ) ) ; } }
public class aTest{ @Test public void testIoNewBill ( ) { int id = 0 ; org . isf . accounting . model . Bill bill = null ; org . isf . patient . model . Patient patient = null ; org . isf . priceslist . model . PriceList priceList = null ; try { patient = org . isf . accounting . test . Tests . testPatient . setup ( false ) ; priceList = org . isf . accounting . test . Tests . testPriceList . setup ( false ) ; bill = org . isf . accounting . test . Tests . testBill . setup ( priceList , patient , false ) ; org . isf . accounting . test . Tests . jpa . beginTransaction ( ) ; org . isf . accounting . test . Tests . jpa . persist ( priceList ) ; org . isf . accounting . test . Tests . jpa . persist ( patient ) ; org . isf . accounting . test . Tests . jpa . commitTransaction ( ) ; id = accountingIoOperation . newBill ( bill ) ; _checkBillIntoDb ( id ) ; } catch ( java . lang . Exception e ) { e . printStackTrace ( ) ; org . junit . Assert . assertEquals ( true , false ) ; } }
public class aTest{ @Test public void testNotNode ( ) { final java . lang . String drl = ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "rule<sp>R2<sp>salience<sp>1\n" 1 + ( org . drools . testcoverage . common . model . Cheese . class . getCanonicalName ( ) ) ) + ";\n" ) + "rule<sp>R2<sp>salience<sp>1\n" 1 ) + ( org . drools . testcoverage . common . model . Person . class . getCanonicalName ( ) ) ) + ";\n" ) + "rule<sp>R2<sp>salience<sp>1\n" 2 ) + "when\n" ) + "<sp>Person(<sp>$age<sp>:<sp>age<sp>)" ) + "<sp>not<sp>Cheese(<sp>price<sp><<sp>$age<sp>)\n" ) + "then\n" ) + "rule<sp>R2<sp>salience<sp>1\n" 3 ) + "rule<sp>R2<sp>salience<sp>1\n" ) + "when\n" ) + "<sp>$p<sp>:<sp>Person(<sp>age<sp>==<sp>10<sp>)" ) + "then\n" ) + "rule<sp>R2<sp>salience<sp>1\n" 0 ) + "rule<sp>R2<sp>salience<sp>1\n" 3 ; final org . kie . api . KieBase kbase = org . drools . testcoverage . common . util . KieBaseUtil . getKieBaseFromKieModuleFromDrl ( "indexing-test" , kieBaseTestConfiguration , drl ) ; final org . kie . api . runtime . KieSession ksession = kbase . newKieSession ( ) ; try { ksession . insert ( new org . drools . testcoverage . common . model . Person ( "mario" , 10 ) ) ; ksession . insert ( new org . drools . testcoverage . common . model . Cheese ( "gorgonzola" , 20 ) ) ; org . junit . Assert . assertEquals ( 3 , ksession . fireAllRules ( ) ) ; } }
public class aTest{ @Test public void testSelectSumNestedTable ( ) { org . verdictdb . core . sqlobject . BaseTable base = new org . verdictdb . core . sqlobject . BaseTable ( "myschema" , "mytable" , "t" ) ; java . lang . String aliasName = "sum(verdictdbalias6.`verdictdbalias8`<sp>*<sp>" 8 ; org . verdictdb . core . sqlobject . SelectQuery nestedSource = org . verdictdb . core . sqlobject . SelectQuery . create ( java . util . Arrays . < org . verdictdb . core . sqlobject . SelectItem > asList ( new org . verdictdb . core . sqlobject . AliasedColumn ( org . verdictdb . core . sqlobject . ColumnOp . multiply ( new org . verdictdb . core . sqlobject . BaseColumn ( "t" , "price" ) , new org . verdictdb . core . sqlobject . BaseColumn ( "t" , "discount" ) ) , "discounted_price" ) ) , base ) ; nestedSource . setAliasName ( ",<sp>" 4 ) ; org . verdictdb . core . sqlobject . SelectQuery relation = org . verdictdb . core . sqlobject . SelectQuery . create ( java . util . Arrays . < org . verdictdb . core . sqlobject . SelectItem > asList ( new org . verdictdb . core . sqlobject . AliasedColumn ( new org . verdictdb . core . sqlobject . ColumnOp ( "sum" , new org . verdictdb . core . sqlobject . BaseColumn ( "s" , "discounted_price" ) ) , aliasName ) ) , nestedSource ) ; org . verdictdb . core . scrambling . ScrambleMetaSet meta = generateTestScrambleMeta ( ) ; org . verdictdb . core . rewriter . query . AggQueryRewriter rewriter = new org . verdictdb . core . rewriter . query . AggQueryRewriter ( meta ) ; java . util . List < org . apache . commons . lang3 . tuple . Pair < org . verdictdb . core . sqlobject . AbstractRelation , org . verdictdb . core . rewriter . query . AggblockMeta > > rewritten = rewriter . rewrite ( relation ) ; java . lang . String aliasForSumEstimate = org . verdictdb . core . rewriter . AliasRenamingRules . sumEstimateAliasName ( aliasName ) ; java . lang . String aliasForSumScaledSubsum = org . verdictdb . core . rewriter . AliasRenamingRules . sumScaledSumAliasName ( aliasName ) ; java . lang . String aliasForSumSquaredScaledSubsum = org . verdictdb . core . rewriter . AliasRenamingRules . sumSquaredScaledSumAliasName ( aliasName ) ; java . lang . String aliasForCountSubsample = org . verdictdb . core . rewriter . AliasRenamingRules . countSubsampleAliasName ( ) ; java . lang . String aliasForSumSubsampleSize = org . verdictdb . core . rewriter . AliasRenamingRules . sumSubsampleSizeAliasName ( ) ; for ( int k = 0 ; k < ( aggblockCount ) ; k ++ ) { java . lang . String expected = ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "discounted_price" 0 + "sum(verdictdbalias6.`verdictdbalias8`)<sp>as<sp>" ) + ( quoteAlias ( aliasForSumEstimate ) ) ) + ",<sp>" ) + "sum(verdictdbalias6.`verdictdbalias8`<sp>*<sp>" ) + "sum(verdictdbalias6.`verdictdbalias8`<sp>*<sp>" 7 ) + ( quoteAlias ( aliasForSumScaledSubsum ) ) ) + ",<sp>" ) + "sum((verdictdbalias6.`verdictdbalias8`<sp>*<sp>verdictdbalias6.`verdictdbalias8`)<sp>*<sp>" ) + "sum(verdictdbalias6.`verdictdbalias8`<sp>*<sp>" 7 ) + ( quoteAlias ( aliasForSumSquaredScaledSubsum ) ) ) + ",<sp>" ) + "count(*)<sp>as<sp>" ) + ( quoteAlias ( aliasForCountSubsample ) ) ) + ",<sp>" ) + "sum(verdictdbalias6.`verdictdbalias9`)<sp>as<sp>" ) + ( quoteAlias ( aliasForSumSubsampleSize ) ) ) + ",<sp>" 0 ) + ",<sp>" 5 ) + "select<sp>s.`verdictdbalias5`<sp>as<sp>`verdictdbalias7`,<sp>" ) + "discounted_price" 2 ) + "sum(verdictdbalias6.`verdictdbalias8`<sp>*<sp>" 6 ) + ",<sp>" 1 ) + "sum(verdictdbalias6.`verdictdbalias8`<sp>*<sp>" 0 ) + "sum(verdictdbalias6.`verdictdbalias8`<sp>*<sp>" 3 ) + ",<sp>" 3 ) + "t.`verdictdbsid`<sp>as<sp>`verdictdbalias2`,<sp>" ) + "sum(verdictdbalias6.`verdictdbalias8`<sp>*<sp>" 4 ) + "sum(verdictdbalias6.`verdictdbalias8`<sp>*<sp>" 1 ) + "sum(verdictdbalias6.`verdictdbalias8`<sp>*<sp>" 9 ) + k ) + "discounted_price" 1 ) + "sum(verdictdbalias6.`verdictdbalias8`<sp>*<sp>" 5 ) + ",<sp>" 7 ; org . verdictdb . sqlwriter . SelectQueryToSql relToSql = new org . verdictdb . sqlwriter . SelectQueryToSql ( new org . verdictdb . sqlsyntax . HiveSyntax ( ) ) ; java . lang . String actual = relToSql . toSql ( rewritten . get ( k ) . getLeft ( ) ) ; org . junit . Assert . assertEquals ( expected , actual ) ; } } }
public class aTest{ @Test public void testAsyncBulkheadQueueFull ( ) { com . ibm . ws . microprofile . faulttolerance . spi . BulkheadPolicy bulkhead = com . ibm . ws . microprofile . faulttolerance . spi . FaultToleranceProvider . newBulkheadPolicy ( ) ; bulkhead . setMaxThreads ( 2 ) ; bulkhead . setQueueSize ( 2 ) ; com . ibm . ws . microprofile . faulttolerance . spi . ExecutorBuilder < java . lang . String > builder = com . ibm . ws . microprofile . faulttolerance . spi . FaultToleranceProvider . newExecutionBuilder ( ) ; builder . setBulkheadPolicy ( bulkhead ) ; com . ibm . ws . microprofile . faulttolerance . spi . Executor < java . util . concurrent . Future < java . lang . String > > executor = builder . buildAsync ( java . util . concurrent . Future . class ) ; java . util . concurrent . Future < java . lang . String > [ ] futures = new java . util . concurrent . Future [ 5 ] ; try { for ( int i = 0 ; i < 4 ; i ++ ) { java . lang . String id = "testAsyncBulkheadQueueFull" + i ; org . eclipse . microprofile . faulttolerance . ExecutionContext context = executor . newExecutionContext ( id , ( ( java . lang . reflect . Method ) ( null ) ) , id ) ; com . ibm . ws . microprofile . faulttolerance . test . util . AsyncTestFunction callable = new com . ibm . ws . microprofile . faulttolerance . test . util . AsyncTestFunction ( java . time . Duration . ofMillis ( 2000 ) , id ) ; futures [ i ] = executor . execute ( callable , context ) ; System . out . println ( ( ( ( ( java . lang . System . currentTimeMillis ( ) ) + "<sp>Test<sp>" ) + context ) + "<sp>-<sp>submitted" ) ) ; org . junit . Assert . assertFalse ( futures [ i ] . isDone ( ) ) ; java . lang . Thread . sleep ( 100 ) ; } }
public class aTest{ @Test public void entitySetTwoPrimNoMetadata ( ) { final org . apache . olingo . commons . api . edm . EdmEntitySet edmEntitySet = org . apache . olingo . server . core . serializer . json . ODataJsonSerializerv01Test . entityContainer . getEntitySet ( "ESTwoPrim" ) ; final org . apache . olingo . commons . api . data . EntityCollection entitySet = data . readAll ( edmEntitySet ) ; final java . lang . String resultString = org . apache . commons . io . IOUtils . toString ( serializerNoMetadata . entityCollection ( org . apache . olingo . server . core . serializer . json . ODataJsonSerializerv01Test . metadata , edmEntitySet . getEntityType ( ) , entitySet , org . apache . olingo . server . api . serializer . EntityCollectionSerializerOptions . with ( ) . contextURL ( org . apache . olingo . commons . api . data . ContextURL . with ( ) . entitySet ( edmEntitySet ) . build ( ) ) . build ( ) ) . getContent ( ) ) ; final java . lang . String expectedResult = "{\"value\":[" + ( ( ( "{\"PropertyInt16\":32766,\"PropertyString\":\"Test<sp>String1\"}," + "{\"PropertyInt16\":-365,\"PropertyString\":\"Test<sp>String2\"}," ) + "{\"PropertyInt16\":-32766,\"PropertyString\":[" 0 ) + "{\"PropertyInt16\":32767,\"PropertyString\":\"Test<sp>String4\"}]}" ) ; org . junit . Assert . assertEquals ( expectedResult , resultString ) ; } }
public class aTest{ @Test public void testAddCisActivity ( ) { org . societies . platform . activityfeed . ActivityFeedTest . LOG . info ( "filterValue" 3 ) ; actFeed . setId ( "filterValue" 0 ) ; actFeed . startUp ( sessionFactory ) ; java . lang . String actor = "filterValue" 2 ; java . lang . String verb = "published" ; org . societies . api . activity . IActivity iact = new org . societies . activity . model . Activity ( ) ; iact . setActor ( actor ) ; iact . setPublished ( java . lang . Long . toString ( java . lang . System . currentTimeMillis ( ) ) ) ; iact . setVerb ( verb ) ; iact . setObject ( "filterValue" 5 ) ; iact . setTarget ( "testTarget" ) ; actFeed . addActivityToDB ( iact ) ; java . util . List < org . societies . api . activity . IActivity > results = null ; try { org . json . JSONObject searchQuery = new org . json . JSONObject ( ) ; java . lang . String timeSeries = "filterValue" 4 + ( java . lang . Long . toString ( java . lang . System . currentTimeMillis ( ) ) ) ; try { searchQuery . append ( "filterBy" , "actor" ) ; searchQuery . append ( "filterOp" , "equals" ) ; searchQuery . append ( "filterValue" , actor ) ; } catch ( org . json . JSONException e ) { e . printStackTrace ( ) ; } org . societies . platform . activityfeed . ActivityFeedTest . LOG . info ( ( ( ( "sending<sp>timeSeries:<sp>" + timeSeries ) + "<sp>act<sp>published:<sp>" ) + ( iact . getPublished ( ) ) ) ) ; results = actFeed . getActivitiesFromDB ( searchQuery . toString ( ) , timeSeries ) ; org . societies . platform . activityfeed . ActivityFeedTest . LOG . info ( ( "filterValue" 1 + ( results . size ( ) ) ) ) ; } catch ( java . lang . Exception e ) { org . societies . platform . activityfeed . ActivityFeedTest . LOG . error ( "exception<sp>in<sp>test:" , e ) ; } org . junit . Assert . assertNotNull ( results ) ; assert ( results . size ( ) ) > 0 ; assert results . get ( 0 ) . getActor ( ) . equals ( actor ) ; } }
public class aTest{ @Test public void renewLeaseTimedOutWithCheck ( ) { org . apache . jackrabbit . oak . plugins . document . ClusterNodeInfo info = newClusterNodeInfo ( 1 ) ; clock . waitUntil ( ( ( info . getLeaseEndTime ( ) ) + ( ClusterNodeInfo . DEFAULT_LEASE_UPDATE_INTERVAL_MILLIS ) ) ) ; try { info . performLeaseCheck ( ) ; org . junit . Assert . fail ( "lease<sp>check<sp>must<sp>fail<sp>with<sp>exception" ) ; } catch ( org . apache . jackrabbit . oak . plugins . document . DocumentStoreException e ) { } try { org . junit . Assert . assertFalse ( info . renewLease ( ) ) ; } }
public class aTest{ @Test public void shouldCreateFilteringSequenceThatIncludesOnlyNonSystemNodes ( ) { org . modeshape . jcr . query . NodeSequence . RowFilter nonSysFilter = rowFilterOfNodesWithKeysHavingWorkspaceKey ( 0 , org . modeshape . jcr . cache . NodeKey . keyForWorkspaceName ( workspaceName ( ) ) ) ; org . modeshape . jcr . query . NodeSequence seq = org . modeshape . jcr . query . NodeSequence . filter ( allNodes ( ) , nonSysFilter ) ; try { org . modeshape . jcr . query . NodeSequence . Batch batch = null ; while ( ( batch = seq . nextBatch ( ) ) != null ) { while ( batch . hasNext ( ) ) { batch . nextRow ( ) ; org . modeshape . jcr . cache . CachedNode node = batch . getNode ( ) ; if ( node != null ) { org . modeshape . jcr . value . Path path = node . getPath ( cache ) ; boolean isSystem = path . getSegment ( 0 ) . getName ( ) . equals ( JcrLexicon . SYSTEM ) ; org . junit . Assert . assertTrue ( ( ( path . isRoot ( ) ) || ( ! isSystem ) ) ) ; } } } } }
public class aTest{ @Test public void testZeroLikeNull ( ) { java . lang . String sql = com . liferay . portal . dao . orm . test . SQLNullTest . _SQL_LIKE_NULL ; if ( isPostgreSQL ( ) ) { sql = transformPostgreSQL ( sql ) ; } else if ( isSybase ( ) ) { sql = transformSybaseSQL ( sql ) ; } else if ( isHypersonic ( ) ) { sql = transformHypersonicSQL ( sql ) ; } com . liferay . portal . kernel . dao . orm . Session session = _sessionFactory . openSession ( ) ; try { com . liferay . portal . kernel . dao . orm . SQLQuery sqlQuery = session . createSynchronizedSQLQuery ( sql ) ; com . liferay . portal . kernel . dao . orm . QueryPos qPos = com . liferay . portal . kernel . dao . orm . QueryPos . getInstance ( sqlQuery ) ; qPos . add ( 0 ) ; java . util . List < java . lang . Object > list = sqlQuery . list ( ) ; org . junit . Assert . assertTrue ( list . toString ( ) , list . isEmpty ( ) ) ; } }
public class aTest{ @Test public void testLoadingNotAKJar ( ) { java . lang . ClassLoader cl = java . lang . Thread . currentThread ( ) . getContextClassLoader ( ) ; java . net . URLClassLoader urlClassLoader = new java . net . URLClassLoader ( new java . net . URL [ ] { this . getClass ( ) . getResource ( "/only-jar-pojo-not-kjar-no-kmodule-1.0.0.jar" ) } ) ; java . lang . Thread . currentThread ( ) . setContextClassLoader ( urlClassLoader ) ; try { org . kie . api . KieServices ks = KieServices . Factory . get ( ) ; org . kie . api . builder . KieRepository kieRepository = ks . getRepository ( ) ; org . kie . api . builder . ReleaseId releaseId = ks . newReleaseId ( "org.test" , "only-jar-pojo-not-kjar-no-kmodule" , "1.0.0" ) ; org . kie . api . builder . KieModule kieModule = kieRepository . getKieModule ( releaseId ) ; org . junit . Assert . assertNull ( kieModule ) ; } }
public class aTest{ @Test public void checkClassGenerationWithUsingJarFile ( ) { java . lang . String pathToJar = this . getClass ( ) . getClassLoader ( ) . getResource ( "ifield_name.jar" ) . getFile ( ) ; java . util . jar . JarFile jarFile = new java . util . jar . JarFile ( pathToJar ) ; java . util . Enumeration < java . util . jar . JarEntry > e = jarFile . entries ( ) ; java . net . URL [ ] urls = new java . net . URL [ ] { new java . net . URL ( ( ( "jar:file:" + pathToJar ) + "!/" ) ) } ; java . net . URLClassLoader cl = java . net . URLClassLoader . newInstance ( urls ) ; while ( e . hasMoreElements ( ) ) { java . util . jar . JarEntry je = e . nextElement ( ) ; if ( ( je . isDirectory ( ) ) || ( ! ( je . getName ( ) . endsWith ( ".class" ) ) ) ) { continue ; } java . lang . String className = je . getName ( ) . substring ( 0 , ( ( je . getName ( ) . length ( ) ) - 6 ) ) ; className = className . replace ( '/' , '.' ) ; java . lang . Class c = cl . loadClass ( className ) ; } info . smart_tools . smartactors . utility_tool . class_generator_with_java_compile_api . ClassGenerator classGenerator = new info . smart_tools . smartactors . utility_tool . class_generator_with_java_compile_api . ClassGenerator ( cl ) ; java . lang . String testSample = "ifield_name.jar" 2 + ( ( ( ( ( ( ( "import<sp>info.smart_tools.smartactors.utility_tool.class_generator_with_java_compile_api.TestInterface;\n" + "import<sp>info.smart_tools.smartactors.core.ifield_name.IFieldName;\n" ) + "ifield_name.jar" 1 ) + "<sp>private<sp>int<sp>a;\n" ) + "<sp>public<sp>Integer<sp>getA()<sp>{\n" ) + "<sp>return<sp>a;\n" ) + "ifield_name.jar" 0 ) + "}\n" ) ; java . lang . Class newClass = classGenerator . generate ( testSample ) ; org . junit . Assert . assertNotNull ( newClass ) ; } }
public class aTest{ @Test public void testLookupPerfJNDI ( ) { javax . naming . ldap . LdapContext ctx = org . apache . directory . server . operations . lookup . LookupPerfIT . getWiredContext ( ldapServer , null ) ; javax . naming . directory . Attributes result = ctx . getAttributes ( "uid=admin,ou=system" ) ; org . junit . Assert . assertNotNull ( result ) ; long t0 = java . lang . System . currentTimeMillis ( ) ; for ( int i = 0 ; i < 50 ; i ++ ) { for ( int j = 0 ; j < 10000 ; j ++ ) { ctx . getAttributes ( "uid=admin,ou=system" ) ; } }
public class aTest{ @Test public void testShouldValidationFailWhenHostGroupConfigurationProvidedAndRequiredConfigTypesAreMissingFromSlaveHostgroup ( ) { masterHostGroupConfigurationMap . put ( "kerberos-env" , new java . util . HashMap ( ) ) ; masterHostGroupConfigurationMap . get ( "kerberos-env" ) . put ( "realm" , "etwas" ) ; masterHostGroupConfigurationMap . get ( "kerberos-env" ) . put ( "kdc_type" , "mit-kdc" ) ; masterHostGroupConfigurationMap . put ( "krb5-conf" , new java . util . HashMap ( ) ) ; masterHostGroupConfigurationMap . get ( "krb5-conf" ) . put ( "smthg" 1 , "smthg" ) ; missingProps . put ( "slave" , com . google . common . collect . Sets . newTreeSet ( com . google . common . collect . Lists . newArrayList ( "kdc_type" , "smthg" 1 , "realm" ) ) ) ; replayAll ( ) ; java . lang . String expectedMsg = java . lang . String . format ( "Missing<sp>required<sp>properties.<sp>Specify<sp>a<sp>value<sp>for<sp>these<sp>properties<sp>in<sp>the<sp>blueprint<sp>or<sp>cluster<sp>creation<sp>template<sp>configuration.<sp>%s" , missingProps ) ; java . lang . String actualMsg = "" ; try { testSubject . validate ( clusterTopologyMock ) ; } catch ( org . apache . ambari . server . topology . InvalidTopologyException e ) { actualMsg = e . getMessage ( ) ; } org . junit . Assert . assertEquals ( "smthg" 0 , expectedMsg , actualMsg ) ; } }
public class aTest{ @Test public void testListRegions ( ) { try { com . fit2cloud . aliyun . ecs . model . response . ListRegionsResponse response = client . listRegions ( ) ; System . out . println ( ( "testListRegions<sp>::<sp>" + response ) ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testTwentyProductsWithDifferentTypes ( ) { org . esa . beam . pixex . Coordinate [ ] coordinates = new org . esa . beam . pixex . Coordinate [ ] { new org . esa . beam . pixex . Coordinate ( "coord3" , 2.5F , 1.0F , null ) , new org . esa . beam . pixex . Coordinate ( "coord4" , 0.5F , 0.5F , null ) } ; int windowSize = 1 ; java . util . HashMap < java . lang . String , java . lang . Object > parameterMap = new java . util . HashMap ( ) ; java . io . File outputDir = org . esa . beam . pixex . PixExOpTest . getOutputDir ( "testTwentyProductsWithDifferentTypes" , getClass ( ) ) ; parameterMap . put ( "outputDir" , outputDir ) ; parameterMap . put ( "exportTiePoints" , false ) ; parameterMap . put ( "exportMasks" , false ) ; parameterMap . put ( "coordinates" , coordinates ) ; parameterMap . put ( "prod_" 0 , windowSize ) ; java . util . List < org . esa . beam . framework . datamodel . Product > productList = new java . util . ArrayList ( ) ; for ( int i = 0 ; i < 20 ; i ++ ) { productList . add ( org . esa . beam . pixex . PixExOpTest . createTestProduct ( ( "prod_" + i ) , ( "type" + i ) , new java . lang . String [ ] { "band" + i } ) ) ; } org . esa . beam . framework . datamodel . Product [ ] products = productList . toArray ( new org . esa . beam . framework . datamodel . Product [ productList . size ( ) ] ) ; org . esa . beam . pixex . PixExOpTest . computeData ( parameterMap , products ) ; try ( org . esa . beam . pixex . PixExMeasurementReader measurementReader = new org . esa . beam . pixex . PixExMeasurementReader ( outputDir ) ) { final java . util . List < org . esa . beam . measurement . Measurement > measurementList = convertToList ( measurementReader ) ; org . junit . Assert . assertEquals ( ( ( ( windowSize * windowSize ) * ( products . length ) ) * ( coordinates . length ) ) , measurementList . size ( ) ) ; testForExistingMeasurement ( measurementList , "coord3" , 1 , 2.5F , 1.5F , 181.5F , 87.5F ) ; testForExistingMeasurement ( measurementList , "coord4" , 2 , 0.5F , 0.5F , 180.5F , 89.5F ) ; } } }
public class aTest{ @Test public void testRecordVTNested ( ) { java . lang . String ns = "testRecordVTNested" ; org . lilyproject . repository . api . FieldType fieldType1 = org . lilyproject . repository . impl . test . ValueTypeTest . typeManager . createFieldType ( org . lilyproject . repository . impl . test . ValueTypeTest . typeManager . newFieldType ( org . lilyproject . repository . impl . test . ValueTypeTest . typeManager . getValueType ( "STRING" ) , new org . lilyproject . repository . api . QName ( ns , "rt3" 1 ) , Scope . NON_VERSIONED ) ) ; org . lilyproject . repository . api . RecordType rt1 = org . lilyproject . repository . impl . test . ValueTypeTest . typeManager . recordTypeBuilder ( ) . name ( new org . lilyproject . repository . api . QName ( ns , "rt1" ) ) . field ( fieldType1 . getId ( ) , false ) . create ( ) ; org . lilyproject . repository . api . ValueType recordVT1 = org . lilyproject . repository . impl . test . ValueTypeTest . typeManager . getValueType ( ( ( "RECORD<{" + ns ) + "}rt1>" ) ) ; org . lilyproject . repository . api . FieldType fieldType2 = org . lilyproject . repository . impl . test . ValueTypeTest . typeManager . createFieldType ( org . lilyproject . repository . impl . test . ValueTypeTest . typeManager . newFieldType ( recordVT1 , new org . lilyproject . repository . api . QName ( ns , "field2" ) , Scope . NON_VERSIONED ) ) ; org . lilyproject . repository . api . RecordType rt2 = org . lilyproject . repository . impl . test . ValueTypeTest . typeManager . recordTypeBuilder ( ) . name ( new org . lilyproject . repository . api . QName ( ns , "rt2" ) ) . field ( fieldType2 . getId ( ) , false ) . create ( ) ; org . lilyproject . repository . api . ValueType recordVT2 = org . lilyproject . repository . impl . test . ValueTypeTest . typeManager . getValueType ( ( ( "RECORD<{" + ns ) + "rt3" 0 ) ) ; org . lilyproject . repository . api . FieldType fieldType3 = org . lilyproject . repository . impl . test . ValueTypeTest . typeManager . createFieldType ( org . lilyproject . repository . impl . test . ValueTypeTest . typeManager . newFieldType ( recordVT2 , new org . lilyproject . repository . api . QName ( ns , "field3" ) , Scope . NON_VERSIONED ) ) ; org . lilyproject . repository . api . RecordType rt3 = org . lilyproject . repository . impl . test . ValueTypeTest . typeManager . recordTypeBuilder ( ) . name ( new org . lilyproject . repository . api . QName ( ns , "rt3" ) ) . field ( fieldType3 . getId ( ) , false ) . create ( ) ; org . lilyproject . repository . api . Record recordField1 = org . lilyproject . repository . impl . test . ValueTypeTest . repository . recordBuilder ( ) . field ( new org . lilyproject . repository . api . QName ( ns , "rt3" 1 ) , "abc" ) . build ( ) ; org . lilyproject . repository . api . Record recordField2 = org . lilyproject . repository . impl . test . ValueTypeTest . repository . recordBuilder ( ) . field ( new org . lilyproject . repository . api . QName ( ns , "field2" ) , recordField1 ) . build ( ) ; org . lilyproject . repository . api . Record createdRecord = org . lilyproject . repository . impl . test . ValueTypeTest . repository . recordBuilder ( ) . recordType ( new org . lilyproject . repository . api . QName ( ns , "rt3" ) ) . field ( new org . lilyproject . repository . api . QName ( ns , "field3" ) , recordField2 ) . create ( ) ; org . lilyproject . repository . api . Record readRecord = org . lilyproject . repository . impl . test . ValueTypeTest . repository . read ( createdRecord . getId ( ) ) ; org . lilyproject . repository . api . Record nestedRecord1 = readRecord . getField ( new org . lilyproject . repository . api . QName ( ns , "field3" ) ) ; org . lilyproject . repository . api . Record nestedRecord2 = nestedRecord1 . getField ( new org . lilyproject . repository . api . QName ( ns , "field2" ) ) ; org . junit . Assert . assertEquals ( "abc" , nestedRecord2 . getField ( new org . lilyproject . repository . api . QName ( ns , "rt3" 1 ) ) ) ; } }
public class aTest{ @Test public void testEmptyResults ( ) { jdbcInputFormat = org . apache . flink . api . java . io . jdbc . JDBCInputFormat . buildJDBCInputFormat ( ) . setDrivername ( org . apache . flink . api . java . io . jdbc . DRIVER_CLASS ) . setDBUrl ( org . apache . flink . api . java . io . jdbc . DB_URL ) . setQuery ( org . apache . flink . api . java . io . jdbc . SELECT_EMPTY ) . setRowTypeInfo ( org . apache . flink . api . java . io . jdbc . ROW_TYPE_INFO ) . setResultSetType ( ResultSet . TYPE_SCROLL_INSENSITIVE ) . finish ( ) ; try { jdbcInputFormat . openInputFormat ( ) ; jdbcInputFormat . open ( null ) ; org . junit . Assert . assertTrue ( jdbcInputFormat . reachedEnd ( ) ) ; } }
public class aTest{ @Test public void toString_ ( ) { com . alibaba . citrus . service . moduleloader . impl . adapter . ScreenEventAdapter screenEvent = ( ( com . alibaba . citrus . service . moduleloader . impl . adapter . ScreenEventAdapter ) ( moduleLoader . getModule ( "<sp>[1/2]<sp><null><sp>=<sp>public<sp>void<sp>com.alibaba.test.app1.module2.screen.MyEventScreenWithDefaultHandler.doPerform()<sp>throws<sp>java.lang.Exception\n" 1 , "MyEventScreenWithDefaultHandler" ) ) ) ; java . lang . String s = "" ; s += "ScreenEventAdapter<sp>{\n" ; s += "<sp>moduleClass<sp>=<sp>com.alibaba.test.app1.module2.screen.MyEventScreenWithDefaultHandler\n" ; s += "<sp>handlers<sp>=<sp>{\n" ; s += "<sp>[1/2]<sp><null><sp>=<sp>public<sp>void<sp>com.alibaba.test.app1.module2.screen.MyEventScreenWithDefaultHandler.doPerform()<sp>throws<sp>java.lang.Exception\n" ; s += "<sp>[2/2]<sp>something<sp>=<sp>public<sp>void<sp>com.alibaba.test.app1.module2.screen.MyEventScreenWithDefaultHandler.doSomething()<sp>throws<sp>java.lang.Exception\n" ; s += "<sp>}\n" ; s += "<sp>[1/2]<sp><null><sp>=<sp>public<sp>void<sp>com.alibaba.test.app1.module2.screen.MyEventScreenWithDefaultHandler.doPerform()<sp>throws<sp>java.lang.Exception\n" 0 ; s += "<sp>postHandler<sp>=<sp>public<sp>void<sp>com.alibaba.test.app1.module2.screen.MyEventScreenWithDefaultHandler.afterExecution()\n" ; s += "}" ; org . junit . Assert . assertEquals ( s , screenEvent . toString ( ) ) ; } }
public class aTest{ @Test public void testUseCustomPackage ( ) { java . io . InputStream is = org . scilab . forge . jlatexmath . examples . macros . FooPackageTest . class . getResourceAsStream ( "/Package_Foo.xml" ) ; org . junit . Assert . assertNotNull ( is ) ; org . scilab . forge . jlatexmath . TeXFormula . addPredefinedCommands ( is ) ; java . lang . String latex = "\\begin{array}{l}" ; latex += "\\fooA{\\pi}{C}\\\\" ; latex += "\\mbox{A<sp>red<sp>circle<sp>}\\fooB{75.3}\\\\" ; latex += "\\mbox{A<sp>red<sp>disk<sp>}\\fooC[abc]{126.7}\\\\" ; latex += "\\mbox{An<sp>other<sp>red<sp>circle<sp>}\\fooD{159.81}[ab]" ; latex += "\\end{array}" ; org . scilab . forge . jlatexmath . TeXFormula formula = new org . scilab . forge . jlatexmath . TeXFormula ( latex ) ; org . scilab . forge . jlatexmath . TeXIcon icon = formula . createTeXIcon ( TeXConstants . STYLE_DISPLAY , 20 ) ; icon . setInsets ( new java . awt . Insets ( 5 , 5 , 5 , 5 ) ) ; java . awt . image . BufferedImage image = new java . awt . image . BufferedImage ( icon . getIconWidth ( ) , icon . getIconHeight ( ) , java . awt . image . BufferedImage . TYPE_INT_ARGB ) ; java . awt . Graphics2D g2 = image . createGraphics ( ) ; g2 . setColor ( Color . white ) ; g2 . fillRect ( 0 , 0 , icon . getIconWidth ( ) , icon . getIconHeight ( ) ) ; javax . swing . JLabel jl = new javax . swing . JLabel ( ) ; jl . setForeground ( new java . awt . Color ( 0 , 0 , 0 ) ) ; icon . paintIcon ( jl , g2 , 0 , 0 ) ; java . io . File file = new java . io . File ( "target/ExampleMacros.png" ) ; try { javax . imageio . ImageIO . write ( image , "png" , file . getAbsoluteFile ( ) ) ; } }
public class aTest{ @Test public void testKeyedCEPOperatorCheckpointingWithRocksDB ( ) { java . lang . String rocksDbPath = tempFolder . newFolder ( ) . getAbsolutePath ( ) ; org . apache . flink . contrib . streaming . state . RocksDBStateBackend rocksDBStateBackend = new org . apache . flink . contrib . streaming . state . RocksDBStateBackend ( new org . apache . flink . runtime . state . memory . MemoryStateBackend ( ) ) ; rocksDBStateBackend . setDbStoragePath ( rocksDbPath ) ; org . apache . flink . streaming . util . OneInputStreamOperatorTestHarness < org . apache . flink . cep . Event , java . util . Map < java . lang . String , java . util . List < org . apache . flink . cep . Event > > > harness = getCepTestHarness ( false ) ; try { harness . setStateBackend ( rocksDBStateBackend ) ; harness . open ( ) ; org . apache . flink . cep . Event startEvent = new org . apache . flink . cep . Event ( 42 , "start" , 1.0 ) ; org . apache . flink . cep . SubEvent middleEvent = new org . apache . flink . cep . SubEvent ( 42 , "foo" , 1.0 , 10.0 ) ; org . apache . flink . cep . Event endEvent = new org . apache . flink . cep . Event ( 42 , "end" , 1.0 ) ; harness . processElement ( new org . apache . flink . streaming . runtime . streamrecord . StreamRecord ( startEvent , 1L ) ) ; harness . processElement ( new org . apache . flink . streaming . runtime . streamrecord . StreamRecord ( new org . apache . flink . cep . Event ( 42 , "foobar" , 1.0 ) , 2L ) ) ; org . apache . flink . runtime . checkpoint . OperatorSubtaskState snapshot = harness . snapshot ( 0L , 0L ) ; harness . close ( ) ; harness = getCepTestHarness ( false ) ; rocksDBStateBackend = new org . apache . flink . contrib . streaming . state . RocksDBStateBackend ( new org . apache . flink . runtime . state . memory . MemoryStateBackend ( ) ) ; rocksDBStateBackend . setDbStoragePath ( rocksDbPath ) ; harness . setStateBackend ( rocksDBStateBackend ) ; harness . setup ( ) ; harness . initializeState ( snapshot ) ; harness . open ( ) ; harness . processWatermark ( new org . apache . flink . streaming . api . watermark . Watermark ( Long . MIN_VALUE ) ) ; harness . processElement ( new org . apache . flink . streaming . runtime . streamrecord . StreamRecord < org . apache . flink . cep . Event > ( new org . apache . flink . cep . SubEvent ( 42 , "barfoo" , 1.0 , 5.0 ) , 3L ) ) ; harness . processWatermark ( new org . apache . flink . streaming . api . watermark . Watermark ( 2L ) ) ; org . apache . flink . runtime . checkpoint . OperatorSubtaskState snapshot2 = harness . snapshot ( 1L , 1L ) ; harness . close ( ) ; harness = getCepTestHarness ( false ) ; rocksDBStateBackend = new org . apache . flink . contrib . streaming . state . RocksDBStateBackend ( new org . apache . flink . runtime . state . memory . MemoryStateBackend ( ) ) ; rocksDBStateBackend . setDbStoragePath ( rocksDbPath ) ; harness . setStateBackend ( rocksDBStateBackend ) ; harness . setup ( ) ; harness . initializeState ( snapshot2 ) ; harness . open ( ) ; harness . processElement ( new org . apache . flink . streaming . runtime . streamrecord . StreamRecord < org . apache . flink . cep . Event > ( middleEvent , 3L ) ) ; harness . processElement ( new org . apache . flink . streaming . runtime . streamrecord . StreamRecord ( new org . apache . flink . cep . Event ( 42 , "start" , 1.0 ) , 4L ) ) ; harness . processElement ( new org . apache . flink . streaming . runtime . streamrecord . StreamRecord ( endEvent , 5L ) ) ; harness . processWatermark ( new org . apache . flink . streaming . api . watermark . Watermark ( Long . MAX_VALUE ) ) ; java . util . Queue < java . lang . Object > result = harness . getOutput ( ) ; org . junit . Assert . assertEquals ( 2 , result . size ( ) ) ; verifyPattern ( result . poll ( ) , startEvent , middleEvent , endEvent ) ; verifyWatermark ( result . poll ( ) , Long . MAX_VALUE ) ; } }
public class aTest{ @Test public void testbuildBarDataSizeString_Hours ( ) { org . junit . Assert . assertEquals ( "1<sp>hour" , com . sumzerotrading . ib . historical . HistoricalDataUtils . buildBarDataSizeString ( 1 , LengthUnit . HOUR ) ) ; try { com . sumzerotrading . ib . historical . HistoricalDataUtils . buildBarDataSizeString ( 2 , LengthUnit . HOUR ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void testIntervalMerge ( ) { com . questdb . std . str . StringSink sink = new com . questdb . std . str . StringSink ( ) ; try ( com . questdb . store . JournalWriter < com . questdb . model . Quote > w = getFactory ( ) . writer ( com . questdb . model . Quote . class ) ) { com . questdb . test . tools . TestUtils . generateQuoteData ( w , 600 , com . questdb . std . time . DateFormatUtils . parseDateTime ( "2014-03-10T07:03:00.000Z\tTLW.L\t245.300086975098\t363.160156250000\t1722093204\t448833342\tFast<sp>trading\tLXE\n" 5 ) , Dates . MINUTE_MILLIS ) ; w . commit ( ) ; com . questdb . ql . RecordSourcePrinter p = new com . questdb . ql . RecordSourcePrinter ( sink ) ; com . questdb . std . LongList intervals = new com . questdb . std . LongList ( ) ; intervals . add ( com . questdb . std . time . DateFormatUtils . parseDateTime ( "2014-03-10T07:03:00.000Z\tTLW.L\t245.300086975098\t363.160156250000\t1722093204\t448833342\tFast<sp>trading\tLXE\n" 3 ) ) ; intervals . add ( com . questdb . std . time . DateFormatUtils . parseDateTime ( "2014-03-10T07:15:00.000Z" ) ) ; p . print ( new com . questdb . ql . JournalRecordSource ( new com . questdb . ql . interval . MultiIntervalPartitionSource ( new com . questdb . ql . JournalPartitionSource ( w . getMetadata ( ) , true ) , intervals ) , new com . questdb . ql . AllRowSource ( ) ) , getFactory ( ) ) ; } final java . lang . String expected = "2014-03-10T07:00:00.000Z\tGKN.L\t290.000000000000\t320.000000000000\t1070060020\t627764827\tFast<sp>trading\tLXE\n" + ( ( ( ( ( ( ( ( ( ( ( ( ( ( "2014-03-10T07:01:00.000Z\tLLOY.L\t0.001271238521\t0.000000010817\t855783502\t444545168\tFast<sp>trading\tLXE\n" + "2014-03-10T07:02:00.000Z\tRRS.L\t0.000010917804\t272.000000000000\t1212565949\t1829154977\tFast<sp>trading\tLXE\n" ) + "2014-03-10T07:03:00.000Z\tTLW.L\t245.300086975098\t363.160156250000\t1722093204\t448833342\tFast<sp>trading\tLXE\n" ) + "2014-03-10T07:04:00.000Z\tTLW.L\t0.025095539168\t0.000000122058\t1703832336\t180642477\tFast<sp>trading\tLXE\n" ) + "2014-03-10T07:03:00.000Z\tTLW.L\t245.300086975098\t363.160156250000\t1722093204\t448833342\tFast<sp>trading\tLXE\n" 0 ) + "2014-03-10T07:03:00.000Z\tTLW.L\t245.300086975098\t363.160156250000\t1722093204\t448833342\tFast<sp>trading\tLXE\n" 8 ) + "2014-03-10T07:03:00.000Z\tTLW.L\t245.300086975098\t363.160156250000\t1722093204\t448833342\tFast<sp>trading\tLXE\n" 2 ) + "2014-03-10T07:08:00.000Z\tAGK.L\t0.207015849650\t0.199165701866\t1090730005\t1076974002\tFast<sp>trading\tLXE\n" ) + "2014-03-10T07:03:00.000Z\tTLW.L\t245.300086975098\t363.160156250000\t1722093204\t448833342\tFast<sp>trading\tLXE\n" 6 ) + "2014-03-10T07:03:00.000Z\tTLW.L\t245.300086975098\t363.160156250000\t1722093204\t448833342\tFast<sp>trading\tLXE\n" 7 ) + "2014-03-10T07:11:00.000Z\tAGK.L\t512.000000000000\t33.973937988281\t1723438228\t349327821\tFast<sp>trading\tLXE\n" ) + "2014-03-10T07:12:00.000Z\tWTB.L\t384.000000000000\t0.000000832384\t2145991300\t1388483923\tFast<sp>trading\tLXE\n" ) + "2014-03-10T07:13:00.000Z\tTLW.L\t0.000000093063\t0.000071085584\t1186156647\t1143726003\tFast<sp>trading\tLXE\n" ) + "2014-03-10T07:03:00.000Z\tTLW.L\t245.300086975098\t363.160156250000\t1722093204\t448833342\tFast<sp>trading\tLXE\n" 4 ) + "2014-03-10T07:03:00.000Z\tTLW.L\t245.300086975098\t363.160156250000\t1722093204\t448833342\tFast<sp>trading\tLXE\n" 1 ) ; org . junit . Assert . assertEquals ( expected , sink . toString ( ) ) ; } }
public class aTest{ @Test public void getRequestSucceed ( tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . DeviceRegistrationState ) { final java . lang . String id = "id-1" ; final java . lang . String registrationPath = "registrations/" + id ; final java . lang . String resultPayload = "validJson" ; tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . RegistrationStatusManager registrationStatusManager = createRegistrationStatusManager ( ) ; new tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . StrictExpectations ( ) { { mockedContractApiHttp . request ( HttpMethod . GET , registrationPath , null , "" ) ; result = mockedHttpResponse ; times = 1 ; mockedHttpResponse . getBody ( ) ; result = resultPayload . getBytes ( ) ; times = 1 ; tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . Deencapsulation . newInstance ( tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . DeviceRegistrationState . class , resultPayload ) ; result = mockedDeviceRegistrationState ; times = 1 ; } } ; tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . DeviceRegistrationState response = tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . Deencapsulation . invoke ( registrationStatusManager , "get" , new java . lang . Class [ ] { java . lang . String . class } , id ) ; org . junit . Assert . assertNotNull ( response ) ; } }
public class aTest{ @Test public void login_bad_credentials ( ) { oakbot . chat . StackExchangeSite site = new oakbot . chat . StackExchangeSite ( ) ; oakbot . util . Http http = new oakbot . util . Http ( new oakbot . chat . MockHttpClientBuilder ( ) . request ( "GET" , "https://stackexchange.com/users/signin" ) . response ( 200 , exampleLoginFormUrl ) . request ( "GET" , exampleLoginFormUrl ) . response ( 200 , oakbot . chat . ResponseSamples . stackExchangeLoginForm ( ) ) . request ( "POST" , "https://openid.stackexchange.com/affiliate/form/login/submit" , "email" , "POST" 2 , "POST" 0 , "POST" 0 , "POST" 1 , "17382ce7-0148-45ae-83e5-d5416ad4f12a" , "affId" , "11" ) . response ( 200 , "If<sp>the<sp>response<sp>is<sp>missing<sp>a<sp>JavaScript<sp>variable<sp>called<sp>\"target\",<sp>then<sp>the<sp>credentials<sp>are<sp>bad." ) . build ( ) ) ; org . junit . Assert . assertFalse ( site . login ( "POST" 2 , "POST" 0 , http ) ) ; oakbot . chat . StackExchangeSiteTest . verifyNumberOfRequestsSent ( http , 3 ) ; } }
public class aTest{ @Test public void testBlob1 ( ) { try { super . testBlob1 ( ) ; org . junit . Assert . assertTrue ( "This<sp>test<sp>should<sp>not<sp>pass<sp>on<sp>sql<sp>server" , false ) ; } }
public class aTest{ @Test public void testWildcardBangWithListLabel ( ) { java . lang . String grammar = "grammar<sp>T;\n" + ( ( ( ( "options<sp>{output=AST;}\n" + "a<sp>:<sp>v=\'void\'<sp>x=.!<sp>\';\'<sp>;\n" ) + "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" ) + "INT<sp>:<sp>\'0\'..\'9\'+;\n" ) + "WS<sp>:<sp>(\'<sp>\'|\'\\n\')<sp>{$channel=HIDDEN;}<sp>;\n" ) ; java . lang . String found = execParser ( "a" 0 , grammar , "TParser" , "TLexer" , "a" , "void<sp>foo;" , debug ) ; org . junit . Assert . assertEquals ( "a" 1 , found ) ; } }
public class aTest{ @Test public void testGetFromAnotherThread ( ) { final com . allanbank . mongodb . client . FutureCallback < java . lang . Object > callback = new com . allanbank . mongodb . client . FutureCallback < java . lang . Object > ( ) ; final java . lang . Object result = new java . lang . Object ( ) ; final java . lang . Thread t = new java . lang . Thread ( new java . lang . Runnable ( ) { @ com . allanbank . mongodb . client . Override public void run ( ) { try { java . lang . Thread . sleep ( 50 ) ; } catch ( final java . lang . InterruptedException e ) { } finally { callback . callback ( result ) ; } } } ) ; t . start ( ) ; try { org . junit . Assert . assertSame ( result , callback . get ( ) ) ; } }
public class aTest{ @Test public void testDeprecatedOverride ( ) { try { for ( org . apache . sentry . binding . hive . conf . HiveAuthzConf . AuthzConfVars currentVar : currentProps ) { authzDepConf . set ( currentVar . getVar ( ) , "current" ) ; org . junit . Assert . assertEquals ( "current" , authzDepConf . get ( currentVar . getVar ( ) ) ) ; } } }
public class aTest{ @Test public void testListVpcs ( ) { try { com . fit2cloud . aliyun . ecs . model . request . ListVpcsRequest request = new com . fit2cloud . aliyun . ecs . model . request . ListVpcsRequest ( "cn-beijing" ) ; com . fit2cloud . aliyun . ecs . model . response . ListVpcsResponse response = client . listVpcs ( request ) ; System . out . println ( ( "testListVpcs<sp>::<sp>" + response ) ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void test6 ( ) { java . lang . String authorization = "Basic<sp>sdfsdfs,<sp>Bearer<sp>,<sp>SOMETHING<sp>ELSE" ; net . nortlam . porcupine . common . token . AccessToken accessToken = new net . nortlam . porcupine . common . token . AccessToken ( ) ; try { accessToken . parseAuthorizationBearer ( authorization ) ; org . junit . Assert . assertEquals ( accessToken . getToken ( ) , "" ) ; net . nortlam . porcupine . test . TestAuthorizationBearerParsing . LOG . log ( Level . INFO , "66666<sp>Token:{0}" , accessToken . getToken ( ) ) ; } }
public class aTest{ @Test public void testDoMapRegex ( ) { System . out . println ( ( ( getTestTraceHead ( "[NGSIGroupingInterceptor.doMapRegex]" ) ) + "--------<sp>A<sp>mapped<sp>ContextElement<sp>can<sp>be<sp>obtained<sp>from<sp>the<sp>Name<sp>Mappings" ) ) ; com . telefonica . iot . cygnus . interceptors . NGSINameMappingsInterceptor nameMappingsInterceptor = new com . telefonica . iot . cygnus . interceptors . NGSINameMappingsInterceptor ( null , false ) ; nameMappingsInterceptor . loadNameMappings ( nameMappingsRegexStr ) ; com . telefonica . iot . cygnus . containers . NotifyContextRequest . ContextElement originalCE ; com . telefonica . iot . cygnus . containers . NotifyContextRequest . ContextElement expectedCE ; try { originalCE = com . telefonica . iot . cygnus . utils . NGSIUtilsForTests . createJsonContextElement ( originalCEStr ) ; expectedCE = com . telefonica . iot . cygnus . utils . NGSIUtilsForTests . createJsonContextElement ( expectedCEStr ) ; } catch ( java . lang . Exception e ) { System . out . println ( ( ( getTestTraceHead ( "[NGSIGroupingInterceptor.doMapRegex]" ) ) + "-<sp>FAIL<sp>-<sp>There<sp>was<sp>some<sp>problem<sp>when<sp>parsing<sp>the<sp>ContextElements" ) ) ; throw new java . lang . AssertionError ( e . getMessage ( ) ) ; } org . apache . commons . lang3 . tuple . ImmutableTriple < java . lang . String , java . lang . String , com . telefonica . iot . cygnus . containers . NotifyContextRequest . ContextElement > map = nameMappingsInterceptor . doMap ( originalService , originalServicePath , originalCE ) ; com . telefonica . iot . cygnus . containers . NotifyContextRequest . ContextElement mappedCE = map . getRight ( ) ; boolean equals = true ; if ( ( ! ( mappedCE . getId ( ) . equals ( expectedCE . getId ( ) ) ) ) || ( ! ( mappedCE . getType ( ) . equals ( expectedCE . getType ( ) ) ) ) ) { equals = false ; } else { for ( int j = 0 ; j < ( mappedCE . getAttributes ( ) . size ( ) ) ; j ++ ) { com . telefonica . iot . cygnus . containers . NotifyContextRequest . ContextAttribute mappedCA = mappedCE . getAttributes ( ) . get ( j ) ; com . telefonica . iot . cygnus . containers . NotifyContextRequest . ContextAttribute expectedCA = expectedCE . getAttributes ( ) . get ( j ) ; if ( ( ! ( mappedCA . getName ( ) . equals ( expectedCA . getName ( ) ) ) ) || ( ! ( mappedCA . getType ( ) . equals ( expectedCA . getType ( ) ) ) ) ) { equals = false ; break ; } } } try { org . junit . Assert . assertTrue ( equals ) ; System . out . println ( ( ( getTestTraceHead ( "[NGSIGroupingInterceptor.doMapRegex]" ) ) + "-<sp>OK<sp>-<sp>The<sp>mapped<sp>NotifyContextRequest<sp>is<sp>equals<sp>to<sp>the<sp>expected<sp>one" ) ) ; } }
public class aTest{ @Test public void testQueryFirstAllShardsAsyncFail ( ) { try { com . ctrip . platform . dal . dao . shard . DalHints hints = new com . ctrip . platform . dal . dao . shard . DalHints ( ) ; com . ctrip . platform . dal . dao . shard . DalQueryDaoTest . ClientTestModel result = dao . queryForObject ( sqlNoResult , parameters ( ) , hints . inAllShards ( ) . asyncExecution ( ) , new com . ctrip . platform . dal . dao . shard . DalQueryDaoTest . ClientTestDalRowMapper ( ) ) ; org . junit . Assert . assertNull ( result ) ; java . util . concurrent . Future < com . ctrip . platform . dal . dao . shard . DalQueryDaoTest . ClientTestModel > fr = ( ( java . util . concurrent . Future < com . ctrip . platform . dal . dao . shard . DalQueryDaoTest . ClientTestModel > ) ( hints . getAsyncResult ( ) ) ) ; result = fr . get ( ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void sessionIdNotSent ( ) { enkan . middleware . SessionMiddleware middleware = new enkan . middleware . SessionMiddleware ( ) ; enkan . middleware . HttpRequest request = builder ( new enkan . middleware . DefaultHttpRequest ( ) ) . build ( ) ; request = enkan . util . MixinUtils . mixin ( request , enkan . middleware . WebSessionAvailable . class ) ; java . util . UUID sessionKey = java . util . UUID . randomUUID ( ) ; enkan . middleware . Session session = new enkan . middleware . Session ( ) ; session . put ( "name" , "kawasima" ) ; tryReflection ( ( ) -> { java . lang . reflect . Field storeField = . class . getDeclaredField ( "store" ) ; storeField . setAccessible ( true ) ; enkan . middleware . session . MemoryStore store = ( ( enkan . middleware . session . MemoryStore ) ( storeField . get ( middleware ) ) ) ; store . write ( sessionKey . toString ( ) , session ) ; return store ; } ) ; middleware . sessionRequest ( request ) ; org . junit . Assert . assertNull ( request . getSession ( ) ) ; } }
public class aTest{ @Test public void testListCollectionsOnSecondary ( ) { final int numCollections = 1000 ; com . allanbank . mongodb . MongoIterator < com . allanbank . mongodb . bson . Document > docsIter = null ; myCollection . insert ( Durability . ACK , com . allanbank . mongodb . bson . builder . BuilderFactory . start ( ) . build ( ) ) ; for ( int i = 0 ; i < numCollections ; ++ i ) { myDb . getCollection ( ( "listCollections_" + i ) ) . insert ( Durability . ACK , com . allanbank . mongodb . bson . builder . BuilderFactory . start ( ) ) ; } try { docsIter = myDb . listCollections ( com . allanbank . mongodb . builder . ListCollections . builder ( ) . readPreference ( ReadPreference . PREFER_SECONDARY ) ) ; int count = 0 ; while ( docsIter . hasNext ( ) ) { docsIter . next ( ) ; count += 1 ; } org . junit . Assert . assertThat ( count , org . hamcrest . Matchers . greaterThan ( ( numCollections + 1 ) ) ) ; } }
public class aTest{ @Test public void testZeroLengthToken ( ) { java . lang . String grammar = "lexer<sp>grammar<sp>L;\n" + ( ( ( ( ( ( ( ( ( ( ( ( "\n" + "BeginString\n" ) + "[@0,0:4=\'\'xxx\'\',<1>,1:0]\n" 3 ) + "[@0,0:4=\'\'xxx\'\',<1>,1:0]\n" 4 ) + "\n" ) + "mode<sp>StringMode;\n" ) + "\n" ) + "\tStringMode_X<sp>:<sp>\'x\'<sp>-><sp>more;\n" ) + "\tStringMode_Done<sp>:<sp>-><sp>more,<sp>mode(EndStringMode);\n" ) + "\n" ) + "mode<sp>EndStringMode;\t\n" ) + "\n" ) + "[@0,0:4=\'\'xxx\'\',<1>,1:0]\n" 1 ) ; java . lang . String found = execLexer ( "L.g4" , grammar , "[@0,0:4=\'\'xxx\'\',<1>,1:0]\n" 0 , "[@0,0:4=\'\'xxx\'\',<1>,1:0]\n" 2 ) ; java . lang . String expecting = "[@0,0:4=\'\'xxx\'\',<1>,1:0]\n" + "[@1,5:4=\'<EOF>\',<-1>,1:5]\n" ; org . junit . Assert . assertEquals ( expecting , found ) ; } }
public class aTest{ @Test public void cancelWhileFlatMappedMapFunctionRunningInterruptTest ( ) { org . threadly . concurrent . SingleThreadScheduler sts = new org . threadly . concurrent . SingleThreadScheduler ( ) ; try { java . util . concurrent . atomic . AtomicBoolean started = new java . util . concurrent . atomic . AtomicBoolean ( ) ; java . util . concurrent . atomic . AtomicBoolean interrupted = new java . util . concurrent . atomic . AtomicBoolean ( ) ; org . threadly . concurrent . future . ListenableFuture < ? > lf = makeListenableFutureFactory ( ) . makeWithResult ( null ) ; org . threadly . concurrent . future . ListenableFuture < java . lang . Void > mappedLF = lf . flatMap ( ( o ) -> { started . set ( true ) ; try { java . lang . Thread . sleep ( 10000 ) ; } catch ( e ) { interrupted . set ( true ) ; } return org . threadly . concurrent . future . FutureUtils . immediateResultFuture ( null ) ; } , sts ) ; new org . threadly . test . concurrent . TestCondition ( ( ) -> started . get ( ) ) . blockTillTrue ( ) ; org . junit . Assert . assertTrue ( mappedLF . cancel ( true ) ) ; new org . threadly . test . concurrent . TestCondition ( ( ) -> interrupted . get ( ) ) . blockTillTrue ( ) ; } }
public class aTest{ @Test public void testAccessTwoServersAtOnceShouldFail ( ) { org . jacorb . test . BasicServer server1 = org . jacorb . test . BasicServerHelper . narrow ( orb . string_to_object ( server1IOR ) ) ; org . junit . Assert . assertEquals ( 10 , server1 . bounce_long ( 10 ) ) ; org . jacorb . test . BasicServer server2 = org . jacorb . test . BasicServerHelper . narrow ( orb . string_to_object ( server2IOR ) ) ; try { server2 . bounce_long ( 10 ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void testSearchNonascii ( ) { int total = 5 ; for ( int i = 0 ; i < total ; i ++ ) { com . liferay . data . engine . service . test . DEDataEngineTestUtil . insertDEDataRecordCollection ( _adminUser , _group , ( "Description" + i ) , ( "Name" + i ) , _deDataDefinitionService , _deDataRecordCollectionService ) ; } com . liferay . data . engine . service . test . DEDataEngineTestUtil . insertDEDataRecordCollection ( _adminUser , _group , "nonascii" , "Name" , _deDataDefinitionService , _deDataRecordCollectionService ) ; java . util . List < com . liferay . data . engine . model . DEDataRecordCollection > deDataRecordCollections = searchDEDataRecordCollection ( _group , "nonascii" ) ; com . liferay . portal . search . test . util . IdempotentRetryAssert . retryAssert ( 6 , TimeUnit . SECONDS , ( ) -> { org . junit . Assert . assertEquals ( deDataRecordCollections . toString ( ) , 1 , deDataRecordCollections . size ( ) ) ; return null ; } }
public class aTest{ @Test public void testPut ( ) { final int cap = 10 ; final java . util . concurrent . BlockingQueue < java . lang . Integer > dbq = new com . conversantmedia . util . concurrent . DisruptorBlockingQueue < java . lang . Integer > ( cap ) ; for ( int i = 0 ; i < cap ; i ++ ) { dbq . offer ( java . lang . Integer . valueOf ( i ) ) ; } executor . execute ( ( ) -> { try { java . lang . Thread . sleep ( 1000 ) ; dbq . poll ( ) ; } catch ( e ) { com . conversantmedia . util . concurrent . e . printStackTrace ( ) ; } } ) ; dbq . put ( java . lang . Integer . valueOf ( cap ) ) ; boolean hasValCap = false ; while ( ! ( dbq . isEmpty ( ) ) ) { if ( dbq . poll ( ) . equals ( java . lang . Integer . valueOf ( cap ) ) ) hasValCap = true ; } org . junit . Assert . assertTrue ( hasValCap ) ; } }
public class aTest{ @Test public void testQueryTaskDeleted ( ) { final java . lang . String queryTaskDocumentSelfLink = com . vmware . xenon . common . UriUtils . buildUriPath ( ServiceUriPaths . CORE_QUERY_TASKS , "/testQueryTaskResultLimit" ) ; final java . lang . String queryTaskLink = queryTaskDocumentSelfLink + 1 ; com . vmware . xenon . services . common . QueryTask . QuerySpecification qs = new com . vmware . xenon . services . common . QueryTask . QuerySpecification ( ) ; qs . query = Query . Builder . create ( ) . addKindFieldClause ( com . vmware . admiral . compute . container . ContainerDescriptionService . ContainerDescription . class ) . build ( ) ; com . vmware . xenon . services . common . QueryTask qt = com . vmware . xenon . services . common . QueryTask . create ( qs ) ; qt . documentSelfLink = queryTaskLink ; int fiveSec = 5000000 ; qt . documentExpirationTimeMicros = com . vmware . admiral . common . util . ServiceUtils . getExpirationTimeFromNowInMicros ( fiveSec ) ; final java . util . concurrent . atomic . AtomicReference < com . vmware . xenon . services . common . QueryTask > q = new java . util . concurrent . atomic . AtomicReference ( ) ; host . testStart ( 1 ) ; new com . vmware . admiral . common . util . ServiceDocumentQuery ( host , com . vmware . admiral . compute . container . ContainerDescriptionService . ContainerDescription . class ) . query ( qt , handler ( false , q , qt . documentSelfLink ) ) ; host . testWait ( ) ; qt = q . getAndSet ( null ) ; org . junit . Assert . assertNotNull ( qt ) ; waitFor ( ( ) -> { com . vmware . xenon . services . common . QueryTask queryTask = getDocumentNoWait ( . class , queryTaskLink ) ; return queryTask == null ; } }
public class aTest{ @Test public void test ( ) { final org . mwg . Graph graph = new org . mwg . GraphBuilder ( ) . withPlugin ( new org . mwg . mlx . MLXPlugin ( ) ) . withScheduler ( new org . mwg . internal . scheduler . NoopScheduler ( ) ) . build ( ) ; graph . connect ( new org . mwg . Callback < java . lang . Boolean > ( ) { @ ml . classifier . Override public void on ( java . lang . Boolean result ) { org . mwg . mlx . algorithm . classifier . GaussianNaiveBayesianNode gaussianNBNode = ( ( org . mwg . mlx . algorithm . classifier . GaussianNaiveBayesianNode ) ( graph . newTypedNode ( 0 , 0 , GaussianNaiveBayesianNode . NAME ) ) ) ; standardSettings ( gaussianNBNode ) ; ml . classifier . ClassificationJumpCallback cjc = runThroughDummyDataset ( gaussianNBNode ) ; gaussianNBNode . free ( ) ; graph . disconnect ( null ) ; org . junit . Assert . assertTrue ( ( ( cjc . errors ) <= 3 ) ) ; } } }
public class aTest{ @Test public void testH2Database ( ) { try { java . lang . String expectedSQL = ( org . pentaho . di . core . database . SelectCountIT . NonHiveSelect ) + ( org . pentaho . di . core . database . SelectCountIT . TableName ) ; org . pentaho . di . core . database . DatabaseMeta databaseMeta = new org . pentaho . di . core . database . DatabaseMeta ( org . pentaho . di . core . database . SelectCountIT . h2DatabaseXML ) ; java . lang . String sql = databaseMeta . getDatabaseInterface ( ) . getSelectCountStatement ( org . pentaho . di . core . database . SelectCountIT . TableName ) ; org . junit . Assert . assertTrue ( sql . equalsIgnoreCase ( expectedSQL ) ) ; } }
public class aTest{ @Test public void testCompareLongGTEDecimal2 ( ) { long ts = nextTimestamp ( ) ; com . salesforce . phoenix . end2end . CompareDecimalToLongTest . initTableValues ( null , ts ) ; java . lang . String query = "SELECT<sp>l<sp>FROM<sp>LongInKeyTest<sp>where<sp>l<sp>>=<sp>2.5" ; java . util . Properties props = new java . util . Properties ( ) ; props . setProperty ( PhoenixRuntime . CURRENT_SCN_ATTRIB , java . lang . Long . toString ( ( ts + 2 ) ) ) ; com . salesforce . phoenix . end2end . Connection conn = com . salesforce . phoenix . end2end . DriverManager . getConnection ( com . salesforce . phoenix . end2end . PHOENIX_JDBC_URL , props ) ; try { com . salesforce . phoenix . end2end . PreparedStatement statement = conn . prepareStatement ( query ) ; com . salesforce . phoenix . end2end . ResultSet rs = statement . executeQuery ( ) ; org . junit . Assert . assertFalse ( rs . next ( ) ) ; } }
public class aTest{ @Test public void testNullNotEqualsNull ( ) { java . lang . String sql = com . liferay . portal . dao . orm . test . SQLNullTest . _SQL_NOT_EQUALS_NULL ; if ( isSybase ( ) ) { sql = transformSybaseSQL ( sql ) ; } else if ( isHypersonic ( ) ) { sql = transformHypersonicSQL ( sql ) ; } com . liferay . portal . kernel . dao . orm . Session session = _sessionFactory . openSession ( ) ; try { com . liferay . portal . kernel . dao . orm . SQLQuery sqlQuery = session . createSynchronizedSQLQuery ( sql ) ; com . liferay . portal . kernel . dao . orm . QueryPos qPos = com . liferay . portal . kernel . dao . orm . QueryPos . getInstance ( sqlQuery ) ; qPos . add ( ( ( java . lang . Object ) ( null ) ) ) ; java . util . List < java . lang . Object > list = sqlQuery . list ( ) ; org . junit . Assert . assertTrue ( list . toString ( ) , list . isEmpty ( ) ) ; } }
public class aTest{ @Test public void testJitting ( ) { java . lang . String str = "<sp>import<sp>org.drools.compiler.integrationtests.Misc2Test.AA;<sp>" + ( ( ( ( ( ( ( ( ( "<sp>import<sp>org.drools.compiler.integrationtests.Misc2Test.BB;<sp>" + "<sp>" 0 ) + "<sp>" ) + "<sp>rule<sp>R<sp>\n<sp>" ) + "<sp>when<sp>\n" ) + "<sp>BB(<sp>$v<sp>:<sp>value<sp>)<sp>\n" ) + "<sp>" 1 ) + "<sp>then<sp>\n" ) + "<sp>list.add(<sp>$a<sp>);<sp>\n" ) + "<sp>end<sp>\n" ) ; org . kie . api . KieBase kbase = loadKnowledgeBaseFromString ( str ) ; org . kie . api . runtime . KieSession ksession = kbase . newKieSession ( ) ; java . util . List list = new java . util . ArrayList ( ) ; ksession . setGlobal ( "list" , list ) ; ksession . insert ( new org . drools . compiler . integrationtests . Misc2Test . BB ( ) ) ; for ( int j = 0 ; j < 100 ; j ++ ) { ksession . insert ( new org . drools . compiler . integrationtests . Misc2Test . AA ( j ) ) ; ksession . fireAllRules ( ) ; } org . junit . Assert . assertEquals ( 100 , list . size ( ) ) ; } }
public class aTest{ @Test public void testGetDefaultPatientExamination ( ) { try { org . isf . patient . model . Patient patient = org . isf . examination . test . Tests . testPatient . setup ( false ) ; org . isf . examination . model . PatientExamination patientExamination = examinationOperations . getDefaultPatientExamination ( patient ) ; org . isf . examination . test . Tests . testPatient . check ( patientExamination . getPatient ( ) ) ; } catch ( java . lang . Exception e ) { e . printStackTrace ( ) ; org . junit . Assert . assertEquals ( true , false ) ; } }
public class aTest{ @Test public void testSearchExactDescription ( ) { int total = 5 ; for ( int i = 0 ; i < total ; i ++ ) { com . liferay . data . engine . service . test . DEDataEngineTestUtil . insertDEDataDefinition ( _adminUser , _group , ( "Description" + i ) , ( "Name" + i ) , _deDataDefinitionService ) ; } java . util . List < com . liferay . data . engine . model . DEDataDefinition > deDataDefinitions = searchDEDataDefinitions ( _group , "Description1" ) ; com . liferay . portal . search . test . util . IdempotentRetryAssert . retryAssert ( 3 , TimeUnit . SECONDS , ( ) -> { org . junit . Assert . assertEquals ( deDataDefinitions . toString ( ) , 1 , deDataDefinitions . size ( ) ) ; return null ; } }
public class aTest{ @Test public void testCircularJoin ( ) { final org . pentaho . metadata . model . LogicalModel model = new org . pentaho . metadata . model . LogicalModel ( ) ; final org . pentaho . metadata . model . LogicalTable bt1 = new org . pentaho . metadata . model . LogicalTable ( ) ; bt1 . setId ( "bt1" ) ; final org . pentaho . metadata . model . LogicalTable bt2 = new org . pentaho . metadata . model . LogicalTable ( ) ; bt2 . setId ( "bt2" ) ; final org . pentaho . metadata . model . LogicalTable bt3 = new org . pentaho . metadata . model . LogicalTable ( ) ; bt3 . setId ( "bt3" ) ; final org . pentaho . metadata . model . LogicalTable bt4 = new org . pentaho . metadata . model . LogicalTable ( ) ; bt4 . setId ( "bt4" ) ; final org . pentaho . metadata . model . LogicalTable bt5 = new org . pentaho . metadata . model . LogicalTable ( ) ; bt5 . setId ( "bt5" ) ; final org . pentaho . metadata . model . LogicalTable bt6 = new org . pentaho . metadata . model . LogicalTable ( ) ; bt6 . setId ( "bt6" ) ; final org . pentaho . metadata . model . LogicalRelationship rl1 = new org . pentaho . metadata . model . LogicalRelationship ( ) ; rl1 . setFromTable ( bt1 ) ; rl1 . setToTable ( bt2 ) ; final org . pentaho . metadata . model . LogicalRelationship rl2 = new org . pentaho . metadata . model . LogicalRelationship ( ) ; rl2 . setToTable ( bt2 ) ; rl2 . setFromTable ( bt3 ) ; final org . pentaho . metadata . model . LogicalRelationship rl3 = new org . pentaho . metadata . model . LogicalRelationship ( ) ; rl3 . setToTable ( bt2 ) ; rl3 . setFromTable ( bt4 ) ; final org . pentaho . metadata . model . LogicalRelationship rl4 = new org . pentaho . metadata . model . LogicalRelationship ( ) ; rl4 . setToTable ( bt3 ) ; rl4 . setFromTable ( bt5 ) ; final org . pentaho . metadata . model . LogicalRelationship rl5 = new org . pentaho . metadata . model . LogicalRelationship ( ) ; rl5 . setToTable ( bt4 ) ; rl5 . setFromTable ( bt5 ) ; final org . pentaho . metadata . model . LogicalRelationship rl6 = new org . pentaho . metadata . model . LogicalRelationship ( ) ; rl6 . setToTable ( bt5 ) ; rl6 . setFromTable ( bt6 ) ; model . getLogicalTables ( ) . add ( bt1 ) ; model . getLogicalTables ( ) . add ( bt2 ) ; model . getLogicalTables ( ) . add ( bt3 ) ; model . getLogicalTables ( ) . add ( bt4 ) ; model . getLogicalTables ( ) . add ( bt5 ) ; model . getLogicalTables ( ) . add ( bt6 ) ; model . getLogicalRelationships ( ) . add ( rl1 ) ; model . getLogicalRelationships ( ) . add ( rl2 ) ; model . getLogicalRelationships ( ) . add ( rl3 ) ; model . getLogicalRelationships ( ) . add ( rl4 ) ; model . getLogicalRelationships ( ) . add ( rl5 ) ; model . getLogicalRelationships ( ) . add ( rl6 ) ; org . pentaho . metadata . SqlGeneratorIT . TestSqlGenerator sqlGenerator = new org . pentaho . metadata . SqlGeneratorIT . TestSqlGenerator ( ) ; java . util . List < org . pentaho . metadata . model . LogicalTable > tbls = new java . util . ArrayList < org . pentaho . metadata . model . LogicalTable > ( ) ; tbls . add ( bt1 ) ; tbls . add ( bt6 ) ; org . pentaho . metadata . query . impl . sql . Path path = sqlGenerator . getShortestPathBetween ( model , tbls ) ; org . junit . Assert . assertEquals ( 4 , path . size ( ) ) ; java . util . ArrayList < org . pentaho . metadata . model . LogicalRelationship > rtns = new java . util . ArrayList < org . pentaho . metadata . model . LogicalRelationship > ( 4 ) ; rtns . add ( rl1 ) ; rtns . add ( rl3 ) ; rtns . add ( rl5 ) ; rtns . add ( rl6 ) ; for ( int i = 0 ; i < ( path . size ( ) ) ; i ++ ) { org . pentaho . metadata . model . LogicalRelationship rel = path . getRelationship ( i ) ; int idx = rtns . indexOf ( rel ) ; if ( idx < 0 ) { org . junit . Assert . fail ( ( "Relationship<sp>returned<sp>twice<sp>-<sp>" + rel ) ) ; } }
public class aTest{ @Test public void parseHttpsMessageFromResponseDoesNotIncludeNonAppProperties ( com . microsoft . azure . sdk . iot . device . transport . https . HttpsResponse , com . microsoft . azure . sdk . iot . device . MessageProperty ) { final byte [ ] body = new byte [ ] { 97 , 98 , 99 } ; final java . util . Map < java . lang . String , java . lang . String > headerFields = new java . util . HashMap ( ) ; final java . lang . String propertyName = "test-property-name" ; final java . lang . String propertyValue = "test-property-value" ; headerFields . put ( propertyName , propertyValue ) ; new mockit . NonStrictExpectations ( ) { { mockResponse . getBody ( ) ; result = body ; mockResponse . getHeaderFields ( ) ; result = headerFields ; } } ; com . microsoft . azure . sdk . iot . device . transport . https . HttpsSingleMessage httpsMsg = com . microsoft . azure . sdk . iot . device . transport . https . HttpsSingleMessage . parseHttpsMessage ( mockResponse ) ; com . microsoft . azure . sdk . iot . device . MessageProperty [ ] testProperties = httpsMsg . getProperties ( ) ; com . microsoft . azure . sdk . iot . device . MessageProperty [ ] expectedProperties = new com . microsoft . azure . sdk . iot . device . MessageProperty [ ] { } ; org . junit . Assert . assertThat ( testProperties , org . hamcrest . CoreMatchers . is ( expectedProperties ) ) ; } }
public class aTest{ @Test public void testJoinArrayElementWithColumn ( ) { java . sql . ResultSet rs = conn . createStatement ( ) . executeQuery ( ( ( "<sp>0<sp>|<sp>0<sp>|\n" 2 + ( joinStrategy ) ) + "inner<sp>join<sp>a<sp>b<sp>on<sp>a.ia[0]<sp>=<sp>b.i" ) ) ; java . lang . String s = TestUtils . FormattedResult . ResultFactory . toString ( rs ) ; rs . close ( ) ; java . lang . String expected = "1<sp>|<sp>I<sp>|\n" + ( ( ( ( ( ( ( ( ( ( "--------\n" + "<sp>0<sp>|<sp>0<sp>|\n" ) + "<sp>1<sp>|<sp>1<sp>|\n" ) + "<sp>0<sp>|<sp>0<sp>|\n" 1 ) + "<sp>3<sp>|<sp>3<sp>|\n" ) + "<sp>4<sp>|<sp>4<sp>|\n" ) + "<sp>5<sp>|<sp>5<sp>|\n" ) + "<sp>0<sp>|<sp>0<sp>|\n" 3 ) + "<sp>0<sp>|<sp>0<sp>|\n" 0 ) + "<sp>8<sp>|<sp>8<sp>|\n" ) + "<sp>9<sp>|<sp>9<sp>|" ) ; org . junit . Assert . assertEquals ( s , expected , s ) ; } }
public class aTest{ @Test public void $metadata ( ) { com . fujitsu . dc . test . utils . TResponse res = com . fujitsu . dc . test . utils . Http . request ( "box/$metadata-$metadata-get.txt" ) . with ( "box/$metadata-$metadata-get.txt" 1 , "\\$metadata" ) . with ( "col" , "box/$metadata-$metadata-get.txt" 3 ) . with ( "accept" , "application/atomsvc+xml" ) . with ( "token" , com . fujitsu . dc . core . DcCoreConfig . getMasterToken ( ) ) . returns ( ) . statusCode ( HttpStatus . SC_OK ) . debug ( ) ; java . lang . String str = res . getBody ( ) ; com . fujitsu . dc . test . jersey . box . Service service = javax . xml . bind . JAXB . unmarshal ( new java . io . StringReader ( str ) , com . fujitsu . dc . test . jersey . box . Service . class ) ; com . fujitsu . dc . test . jersey . box . Service rightService = new com . fujitsu . dc . test . jersey . box . Service ( ) ; rightService . workspace = new com . fujitsu . dc . test . jersey . box . Workspace ( "box/$metadata-$metadata-get.txt" 2 , new java . util . HashSet < com . fujitsu . dc . test . jersey . box . Collection > ( java . util . Arrays . asList ( new com . fujitsu . dc . test . jersey . box . Collection ( "box/$metadata-$metadata-get.txt" 0 , "box/$metadata-$metadata-get.txt" 0 ) , new com . fujitsu . dc . test . jersey . box . Collection ( "ComplexTypeProperty" , "ComplexTypeProperty" ) , new com . fujitsu . dc . test . jersey . box . Collection ( "AssociationEnd" , "AssociationEnd" ) , new com . fujitsu . dc . test . jersey . box . Collection ( "EntityType" , "EntityType" ) , new com . fujitsu . dc . test . jersey . box . Collection ( "Property" , "Property" ) ) ) ) ; org . junit . Assert . assertTrue ( rightService . isEqualTo ( service ) ) ; } }
public class aTest{ @Test public void testClosedCondition ( ) { final byte [ ] input = new byte [ ] { 'a' , 'b' , 'c' } ; final java . io . ByteArrayInputStream inputStream = new java . io . ByteArrayInputStream ( input ) ; final org . apache . hc . core5 . http . io . SessionInputBuffer inBuffer = new org . apache . hc . core5 . http . impl . io . SessionInputBufferImpl ( 16 ) ; final org . apache . hc . core5 . http . impl . io . IdentityInputStream in = new org . apache . hc . core5 . http . impl . io . IdentityInputStream ( inBuffer , inputStream ) ; in . close ( ) ; in . close ( ) ; org . junit . Assert . assertEquals ( 0 , in . available ( ) ) ; final byte [ ] tmp = new byte [ 2 ] ; try { in . read ( tmp , 0 , tmp . length ) ; org . junit . Assert . fail ( "StreamClosedException<sp>expected" ) ; } }
public class aTest{ @Test public void testReentrace ( ) { java . io . File tmp = java . io . File . createTempFile ( "bla" , ".lock" ) ; try ( org . spf4j . concurrent . FileBasedLock lock = org . spf4j . concurrent . FileBasedLock . getLock ( tmp ) ) { lock . lock ( ) ; try { lock . lock ( ) ; try { org . spf4j . concurrent . FileBasedLockTest . LOG . debug ( "Reentered<sp>lock" ) ; } finally { lock . unlock ( ) ; } boolean tryLock = lock . tryLock ( ) ; org . junit . Assert . assertTrue ( tryLock ) ; lock . unlock ( ) ; } }
public class aTest{ @Test public void autoSizeColumn_trackColumnForAutoSizing ( ) { workbook = new org . apache . poi . xssf . streaming . SXSSFWorkbook ( ) ; sheet = workbook . createSheet ( ) ; sheet . trackColumnForAutoSizing ( 0 ) ; java . util . SortedSet < java . lang . Integer > expected = new java . util . TreeSet ( ) ; expected . add ( 0 ) ; org . junit . Assert . assertEquals ( expected , sheet . getTrackedColumnsForAutoSizing ( ) ) ; sheet . autoSizeColumn ( 0 , useMergedCells ) ; try { sheet . autoSizeColumn ( 1 , useMergedCells ) ; org . junit . Assert . fail ( "Should<sp>not<sp>be<sp>able<sp>to<sp>auto-size<sp>an<sp>untracked<sp>column" ) ; } }
public class aTest{ @Test public void testToUnmodifiableList ( ) { java . util . List < java . lang . Integer > list = com . annimon . stream . Stream . range ( 0 , 6 ) . collect ( com . annimon . stream . Collectors . < java . lang . Integer > toUnmodifiableList ( ) ) ; org . junit . Assert . assertThat ( list , is ( java . util . Arrays . asList ( 0 , 1 , 2 , 3 , 4 , 5 ) ) ) ; try { list . add ( 1 ) ; org . junit . Assert . fail ( "Expected<sp>an<sp>UnsupportedOperationException<sp>to<sp>be<sp>thrown<sp>when<sp>add<sp>item<sp>to<sp>list" ) ; } }
public class aTest{ @Test public void testComplexDump ( ) { com . bah . culvert . constraints . Constraint constraint = org . powermock . api . easymock . PowerMock . createPartialMock ( com . bah . culvert . constraints . Constraint . class , "getResultIterator" ) ; java . util . List < com . bah . culvert . data . Result > resList = new java . util . ArrayList < com . bah . culvert . data . Result > ( ) ; java . util . List < com . bah . culvert . data . CKeyValue > ckv = new java . util . ArrayList < com . bah . culvert . data . CKeyValue > ( ) ; for ( int i = 0 ; i < 10 ; i ++ ) { ckv . add ( new com . bah . culvert . data . CKeyValue ( com . bah . culvert . util . Bytes . toBytes ( java . lang . String . format ( "%d" , i ) ) , com . bah . culvert . util . Bytes . toBytes ( "cf" ) , "cq" . getBytes ( ) ) ) ; } resList . add ( new com . bah . culvert . data . Result ( ckv ) ) ; org . easymock . EasyMock . expect ( constraint . getResultIterator ( ) ) . andReturn ( new com . bah . culvert . iterators . DecoratingCurrentIterator ( resList . iterator ( ) ) ) ; com . bah . culvert . adapter . TableAdapter dumpTable = new com . bah . culvert . inmemory . InMemoryTable ( ) ; class DropFilter extends com . bah . culvert . constraints . write . Handler { @ com . bah . culvert . constraints . Override public java . util . List < com . bah . culvert . data . CKeyValue > apply ( com . bah . culvert . data . Result write ) { return java . util . Collections . EMPTY_LIST ; } } org . powermock . api . easymock . PowerMock . replayAll ( ) ; constraint . writeToTable ( dumpTable , new DropFilter ( ) ) ; org . powermock . api . easymock . PowerMock . verifyAll ( ) ; java . util . Iterator < com . bah . culvert . data . Result > results = dumpTable . get ( new com . bah . culvert . transactions . Get ( new com . bah . culvert . data . CRange ( new byte [ 0 ] ) ) ) ; org . junit . Assert . assertFalse ( results . hasNext ( ) ) ; org . powermock . api . easymock . PowerMock . resetAll ( ) ; org . easymock . EasyMock . expect ( constraint . getResultIterator ( ) ) . andReturn ( new com . bah . culvert . iterators . DecoratingCurrentIterator ( resList . iterator ( ) ) ) ; dumpTable = new com . bah . culvert . inmemory . InMemoryTable ( ) ; class MultiplicateHandler extends com . bah . culvert . constraints . write . Handler { @ com . bah . culvert . constraints . Override public java . util . List < com . bah . culvert . data . CKeyValue > apply ( com . bah . culvert . data . Result write ) { if ( ! ( write . getKeyValues ( ) . iterator ( ) . hasNext ( ) ) ) return java . util . Collections . EMPTY_LIST ; java . util . List < com . bah . culvert . data . CKeyValue > newKvs = new java . util . ArrayList < com . bah . culvert . data . CKeyValue > ( ) ; for ( com . bah . culvert . data . CKeyValue kv : write . getKeyValues ( ) ) newKvs . add ( new com . bah . culvert . data . CKeyValue ( kv . getRowId ( ) , com . bah . culvert . util . Bytes . lexIncrement ( kv . getFamily ( ) ) , "add" . getBytes ( ) ) ) ; newKvs . addAll ( write . getKeyValues ( ) ) ; return newKvs ; } } }
public class aTest{ @Test public void testComplexArrayValuesCreate ( ) { software . amazon . kinesis . multilang . config . BuilderDynaBeanTest . TestVariantCreate variant1 = software . amazon . kinesis . multilang . config . BuilderDynaBeanTest . TestVariantCreate . create ( "variant-1" , 10L , "vary-1" ) ; software . amazon . kinesis . multilang . config . BuilderDynaBeanTest . TestVariantCreate variant2 = software . amazon . kinesis . multilang . config . BuilderDynaBeanTest . TestVariantCreate . create ( "tests[1].[0]" 3 , 20L , "vary-2" ) ; software . amazon . kinesis . multilang . config . BuilderDynaBeanTest . ComplexArrayClassVariant expected = software . amazon . kinesis . multilang . config . BuilderDynaBeanTest . ComplexArrayClassVariant . builder ( ) . variantName ( "create-test" ) . tests ( new software . amazon . kinesis . multilang . config . BuilderDynaBeanTest . TestInterface [ ] { variant1 , variant2 } ) . build ( ) ; software . amazon . kinesis . multilang . config . BuilderDynaBean builderDynaBean = new software . amazon . kinesis . multilang . config . BuilderDynaBean ( software . amazon . kinesis . multilang . config . BuilderDynaBeanTest . ComplexArrayClassVariant . class , convertUtilsBean ) ; utilsBean . setProperty ( builderDynaBean , "tests[1].[0]" 2 , expected . variantName ) ; utilsBean . setProperty ( builderDynaBean , "tests[0].class" , variant1 . getClass ( ) . getName ( ) ) ; utilsBean . setProperty ( builderDynaBean , "tests[0].[0]" , variant1 . variantCreateName ) ; utilsBean . setProperty ( builderDynaBean , "tests[1].[0]" 0 , variant1 . longClass ) ; utilsBean . setProperty ( builderDynaBean , "tests[1].[0]" 1 , variant1 . varyString ) ; utilsBean . setProperty ( builderDynaBean , "tests[1].class" , variant2 . getClass ( ) . getName ( ) ) ; utilsBean . setProperty ( builderDynaBean , "tests[1].[0]" , variant2 . variantCreateName ) ; utilsBean . setProperty ( builderDynaBean , "tests[1].[1]" , variant2 . longClass ) ; utilsBean . setProperty ( builderDynaBean , "tests[1].[2]" , variant2 . varyString ) ; software . amazon . kinesis . multilang . config . BuilderDynaBeanTest . ComplexArrayClassVariant actual = builderDynaBean . build ( software . amazon . kinesis . multilang . config . BuilderDynaBeanTest . ComplexArrayClassVariant . class ) ; org . junit . Assert . assertThat ( actual , org . hamcrest . CoreMatchers . equalTo ( expected ) ) ; } }
public class aTest{ @Test public void testNewPacket ( ) { try { org . pcap4j . packet . IcmpV6EchoRequestPacket p = org . pcap4j . packet . IcmpV6EchoRequestPacket . newPacket ( packet . getRawData ( ) , 0 , packet . getRawData ( ) . length ) ; org . junit . Assert . assertEquals ( packet , p ) ; } }
public class aTest{ @Test public void LastAuthenticated ( ) { java . lang . String updateUserName = "account1999" ; java . lang . String updatePass = "password19999" ; try { com . fujitsu . dc . test . utils . AccountUtils . create ( AbstractCase . MASTER_TOKEN_NAME , cellName , updateUserName , updatePass , HttpStatus . SC_CREATED ) ; com . fujitsu . dc . test . utils . AccountUtils . update ( AbstractCase . MASTER_TOKEN_NAME , cellName , updateUserName , updateUserName , orgPass , "/Date(1414656074074)/" , HttpStatus . SC_NO_CONTENT ) ; com . fujitsu . dc . test . utils . TResponse res = com . fujitsu . dc . test . utils . AccountUtils . get ( AbstractCase . MASTER_TOKEN_NAME , HttpStatus . SC_OK , cellName , updateUserName ) ; java . lang . String getLastAuthenticated = ( ( java . lang . String ) ( ( ( org . json . simple . JSONObject ) ( ( ( org . json . simple . JSONObject ) ( res . bodyAsJson ( ) . get ( "d" ) ) ) . get ( "results" ) ) ) . get ( "LastAuthenticated" ) ) ) ; org . junit . Assert . assertEquals ( "/Date(1414656074074)/" , getLastAuthenticated ) ; } }
public class aTest{ @Test public void testLenientLimit ( ) { org . apache . drill . test . LogFixture . LogFixtureBuilder logBuilder = org . apache . drill . test . LogFixture . builder ( ) . logger ( org . apache . drill . exec . memory . Accountant . class , Level . WARN ) ; try ( org . apache . drill . test . LogFixture logFixture = logBuilder . build ( ) ) { org . junit . Assert . assertTrue ( org . apache . drill . exec . util . AssertionUtil . isAssertionsEnabled ( ) ) ; org . apache . drill . exec . memory . BufferAllocator allocator = fixture . allocator ( ) . newChildAllocator ( "test" , ( 10 * ( org . apache . drill . exec . physical . impl . xsort . managed . TestLenientAllocation . ONE_MEG ) ) , ( 128 * ( org . apache . drill . exec . physical . impl . xsort . managed . TestLenientAllocation . ONE_MEG ) ) ) ; ( ( org . apache . drill . exec . memory . Accountant ) ( allocator ) ) . forceLenient ( ) ; io . netty . buffer . DrillBuf buf1 = allocator . buffer ( ( 64 * ( org . apache . drill . exec . physical . impl . xsort . managed . TestLenientAllocation . ONE_MEG ) ) ) ; io . netty . buffer . DrillBuf buf2 = allocator . buffer ( ( 128 * ( org . apache . drill . exec . physical . impl . xsort . managed . TestLenientAllocation . ONE_MEG ) ) ) ; try { allocator . buffer ( ( 64 * ( org . apache . drill . exec . physical . impl . xsort . managed . TestLenientAllocation . ONE_MEG ) ) ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void fileLengthUnknown ( ) { long lengthActual = ( ( alluxio . client . block . stream . GrpcDataReaderTest . CHUNK_SIZE ) * 1024 ) + ( ( alluxio . client . block . stream . GrpcDataReaderTest . CHUNK_SIZE ) / 3 ) ; long checksumStart = 100 ; long bytesToRead = lengthActual / 3 ; try ( alluxio . client . block . stream . DataReader reader = create ( 0 , Long . MAX_VALUE ) ) { long checksum = setReadResponses ( mClient , lengthActual , checksumStart , ( bytesToRead - 1 ) ) ; long checksumActual = checkChunks ( reader , checksumStart , bytesToRead ) ; org . junit . Assert . assertEquals ( checksum , checksumActual ) ; } }
public class aTest{ @Test public void test1 ( ) { com . taobao . tddl . executor . cursor . MockArrayCursor cursor = new com . taobao . tddl . executor . cursor . MockArrayCursor ( "school3" 2 ) ; cursor . addColumn ( "school3" 1 , DataType . IntegerType ) ; cursor . addColumn ( "name" , DataType . StringType ) ; cursor . addColumn ( "school" , DataType . StringType ) ; cursor . initMeta ( ) ; cursor . addRow ( new java . lang . Object [ ] { 1 , "school3" 7 , "school1" } ) ; cursor . addRow ( new java . lang . Object [ ] { 2 , "school3" 3 , "school3" 0 } ) ; cursor . addRow ( new java . lang . Object [ ] { 3 , "school3" 4 , "school3" } ) ; cursor . addRow ( new java . lang . Object [ ] { 4 , "school3" 6 , "school3" 5 } ) ; cursor . addRow ( new java . lang . Object [ ] { 5 , "name5" , "school5" } ) ; cursor . addRow ( new java . lang . Object [ ] { 6 , "name6" , "school6" } ) ; cursor . addRow ( new java . lang . Object [ ] { 7 , "name7" , "school7" } ) ; cursor . init ( ) ; com . taobao . tddl . executor . rowset . IRowSet row = null ; int count = 0 ; while ( ( row = cursor . next ( ) ) != null ) { System . out . println ( row ) ; count ++ ; } org . junit . Assert . assertEquals ( 7 , count ) ; } }
public class aTest{ @Test public void testBuildTableNameRootServicePathDataModelByServicePathNewEncoding ( ) { System . out . println ( ( ( ( getTestTraceHead ( "[NGSIPostgreSQLSink.buildTableName]" ) ) + "--------<sp>When<sp>a<sp>root<sp>service-path<sp>is<sp>notified/defaulted<sp>and<sp>data_model<sp>is<sp>" ) + "'dm-by-service-path'<sp>the<sp>MySQL<sp>table<sp>name<sp>is<sp>the<sp>encoding<sp>of<sp><service-path>" ) ) ; java . lang . String attrPersistence = null ; java . lang . String batchSize = null ; java . lang . String batchTime = null ; java . lang . String batchTTL = null ; java . lang . String dataModel = "dm-by-service-path" ; java . lang . String enableEncoding = "true" ; java . lang . String enableGrouping = null ; java . lang . String enableLowercase = null ; java . lang . String host = null ; java . lang . String password = null ; java . lang . String port = null ; java . lang . String username = null ; java . lang . String cache = null ; com . telefonica . iot . cygnus . sinks . NGSIPostgreSQLSink sink = new com . telefonica . iot . cygnus . sinks . NGSIPostgreSQLSink ( ) ; sink . configure ( createContext ( attrPersistence , batchSize , batchTime , batchTTL , dataModel , enableEncoding , enableGrouping , enableLowercase , host , password , port , username , cache ) ) ; java . lang . String servicePath = "/" ; java . lang . String entity = null ; java . lang . String attribute = null ; try { java . lang . String builtTableName = sink . buildTableName ( servicePath , entity , attribute ) ; java . lang . String expecetedTableName = "x002f" ; try { org . junit . Assert . assertEquals ( expecetedTableName , builtTableName ) ; System . out . println ( ( ( ( ( getTestTraceHead ( "[NGSIPostgreSQLSink.buildTableName]" ) ) + "-<sp>OK<sp>-<sp>'" ) + builtTableName ) + "'<sp>is<sp>equals<sp>to<sp>the<sp>encoding<sp>of<sp><service-path>" ) ) ; } }
public class aTest{ @Test public void testEncodeMySQLSinglex ( ) { System . out . println ( ( ( com . telefonica . iot . cygnus . utils . CommonUtilsForTests . getTestTraceHead ( "[NGSICharsets.encodeMySQL]" ) ) + "--------<sp>A<sp>single<sp>'x'<sp>is<sp>not<sp>encoded" ) ) ; java . lang . String in = "x" ; java . lang . String expected = "x" ; java . lang . String out = com . telefonica . iot . cygnus . utils . NGSICharsets . encodeMySQL ( in ) ; try { org . junit . Assert . assertEquals ( expected , out ) ; System . out . println ( ( ( ( ( com . telefonica . iot . cygnus . utils . CommonUtilsForTests . getTestTraceHead ( "[NGSICharsets.encodeMySQL]" ) ) + "-<sp>OK<sp>-<sp>'" ) + in ) + "'<sp>has<sp>not<sp>been<sp>encoded" ) ) ; } }
public class aTest{ @Test public void testGetTargetContentEncoding ( ) { org . jboss . netty . handler . codec . http . HttpContentCompressor compressor = new org . jboss . netty . handler . codec . http . HttpContentCompressor ( ) ; java . lang . String [ ] tests = new java . lang . String [ ] { -> Content - Encoding "" , null , "*" , "gzip" , "*;q=0.0" , null , "gzip" , "gzip" , "compress,<sp>gzip;q=0.5" , "gzip" , "gzip;<sp>q=0.5,<sp>identity" , "gzip" , "gzip<sp>;<sp>q=0.1" , "gzip" , "gzip;<sp>q=0,<sp>deflate" , "deflate" , "<sp>deflate<sp>;<sp>q=0<sp>,<sp>*;q=0.5" , "gzip" } ; for ( int i = 0 ; i < ( tests . length ) ; i += 2 ) { java . lang . String acceptEncoding = tests [ i ] ; java . lang . String contentEncoding = tests [ ( i + 1 ) ] ; java . lang . String targetEncoding = compressor . getTargetContentEncoding ( acceptEncoding ) ; org . junit . Assert . assertEquals ( contentEncoding , targetEncoding ) ; } } }
public class aTest{ @Test public void testZCompressionProvider ( ) { java . io . InputStream in = java . lang . Thread . currentThread ( ) . getContextClassLoader ( ) . getResourceAsStream ( "nom/tam/fits/test/test.fits.Z" ) ; nom . tam . fits . Fits f = null ; try { f = new nom . tam . fits . Fits ( new nom . tam . fits . compress . ZCompressionProvider ( ) . decompress ( in ) ) ; org . junit . Assert . assertNotNull ( f . readHDU ( ) ) ; } }
public class aTest{ @Test public void generateDDL_whenTableHasACustomFormatType_returnsDDLStringWithACustomFormat ( ) { org . jkiss . dbeaver . ext . greenplum . model . PostgreTableColumn mockPostgreTableColumn = mockDbColumn ( "column1" , "int4" , 1 ) ; java . util . List < org . jkiss . dbeaver . ext . greenplum . model . PostgreTableColumn > tableColumns = java . util . Collections . singletonList ( mockPostgreTableColumn ) ; org . mockito . Mockito . when ( mockResults . getString ( "b" 1 ) ) . thenReturn ( "b" ) ; org . mockito . Mockito . when ( mockResults . getString ( "fmtopts" ) ) . thenReturn ( "FORMATTER<sp>'formatter_export_s'" ) ; org . jkiss . dbeaver . ext . greenplum . model . GreenplumExternalTable table = new org . jkiss . dbeaver . ext . greenplum . model . GreenplumExternalTable ( mockSchema , mockResults ) ; addMockColumnsToTableCache ( tableColumns , table ) ; java . lang . String expectedDDL = "b" 0 + ( ( ( ( "LOCATION<sp>(\n" + "\t\'gpfdist://filehost:8081/*.txt\'\n" ) + ")<sp>ON<sp>ALL\n" ) + "FORMAT<sp>\'CUSTOM\'<sp>(<sp>FORMATTER=\'formatter_export_s\'<sp>)\n" ) + "ENCODING<sp>'UTF8'" ) ; org . junit . Assert . assertEquals ( expectedDDL , table . generateDDL ( monitor ) ) ; } }
public class aTest{ @Test public void testOrder ( ) { org . apache . hadoop . hbase . TableName tableName = sharedTestEnv . newTestTableName ( ) ; java . util . List < java . lang . String > randomFamilies = new java . util . ArrayList ( ) ; for ( int i = 0 ; i < 10 ; i ++ ) { randomFamilies . add ( java . util . UUID . randomUUID ( ) . toString ( ) ) ; } try ( org . apache . hadoop . hbase . client . Admin admin = getConnection ( ) . getAdmin ( ) ) { org . apache . hadoop . hbase . HTableDescriptor tableDescriptor = new org . apache . hadoop . hbase . HTableDescriptor ( tableName ) ; for ( java . lang . String family : randomFamilies ) { tableDescriptor . addFamily ( new org . apache . hadoop . hbase . HColumnDescriptor ( family ) ) ; } admin . createTable ( tableDescriptor ) ; try ( org . apache . hadoop . hbase . client . Table table = getConnection ( ) . getTable ( tableName ) ) { java . util . Collections . shuffle ( randomFamilies ) ; byte [ ] rowKey = org . apache . hadoop . hbase . util . Bytes . toBytes ( "rowKey" ) ; org . apache . hadoop . hbase . client . Put put = new org . apache . hadoop . hbase . client . Put ( rowKey ) ; for ( java . lang . String family : randomFamilies ) { put . addColumn ( org . apache . hadoop . hbase . util . Bytes . toBytes ( family ) , org . apache . hadoop . hbase . util . Bytes . toBytes ( java . util . UUID . randomUUID ( ) . toString ( ) ) , org . apache . hadoop . hbase . util . Bytes . toBytes ( java . util . UUID . randomUUID ( ) . toString ( ) ) ) ; put . addColumn ( org . apache . hadoop . hbase . util . Bytes . toBytes ( family ) , org . apache . hadoop . hbase . util . Bytes . toBytes ( java . util . UUID . randomUUID ( ) . toString ( ) ) , org . apache . hadoop . hbase . util . Bytes . toBytes ( java . util . UUID . randomUUID ( ) . toString ( ) ) ) ; } table . put ( put ) ; org . apache . hadoop . hbase . client . Result result = table . get ( new org . apache . hadoop . hbase . client . Get ( rowKey ) ) ; org . apache . hadoop . hbase . Cell [ ] rawCells = result . rawCells ( ) ; java . util . Set < org . apache . hadoop . hbase . Cell > ordered = new java . util . TreeSet ( org . apache . hadoop . hbase . KeyValue . COMPARATOR ) ; ordered . addAll ( java . util . Arrays . asList ( rawCells ) ) ; org . apache . hadoop . hbase . Cell [ ] orderedCells = ordered . toArray ( new org . apache . hadoop . hbase . Cell [ 0 ] ) ; org . junit . Assert . assertArrayEquals ( orderedCells , rawCells ) ; } }
public class aTest{ @Test public void testGetters ( ) { edu . emory . clir . clearnlp . constituent . CTNode curr = new edu . emory . clir . clearnlp . constituent . CTNode ( "NP-PRD-1-LOC=2" ) ; testGetTags ( curr ) ; edu . emory . clir . clearnlp . constituent . CTNode [ ] children = new edu . emory . clir . clearnlp . constituent . CTNode [ 4 ] ; children [ 0 ] = new edu . emory . clir . clearnlp . constituent . CTNode ( "NP-LOC" ) ; children [ 1 ] = new edu . emory . clir . clearnlp . constituent . CTNode ( "and" 0 , "and" ) ; children [ 2 ] = new edu . emory . clir . clearnlp . constituent . CTNode ( "and" 1 ) ; children [ 3 ] = new edu . emory . clir . clearnlp . constituent . CTNode ( "PP-LOC-PRD" ) ; for ( edu . emory . clir . clearnlp . constituent . CTNode child : children ) curr . addChild ( child ) ; testGetChildren ( curr , children ) ; edu . emory . clir . clearnlp . constituent . CTNode gChild = new edu . emory . clir . clearnlp . constituent . CTNode ( "-NONE-" , "*ICH*" ) ; children [ 0 ] . addChild ( new edu . emory . clir . clearnlp . constituent . CTNode ( "-NONE-" , "*" ) ) ; children [ 2 ] . addChild ( new edu . emory . clir . clearnlp . constituent . CTNode ( "NNP" , "Jinho" ) ) ; children [ 2 ] . addChild ( gChild ) ; children [ 3 ] . addChild ( new edu . emory . clir . clearnlp . constituent . CTNode ( "PP" ) ) ; testGetAncestors ( curr , children , gChild ) ; testGetDescendants ( curr , children ) ; testGetSiblings ( curr , children ) ; testGetSubtree ( curr ) ; testGetEmptyCategories ( curr ) ; org . junit . Assert . assertEquals ( 2 , gChild . getDistanceToTop ( ) ) ; } }
public class aTest{ @Test public void testBug305 ( ) { java . util . Properties p = new java . util . Properties ( ) ; p . putAll ( com . enioka . jqm . jdbc . Db . loadProperties ( ) ) ; com . enioka . jqm . jdbc . Db db = new com . enioka . jqm . jdbc . Db ( p ) ; com . enioka . jqm . jdbc . DbConn cnx = null ; try { cnx = db . getConn ( ) ; int qId = com . enioka . jqm . model . Queue . create ( cnx , "q1" , "q1<sp>description" , true ) ; int jobDefdId = com . enioka . jqm . model . JobDef . create ( cnx , "test<sp>description" , "class" , null , "jar" , qId , 1 , "appName" , null , null , null , null , null , false , null , PathType . FS ) ; com . enioka . jqm . model . JobInstance . enqueue ( cnx , com . enioka . jqm . model . State . RUNNING , qId , jobDefdId , null , null , null , null , null , null , null , null , null , false , false , null , 1 , Instruction . RUN , new java . util . HashMap ( ) ) ; com . enioka . jqm . model . JobInstance . enqueue ( cnx , com . enioka . jqm . model . State . RUNNING , qId , jobDefdId , null , null , null , null , null , null , null , null , null , false , false , null , 1 , Instruction . RUN , new java . util . HashMap ( ) ) ; cnx . commit ( ) ; java . util . Properties p2 = new java . util . Properties ( ) ; p2 . put ( "com.enioka.jqm.jdbc.contextobject" , db ) ; java . util . List < com . enioka . jqm . model . com . enioka . jqm . api . JobInstance > res = com . enioka . jqm . api . JqmClientFactory . getClient ( "test" , p2 , false ) . getJobs ( com . enioka . jqm . api . Query . create ( ) . setQueryHistoryInstances ( false ) . setQueryLiveInstances ( true ) . addSortDesc ( Query . Sort . ID ) . setPageSize ( 1 ) . setApplicationName ( "appName" ) ) ; org . junit . Assert . assertEquals ( 1 , res . size ( ) ) ; } }
public class aTest{ @Test public void testNamedInjectedFieldUsesFieldName ( org . jboss . weld . tests . resolution . named . NamedBeanConsumer ) { org . junit . Assert . assertNotNull ( consumer . getFoo ( ) ) ; } }
public class aTest{ @Test public void testCreateReferenceTwoObjects ( ) { com . sun . sgs . test . impl . service . data . TestDataServiceImpl . txnScheduler . runTask ( new com . sun . sgs . test . util . TestAbstractKernelRunnable ( ) { public void run ( ) { com . sun . sgs . test . util . DummyManagedObject x = new com . sun . sgs . test . util . DummyManagedObject ( ) ; com . sun . sgs . test . util . DummyManagedObject y = new com . sun . sgs . test . util . DummyManagedObject ( ) ; org . junit . Assert . assertFalse ( com . sun . sgs . test . impl . service . data . TestDataServiceImpl . service . createReference ( x ) . equals ( com . sun . sgs . test . impl . service . data . TestDataServiceImpl . service . createReference ( y ) ) ) ; } } }
public class aTest{ @Test public void testKeypointImageTransform ( ) { final java . io . File featureSeqFile = folder . newFile ( "seq-testKeypointImageTransform.features" ) ; final java . io . File featureSeqFileDouble = folder . newFile ( "seq-testKeypointImageTransform2.features" ) ; org . openimaj . hadoop . tools . localfeature . HadoopLocalFeaturesTool . main ( new java . lang . String [ ] { "ASIFTENRICHED" 0 , "mapred.child.java.opts=\"-Xmx3000M\"" , "-i" , imageSeqFile . getAbsolutePath ( ) , "-o" , featureSeqFileDouble . getAbsolutePath ( ) , "-m" , "ASIFTENRICHED" , "-rm" } ) ; org . openimaj . hadoop . tools . localfeature . HadoopLocalFeaturesTool . main ( new java . lang . String [ ] { "ASIFTENRICHED" 0 , "mapred.child.java.opts=\"-Xmx3000M\"" , "-i" , imageSeqFile . getAbsolutePath ( ) , "-o" , featureSeqFile . getAbsolutePath ( ) , "-m" , "ASIFTENRICHED" , "-rm" , "-nds" } ) ; final org . openimaj . feature . local . list . LocalFeatureList < org . openimaj . image . feature . local . keypoints . Keypoint > firstKPL = getKPLFromSequence ( keys . get ( 0 ) , featureSeqFile ) ; final org . openimaj . feature . local . list . LocalFeatureList < org . openimaj . image . feature . local . keypoints . Keypoint > firstKPLDouble = getKPLFromSequence ( keys . get ( 0 ) , featureSeqFileDouble ) ; org . junit . Assert . assertTrue ( ( ( firstKPL . size ( ) ) < ( firstKPLDouble . size ( ) ) ) ) ; } }
public class aTest{ @Test public void testQueryRawMappedThrow ( ) { com . j256 . ormlite . dao . Dao < com . j256 . ormlite . dao . Foo , java . lang . Integer > dao = createDao ( com . j256 . ormlite . dao . Foo . class , true ) ; com . j256 . ormlite . dao . Foo foo = new com . j256 . ormlite . dao . Foo ( ) ; org . junit . Assert . assertEquals ( 1 , dao . create ( foo ) ) ; com . j256 . ormlite . support . DatabaseConnection conn = connectionSource . getReadWriteConnection ( com . j256 . ormlite . dao . FOO_TABLE_NAME ) ; try { conn . close ( ) ; dao . queryRaw ( "SELECT<sp>*<sp>FROM<sp>FOO" , new com . j256 . ormlite . dao . RawRowMapper < com . j256 . ormlite . dao . Foo > ( ) { @ com . j256 . ormlite . dao . Override public com . j256 . ormlite . dao . Foo mapRow ( java . lang . String [ ] columnNames , java . lang . String [ ] resultColumns ) { return null ; } } }
public class aTest{ @Test public void testGetActiveData ( ) { byte [ ] data = new byte [ 8 ] ; org . mockito . Mockito . when ( mockZK . getData ( org . mockito . Mockito . eq ( org . apache . hadoop . ha . TestActiveStandbyElector . ZK_LOCK_NAME ) , org . mockito . Mockito . eq ( false ) , any ( ) ) ) . thenReturn ( data ) ; org . junit . Assert . assertEquals ( data , elector . getActiveData ( ) ) ; org . mockito . Mockito . verify ( mockZK , org . mockito . Mockito . times ( 1 ) ) . getData ( org . mockito . Mockito . eq ( org . apache . hadoop . ha . TestActiveStandbyElector . ZK_LOCK_NAME ) , org . mockito . Mockito . eq ( false ) , any ( ) ) ; org . mockito . Mockito . when ( mockZK . getData ( org . mockito . Mockito . eq ( org . apache . hadoop . ha . TestActiveStandbyElector . ZK_LOCK_NAME ) , org . mockito . Mockito . eq ( false ) , any ( ) ) ) . thenThrow ( new org . apache . zookeeper . KeeperException . NoNodeException ( ) ) ; try { elector . getActiveData ( ) ; org . junit . Assert . fail ( "ActiveNotFoundException<sp>expected" ) ; } }
public class aTest{ @Test public void testCreateProjectWithAnInvalidWorkspaceId ( ) { org . eclipse . orion . server . core . metastore . IMetaStore metaStore = org . eclipse . orion . server . core . OrionConfiguration . getMetaStore ( ) ; java . lang . String projectName = "Orion<sp>Project" ; org . eclipse . orion . server . core . metastore . ProjectInfo projectInfo = new org . eclipse . orion . server . core . metastore . ProjectInfo ( ) ; projectInfo . setFullName ( projectName ) ; try { projectInfo . setContentLocation ( new java . net . URI ( "file:/home/anthony/orion/project" ) ) ; } catch ( java . net . URISyntaxException e ) { } projectInfo . setWorkspaceId ( "77" ) ; try { metaStore . createProject ( projectInfo ) ; org . junit . Assert . assertTrue ( false ) ; } } }
public class aTest{ @Test public void testFindWildcardMatchJMX ( ) { org . hawkular . agent . monitor . protocol . jmx . JMXNodeLocation multiTargetLocation = new org . hawkular . agent . monitor . protocol . jmx . JMXNodeLocation ( new javax . management . ObjectName ( "domain:name=value,matchme=*" ) ) ; org . hawkular . agent . monitor . protocol . jmx . JMXNodeLocation singleLocation = new org . hawkular . agent . monitor . protocol . jmx . JMXNodeLocation ( new javax . management . ObjectName ( "domain:name2=value2,name3=value3,matchme=foo,name=value" ) ) ; org . hawkular . agent . monitor . protocol . jmx . JMXLocationResolver resolver = new org . hawkular . agent . monitor . protocol . jmx . JMXLocationResolver ( ) ; org . junit . Assert . assertEquals ( "foo" , resolver . findWildcardMatch ( multiTargetLocation , singleLocation ) ) ; singleLocation = new org . hawkular . agent . monitor . protocol . jmx . JMXNodeLocation ( new javax . management . ObjectName ( "domain:name2=value2,name3=value3,name=value" ) ) ; try { resolver . findWildcardMatch ( multiTargetLocation , singleLocation ) ; org . junit . Assert . fail ( "Single<sp>location<sp>was<sp>missing<sp>'matchme'<sp>key<sp>-<sp>should<sp>have<sp>failed" ) ; } }
public class aTest{ @Test public void testPatternOnClass ( ) { final java . lang . String drl = ( ( ( ( ( ( ( ( ( ( ( "import<sp>" + ( org . drools . core . reteoo . InitialFactImpl . class . getCanonicalName ( ) ) ) + "\n" ) + "import<sp>" ) + ( org . drools . testcoverage . common . model . FactB . class . getCanonicalName ( ) ) ) + "\n" ) + "rule<sp>\"Clear\"<sp>when\n" ) + "<sp>$f:<sp>Object(getClass()<sp>!=<sp>FactB.class)\n" ) + "then\n" ) + "<sp>}\n" 0 ) + "<sp>delete(<sp>$f<sp>);\n" ) + "<sp>}\n" ) + "end" ; final org . kie . api . KieBase kbase = org . drools . testcoverage . common . util . KieBaseUtil . getKieBaseFromKieModuleFromDrl ( "pattern-test" , kieBaseTestConfiguration , drl ) ; final org . kie . api . runtime . KieSession ksession = kbase . newKieSession ( ) ; try { ksession . insert ( new org . drools . testcoverage . common . model . FactA ( ) ) ; ksession . insert ( new org . drools . testcoverage . common . model . FactA ( ) ) ; ksession . insert ( new org . drools . testcoverage . common . model . FactB ( ) ) ; ksession . insert ( new org . drools . testcoverage . common . model . FactB ( ) ) ; ksession . insert ( new org . drools . testcoverage . common . model . FactC ( ) ) ; ksession . insert ( new org . drools . testcoverage . common . model . FactC ( ) ) ; ksession . fireAllRules ( ) ; for ( final org . kie . api . runtime . rule . FactHandle fact : ksession . getFactHandles ( ) ) { final org . drools . core . common . InternalFactHandle internalFact = ( ( org . drools . core . common . InternalFactHandle ) ( fact ) ) ; org . junit . Assert . assertTrue ( ( ( internalFact . getObject ( ) ) instanceof org . drools . testcoverage . common . model . FactB ) ) ; } } }
public class aTest{ @Test public void testReplaceWith ( ) { try { search = new org . odftoolkit . simple . common . navigation . TextNavigation ( "ReplaceDateTarget" , doc ) ; int i = 0 ; while ( search . hasNext ( ) ) { org . odftoolkit . simple . common . navigation . TextSelection item = ( ( org . odftoolkit . simple . common . navigation . TextSelection ) ( search . nextSelection ( ) ) ) ; org . odftoolkit . simple . common . navigation . FieldSelection fieldSelection = new org . odftoolkit . simple . common . navigation . FieldSelection ( item ) ; fieldSelection . replaceWith ( "hello<sp>world." ) ; i ++ ; } doc . save ( org . odftoolkit . simple . utils . ResourceUtilities . newTestOutputFile ( org . odftoolkit . simple . common . navigation . FieldSelectionTest . SAVE_FILE_REPLACE ) ) ; org . odftoolkit . simple . TextDocument doc1 = ( ( org . odftoolkit . simple . TextDocument ) ( org . odftoolkit . simple . Document . loadDocument ( org . odftoolkit . simple . utils . ResourceUtilities . getAbsolutePath ( org . odftoolkit . simple . common . navigation . FieldSelectionTest . SAVE_FILE_REPLACE ) ) ) ) ; search = new org . odftoolkit . simple . common . navigation . TextNavigation ( "ReplaceDateTarget" , doc1 ) ; if ( search . hasNext ( ) ) { org . junit . Assert . fail ( ) ; } search = new org . odftoolkit . simple . common . navigation . TextNavigation ( "hello<sp>world." , doc1 ) ; int j = 0 ; while ( search . hasNext ( ) ) { org . odftoolkit . simple . common . navigation . TextSelection item = ( ( org . odftoolkit . simple . common . navigation . TextSelection ) ( search . nextSelection ( ) ) ) ; item . replaceWith ( "hi<sp>world." ) ; j ++ ; } org . junit . Assert . assertEquals ( i , j ) ; } }
public class aTest{ @Test public void testConcurrentReading ( ) { for ( org . apache . hadoop . hbase . io . hfile . Compression . Algorithm compressAlgo : org . apache . hadoop . hbase . io . hfile . TestHFileBlock . COMPRESSION_ALGORITHMS ) { org . apache . hadoop . fs . Path path = new org . apache . hadoop . fs . Path ( org . apache . hadoop . hbase . io . hfile . TestHFileBlock . TEST_UTIL . getDataTestDir ( ) , "concurrent_reading" ) ; java . util . Random rand = defaultRandom ( ) ; java . util . List < java . lang . Long > offsets = new java . util . ArrayList < java . lang . Long > ( ) ; java . util . List < org . apache . hadoop . hbase . io . hfile . BlockType > types = new java . util . ArrayList < org . apache . hadoop . hbase . io . hfile . BlockType > ( ) ; writeBlocks ( rand , compressAlgo , path , offsets , null , types , null ) ; org . apache . hadoop . fs . FSDataInputStream is = fs . open ( path ) ; long fileSize = fs . getFileStatus ( path ) . getLen ( ) ; org . apache . hadoop . hbase . io . hfile . HFileBlock . FSReader hbr = new org . apache . hadoop . hbase . io . hfile . HFileBlock . FSReaderV2 ( is , compressAlgo , fileSize ) ; java . util . concurrent . Executor exec = java . util . concurrent . Executors . newFixedThreadPool ( org . apache . hadoop . hbase . io . hfile . TestHFileBlock . NUM_READER_THREADS ) ; java . util . concurrent . ExecutorCompletionService < java . lang . Boolean > ecs = new java . util . concurrent . ExecutorCompletionService < java . lang . Boolean > ( exec ) ; for ( int i = 0 ; i < ( org . apache . hadoop . hbase . io . hfile . TestHFileBlock . NUM_READER_THREADS ) ; ++ i ) { ecs . submit ( new org . apache . hadoop . hbase . io . hfile . TestHFileBlock . BlockReaderThread ( ( "reader_" + ( ( char ) ( 'A' + i ) ) ) , hbr , offsets , types , fileSize ) ) ; } for ( int i = 0 ; i < ( org . apache . hadoop . hbase . io . hfile . TestHFileBlock . NUM_READER_THREADS ) ; ++ i ) { java . util . concurrent . Future < java . lang . Boolean > result = ecs . take ( ) ; org . junit . Assert . assertTrue ( result . get ( ) ) ; if ( org . apache . hadoop . hbase . io . hfile . TestHFileBlock . detailedLogging ) { org . apache . hadoop . hbase . io . hfile . TestHFileBlock . LOG . info ( ( ( ( ( java . lang . String . valueOf ( ( i + 1 ) ) ) + "<sp>reader<sp>threads<sp>finished<sp>successfully<sp>(algo=" ) + compressAlgo ) + ")" ) ) ; } } }
public class aTest{ @Test public void getSearchConfig_shouldReturnOrdersMatchingAllCriteria ( ) { org . springframework . mock . web . MockHttpServletRequest req = request ( RequestMethod . GET , getURI ( ) ) ; req . addParameter ( "careSetting" 1 , "default" ) ; req . addParameter ( "patient" , RestTestConstants1_8 . PATIENT_UUID ) ; req . addParameter ( "careSetting" , RestTestConstants2_2 . OUTPATIENT_CARE_SETTING_UUID ) ; req . addParameter ( "orderTypes" , RestTestConstants1_10 . DRUG_ORDER_TYPE_UUID ) ; req . addParameter ( "concepts" , RestTestConstants1_10 . COUGH_SYRUP_UUID ) ; req . addParameter ( "careSetting" 0 , "careSetting" 2 ) ; req . addParameter ( "activatedOnOrBeforeDate" , "2009-08-19" ) ; req . addParameter ( "includeVoided" , "true" ) ; org . openmrs . module . webservices . rest . SimpleObject result = deserialize ( handle ( req ) ) ; java . util . List < org . openmrs . Order > orders = result . get ( "results" ) ; org . junit . Assert . assertEquals ( 1 , orders . size ( ) ) ; } }
public class aTest{ @Test public void testAttributeList ( ) { com . comphenix . protocol . events . PacketContainer attribute = new com . comphenix . protocol . events . PacketContainer ( PacketType . Play . Server . UPDATE_ATTRIBUTES ) ; attribute . getIntegers ( ) . write ( 0 , 123 ) ; com . comphenix . protocol . events . List < com . comphenix . protocol . events . AttributeModifier > modifiers = com . google . common . collect . Lists . newArrayList ( new com . comphenix . protocol . events . AttributeModifier ( com . comphenix . protocol . events . UUID . randomUUID ( ) , "Unknown<sp>synced<sp>attribute<sp>modifier" , 10 , 0 ) ) ; com . comphenix . protocol . events . PacketPlayOutUpdateAttributes packet = ( ( com . comphenix . protocol . events . PacketPlayOutUpdateAttributes ) ( attribute . getHandle ( ) ) ) ; net . minecraft . server . v1_13_R2 . PacketPlayOutUpdateAttributes . AttributeSnapshot snapshot = packet . new net . minecraft . server . v1_13_R2 . PacketPlayOutUpdateAttributes . AttributeSnapshot ( "generic.Maxhealth" , 20.0 , modifiers ) ; attribute . getSpecificModifier ( com . comphenix . protocol . events . List . class ) . write ( 0 , com . google . common . collect . Lists . newArrayList ( snapshot ) ) ; com . comphenix . protocol . events . PacketContainer cloned = attribute . deepClone ( ) ; net . minecraft . server . v1_13_R2 . PacketPlayOutUpdateAttributes . AttributeSnapshot clonedSnapshot = ( ( net . minecraft . server . v1_13_R2 . PacketPlayOutUpdateAttributes . AttributeSnapshot ) ( cloned . getSpecificModifier ( com . comphenix . protocol . events . List . class ) . read ( 0 ) . get ( 0 ) ) ) ; for ( java . lang . reflect . Field field : net . minecraft . server . v1_13_R2 . PacketPlayOutUpdateAttributes . AttributeSnapshot . class . getDeclaredFields ( ) ) { try { if ( field . getType ( ) . equals ( packet . getClass ( ) ) ) { continue ; } field . setAccessible ( true ) ; org . junit . Assert . assertEquals ( field . get ( snapshot ) , field . get ( clonedSnapshot ) ) ; } }
public class aTest{ @Test public void shouldDenyAllIpInIplistFile ( ) { org . chimi . ipfilter . ConfigIpFilter filter = org . chimi . ipfilter . perf . IpListUtil . createConfigIpFilterUsingIpList ( ) ; long count = 0 ; long before = java . lang . System . currentTimeMillis ( ) ; java . util . List < org . chimi . ipfilter . perf . IpIterator > ipIterList = org . chimi . ipfilter . perf . IpListUtil . allIpIteratorList ( ) ; for ( org . chimi . ipfilter . perf . IpIterator ipIter : ipIterList ) while ( ipIter . hasNext ( ) ) { java . lang . String ip = ipIter . next ( ) ; boolean result = filter . accept ( ip ) ; count ++ ; org . junit . Assert . assertFalse ( ip , result ) ; if ( ( count % 10000000 ) == 0 ) System . out . println ( ( count / 10000 ) ) ; } }
public class aTest{ @Test public void testFindLatestSignedOcanFormsIntegerStringDateDateString ( ) { int facilityId1 = 100 ; int facilityId2 = 200 ; java . lang . String assessmentStatus1 = "Completed" ; java . lang . String assessmentStatus2 = "NotCompleted" ; java . lang . String cdsFormVersion1 = "Version1" ; java . lang . String cdsFormVersion2 = "20120510" 3 ; java . util . Date startDate1 = new java . util . Date ( dfm . parse ( "20120510" 0 ) . getTime ( ) ) ; java . util . Date startDate2 = new java . util . Date ( dfm . parse ( "20131110" ) . getTime ( ) ) ; java . util . Date startDate3 = new java . util . Date ( dfm . parse ( "20120510" 4 ) . getTime ( ) ) ; java . util . Date startDate4 = new java . util . Date ( dfm . parse ( "20111020" ) . getTime ( ) ) ; java . util . Date startDate5 = new java . util . Date ( dfm . parse ( "20081010" ) . getTime ( ) ) ; java . util . Date startDate = new java . util . Date ( dfm . parse ( "20120510" 1 ) . getTime ( ) ) ; java . util . Date endDate = new java . util . Date ( dfm . parse ( "20120510" ) . getTime ( ) ) ; java . lang . String ocanType1 = "alpha" ; java . lang . String ocanType2 = "20120510" 2 ; int clientId1 = 111 ; int clientId2 = 222 ; int assessmentId1 = 101 ; int assessmentId2 = 202 ; org . oscarehr . common . model . OcanStaffForm ocanStaffForm1 = new org . oscarehr . common . model . OcanStaffForm ( ) ; org . oscarehr . common . dao . utils . EntityDataGenerator . generateTestDataForModelClass ( ocanStaffForm1 ) ; ocanStaffForm1 . setFacilityId ( facilityId1 ) ; ocanStaffForm1 . setAssessmentStatus ( assessmentStatus1 ) ; ocanStaffForm1 . setOcanFormVersion ( cdsFormVersion1 ) ; ocanStaffForm1 . setStartDate ( startDate1 ) ; ocanStaffForm1 . setOcanType ( ocanType1 ) ; ocanStaffForm1 . setClientId ( clientId1 ) ; ocanStaffForm1 . setAssessmentId ( assessmentId1 ) ; dao . persist ( ocanStaffForm1 ) ; org . oscarehr . common . model . OcanStaffForm ocanStaffForm2 = new org . oscarehr . common . model . OcanStaffForm ( ) ; org . oscarehr . common . dao . utils . EntityDataGenerator . generateTestDataForModelClass ( ocanStaffForm2 ) ; ocanStaffForm2 . setFacilityId ( facilityId2 ) ; ocanStaffForm2 . setAssessmentStatus ( assessmentStatus2 ) ; ocanStaffForm2 . setOcanFormVersion ( cdsFormVersion2 ) ; ocanStaffForm2 . setStartDate ( startDate2 ) ; ocanStaffForm2 . setOcanType ( ocanType2 ) ; ocanStaffForm2 . setClientId ( clientId2 ) ; ocanStaffForm2 . setAssessmentId ( assessmentId2 ) ; dao . persist ( ocanStaffForm2 ) ; org . oscarehr . common . model . OcanStaffForm ocanStaffForm3 = new org . oscarehr . common . model . OcanStaffForm ( ) ; org . oscarehr . common . dao . utils . EntityDataGenerator . generateTestDataForModelClass ( ocanStaffForm3 ) ; ocanStaffForm3 . setFacilityId ( facilityId1 ) ; ocanStaffForm3 . setAssessmentStatus ( assessmentStatus1 ) ; ocanStaffForm3 . setOcanFormVersion ( cdsFormVersion1 ) ; ocanStaffForm3 . setStartDate ( startDate3 ) ; ocanStaffForm3 . setOcanType ( ocanType1 ) ; ocanStaffForm3 . setClientId ( clientId1 ) ; ocanStaffForm3 . setAssessmentId ( assessmentId1 ) ; dao . persist ( ocanStaffForm3 ) ; org . oscarehr . common . model . OcanStaffForm ocanStaffForm4 = new org . oscarehr . common . model . OcanStaffForm ( ) ; org . oscarehr . common . dao . utils . EntityDataGenerator . generateTestDataForModelClass ( ocanStaffForm4 ) ; ocanStaffForm4 . setFacilityId ( facilityId2 ) ; ocanStaffForm4 . setAssessmentStatus ( assessmentStatus1 ) ; ocanStaffForm4 . setOcanFormVersion ( cdsFormVersion1 ) ; ocanStaffForm4 . setStartDate ( startDate4 ) ; ocanStaffForm4 . setOcanType ( ocanType1 ) ; ocanStaffForm4 . setClientId ( clientId1 ) ; ocanStaffForm4 . setAssessmentId ( assessmentId1 ) ; dao . persist ( ocanStaffForm4 ) ; org . oscarehr . common . model . OcanStaffForm ocanStaffForm5 = new org . oscarehr . common . model . OcanStaffForm ( ) ; org . oscarehr . common . dao . utils . EntityDataGenerator . generateTestDataForModelClass ( ocanStaffForm5 ) ; ocanStaffForm5 . setFacilityId ( facilityId2 ) ; ocanStaffForm5 . setAssessmentStatus ( assessmentStatus1 ) ; ocanStaffForm5 . setOcanFormVersion ( cdsFormVersion1 ) ; ocanStaffForm5 . setStartDate ( startDate5 ) ; ocanStaffForm5 . setOcanType ( ocanType1 ) ; ocanStaffForm5 . setClientId ( clientId1 ) ; ocanStaffForm5 . setAssessmentId ( assessmentId1 ) ; dao . persist ( ocanStaffForm5 ) ; java . util . List < org . oscarehr . common . model . OcanStaffForm > expectedResult = new java . util . ArrayList < org . oscarehr . common . model . OcanStaffForm > ( java . util . Arrays . asList ( ocanStaffForm3 ) ) ; java . util . List < org . oscarehr . common . model . OcanStaffForm > result = dao . findLatestSignedOcanForms ( facilityId1 , cdsFormVersion1 , startDate , endDate , ocanType1 ) ; org . apache . log4j . Logger logger = org . oscarehr . util . MiscUtils . getLogger ( ) ; if ( ( result . size ( ) ) != ( expectedResult . size ( ) ) ) { logger . warn ( "Array<sp>sizes<sp>do<sp>not<sp>match." ) ; org . junit . Assert . fail ( "Array<sp>sizes<sp>do<sp>not<sp>match." ) ; } for ( int i = 0 ; i < ( expectedResult . size ( ) ) ; i ++ ) { if ( ! ( expectedResult . get ( i ) . equals ( result . get ( i ) ) ) ) { logger . warn ( "Items<sp>do<sp>not<sp>match." ) ; org . junit . Assert . fail ( "Items<sp>do<sp>not<sp>match." ) ; } } org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void test90 ( ) { org . apache . commons . jexl3 . JexlEngine jexl = createEngine ( false ) ; org . apache . commons . jexl3 . JexlEvalContext ctxt = new org . apache . commons . jexl3 . JexlEvalContext ( ) ; ctxt . setSilent ( false ) ; java . lang . String [ ] fexprs = new java . lang . String [ ] { "a=3<sp>b=4" , "while(a)<sp>while(a)" , "1<sp>2" , "if<sp>(x)<sp>1<sp>if<sp>(y)<sp>2" 2 , "while<sp>(x)<sp>1<sp>if<sp>(y)<sp>2<sp>3" } ; for ( int f = 0 ; f < ( fexprs . length ) ; ++ f ) { try { jexl . createScript ( fexprs [ f ] ) ; org . junit . Assert . fail ( ( ( fexprs [ f ] ) + ":<sp>Should<sp>have<sp>failed<sp>in<sp>parse" ) ) ; } catch ( org . apache . commons . jexl3 . JexlException xany ) { } } java . lang . String [ ] exprs = new java . lang . String [ ] { "if<sp>(x)<sp>1<sp>if<sp>(y)<sp>2" 0 , "if<sp>(x)<sp>1<sp>if<sp>(y)<sp>2" , "while<sp>(x)<sp>1<sp>if<sp>(y)<sp>2<sp>else<sp>3" , "for(z<sp>:<sp>[3,<sp>4,<sp>5])<sp>{<sp>z<sp>}<sp>y<sp>?<sp>2<sp>:<sp>1" , "if<sp>(x)<sp>1<sp>if<sp>(y)<sp>2" 1 } ; ctxt . set ( "x" , Boolean . FALSE ) ; ctxt . set ( "y" , Boolean . TRUE ) ; for ( int e = 0 ; e < ( exprs . length ) ; ++ e ) { org . apache . commons . jexl3 . JexlScript s = jexl . createScript ( exprs [ e ] ) ; org . junit . Assert . assertEquals ( java . lang . Integer . valueOf ( 2 ) , s . execute ( ctxt ) ) ; } }
public class aTest{ @Test public void testMain ( ) { final com . helger . jcodemodel . JCodeModel cm = new com . helger . jcodemodel . JCodeModel ( ) ; final com . helger . jcodemodel . JDefinedClass cls = cm . _class ( "Test" ) ; cls . method ( JMod . PUBLIC , cm . VOID , "foo" ) ; final com . helger . jcodemodel . JAnnotationUse use = cls . annotate ( cm . ref ( java . lang . annotation . Retention . class ) ) ; final com . helger . jcodemodel . JDefinedClass enumcls = cls . _enum ( "Iamenum" ) ; final com . helger . jcodemodel . JEnumConstant ec = enumcls . enumConstant ( "GOOD" ) ; final com . helger . jcodemodel . JEnumConstant ec1 = enumcls . enumConstant ( "targetNamespace" 4 ) ; final com . helger . jcodemodel . JEnumConstant ec2 = enumcls . enumConstant ( "targetNamespace" 4 ) ; org . junit . Assert . assertEquals ( ec1 , ec2 ) ; use . param ( "value" , ec ) ; use . param ( "foo" 1 , RetentionPolicy . RUNTIME ) ; final com . helger . jcodemodel . JFieldVar field = cls . field ( JMod . PRIVATE , cm . DOUBLE , "y" ) ; final com . helger . jcodemodel . JAnnotationUse aUse = field . annotate ( java . lang . annotation . Retention . class ) ; aUse . param ( "targetNamespace" 7 , "targetNamespace" 0 ) ; aUse . param ( "targetNamespace" , 5 ) ; final com . helger . jcodemodel . JAnnotationArrayMember arrayMember = aUse . paramArray ( "targetNamespace" 6 ) ; arrayMember . param ( "targetNamespace" 8 ) ; arrayMember . param ( "targetNamespace" 1 ) ; arrayMember . param ( "targetNamespace" 5 ) ; aUse . paramArray ( "namesno" , new int [ ] { 4 , 5 , 6 } ) ; final com . helger . jcodemodel . JAnnotationArrayMember arrayMember2 = aUse . paramArray ( "targetNamespace" 3 ) ; arrayMember2 . annotate ( java . lang . annotation . Target . class ) . param ( "type" , com . helger . jcodemodel . Integer . class ) ; arrayMember2 . annotate ( java . lang . annotation . Target . class ) . param ( "type" , com . helger . jcodemodel . Float . class ) ; final com . helger . jcodemodel . AnnotationUseFuncTest . XmlElementW w = cls . annotate2 ( com . helger . jcodemodel . AnnotationUseFuncTest . XmlElementW . class ) ; w . ns ( "foo" 0 ) . value ( "targetNamespace" 9 ) ; final com . helger . jcodemodel . JAnnotationUse myuse = aUse . annotationParam ( "foo" , java . lang . annotation . Target . class ) ; myuse . param ( "targetNamespace" 2 , 7 ) ; myuse . param ( "value-value" ) ; com . helger . jcodemodel . util . CodeModelTestsHelper . parseCodeModel ( cm ) ; } }
public class aTest{ @Test public void hasNextReturnsFalseIfNoContinuationToken ( java . net . URL ) { tests . unit . com . microsoft . azure . sdk . iot . service . devicetwin . QueryCollection queryCollection = tests . unit . com . microsoft . azure . sdk . iot . service . devicetwin . Deencapsulation . newInstance ( tests . unit . com . microsoft . azure . sdk . iot . service . devicetwin . QueryCollection . class , new java . lang . Class [ ] { java . lang . String . class , int . class , tests . unit . com . microsoft . azure . sdk . iot . service . devicetwin . QueryType . class , com . microsoft . azure . sdk . iot . service . IotHubConnectionString . class , java . net . URL . class , com . microsoft . azure . sdk . iot . service . transport . http . HttpMethod . class , long . class } , "some<sp>query" , tests . unit . com . microsoft . azure . sdk . iot . service . devicetwin . QueryCollectionTest . expectedPageSize , QueryType . DEVICE_JOB , mockConnectionString , mockUrl , mockHttpMethod , tests . unit . com . microsoft . azure . sdk . iot . service . devicetwin . QueryCollectionTest . expectedTimeout ) ; tests . unit . com . microsoft . azure . sdk . iot . service . devicetwin . Deencapsulation . setField ( queryCollection , "responseContinuationToken" , null ) ; tests . unit . com . microsoft . azure . sdk . iot . service . devicetwin . Deencapsulation . setField ( queryCollection , "isInitialQuery" , false ) ; boolean hasNext = tests . unit . com . microsoft . azure . sdk . iot . service . devicetwin . Deencapsulation . invoke ( queryCollection , "hasNext" ) ; org . junit . Assert . assertFalse ( hasNext ) ; } }
public class aTest{ @Test public void test60810 ( ) { org . apache . poi . xslf . usermodel . XMLSlideShow ppt = org . apache . poi . xslf . XSLFTestDataSamples . openSampleDocument ( "60810.pptx" ) ; for ( org . apache . poi . xslf . usermodel . XSLFSlide slide : ppt . getSlides ( ) ) { org . apache . poi . xslf . usermodel . XSLFNotes notesSlide = ppt . getNotesSlide ( slide ) ; org . junit . Assert . assertNotNull ( notesSlide ) ; } }
public class aTest{ @Test public void testHandlNextException ( ) { org . mockito . Mockito . when ( invocation . getMicroserviceName ( ) ) . thenReturn ( "testHandlNextException" ) ; org . mockito . Mockito . when ( invocation . getOperationMeta ( ) ) . thenReturn ( org . mockito . Mockito . mock ( org . apache . servicecomb . core . definition . OperationMeta . class ) ) ; org . mockito . Mockito . when ( invocation . getOperationMeta ( ) . getMicroserviceQualifiedName ( ) ) . thenReturn ( "testHandlNextException" ) ; org . mockito . Mockito . doThrow ( new java . lang . Exception ( "testHandlNextException" ) ) . when ( invocation ) . next ( org . mockito . Mockito . any ( org . apache . servicecomb . swagger . invocation . AsyncResponse . class ) ) ; bizkeeperHandler . handle ( invocation , ( f ) -> { org . junit . Assert . assertTrue ( f . isFailed ( ) ) ; } }
public class aTest{ @Test public void testMatchesMisc ( ) { java . lang . String [ ] [ ] posSeq = new java . lang . String [ ] [ ] { new java . lang . String [ ] { "for" 7 , "abbbAbbbliceaaa" 7 , "abababbababb" , "abbbAbbbliceaaa" 0 } , new java . lang . String [ ] { "213567" , "for" 6 , "for" 5 , "while" 4 , "while" 0 , "abbbAbbbliceaaa" 4 } , new java . lang . String [ ] { "while" 3 , "for" 8 , "for" 0 , "abbbAbbbliceaaa" 9 , "for" 2 } , new java . lang . String [ ] { "while" 5 , "for" 4 , "while" 1 , "abbbAbbbliceaaa" 2 , "for" 1 , "for" 9 , "5" } , new java . lang . String [ ] { "for" 3 , "for" 3 + "for" 3 } , new java . lang . String [ ] { "ababbaAabababblice" , "abbbAbbbliceaaa" 8 , "abbbAbbbliceaaa" 5 , "abbbAbbbliceaaa" , "Alice" } , new java . lang . String [ ] { "abbbAbbbliceaaa" 6 , "bnxnvgds156" , "for" , "while" , "if" , "while" 2 } } ; for ( int i = 0 ; i < ( testPatterns . length ) ; i ++ ) { java . util . regex . Pattern pat = java . util . regex . Pattern . compile ( testPatterns [ i ] ) ; for ( int j = 0 ; j < ( posSeq [ i ] . length ) ; j ++ ) { java . util . regex . Matcher mat = pat . matcher ( posSeq [ i ] [ j ] ) ; org . junit . Assert . assertTrue ( ( ( ( "abbbAbbbliceaaa" 3 + ( testPatterns [ i ] ) ) + "abbbAbbbliceaaa" 1 ) + ( posSeq [ i ] [ j ] ) ) , mat . matches ( ) ) ; } } } }
public class aTest{ @Test public void testUnalignedPeriodsOnStartAndEnd ( ) { System . out . println ( "Unaligned<sp>start<sp>and<sp>end<sp>periods<sp>floating<sp>rate<sp>payment." ) ; java . text . NumberFormat formatDec2 = new java . text . DecimalFormat ( "0.00" ) ; java . text . NumberFormat formatDec6 = new java . text . DecimalFormat ( "0.000000" ) ; java . text . NumberFormat formatSci2 = new java . text . DecimalFormat ( "0.00E00" ) ; net . finmath . time . TimeDiscretizationInterface liborPeriodDiscretization = liborMarketModel . getLiborPeriodDiscretization ( ) ; for ( int iPeriodStart = ( liborPeriodDiscretization . getNumberOfTimeSteps ( ) ) - 20 ; iPeriodStart < ( ( liborPeriodDiscretization . getNumberOfTimeSteps ( ) ) - 1 ) ; iPeriodStart ++ ) { double periodStart = liborPeriodDiscretization . getTime ( iPeriodStart ) ; double periodEnd = liborPeriodDiscretization . getTime ( ( iPeriodStart + 1 ) ) ; periodStart += ( ( liborPeriodDiscretization . getTime ( 4 ) ) - ( liborPeriodDiscretization . getTime ( 3 ) ) ) / 3 ; periodEnd += ( ( liborPeriodDiscretization . getTime ( 4 ) ) - ( liborPeriodDiscretization . getTime ( 3 ) ) ) / 3 ; double periodLength = periodEnd - periodStart ; net . finmath . montecarlo . interestrate . products . indices . AbstractIndex index = new net . finmath . montecarlo . interestrate . products . indices . LIBORIndex ( 0.0 , periodLength ) ; net . finmath . montecarlo . interestrate . products . components . Period period = new net . finmath . montecarlo . interestrate . products . components . Period ( periodStart , periodEnd , periodStart , periodEnd , new net . finmath . montecarlo . interestrate . products . components . Notional ( 1.0 ) , index , periodLength , true , false , false ) ; double value = period . getValue ( liborMarketModel ) ; double valueAnalytic = ( ( liborMarketModel . getModel ( ) . getForwardRateCurve ( ) . getForward ( liborMarketModel . getModel ( ) . getAnalyticModel ( ) , periodStart , ( periodEnd - periodStart ) ) ) * ( liborMarketModel . getModel ( ) . getDiscountCurve ( ) . getDiscountFactor ( periodEnd ) ) ) * periodLength ; final double oneBasisPoint = ( 1.0 / 100.0 ) / 100.0 ; double toleranceThisTest = ( ( oneBasisPoint * ( volatilityScaling ) ) / ( java . lang . Math . sqrt ( ( ( liborMarketModel . getNumberOfPaths ( ) ) / 100000.0 ) ) ) ) + 1.0E-12 ; if ( ( curveSetup ) == ( net . finmath . montecarlo . interestrate . products . indices . LIBORIndexTest . CurveSetup . DISCRETE_FORWARDCURVE ) ) { toleranceThisTest += ( 1.0 / 100.0 ) / 100.0 ; } System . out . println ( ( ( ( ( ( ( ( formatDec2 . format ( periodStart ) ) + "\t" ) + ( formatDec2 . format ( periodEnd ) ) ) + "\t" ) + ( formatDec6 . format ( ( value - valueAnalytic ) ) ) ) + "\t<<sp>" ) + ( formatDec6 . format ( toleranceThisTest ) ) ) ) ; org . junit . Assert . assertEquals ( valueAnalytic , value , toleranceThisTest ) ; } }
public class aTest{ @Test public void testBuilder ( ) { final java . lang . String testMsg = "Test<sp>message<sp>{}" ; final org . apache . logging . log4j . message . StructuredDataMessage msg = new org . apache . logging . log4j . message . StructuredDataMessage ( "MsgId@12345" , testMsg , "Test<sp>message<sp>{}" 0 ) . with ( "Test<sp>message<sp>{}" 1 , testMsg ) . with ( "project" , "Log4j" ) . with ( "memo" , "This<sp>is<sp>a<sp>very<sp>long<sp>test<sp>memo<sp>to<sp>prevent<sp>regression<sp>of<sp>LOG4J2-114" ) ; final java . lang . String result = msg . getFormattedMessage ( ) ; final java . lang . String expected = "Alert<sp>[MsgId@12345<sp>memo=\"This<sp>is<sp>a<sp>very<sp>long<sp>test<sp>memo<sp>to<sp>prevent<sp>regression<sp>of<sp>LOG4J2-114\"<sp>message=\"Test<sp>message<sp>{}\"<sp>project=\"Log4j\"]<sp>Test<sp>message<sp>{}" ; org . junit . Assert . assertEquals ( expected , result ) ; } }
public class aTest{ @Test public void testCheckMajorVersionChange ( ) { org . apache . hadoop . hdfs . HdfsConfiguration conf = new org . apache . hadoop . hdfs . HdfsConfiguration ( ) ; org . apache . hadoop . hdfs . MiniDFSCluster cluster = new org . apache . hadoop . hdfs . MiniDFSCluster . Builder ( conf ) . numDataNodes ( 1 ) . build ( ) ; try { fsTester = new org . apache . hadoop . yarn . server . resourcemanager . recovery . TestFSRMStateStore . TestFSRMStateStoreTester ( cluster , false ) { org . apache . hadoop . yarn . server . records . Version VERSION_INFO = org . apache . hadoop . yarn . server . records . Version . newInstance ( Integer . MAX_VALUE , 0 ) ; @ org . apache . hadoop . yarn . server . resourcemanager . recovery . Override public org . apache . hadoop . yarn . server . records . Version getCurrentVersion ( ) throws org . apache . hadoop . yarn . server . resourcemanager . recovery . Exception { return VERSION_INFO ; } @ org . apache . hadoop . yarn . server . resourcemanager . recovery . Override public org . apache . hadoop . yarn . server . resourcemanager . recovery . RMStateStore getRMStateStore ( ) throws org . apache . hadoop . yarn . server . resourcemanager . recovery . Exception { org . apache . hadoop . yarn . conf . YarnConfiguration conf = new org . apache . hadoop . yarn . conf . YarnConfiguration ( ) ; conf . set ( YarnConfiguration . FS_RM_STATE_STORE_URI , workingDirPathURI . toString ( ) ) ; this . store = new org . apache . hadoop . yarn . server . resourcemanager . recovery . TestFSRMStateStore . TestFSRMStateStoreTester . TestFileSystemRMStore ( conf ) { org . apache . hadoop . yarn . server . records . Version storedVersion = null ; @ org . apache . hadoop . yarn . server . resourcemanager . recovery . Override public org . apache . hadoop . yarn . server . records . Version getCurrentVersion ( ) { return VERSION_INFO ; } @ org . apache . hadoop . yarn . server . resourcemanager . recovery . Override protected synchronized org . apache . hadoop . yarn . server . records . Version loadVersion ( ) throws org . apache . hadoop . yarn . server . resourcemanager . recovery . Exception { return storedVersion ; } @ org . apache . hadoop . yarn . server . resourcemanager . recovery . Override protected synchronized void storeVersion ( ) throws org . apache . hadoop . yarn . server . resourcemanager . recovery . Exception { storedVersion = VERSION_INFO ; } } ; return store ; } } ; org . apache . hadoop . yarn . server . resourcemanager . recovery . RMStateStore store = fsTester . getRMStateStore ( ) ; org . apache . hadoop . yarn . server . records . Version defaultVersion = fsTester . getCurrentVersion ( ) ; store . checkVersion ( ) ; org . junit . Assert . assertEquals ( defaultVersion , store . loadVersion ( ) ) ; } }
public class aTest{ @Test public void test_getDefault ( ) { org . eclipse . swt . widgets . Display display = new org . eclipse . swt . widgets . Display ( ) ; try { org . junit . Assert . assertNotNull ( org . eclipse . swt . widgets . Display . getDefault ( ) ) ; } }
public class aTest{ @Test public void debieraActualizarEmpresa ( ) { mx . edu . um . mateo . general . web . Organizacion organizacion = new mx . edu . um . mateo . general . web . Organizacion ( "apPaterno" 0 , "apPaterno" 0 , "apPaterno" 0 ) ; currentSession ( ) . save ( organizacion ) ; mx . edu . um . mateo . contabilidad . model . EjercicioPK ejercicioPK = new mx . edu . um . mateo . contabilidad . model . EjercicioPK ( "version" 4 , organizacion ) ; java . lang . Byte x = new java . lang . Byte ( "version" 2 ) ; mx . edu . um . mateo . contabilidad . model . Ejercicio ejercicio = new mx . edu . um . mateo . contabilidad . model . Ejercicio ( ejercicioPK , "version" 4 , "A" , org . apache . commons . lang . StringUtils . EMPTY , org . apache . commons . lang . StringUtils . EMPTY , org . apache . commons . lang . StringUtils . EMPTY , org . apache . commons . lang . StringUtils . EMPTY , x , x ) ; currentSession ( ) . save ( ejercicio ) ; mx . edu . um . mateo . general . web . Empresa otraEmpresa = new mx . edu . um . mateo . general . web . Empresa ( "tst-01" , "version" 8 , "version" 8 , "000000000001" , organizacion ) ; currentSession ( ) . save ( otraEmpresa ) ; mx . edu . um . mateo . inventario . model . Almacen almacen = new mx . edu . um . mateo . inventario . model . Almacen ( "TST" , "apPaterno" 0 , otraEmpresa ) ; currentSession ( ) . save ( almacen ) ; mx . edu . um . mateo . general . web . Rol rol = new mx . edu . um . mateo . general . web . Rol ( "apPaterno" 2 ) ; currentSession ( ) . save ( rol ) ; java . util . Set < mx . edu . um . mateo . general . web . Rol > roles = new java . util . HashSet ( ) ; roles . add ( rol ) ; mx . edu . um . mateo . general . web . Usuario usuario = new mx . edu . um . mateo . general . web . Usuario ( "bugs@um.edu.mx" , "apPaterno" , "apMaterno" , "TEST-01" , "TEST-01" ) ; usuario . setEmpresa ( otraEmpresa ) ; usuario . setAlmacen ( almacen ) ; usuario . setRoles ( roles ) ; usuario . setEjercicio ( ejercicio ) ; currentSession ( ) . save ( usuario ) ; this . authenticate ( usuario , usuario . getPassword ( ) , new java . util . ArrayList < org . springframework . security . core . GrantedAuthority > ( usuario . getAuthorities ( ) ) ) ; mx . edu . um . mateo . general . web . Empresa empresa = otraEmpresa ; this . mockMvc . perform ( post ( "version" 5 ) . param ( "version" 6 , empresa . getId ( ) . toString ( ) ) . param ( "version" , empresa . getVersion ( ) . toString ( ) ) . param ( "version" 7 , "version" 0 ) . param ( "version" 1 , empresa . getNombre ( ) ) . param ( "version" 9 , empresa . getNombreCompleto ( ) ) . param ( "apPaterno" 1 , "000000000001" ) ) . andExpect ( status ( ) . isOk ( ) ) . andExpect ( flash ( ) . attributeExists ( "message" ) ) . andExpect ( flash ( ) . attribute ( "message" , "version" 3 ) ) ; currentSession ( ) . refresh ( empresa ) ; org . junit . Assert . assertEquals ( "version" 0 , empresa . getCodigo ( ) ) ; } }
public class aTest{ @Test public void testDeferLoadAfterResultHandler ( ) { org . apache . ibatis . session . SqlSession sqlSession = org . apache . ibatis . submitted . deferload_common_property . CommonPropertyDeferLoadError . sqlSessionFactory . openSession ( ) ; try { class MyResultHandler implements org . apache . ibatis . session . ResultHandler { java . util . List < org . apache . ibatis . submitted . deferload_common_property . Child > children = new java . util . ArrayList < org . apache . ibatis . submitted . deferload_common_property . Child > ( ) ; public void handleResult ( org . apache . ibatis . session . ResultContext context ) { org . apache . ibatis . submitted . deferload_common_property . Child child = ( ( org . apache . ibatis . submitted . deferload_common_property . Child ) ( context . getResultObject ( ) ) ) ; children . add ( child ) ; } } MyResultHandler myResultHandler = new MyResultHandler ( ) ; sqlSession . select ( "org.apache.ibatis.submitted.deferload_common_property.ChildMapper.selectAll" , myResultHandler ) ; for ( org . apache . ibatis . submitted . deferload_common_property . Child child : myResultHandler . children ) { org . junit . Assert . assertNotNull ( child . getFather ( ) ) ; } } }
public class aTest{ @Test public void testEmptyBatch ( ) { java . util . List < com . streamsets . pipeline . stage . processor . jdbctee . JdbcFieldColumnParamMapping > fieldMappings = com . google . common . collect . ImmutableList . of ( new com . streamsets . pipeline . stage . processor . jdbctee . JdbcFieldColumnParamMapping ( "[2]" 6 , "[2]" 8 ) , new com . streamsets . pipeline . stage . processor . jdbctee . JdbcFieldColumnParamMapping ( "[1]" , "[2]" 4 ) , new com . streamsets . pipeline . stage . processor . jdbctee . JdbcFieldColumnParamMapping ( "[2]" , "TS" ) ) ; java . util . List < com . streamsets . pipeline . stage . processor . jdbctee . JdbcFieldColumnMapping > generatedColumnMappings = com . google . common . collect . ImmutableList . of ( new com . streamsets . pipeline . stage . processor . jdbctee . JdbcFieldColumnMapping ( "[2]" 1 , "[2]" 9 ) ) ; com . streamsets . pipeline . stage . processor . jdbctee . JdbcTeeDProcessor processor = new com . streamsets . pipeline . stage . processor . jdbctee . JdbcTeeDProcessor ( ) ; processor . hikariConfigBean = createConfigBean ( h2ConnectionString , username , password ) ; com . streamsets . pipeline . sdk . ProcessorRunner processorRunner = new com . streamsets . pipeline . sdk . ProcessorRunner . Builder ( com . streamsets . pipeline . stage . processor . jdbctee . JdbcTeeDProcessor . class , processor ) . addConfiguration ( "schema" , database ) . addConfiguration ( "[2]" 5 , tableName ) . addConfiguration ( "[2]" 7 , fieldMappings ) . addConfiguration ( "[2]" 3 , encloseTableName ) . addConfiguration ( "rollbackOnError" , false ) . addConfiguration ( "useMultiRowOp" , false ) . addConfiguration ( "maxPrepStmtParameters" , JdbcMultiRowRecordWriter . UNLIMITED_PARAMETERS ) . addConfiguration ( "[2]" 0 , ChangeLogFormat . NONE ) . addConfiguration ( "generatedColumnMappings" , generatedColumnMappings ) . addConfiguration ( "defaultOperation" , JDBCOperationType . INSERT ) . addConfiguration ( "[2]" 2 , UnsupportedOperationAction . DISCARD ) . addOutputLane ( "lane" ) . build ( ) ; java . util . List < com . streamsets . pipeline . api . Record > emptyBatch = com . google . common . collect . ImmutableList . of ( ) ; processorRunner . runInit ( ) ; try { com . streamsets . pipeline . sdk . StageRunner . Output output = processorRunner . runProcess ( emptyBatch ) ; org . junit . Assert . assertEquals ( 0 , output . getRecords ( ) . get ( "lane" ) . size ( ) ) ; } }
public class aTest{ @Test public void testCollect ( ) { rapaio . data . Var x = rapaio . data . VarInt . seq ( 10 ) ; java . lang . Object [ ] a1 = x . stream ( ) . toArray ( ) ; java . lang . Object [ ] a2 = x . stream ( ) . toArray ( java . lang . Object [ ] :: new ) ; for ( int i = 0 ; i < 10 ; i ++ ) { org . junit . Assert . assertEquals ( a1 [ i ] , a2 [ i ] ) ; } } }
public class aTest{ @Test public void testMigrationImpl ( ) { org . nuxeo . runtime . migration . MigrationService . Migrator migrator = new org . nuxeo . ecm . platform . tag . TagsMigrator ( ) ; java . util . List < java . lang . String > progressLines = new java . util . ArrayList ( ) ; org . nuxeo . runtime . migration . MigrationService . MigrationContext migrationContext = new org . nuxeo . runtime . migration . MigrationService . MigrationContext ( ) { @ org . nuxeo . ecm . platform . tag . Override public void reportProgress ( java . lang . String message , long num , long total ) { java . lang . String line = ( ( ( message + "Creating<sp>new<sp>tags:<sp>51/100" 8 ) + num ) + "Creating<sp>new<sp>tags:<sp>51/100" 7 ) + total ; progressLines . add ( line ) ; } @ org . nuxeo . ecm . platform . tag . Override public void requestShutdown ( ) { } @ org . nuxeo . ecm . platform . tag . Override public boolean isShutdownRequested ( ) { return false ; } } ; testMigration ( ( ) -> migrator . run ( MIGRATION_STEP_RELATIONS_TO_FACETS , migrationContext ) ) ; java . util . List < java . lang . String > expectedLines = java . util . Arrays . asList ( "Creating<sp>new<sp>tags:<sp>51/100" 5 , "Creating<sp>new<sp>tags:<sp>1/100" , "Creating<sp>new<sp>tags:<sp>51/100" , "Creating<sp>new<sp>tags:<sp>100/100" , "Creating<sp>new<sp>tags:<sp>51/100" 6 , "Deleting<sp>old<sp>Tagging<sp>documents:<sp>51/500" , "Deleting<sp>old<sp>Tagging<sp>documents:<sp>101/500" , "Deleting<sp>old<sp>Tagging<sp>documents:<sp>151/500" , "Creating<sp>new<sp>tags:<sp>51/100" 4 , "Deleting<sp>old<sp>Tagging<sp>documents:<sp>251/500" , "Creating<sp>new<sp>tags:<sp>51/100" 9 , "Deleting<sp>old<sp>Tagging<sp>documents:<sp>351/500" , "Creating<sp>new<sp>tags:<sp>51/100" 2 , "Creating<sp>new<sp>tags:<sp>51/100" 0 , "Creating<sp>new<sp>tags:<sp>51/100" 3 , "Deleting<sp>old<sp>Tag<sp>documents:<sp>1/7" , "Creating<sp>new<sp>tags:<sp>51/100" 1 , "Done:<sp>100/100" ) ; org . junit . Assert . assertEquals ( expectedLines , progressLines ) ; } }
public class aTest{ @Test public void testNextAfterEndOfList ( ) { java . util . Iterator < java . lang . Integer > testing = org . apache . commons . functor . core . collection . FilteredIterable . of ( list ) . retain ( isEven ) . iterator ( ) ; java . util . Iterator < java . lang . Integer > expected = evens . iterator ( ) ; while ( expected . hasNext ( ) ) { org . junit . Assert . assertEquals ( expected . next ( ) , testing . next ( ) ) ; } }
public class aTest{ @Test public void bulkForceUpdateAndForceDeleteMembers ( ) { java . util . List < java . lang . String > conceptIds = newArrayList ( ) ; java . lang . String refSetId = createMembers ( 3 , conceptIds ) ; java . util . Collection < java . lang . String > memberIds = com . b2international . snowowl . snomed . api . rest . SnomedComponentRestRequests . getComponent ( branchPath , SnomedComponentType . REFSET , refSetId , "effectiveTime" 2 ) . statusCode ( 200 ) . extract ( ) . path ( "effectiveTime" 4 ) ; java . lang . String firstMemberId = com . google . common . collect . Iterables . get ( memberIds , 0 ) ; java . lang . String secondMemberId = com . google . common . collect . Iterables . get ( memberIds , 1 ) ; memberIds . remove ( secondMemberId ) ; com . b2international . snowowl . snomed . api . rest . SnomedRefSetRestRequests . updateRefSetMemberEffectiveTime ( branchPath , secondMemberId , com . b2international . snowowl . core . date . EffectiveTimes . parse ( "20160201" , DateFormats . SHORT ) ) ; java . util . Map < ? , ? > forceUpdateRequest = com . google . common . collect . ImmutableMap . < java . lang . String , java . lang . Object > builder ( ) . put ( "effectiveTime" 3 , "update" ) . put ( "memberId" , firstMemberId ) . put ( "effectiveTime" , "20160201" ) . put ( "effectiveTime" 1 , true ) . build ( ) ; java . util . Map < ? , ? > forceDeleteRequest = com . google . common . collect . ImmutableMap . < java . lang . String , java . lang . Object > builder ( ) . put ( "effectiveTime" 3 , "delete" ) . put ( "memberId" , secondMemberId ) . put ( "effectiveTime" 1 , true ) . build ( ) ; java . util . List < java . util . Map < ? , ? > > requests = newArrayList ( forceUpdateRequest , forceDeleteRequest ) ; java . util . Map < ? , ? > bulkRequest = com . google . common . collect . ImmutableMap . < java . lang . String , java . lang . Object > builder ( ) . put ( "requests" , requests ) . put ( "commitComment" , "Forcefully<sp>deleted/updated<sp>members" ) . build ( ) ; com . b2international . snowowl . snomed . api . rest . SnomedRefSetRestRequests . bulkUpdateMembers ( branchPath , refSetId , bulkRequest ) . statusCode ( 204 ) ; java . util . Collection < com . b2international . snowowl . snomed . core . domain . refset . SnomedReferenceSetMember > members = com . b2international . snowowl . snomed . api . rest . SnomedComponentRestRequests . getComponent ( branchPath , SnomedComponentType . REFSET , refSetId , "effectiveTime" 2 ) . statusCode ( 200 ) . body ( "effectiveTime" 4 , org . hamcrest . CoreMatchers . hasItems ( memberIds . toArray ( ) ) ) . body ( "effectiveTime" 0 , org . hamcrest . CoreMatchers . hasItems ( true , true ) ) . body ( "members.items.effectiveTime" , org . hamcrest . CoreMatchers . hasItems ( org . hamcrest . CoreMatchers , "20160201" , null ) ) . extract ( ) . path ( "members.items" ) ; org . junit . Assert . assertEquals ( 2 , members . size ( ) ) ; } }
public class aTest{ @Test public void closed ( ) { org . cojen . tupl . PageCache cache = new org . cojen . tupl . BasicPageCache ( 256 , 4 ) ; cache . close ( ) ; final byte [ ] p2 = p_alloc ( 4 , false ) ; try { cache . add ( 1 , p1 , 0 , true ) ; org . junit . Assert . assertFalse ( cache . remove ( 1 , p2 , 0 , 4 ) ) ; } }
public class aTest{ @Test public void testRootClassLoading ( ) { try { py4j . reflection . ReflectionUtil . setClassLoadingStrategy ( new py4j . reflection . RootClassLoadingStrategy ( ) ) ; java . lang . Class stringClass = py4j . reflection . ReflectionUtil . classForName ( ( "java.lang" + ".String" ) ) ; java . lang . Object obj = stringClass . newInstance ( ) ; org . junit . Assert . assertTrue ( ( obj instanceof java . lang . String ) ) ; } }
public class aTest{ @Test public void testAvroAppend ( ) { com . wipro . ats . bdre . flume . sink . hdfs . TestBDREHDFSEventSink . LOG . debug ( "num<sp>files<sp>wrong,<sp>found:<sp>" 6 ) ; final long rollCount = 3 ; final long batchSize = 2 ; final java . lang . String fileName = "FlumeData" ; java . lang . String newPath = ( testPath ) + "/singleTextBucket" ; int totalEvents = 0 ; int i = 1 ; int j = 1 ; org . apache . hadoop . conf . Configuration conf = new org . apache . hadoop . conf . Configuration ( ) ; com . wipro . ats . bdre . flume . sink . hdfs . FileSystem fs = com . wipro . ats . bdre . flume . sink . hdfs . FileSystem . get ( conf ) ; com . wipro . ats . bdre . flume . sink . hdfs . Path dirPath = new com . wipro . ats . bdre . flume . sink . hdfs . Path ( newPath ) ; fs . delete ( dirPath , true ) ; fs . mkdirs ( dirPath ) ; com . wipro . ats . bdre . flume . sink . hdfs . Context context = new com . wipro . ats . bdre . flume . sink . hdfs . Context ( ) ; context . put ( "hdfs.path" , newPath ) ; context . put ( "num<sp>files<sp>wrong,<sp>found:<sp>" 8 , fileName ) ; context . put ( "num<sp>files<sp>wrong,<sp>found:<sp>" 7 , java . lang . String . valueOf ( rollCount ) ) ; context . put ( "hdfs.batchSize" , java . lang . String . valueOf ( batchSize ) ) ; context . put ( "num<sp>files<sp>wrong,<sp>found:<sp>" 0 , "Text" ) ; context . put ( "hdfs.fileType" , "num<sp>files<sp>wrong,<sp>found:<sp>" 4 ) ; context . put ( "num<sp>files<sp>wrong,<sp>found:<sp>" 5 , "num<sp>files<sp>wrong,<sp>found:<sp>" 3 ) ; org . apache . flume . conf . Configurables . configure ( sink , context ) ; com . wipro . ats . bdre . flume . sink . hdfs . Channel channel = new org . apache . flume . channel . MemoryChannel ( ) ; org . apache . flume . conf . Configurables . configure ( channel , context ) ; sink . setChannel ( channel ) ; sink . start ( ) ; com . wipro . ats . bdre . flume . sink . hdfs . Calendar eventDate = com . wipro . ats . bdre . flume . sink . hdfs . Calendar . getInstance ( ) ; com . wipro . ats . bdre . flume . sink . hdfs . List < java . lang . String > bodies = com . google . common . collect . Lists . newArrayList ( ) ; for ( i = 1 ; i < 4 ; i ++ ) { com . wipro . ats . bdre . flume . sink . hdfs . Transaction txn = channel . getTransaction ( ) ; txn . begin ( ) ; for ( j = 1 ; j <= batchSize ; j ++ ) { com . wipro . ats . bdre . flume . sink . hdfs . Event event = new org . apache . flume . event . SimpleEvent ( ) ; eventDate . clear ( ) ; eventDate . set ( 2011 , i , i , i , 0 ) ; event . getHeaders ( ) . put ( "num<sp>files<sp>wrong,<sp>found:<sp>" 2 , java . lang . String . valueOf ( eventDate . getTimeInMillis ( ) ) ) ; event . getHeaders ( ) . put ( "num<sp>files<sp>wrong,<sp>found:<sp>" 1 , ( "num<sp>files<sp>wrong,<sp>found:<sp>" 9 + i ) ) ; java . lang . String body = ( ( "Test." + i ) + "." ) + j ; event . setBody ( body . getBytes ( ) ) ; bodies . add ( body ) ; channel . put ( event ) ; totalEvents ++ ; } txn . commit ( ) ; txn . close ( ) ; sink . process ( ) ; } sink . stop ( ) ; com . wipro . ats . bdre . flume . sink . hdfs . FileStatus [ ] dirStat = fs . listStatus ( dirPath ) ; com . wipro . ats . bdre . flume . sink . hdfs . Path [ ] fList = com . wipro . ats . bdre . flume . sink . hdfs . FileUtil . stat2Paths ( dirStat ) ; long expectedFiles = totalEvents / rollCount ; if ( ( totalEvents % rollCount ) > 0 ) expectedFiles ++ ; org . junit . Assert . assertEquals ( ( "num<sp>files<sp>wrong,<sp>found:<sp>" + ( com . google . common . collect . Lists . newArrayList ( fList ) ) ) , expectedFiles , fList . length ) ; verifyOutputAvroFiles ( fs , conf , dirPath . toUri ( ) . getPath ( ) , fileName , bodies ) ; } }
public class aTest{ @Test public void testMain1 ( ) { java . io . FileOutputStream fos = null ; java . io . BufferedInputStream bis = null ; try { fos = new java . io . FileOutputStream ( file ) ; java . lang . System . setErr ( new java . io . PrintStream ( fos ) ) ; com . intel . mtwilson . client . TextConsole . main ( args1 ) ; bis = new java . io . BufferedInputStream ( new java . io . FileInputStream ( new java . io . File ( file . toString ( ) ) ) ) ; byte [ ] b = new byte [ 1024 ] ; int length = bis . read ( b ) ; java . lang . String temp = new java . lang . String ( b , 0 , length ) . trim ( ) ; java . lang . StringBuffer sb = new java . lang . StringBuffer ( ) ; sb . append ( "Unrecognized<sp>command:" ) . append ( "<sp>oat_os:" ) . append ( "<sp>com.intel.mtwilson.client.cmd.oat_os" ) ; org . junit . Assert . assertEquals ( sb . toString ( ) , temp ) ; } }
public class aTest{ @Test public void test1 ( ) { try { org . apache . lucene . analysis . Analyzer analyzer = new org . apdplat . word . lucene . ChineseWordAnalyzer ( ) ; org . apache . lucene . analysis . TokenStream tokenStream = analyzer . tokenStream ( "text" , "APDPlat" ) ; java . util . List < java . lang . String > words = new java . util . ArrayList ( ) ; tokenStream . reset ( ) ; while ( tokenStream . incrementToken ( ) ) { org . apache . lucene . analysis . tokenattributes . CharTermAttribute charTermAttribute = tokenStream . getAttribute ( org . apache . lucene . analysis . tokenattributes . CharTermAttribute . class ) ; words . add ( charTermAttribute . toString ( ) ) ; } tokenStream . close ( ) ; java . lang . String expResult = "[,<sp>,<sp>apdplat,<sp>,<sp>,<sp>,<sp>,<sp>,<sp>]" ; if ( "bigram" . equals ( org . apdplat . word . util . WordConfTools . get ( "ngram" , "bigram" ) ) ) { expResult = "[,<sp>,<sp>apdplat,<sp>,<sp>,<sp>,<sp>,<sp>,<sp>,<sp>]" ; } org . junit . Assert . assertEquals ( expResult , words . toString ( ) ) ; } }
public class aTest{ @Test public void compareImageWithAnotherFails ( ) { java . awt . image . BufferedImage bi1 = test . loadBufferedImage ( "data/test-test/testimage.png" ) ; java . awt . image . BufferedImage bi2 = test . loadBufferedImage ( "data/test-test/testimage2.png" ) ; try { test . compare ( bi1 , bi2 ) ; } catch ( org . jzy3d . junit . ChartTestFailed e ) { org . junit . Assert . assertTrue ( e . getMessage ( ) , true ) ; return ; } }
public class aTest{ @Test public void testGetAttribute ( ) { java . lang . String [ ] [ ] testStrings = new java . lang . String [ ] [ ] { new java . lang . String [ ] { "http://yacy.net?&test" , "unencodedlatinchars" 5 , "" } , new java . lang . String [ ] { "unencodedlatinchars" 7 , "unencodedlatinchars" 4 , ":/?[]@!$'()*,;=" } } ; for ( java . lang . String [ ] testString : testStrings ) { System . out . print ( ( "test<sp>getAttribute:<sp>" + ( testString [ 0 ] ) ) ) ; java . lang . String shouldBe = testString [ 1 ] ; net . yacy . cora . document . id . MultiProtocolURL resultUrl = new net . yacy . cora . document . id . MultiProtocolURL ( testString [ 0 ] ) ; System . out . println ( ( "unencodedlatinchars" 3 + ( resultUrl . toNormalform ( false ) ) ) ) ; java . util . Map < java . lang . String , java . lang . String > attr = resultUrl . getAttributes ( ) ; org . junit . Assert . assertEquals ( testString [ 2 ] , attr . get ( shouldBe ) ) ; } } }
public class aTest{ @Test public void testFileIsChangedToEmpty ( ) { final org . tmatesoft . svn . test . TestOptions options = org . tmatesoft . svn . test . TestOptions . getInstance ( ) ; final org . tmatesoft . svn . core . wc2 . SvnOperationFactory svnOperationFactory = new org . tmatesoft . svn . core . wc2 . SvnOperationFactory ( ) ; final org . tmatesoft . svn . test . Sandbox sandbox = org . tmatesoft . svn . test . Sandbox . createWithCleanup ( ( ( getTestName ( ) ) + ".testFileIsChangedToEmpty" ) , options ) ; try { final org . tmatesoft . svn . core . SVNURL url = sandbox . createSvnRepository ( ) ; final org . tmatesoft . svn . test . CommitBuilder commitBuilder = new org . tmatesoft . svn . test . CommitBuilder ( url ) ; commitBuilder . addFile ( "file" , "originalContents" . getBytes ( ) ) ; commitBuilder . commit ( ) ; final org . tmatesoft . svn . test . WorkingCopy workingCopy = sandbox . checkoutNewWorkingCopy ( url , SVNRevision . HEAD . getNumber ( ) ) ; final java . io . File workingCopyDirectory = workingCopy . getWorkingCopyDirectory ( ) ; final java . io . File file = new java . io . File ( workingCopyDirectory , "file" ) ; final java . lang . String emptyString = "" ; org . tmatesoft . svn . test . TestUtil . writeFileContentsString ( file , emptyString ) ; final org . tmatesoft . svn . core . wc2 . SvnCommit commit = svnOperationFactory . createCommit ( ) ; commit . setSingleTarget ( org . tmatesoft . svn . core . wc2 . SvnTarget . fromFile ( workingCopyDirectory ) ) ; commit . setCommitMessage ( "File<sp>contents<sp>is<sp>changed<sp>to<sp>empty" ) ; final org . tmatesoft . svn . core . SVNCommitInfo commitInfo = commit . run ( ) ; org . junit . Assert . assertEquals ( 2 , commitInfo . getNewRevision ( ) ) ; } }
public class aTest{ @Test public void test_shouldNotUpdateEnvironmentVariableEmptyOldKey ( ) { connect ( ) ; createApplication ( ) ; try { createEnvironmentVariable ( "key" , "value" ) ; org . springframework . shell . core . CommandResult result = updateEnvironmentVariable ( "" , "key" , "value" ) ; org . junit . Assert . assertThat ( result , isFailedCommand ( ) ) ; } }
public class aTest{ @Test public void testvalue ( ) { fr . inria . corese . core . Graph gg = fr . inria . corese . core . Graph . create ( ) ; fr . inria . corese . core . query . QueryProcess exec = fr . inria . corese . core . query . QueryProcess . create ( gg ) ; java . lang . String i = "insert<sp>data<sp>{<sp>" + ( ( "graph<sp>us:g1<sp>{<sp>us:John<sp>us:age<sp>10<sp>}<sp>" + "graph<sp>us:g2<sp>{<sp>us:John<sp>us:knows<sp>us:Jack<sp>}" ) + "}" ) ; java . lang . String q = "select<sp>*<sp>(xt:index(?x)<sp>as<sp>?i)<sp>(xt:index(?a)<sp>as<sp>?j)<sp>(xt:index(us:age)<sp>as<sp>?k)<sp>" + ( ( ( ( ( ( "where<sp>{" + "?x<sp>us:age<sp>?a<sp>" ) + "graph<sp>us:g2<sp>{<sp>us:John<sp>us:knows<sp>us:Jack<sp>}" 0 ) + "filter<sp>(?a<sp>=<sp>xt:value(?x,<sp>us:age,<sp>1))" ) + "filter<sp>(?x<sp>=<sp>xt:value(?x,<sp>us:age,<sp>0))" ) + "}" ) + "" ) ; exec . query ( i ) ; fr . inria . corese . kgram . core . Mappings map = exec . query ( q ) ; org . junit . Assert . assertEquals ( 1 , map . size ( ) ) ; } }
public class aTest{ @Test public void StreamLengthCorrection9 ( ) { synchronized ( this ) { java . lang . String filename = ( com . itextpdf . kernel . pdf . PdfReaderTest . sourceFolder ) + "10PagesDocumentWithInvalidStreamLength2.pdf" ; PdfReader . correctStreamLength = false ; com . itextpdf . kernel . pdf . PdfDocument pdfDoc = new com . itextpdf . kernel . pdf . PdfDocument ( new com . itextpdf . kernel . pdf . PdfReader ( filename ) ) ; int pageCount = pdfDoc . getNumberOfPages ( ) ; for ( int k = 1 ; k < ( pageCount + 1 ) ; k ++ ) { com . itextpdf . kernel . pdf . PdfPage page = pdfDoc . getPage ( k ) ; page . getPdfObject ( ) . get ( PdfName . MediaBox ) ; byte [ ] content = page . getFirstContentStream ( ) . getBytes ( ) ; org . junit . Assert . assertEquals ( 20 , content . length ) ; } }
public class aTest{ @Test public void testLexerPredsInCyclicDFA2 ( ) { java . lang . String grammar = "grammar<sp>foo;" + ( ( ( "@lexer::members<sp>{boolean<sp>p=false;}\n" + "a<sp>:<sp>(A|B)+<sp>;\n" ) + "A<sp>:<sp>{p}?<sp>(\'a\')+<sp>\'x\'<sp>(\'y\')?<sp>{System.out.println(\"token<sp>1\");}<sp>;\n" ) + "B<sp>:<sp>(\'a\')+<sp>\'x\'<sp>{System.out.println(\"token<sp>2\");}<sp>;\n" ) ; java . lang . String found = execParser ( "token<sp>2\n" 1 , grammar , "token<sp>2\n" 0 , "fooLexer" , "a" , "aax" , false ) ; org . junit . Assert . assertEquals ( "token<sp>2\n" , found ) ; } }
public class aTest{ @Test public void testRemovePlugin ( ) { java . util . List < java . lang . String > list = new java . util . LinkedList < java . lang . String > ( ) ; list . add ( "org.walkmod:javalang" ) ; java . io . File file = new java . io . File ( "src/test/resources/yaml/removePlugin.yml" ) ; if ( file . exists ( ) ) { file . delete ( ) ; } file . createNewFile ( ) ; java . lang . String input = "plugins:\n" ; input += "-<sp>\"org.walkmod:imports-cleaner:2.0\"" ; org . apache . commons . io . FileUtils . write ( file , input ) ; try { org . walkmod . conf . providers . YAMLConfigurationProvider provider = new org . walkmod . conf . providers . YAMLConfigurationProvider ( file . getPath ( ) ) ; org . walkmod . conf . entities . Configuration conf = new org . walkmod . conf . entities . impl . ConfigurationImpl ( ) ; provider . init ( conf ) ; org . walkmod . conf . entities . PluginConfig pc = new org . walkmod . conf . entities . impl . PluginConfigImpl ( ) ; pc . setGroupId ( "org.walkmod" ) ; pc . setArtifactId ( "imports-cleaner" ) ; provider . removePluginConfig ( pc , false ) ; java . lang . String output = org . apache . commons . io . FileUtils . readFileToString ( file ) ; System . out . println ( output ) ; org . junit . Assert . assertTrue ( ( ! ( output . contains ( "imports-cleaner" ) ) ) ) ; } }
public class aTest{ @Test public void testDeleteCustomer ( ) { try { boolean deleted = runFlowAndGetPayload ( "delete-customer" ) ; org . junit . Assert . assertTrue ( deleted ) ; } }
public class aTest{ @Test public void shouldOutputTruncatedLogLinesOfFailingTest ( ) { com . facebook . buck . event . listener . TestResultFormatter formatter = createFormatterWithMaxLogLines ( 3 ) ; com . facebook . buck . test . TestCaseSummary summary = new com . facebook . buck . test . TestCaseSummary ( "com.example.FooTest" , com . google . common . collect . ImmutableList . of ( failingTest ) ) ; java . nio . file . Files . write ( logPath , com . google . common . collect . ImmutableList . of ( "This<sp>log<sp>won't<sp>appear" , "This<sp>one<sp>will" , "Another<sp>one" , "Should<sp>be<sp>last" ) , StandardCharsets . UTF_8 ) ; com . facebook . buck . test . TestResults results = com . facebook . buck . test . TestResults . builder ( ) . setBuildTarget ( com . facebook . buck . core . model . BuildTargetFactory . newInstance ( "//foo:bar" ) ) . setTestCases ( com . google . common . collect . ImmutableList . of ( summary ) ) . addTestLogPaths ( logPath ) . build ( ) ; com . google . common . collect . ImmutableList . Builder < java . lang . String > builder = com . google . common . collect . ImmutableList . builder ( ) ; formatter . reportResult ( builder , results ) ; java . lang . String expected = java . lang . String . format ( com . google . common . base . Joiner . on ( '\n' ) . join ( "FAIL<sp>200ms<sp>0<sp>Passed<sp>0<sp>Skipped<sp>1<sp>Failed<sp>com.example.FooTest" , "FAILURE<sp>%s<sp>%s:<sp>%s" , "This<sp>one<sp>will" 0 , "====TEST<sp>LOGS====" , "Last<sp>3<sp>test<sp>log<sp>lines<sp>from<sp>log.txt:" , "This<sp>one<sp>will" , "Another<sp>one" , "Should<sp>be<sp>last" ) , failingTest . getTestCaseName ( ) , failingTest . getTestName ( ) , failingTest . getMessage ( ) , stackTrace ) ; org . junit . Assert . assertEquals ( expected , toString ( builder ) ) ; } }
public class aTest{ @Test public void createEnrollmentGroupQuerySucceed ( tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . QuerySpecification , tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . Query ) { final java . lang . String enrollmentGroupId = "enrollmentGroupId-1" ; final java . lang . String registrationPath = "registrations/" + enrollmentGroupId ; tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . RegistrationStatusManager registrationStatusManager = createRegistrationStatusManager ( ) ; new tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . NonStrictExpectations ( ) { { tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . Deencapsulation . newInstance ( tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . Query . class , new java . lang . Class [ ] { com . microsoft . azure . sdk . iot . provisioning . service . contract . ContractApiHttp . class , java . lang . String . class , tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . QuerySpecification . class , tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . Integer . class } , mockedContractApiHttp , registrationPath , mockedQuerySpecification , 0 ) ; result = mockedQuery ; times = 1 ; } } ; tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . Query query = tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . Deencapsulation . invoke ( registrationStatusManager , "createEnrollmentGroupQuery" , new java . lang . Class [ ] { tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . QuerySpecification . class , java . lang . String . class , tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . Integer . class } , mockedQuerySpecification , enrollmentGroupId , 0 ) ; org . junit . Assert . assertNotNull ( query ) ; } }
public class aTest{ @Test public void testDefaultSourceValidResource ( ) { org . eclipse . ceylon . common . FileUtil . delete ( new java . io . File ( "build/test-modules" ) ) ; org . eclipse . ceylon . compiler . js . ToolModel < org . eclipse . ceylon . compiler . js . CeylonCompileJsTool > tool = pluginLoader . loadToolModel ( "compile-js" ) ; org . junit . Assert . assertNotNull ( tool ) ; org . eclipse . ceylon . compiler . js . CeylonCompileJsTool jsc = pluginFactory . bindArguments ( tool , getMainTool ( ) , args ( "m1res.txt" 1 , "m1res.txt" 2 , "m1res.txt" 0 , "src/test/resources/res_test/test.txt" ) ) ; jsc . run ( ) ; checkCompilerResult ( "m1res.txt" 3 , "default" ) ; checkResources ( "m1res.txt" 3 , "default" , "test.txt" ) ; checkExcludedResources ( "m1res.txt" 3 , "default" , "m1res.txt" , "m1/m1res.txt" , "subdir/third.txt" , "ROOT/inroot.txt" , "ALTROOT/altroot.txt" ) ; } }
public class aTest{ @Test public void indexesAreOrderedAccordingToExpression ( ) { java . util . Map < java . lang . Character , java . lang . String > letterIndex = artistIndexService . createIndexesFromExpression ( "A<sp>B<sp>C<sp>#<sp>OtherLetters(DEFGHIJKLMNOPQRSTUVWXYZ)" ) ; com . github . hakko . musiccabinet . domain . model . music . Artist a ; com . github . hakko . musiccabinet . domain . model . music . Artist c ; com . github . hakko . musiccabinet . domain . model . music . Artist s ; com . github . hakko . musiccabinet . domain . model . music . Artist o ; com . github . hakko . musiccabinet . domain . model . music . Artist hash ; java . util . List < com . github . hakko . musiccabinet . domain . model . music . Artist > artists = java . util . Arrays . asList ( ( a = new com . github . hakko . musiccabinet . domain . model . music . Artist ( "A.A.<sp>Bondy" ) ) , ( c = new com . github . hakko . musiccabinet . domain . model . music . Artist ( "Cat<sp>Power" ) ) , ( s = new com . github . hakko . musiccabinet . domain . model . music . Artist ( "Swans" ) ) , ( o = new com . github . hakko . musiccabinet . domain . model . music . Artist ( "of<sp>Montreal" ) ) , ( hash = new com . github . hakko . musiccabinet . domain . model . music . Artist ( "16<sp>Horsepower" ) ) ) ; artistIndexService . setArtistSortName ( artists , new java . lang . String [ ] { "of" } ) ; java . util . Map < java . lang . String , java . util . List < com . github . hakko . musiccabinet . domain . model . music . Artist > > artistIndex = artistIndexService . createArtistIndex ( letterIndex , artists ) ; java . util . Map < java . lang . String , java . util . List < com . github . hakko . musiccabinet . domain . model . music . Artist > > expectedIndex = new java . util . LinkedHashMap ( ) ; expectedIndex . put ( "A" , java . util . Arrays . asList ( a ) ) ; expectedIndex . put ( "OtherLetters" 0 , java . util . Arrays . asList ( c ) ) ; expectedIndex . put ( "#" , java . util . Arrays . asList ( hash ) ) ; expectedIndex . put ( "OtherLetters" , java . util . Arrays . asList ( s , o ) ) ; org . junit . Assert . assertEquals ( expectedIndex , artistIndex ) ; } }
public class aTest{ @Test public void test ( ) { java . lang . String t_url = "" ; t_url = "http://54.222.142.17:8000/api/GetKey/" ; com . li3huo . sdk . domain . AgentKey key = new com . li3huo . sdk . domain . AgentKey ( ) ; key . channelId = "huawei" ; key . appid = "500006" ; key . keys = new com . alibaba . fastjson . JSONObject ( ) ; key . keys . put ( "param" , new java . lang . String [ ] { "paykey" , "paykey" 1 } ) ; com . li3huo . sdk . PayKeyTest . logger . debug ( ( "request<sp>JSON=" + ( key . toJSONString ( ) ) ) ) ; java . lang . String response = com . li3huo . sdk . tools . HttpUtil . doPost ( t_url , key . toJSONString ( ) ) ; key = com . li3huo . sdk . domain . AgentKey . parse ( response ) ; com . li3huo . sdk . PayKeyTest . logger . debug ( ( "paykey" 0 + ( key . code ) ) ) ; org . junit . Assert . assertEquals ( 0 , key . code ) ; com . li3huo . sdk . PayKeyTest . logger . debug ( ( "response<sp>JSON=" + ( key . toJSONString ( ) ) ) ) ; } }
public class aTest{ @Test public void test_search_with_moduleNames_componentName_success ( ) { java . nio . file . Path path = createTempDir ( ) ; org . apache . solr . core . SolrResourceLoader loader = new org . apache . solr . core . SolrResourceLoader ( path ) ; org . apache . solr . core . NodeConfig config = new org . apache . solr . core . NodeConfig . NodeConfigBuilder ( "testnode" , loader ) . setConfigSetBaseDirectory ( java . nio . file . Paths . get ( org . ikasan . wiretap . dao . SolrWiretapDaoTest . TEST_HOME ( ) ) . resolve ( "configsets" ) . toString ( ) ) . build ( ) ; try ( org . apache . solr . client . solrj . embedded . EmbeddedSolrServer server = new org . apache . solr . client . solrj . embedded . EmbeddedSolrServer ( config , "ikasan" ) ) { org . apache . solr . client . solrj . request . CoreAdminRequest . Create createRequest = new org . apache . solr . client . solrj . request . CoreAdminRequest . Create ( ) ; createRequest . setCoreName ( "ikasan" ) ; createRequest . setConfigSet ( "minimal" ) ; server . request ( createRequest ) ; java . util . HashMap < java . lang . String , java . lang . Object > fields = new java . util . HashMap ( ) ; fields . put ( "testnode" 1 , new java . lang . Integer ( 1 ) ) ; org . apache . solr . client . solrj . request . schema . SchemaRequest . AddField schemaRequest = new org . apache . solr . client . solrj . request . schema . SchemaRequest . AddField ( fields ) ; server . request ( schemaRequest ) ; org . ikasan . wiretap . dao . SolrWiretapDao solrCloudBase = new org . ikasan . wiretap . dao . SolrWiretapDao ( ) ; solrCloudBase . setSolrClient ( server ) ; solrCloudBase . setDaysToKeep ( 0 ) ; org . ikasan . wiretap . model . SolrWiretapEvent event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 1L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "event" ) ; solrCloudBase . save ( event ) ; event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 2L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "event" ) ; solrCloudBase . save ( event ) ; event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 3L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "event" ) ; solrCloudBase . save ( event ) ; event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 4L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "event" ) ; solrCloudBase . save ( event ) ; event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 5L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "event" ) ; solrCloudBase . save ( event ) ; java . util . HashSet < java . lang . String > moduleNames = new java . util . HashSet < java . lang . String > ( ) ; moduleNames . add ( "moduleName" ) ; org . ikasan . spec . search . PagedSearchResult < org . ikasan . spec . wiretap . WiretapEvent > results = solrCloudBase . findWiretapEvents ( 0 , 10 , "testnode" 2 , true , moduleNames , "flowName" , null , null , null , new java . util . Date ( ( ( java . lang . System . currentTimeMillis ( ) ) - 100000000 ) ) , new java . util . Date ( ( ( java . lang . System . currentTimeMillis ( ) ) + 100000000 ) ) , "testnode" 0 ) ; org . junit . Assert . assertEquals ( "testnode" 3 , results . getResultSize ( ) , 10 ) ; server . close ( ) ; } } }
public class aTest{ @Test public void validationOfDoublePropertyDifferentTagHierachy ( ) { java . lang . String room = "<sp><Name>Room<sp>42</Name>" 0 + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "<sp><Name>Room<sp>42</Name>" 4 http : com . sap . core . odata . api . edm . EdmEntitySet entitySet = com . sap . core . odata . testutil . mock . MockFacade . getMockEdm ( ) . getDefaultEntityContainer ( ) . getEntitySet ( "Rooms" ) ; java . io . InputStream reqContent = createContentAsStream ( room ) ; com . sap . core . odata . core . ep . consumer . XmlEntityConsumer xec = new com . sap . core . odata . core . ep . consumer . XmlEntityConsumer ( ) ; com . sap . core . odata . api . ep . entry . ODataEntry result = xec . readEntry ( entitySet , reqContent , com . sap . core . odata . api . ep . EntityProviderReadProperties . init ( ) . mergeSemantic ( false ) . build ( ) ) ; org . junit . Assert . assertNotNull ( result ) ; } }
public class aTest{ @Test public void testStrings ( ) { for ( int i = 1 ; i < 150000 ; i += 10000 ) { java . lang . String value = org . apache . commons . lang3 . RandomStringUtils . random ( i ) ; try { java . io . ByteArrayOutputStream baos = new java . io . ByteArrayOutputStream ( ) ; java . io . DataOutput dataOutput = new java . io . DataOutputStream ( baos ) ; org . apache . oozie . util . StringSerializationUtil . writeString ( dataOutput , value ) ; java . io . DataInput dataInput = new java . io . DataInputStream ( new java . io . ByteArrayInputStream ( baos . toByteArray ( ) ) ) ; org . junit . Assert . assertEquals ( ( "Error<sp>in<sp>serialization<sp>for<sp>size<sp>" + i ) , value , org . apache . oozie . util . StringSerializationUtil . readString ( dataInput ) ) ; } }
public class aTest{ @Test public void testCheckWrongSupplierOfCustomer ( ) { final org . oscm . domobjects . Organization customer = runTX ( new java . util . concurrent . Callable < org . oscm . domobjects . Organization > ( ) { @ org . oscm . accountservice . bean . Override public org . oscm . domobjects . Organization call ( ) throws org . oscm . accountservice . bean . Exception { org . oscm . domobjects . Organization supplier = org . oscm . test . data . Organizations . findOrganization ( mgr , supplierIds . get ( 0 ) ) ; org . oscm . domobjects . Organization supplier1 = org . oscm . test . data . Organizations . findOrganization ( mgr , supplierIds . get ( 1 ) ) ; org . oscm . domobjects . Organization customer = org . oscm . test . data . Organizations . createCustomer ( mgr , supplier ) ; org . oscm . domobjects . OrganizationReference organizationReference = new org . oscm . domobjects . OrganizationReference ( supplier1 , customer , org . oscm . domobjects . enums . OrganizationReferenceType . SUPPLIER_TO_CUSTOMER ) ; mgr . persist ( organizationReference ) ; createAvailablePayment ( customer , OrganizationRoleType . CUSTOMER ) ; return customer ; } } ) ; final org . oscm . domobjects . Organization customer1 = runTX ( new java . util . concurrent . Callable < org . oscm . domobjects . Organization > ( ) { @ org . oscm . accountservice . bean . Override public org . oscm . domobjects . Organization call ( ) throws org . oscm . accountservice . bean . Exception { org . oscm . domobjects . Organization customer1 = ( ( org . oscm . domobjects . Organization ) ( mgr . getReferenceByBusinessKey ( customer ) ) ) ; customer1 . getSources ( ) . size ( ) ; return customer1 ; } } ) ; org . junit . Assert . assertEquals ( customer1 . getSources ( ) . size ( ) , 2 ) ; org . oscm . domobjects . PlatformUser customer1User = runTX ( new java . util . concurrent . Callable < org . oscm . domobjects . PlatformUser > ( ) { @ org . oscm . accountservice . bean . Override public org . oscm . domobjects . PlatformUser call ( ) throws org . oscm . accountservice . bean . Exception { return org . oscm . test . data . Organizations . createUserForOrg ( mgr , customer1 , true , "CustomerAdmin1" ) ; } } }
public class aTest{ @Test public void getSummaryText ( ) { java . lang . String expected = "Result<sp>values<sp>:<sp>\n" 0 + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "Result<sp>values<sp>:<sp>\n" 3 + "Check<sp>this<sp>page<sp>:<sp>http://localhost:8080/#/cluster/clusterName\n" ) + "\n" ) + "#1.\n" ) + "Result<sp>values<sp>:<sp>\n" 5 ) + "Metrics<sp>name<sp>:<sp>cluster_state\n" ) + "Notification<sp>condition<sp>:<sp>cluster_state<sp>!=<sp>\"ok\"\n" ) + "Result<sp>values<sp>:<sp>\n" ) + "Result<sp>values<sp>:<sp>\n" 4 ) + "\n" ) + "Result<sp>values<sp>:<sp>\n" 1 ) + "Metrics<sp>type<sp>:<sp>node_info\n" ) + "Metrics<sp>name<sp>:<sp>used_memory\n" ) + "Notification<sp>condition<sp>:<sp>used_memory<sp><=<sp>1000\n" ) + "Result<sp>values<sp>:<sp>\n" ) + "<sp>100<sp>(on<sp>localhost:10000)\n" ) + "Result<sp>values<sp>:<sp>\n" 2 ) + "\n" ) + "Result<sp>values<sp>:<sp>\n" 0 ) ; java . lang . String result = service . getSummaryText ( cluster , jobs ) ; org . junit . Assert . assertThat ( result , org . hamcrest . core . Is . is ( expected ) ) ; } }
public class aTest{ @Test public void testArchiveUpload_Empty ( ) { org . artificer . atom . archive . ArtificerArchive archive = null ; try { archive = new org . artificer . atom . archive . ArtificerArchive ( ) ; } catch ( java . lang . Exception e ) { org . artificer . atom . archive . ArtificerArchive . closeQuietly ( archive ) ; throw e ; } finally { } try { org . artificer . client . ArtificerAtomApiClient client = client ( ) ; java . util . Map < java . lang . String , ? > results = client . uploadBatch ( archive ) ; org . junit . Assert . assertTrue ( results . isEmpty ( ) ) ; } }
public class aTest{ @Test public void testDumping ( ) { for ( int level = org . graalvm . compiler . debug . DebugContext . BASIC_LEVEL ; level <= ( org . graalvm . compiler . debug . DebugContext . VERY_DETAILED_LEVEL ) ; level ++ ) { org . graalvm . compiler . options . OptionValues options = new org . graalvm . compiler . options . OptionValues ( jdk . internal . vm . compiler . collections . EconomicMap . create ( ) ) ; options = new org . graalvm . compiler . options . OptionValues ( options , org . graalvm . compiler . debug . DebugOptions . Dump , ( ( ( "Scope5" 1 + level ) + ":" ) + level ) ) ; org . graalvm . compiler . debug . test . DebugContextTest . DebugContextSetup setup = new org . graalvm . compiler . debug . test . DebugContextTest . DebugContextSetup ( ) ; try ( org . graalvm . compiler . debug . DebugContext debug = setup . openDebugContext ( options ) ; org . graalvm . compiler . debug . DebugContext . Scope s0 = debug . scope ( "Scope5" 0 ) ) { try ( org . graalvm . compiler . debug . DebugContext . Scope s1 = debug . scope ( "Scope1" ) ) { try ( org . graalvm . compiler . debug . DebugContext . Scope s2 = debug . scope ( "Scope2" ) ) { try ( org . graalvm . compiler . debug . DebugContext . Scope s3 = debug . scope ( "Scope3" ) ) { try ( org . graalvm . compiler . debug . DebugContext . Scope s4 = debug . scope ( "Scope4" ) ) { try ( org . graalvm . compiler . debug . DebugContext . Scope s5 = debug . scope ( "Scope5" ) ) { debug . dump ( level , "an<sp>object" , "at<sp>level<sp>%d" , level ) ; } } } } } } java . lang . String expect = java . lang . String . format ( "Dumping<sp>an<sp>object<sp>with<sp>label<sp>\"at<sp>level<sp>%d\"%n" , level ) ; java . lang . String dump = setup . dumpOutput . toString ( ) ; org . junit . Assert . assertEquals ( expect , dump ) ; } } }
public class aTest{ @Test public void testRemoveAfterDone ( ) { com . j256 . ormlite . dao . Dao < com . j256 . ormlite . dao . JdbcBaseDaoImplTest . Foo , java . lang . Integer > fooDao = createDao ( com . j256 . ormlite . dao . JdbcBaseDaoImplTest . Foo . class , true ) ; java . util . Iterator < com . j256 . ormlite . dao . JdbcBaseDaoImplTest . Foo > iterator = fooDao . iterator ( ) ; org . junit . Assert . assertFalse ( iterator . hasNext ( ) ) ; try { iterator . remove ( ) ; org . junit . Assert . fail ( "expected<sp>exception" ) ; } }
public class aTest{ @Test public void testHappyFlowOmzetting ( ) { final java . lang . String bericht = org . apache . commons . io . IOUtils . toString ( nl . bzk . migratiebrp . synchronisatie . runtime . service . VerwerkToevalligeGebeurtenisService . class . getResourceAsStream ( "VerwerkToevalligeGebeurtenisVerzoekBericht.xml" ) ) ; final nl . bzk . migratiebrp . bericht . model . sync . impl . VerwerkToevalligeGebeurtenisVerzoekBericht verzoek = ( ( nl . bzk . migratiebrp . bericht . model . sync . impl . VerwerkToevalligeGebeurtenisVerzoekBericht ) ( factory . getBericht ( bericht ) ) ) ; final java . util . List < nl . bzk . migratiebrp . conversie . model . lo3 . syntax . Lo3CategorieWaarde > lo3Inhoud = nl . bzk . migratiebrp . bericht . model . lo3 . Lo3Inhoud . parseInhoud ( verzoek . getTb02InhoudAlsTeletex ( ) ) ; final nl . bzk . migratiebrp . conversie . model . brp . toevalligegebeurtenis . BrpToevalligeGebeurtenisPersoon tgbPersoon = nl . bzk . migratiebrp . synchronisatie . runtime . service . toevalligegebeurtenis . VerwerkToevalligeGebeurtenisVerzoekHelper . maakPersoon ( "2354545000" , "456245245" , null , "Vries" 2 , "van" , '<sp>' , null , "Heusden" , 19581512 , "0599" , "6030" , null , null , new nl . bzk . migratiebrp . conversie . model . brp . attribuut . BrpGeslachtsaanduidingCode ( Geslachtsaanduiding . MAN . getCode ( ) ) ) ; final nl . bzk . migratiebrp . conversie . model . brp . toevalligegebeurtenis . BrpToevalligeGebeurtenisPersoon partner = nl . bzk . migratiebrp . synchronisatie . runtime . service . toevalligegebeurtenis . VerwerkToevalligeGebeurtenisVerzoekHelper . maakPersoon ( null , null , null , "Truus" , null , null , new nl . bzk . migratiebrp . conversie . model . brp . attribuut . BrpAdellijkeTitelCode ( AdellijkeTitel . M . getCode ( ) ) , "Vries" , 19581501 , "0599" , "6030" , null , null , new nl . bzk . migratiebrp . conversie . model . brp . attribuut . BrpGeslachtsaanduidingCode ( Geslachtsaanduiding . VROUW . getCode ( ) ) ) ; final nl . bzk . migratiebrp . conversie . model . brp . toevalligegebeurtenis . BrpToevalligeGebeurtenisVerbintenisSluiting sluiting = nl . bzk . migratiebrp . synchronisatie . runtime . service . toevalligegebeurtenis . VerwerkToevalligeGebeurtenisVerzoekHelper . maakSluiting ( BrpSoortRelatieCode . GEREGISTREERD_PARTNERSCHAP , 20110101 , "0599" , null , "6030" ) ; final nl . bzk . migratiebrp . conversie . model . brp . toevalligegebeurtenis . BrpToevalligeGebeurtenisVerbintenisSluiting omzetting = nl . bzk . migratiebrp . synchronisatie . runtime . service . toevalligegebeurtenis . VerwerkToevalligeGebeurtenisVerzoekHelper . maakSluiting ( BrpSoortRelatieCode . HUWELIJK , 20161001 , "Vries" 1 , null , "6030" ) ; final nl . bzk . migratiebrp . conversie . model . brp . toevalligegebeurtenis . BrpToevalligeGebeurtenisVerbintenis verbintenis = new nl . bzk . migratiebrp . conversie . model . brp . toevalligegebeurtenis . BrpToevalligeGebeurtenisVerbintenis ( partner , sluiting , null , omzetting ) ; org . powermock . api . mockito . PowerMockito . when ( syntaxControle . controleer ( lo3Inhoud ) ) . thenReturn ( lo3Inhoud ) ; org . powermock . api . mockito . PowerMockito . when ( converteerLo3NaarBrpService . converteerLo3ToevalligeGebeurtenis ( org . mockito . Matchers . any ( nl . bzk . migratiebrp . conversie . model . lo3 . Lo3ToevalligeGebeurtenis . class ) ) ) . thenReturn ( brpToevalligeGebeurtenis ) ; org . powermock . api . mockito . PowerMockito . when ( brpToevalligeGebeurtenis . getPersoon ( ) ) . thenReturn ( tgbPersoon ) ; org . powermock . api . mockito . PowerMockito . when ( brpToevalligeGebeurtenis . getVerbintenis ( ) ) . thenReturn ( verbintenis ) ; org . powermock . api . mockito . PowerMockito . when ( brpToevalligeGebeurtenis . getPersoon ( ) ) . thenReturn ( tgbPersoon ) ; org . powermock . api . mockito . PowerMockito . when ( brpToevalligeGebeurtenis . getDatumAanvang ( ) ) . thenReturn ( new nl . bzk . migratiebrp . conversie . model . brp . attribuut . BrpDatum ( 20161001 , null ) ) ; org . powermock . api . mockito . PowerMockito . when ( brpToevalligeGebeurtenis . getRegisterGemeente ( ) ) . thenReturn ( new nl . bzk . migratiebrp . conversie . model . brp . attribuut . BrpPartijCode ( "Vries" 0 ) ) ; org . powermock . api . mockito . PowerMockito . when ( brpToevalligeGebeurtenis . getNummerAkte ( ) ) . thenReturn ( new nl . bzk . migratiebrp . conversie . model . brp . attribuut . BrpString ( "5H" , null ) ) ; org . powermock . api . mockito . PowerMockito . when ( persoonslijstService . zoekPersoonOpAnummerFoutiefOpgeschortUitsluiten ( org . mockito . Matchers . anyString ( ) ) ) . thenReturn ( brpPersoonslijst ) ; org . powermock . api . mockito . PowerMockito . when ( brpToevalligeGebeurtenis . getDoelPartijCode ( ) ) . thenReturn ( new nl . bzk . migratiebrp . conversie . model . brp . attribuut . BrpPartijCode ( "Vries" 0 ) ) ; final nl . bzk . migratiebrp . bericht . model . sync . impl . VerwerkToevalligeGebeurtenisAntwoordBericht antwoord = subject . verwerkBericht ( verzoek ) ; org . junit . Assert . assertNull ( antwoord ) ; } }
public class aTest{ @Test public void testMultiByteCharacterMarkReset002 ( ) { final java . lang . String expected = "Aß東Aß東Aß東Aß東Aß東Aß東Aß東Aß東" ; org . glassfish . grizzly . http2 . HttpInputStreamsTest . ReadStrategy reader = new org . glassfish . grizzly . http2 . HttpInputStreamsTest . ReadStrategy ( ) { @ org . glassfish . grizzly . http2 . Override public boolean doRead ( org . glassfish . grizzly . http . server . Request request ) throws java . io . IOException { java . lang . StringBuilder sb = new java . lang . StringBuilder ( 5 ) ; java . io . Reader in = request . getReader ( ) ; for ( int j = 0 ; j < 5 ; j ++ ) { sb . append ( ( ( char ) ( in . read ( ) ) ) ) ; } org . junit . Assert . assertEquals ( expected . substring ( 0 , 5 ) , sb . toString ( ) ) ; in . mark ( 2 ) ; for ( int j = 0 ; j < 5 ; j ++ ) { sb . append ( ( ( char ) ( in . read ( ) ) ) ) ; } }
public class aTest{ @Test public void testEvaluateSingleIndex ( ) { final java . lang . String indexSparqlString = "" + ( ( ( ( "{" 1 + "{" ) + "<sp>?e<sp>a<sp>?c<sp>.<sp>" ) + "{" 0 ) + "}" ) ; org . apache . rya . indexing . external . PcjIntegrationTestingUtil . createAndPopulatePcj ( conn , accCon , ( ( tablename ) + 1 ) , indexSparqlString , new java . lang . String [ ] { "e" , "l" , "c" } , com . google . common . base . Optional . absent ( ) ) ; final java . lang . String queryString = "" + ( ( ( ( ( "SELECT<sp>?e<sp>?c<sp>?l<sp>?o<sp>" + "{" ) + "<sp>?e<sp>a<sp>?c<sp>.<sp>" ) + "<sp>?e<sp><http://www.w3.org/2000/01/rdf-schema#label><sp>?l<sp>.<sp>" ) + "<sp>?e<sp><uri:talksTo><sp>?o<sp>.<sp>" ) + "}" ) ; final org . apache . rya . indexing . external . AccumuloPcjIT . CountingResultHandler crh1 = new org . apache . rya . indexing . external . AccumuloPcjIT . CountingResultHandler ( ) ; final org . apache . rya . indexing . external . AccumuloPcjIT . CountingResultHandler crh2 = new org . apache . rya . indexing . external . AccumuloPcjIT . CountingResultHandler ( ) ; conn . prepareTupleQuery ( QueryLanguage . SPARQL , queryString ) . evaluate ( crh1 ) ; pcjConn . prepareTupleQuery ( QueryLanguage . SPARQL , queryString ) . evaluate ( crh2 ) ; org . junit . Assert . assertEquals ( crh1 . getCount ( ) , crh2 . getCount ( ) ) ; } }
public class aTest{ @Test public void testBootstrapExistingLimits ( ) { java . io . File existingLimit = new java . io . File ( ( ( ( CGroupsHandler . CGroupController . CPU . getName ( ) ) + "." ) + ( CGroupsHandler . CGROUP_CPU_QUOTA_US ) ) ) ; try { org . apache . commons . io . FileUtils . write ( existingLimit , "10000" ) ; when ( mockCGroupsHandler . getPathForCGroup ( CGroupsHandler . CGroupController . CPU , "" ) ) . thenReturn ( "." ) ; org . apache . hadoop . conf . Configuration conf = new org . apache . hadoop . yarn . conf . YarnConfiguration ( ) ; java . util . List < org . apache . hadoop . yarn . server . nodemanager . containermanager . linux . privileged . PrivilegedOperation > ret = cGroupsCpuResourceHandler . bootstrap ( plugin , conf ) ; verify ( mockCGroupsHandler , times ( 1 ) ) . initializeCGroupController ( CGroupsHandler . CGroupController . CPU ) ; verify ( mockCGroupsHandler , times ( 1 ) ) . updateCGroupParam ( CGroupsHandler . CGroupController . CPU , "" , CGroupsHandler . CGROUP_CPU_QUOTA_US , "-1" ) ; org . junit . Assert . assertNull ( ret ) ; } }
public class aTest{ @Test public void testDynamicSelectZero ( ) { org . apache . ibatis . session . SqlSession sqlSession = tk . mybatis . mapper . mapper . MybatisHelper . getSqlSession ( ) ; try { tk . mybatis . mapper . mapper . CountryMapper mapper = sqlSession . getMapper ( tk . mybatis . mapper . mapper . CountryMapper . class ) ; tk . mybatis . mapper . model . Country country = new tk . mybatis . mapper . model . Country ( ) ; country . setCountrycode ( "CN" ) ; country . setCountryname ( "" ) ; java . util . List < tk . mybatis . mapper . model . Country > countryList = mapper . select ( country ) ; org . junit . Assert . assertEquals ( 0 , countryList . size ( ) ) ; } }
public class aTest{ @Test public void testClientKeyStoreWithPrivateKeyPwd ( ) { org . junit . Assume . assumeTrue ( ( ( haveSsl ( sharedConnection ) ) && ( isMariadbServer ( ) ) ) ) ; java . lang . String clientKeyStore2Path = java . lang . System . getProperty ( "keyStorePassword" 0 ) ; java . lang . String clientKeyStore2Password = java . lang . System . getProperty ( "keystore2Password" ) ; java . lang . String clientKeyPassword = java . lang . System . getProperty ( "keyPassword" ) ; org . junit . Assume . assumeTrue ( ( ( ( clientKeyPassword != null ) && ( clientKeyStore2Password != null ) ) && ( clientKeyStore2Path != null ) ) ) ; java . lang . String testUser = "testKeystore" ; createSslTestUser ( testUser ) ; try { java . util . Properties info = new java . util . Properties ( ) ; info . setProperty ( "useSSL" , "true" ) ; info . setProperty ( "serverSslCert" , serverCertificatePath ) ; info . setProperty ( "keyStorePassword" 2 , ( "file:///" + clientKeyStore2Path ) ) ; info . setProperty ( "keyStorePassword" , clientKeyStore2Password ) ; testConnect ( info , true , testUser , "ssltestpassword" ) ; org . junit . Assert . fail ( "Must<sp>have<sp>Error<sp>since<sp>client<sp>private<sp>key<sp>is<sp>protected<sp>with<sp>a<sp>password<sp>different<sp>than<sp>keystore" ) ; } catch ( java . sql . SQLException sqle ) { sqle . printStackTrace ( ) ; org . junit . Assert . assertTrue ( sqle . getMessage ( ) . contains ( "keyStorePassword" 1 ) ) ; } }
public class aTest{ @Test public void nestedAndArray ( ) { final org . apache . commons . lang3 . builder . MultilineRecursiveToStringStyleTest . Account acc = new org . apache . commons . lang3 . builder . MultilineRecursiveToStringStyleTest . Account ( ) ; final org . apache . commons . lang3 . builder . MultilineRecursiveToStringStyleTest . Transaction tx1 = new org . apache . commons . lang3 . builder . MultilineRecursiveToStringStyleTest . Transaction ( "2014.10.14" , 100 ) ; final org . apache . commons . lang3 . builder . MultilineRecursiveToStringStyleTest . Transaction tx2 = new org . apache . commons . lang3 . builder . MultilineRecursiveToStringStyleTest . Transaction ( "<sp>owner=<null>," 4 , 50 ) ; acc . transactions . add ( tx1 ) ; acc . transactions . add ( tx2 ) ; final java . lang . String expected = ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( getClassPrefix ( acc ) ) + "[" ) + ( BR ) ) + "<sp>owner=<null>," ) + ( BR ) ) + "<sp>owner=<null>," 2 ) + ( getClassPrefix ( acc . transactions ) ) ) + "{" ) + ( BR ) ) + "<sp>owner=<null>," 0 ) + ( getClassPrefix ( tx1 ) ) ) + "[" ) + ( BR ) ) + "<sp>amount=100.0," ) + ( BR ) ) + "<sp>date=2014.10.14" ) + ( BR ) ) + "<sp>]," ) + ( BR ) ) + "<sp>owner=<null>," 0 ) + ( getClassPrefix ( tx2 ) ) ) + "[" ) + ( BR ) ) + "<sp>amount=50.0," ) + ( BR ) ) + "<sp>owner=<null>," 3 ) + ( BR ) ) + "<sp>]" ) + ( BR ) ) + "<sp>owner=<null>," 1 ) + ( BR ) ) + "]" ; org . junit . Assert . assertEquals ( expected , toString ( acc ) ) ; } }
public class aTest{ @Test public void testEcho ( ) { doCmdTest ( new org . cyy . fw . nedis . test . cmd . TestAction ( ) { @ org . cyy . fw . nedis . test . cmd . Override public void doTest ( ) throws org . cyy . fw . nedis . test . cmd . InterruptedException , org . cyy . fw . nedis . util . NedisException { final java . lang . String message = "Hello<sp>world." ; client . echo ( new org . cyy . fw . nedis . ResponseCallback < java . lang . String > ( ) { @ org . cyy . fw . nedis . test . cmd . Override public void failed ( java . lang . Throwable cause ) { org . junit . Assert . fail ( cause ) ; controller . countDown ( ) ; } @ org . cyy . fw . nedis . test . cmd . Override public void done ( java . lang . String result ) { org . junit . Assert . assertEquals ( message , result ) ; controller . countDown ( ) ; } } }
public class aTest{ @Test public void testToken2 ( ) { java . lang . String grammar = "grammar<sp>T;\n" + ( ( ( ( ( "s\n" + "@init<sp>{setBuildParseTree(true);}\n" ) + "@after<sp>{System.out.println($r.ctx.toStringTree(this));}\n" ) + "<sp>:r=a<sp>;\n" ) + "a<sp>:<sp>\'x\'<sp>\'y\'\n" ) + "<sp>;\n" ) ; java . lang . String result = execParser ( "T.g4" , grammar , "TParser" , "s\n" 1 , "s\n" 2 , "xy" , false ) ; java . lang . String expecting = "s\n" 0 ; org . junit . Assert . assertEquals ( expecting , result ) ; } }
public class aTest{ @Test public void canMailPollerHandleException ( ) { gov . hhs . fha . nhinc . mail . MailReceiver mockMailReceiver = mock ( gov . hhs . fha . nhinc . mail . MailReceiver . class ) ; gov . hhs . fha . nhinc . mail . MessageHandler mockMessageHandler = mock ( gov . hhs . fha . nhinc . mail . MessageHandler . class ) ; gov . hhs . fha . nhinc . mail . MailClientException mockException = mock ( gov . hhs . fha . nhinc . mail . MailClientException . class ) ; when ( mockException . getMessage ( ) ) . thenReturn ( gov . hhs . fha . nhinc . mail . AbstractMailPollerTest . EXCEPTION_MSG ) ; when ( mockMailReceiver . handleMessages ( mockMessageHandler ) ) . thenThrow ( mockException ) ; gov . hhs . fha . nhinc . mail . AbstractMailPoller testMailPoller = new gov . hhs . fha . nhinc . mail . AbstractMailPoller ( mockMailReceiver , mockMessageHandler ) { @ gov . hhs . fha . nhinc . mail . Override public void handleException ( gov . hhs . fha . nhinc . mail . MailClientException e ) { org . junit . Assert . assertEquals ( gov . hhs . fha . nhinc . mail . AbstractMailPollerTest . EXCEPTION_MSG , e . getMessage ( ) ) ; } } }
public class aTest{ @Test public void should_create_node_atom ( ) { java . util . concurrent . ThreadPoolExecutor threadPoolExecutor = new java . util . concurrent . ThreadPoolExecutor ( 5 , 5 , 1 , java . util . concurrent . TimeUnit . SECONDS , new java . util . concurrent . ArrayBlockingQueue ( 5 ) ) ; final java . util . concurrent . atomic . AtomicInteger size = new java . util . concurrent . atomic . AtomicInteger ( 0 ) ; final java . util . concurrent . CountDownLatch latch = new java . util . concurrent . CountDownLatch ( 5 ) ; final java . lang . String root = "/flow-agent" ; zkClient . create ( root , null ) ; java . lang . String agentNodePath = org . apache . curator . utils . ZKPaths . makePath ( root , "flow-atom" ) ; zkClient . delete ( agentNodePath , false ) ; for ( int i = 0 ; i < 5 ; i ++ ) { threadPoolExecutor . execute ( ( ) -> { try { zkClient . createEphemeral ( agentNodePath ) ; size . incrementAndGet ( ) ; } catch ( e ) { System . out . println ( com . flow . platform . util . zk . test . e ) ; } finally { latch . countDown ( ) ; } } ) ; } latch . await ( 30 , TimeUnit . SECONDS ) ; org . junit . Assert . assertEquals ( 1 , size . get ( ) ) ; zkClient . delete ( root , true ) ; } }
public class aTest{ @Test public void testSystemProperties1 ( ) { final java . lang . String tempFileName = ( java . lang . System . getProperty ( "java.io.tmpdir" ) ) + "/hadoop.log" ; final java . nio . file . Path tempFilePath = new java . io . File ( tempFileName ) . toPath ( ) ; java . nio . file . Files . deleteIfExists ( tempFilePath ) ; try { final org . apache . logging . log4j . core . config . Configuration configuration = getConfiguration ( "config-1.2/log4j-system-properties-1.properties" ) ; final org . apache . logging . log4j . core . appender . RollingFileAppender appender = configuration . getAppender ( "RFA" ) ; appender . stop ( 10 , TimeUnit . SECONDS ) ; System . out . println ( ( ( ( "expected:<sp>" + tempFileName ) + "<sp>Actual:<sp>" ) + ( appender . getFileName ( ) ) ) ) ; org . junit . Assert . assertEquals ( tempFileName , appender . getFileName ( ) ) ; } }
public class aTest{ @Test public void testShouldDetectUsersAsParameterInsideAMap ( ) { org . apache . ibatis . session . SqlSession sqlSession = sqlSessionFactory . openSession ( ) ; try { org . apache . ibatis . submitted . parametrizedlist . Mapper mapper = sqlSession . getMapper ( org . apache . ibatis . submitted . parametrizedlist . Mapper . class ) ; java . util . Map < java . lang . Integer , org . apache . ibatis . submitted . parametrizedlist . User < java . lang . String > > map = mapper . getAMapOfUsers ( ) ; org . junit . Assert . assertEquals ( org . apache . ibatis . submitted . parametrizedlist . User . class , map . get ( 1 ) . getClass ( ) ) ; } }
public class aTest{ @Test public void testLogoutRequestAndDisconnectedCallback ( ) { final java . lang . String name = "logout" ; com . sun . sgs . test . impl . service . session . TestClientSessionServiceImplv4 . DummyClient client = createDummyClient ( name ) ; try { client . connect ( serverNode . getAppPort ( ) ) ; org . junit . Assert . assertTrue ( client . login ( ) ) ; checkBindings ( 1 ) ; client . logout ( ) ; client . checkDisconnectedCallback ( true ) ; checkBindings ( 0 ) ; txnScheduler . runTask ( new com . sun . sgs . test . util . TestAbstractKernelRunnable ( ) { public void run ( ) { try { dataService . getBinding ( name ) ; org . junit . Assert . fail ( ( "expected<sp>ObjectNotFoundException:<sp>" + "object<sp>not<sp>removed" ) ) ; } }
public class aTest{ @Test public void testRegisterComponentInSpaceComponentManager ( ) { final org . jmock . States state = getMockery ( ) . states ( "test" ) ; getMockery ( ) . checking ( new org . jmock . Expectations ( ) { { allowing ( mockWikiDescriptorManager ) . getCurrentWikiId ( ) ; when ( state . isNot ( "otherspace" ) ) ; will ( returnValue ( "space2" 3 ) ) ; allowing ( mockCurrentSpaceReferenceProvider ) . get ( ) ; when ( state . isNot ( "otherspace" ) ) ; will ( returnValue ( new org . xwiki . model . reference . SpaceReference ( "space1" , new org . xwiki . model . reference . WikiReference ( "space2" 3 ) ) ) ) ; allowing ( mockCurrentDocumentReferenceProvider ) . get ( ) ; when ( state . isNot ( "otherspace" ) ) ; will ( returnValue ( new org . xwiki . model . reference . DocumentReference ( "space2" 3 , "space1" , "document1" ) ) ) ; allowing ( mockDocumentAccessBridge ) . getCurrentUserReference ( ) ; when ( state . isNot ( "otherspace" ) ) ; will ( returnValue ( new org . xwiki . model . reference . DocumentReference ( "space2" 2 , "XWiki" , "user" ) ) ) ; } } ) ; org . xwiki . component . manager . ComponentManager userCM = getComponentManager ( ) . getInstance ( org . xwiki . component . manager . ComponentManager . class , "space" ) ; org . xwiki . component . descriptor . DefaultComponentDescriptor < org . xwiki . component . internal . ContextComponentManagerTest . Role > cd = new org . xwiki . component . descriptor . DefaultComponentDescriptor < org . xwiki . component . internal . ContextComponentManagerTest . Role > ( ) ; cd . setRoleType ( org . xwiki . component . internal . ContextComponentManagerTest . Role . class ) ; cd . setImplementation ( org . xwiki . component . internal . ContextComponentManagerTest . RoleImpl . class ) ; userCM . registerComponent ( cd ) ; org . xwiki . component . manager . ComponentManager contextCM = getComponentManager ( ) . getInstance ( org . xwiki . component . manager . ComponentManager . class , "context" ) ; org . junit . Assert . assertNotNull ( contextCM . getInstance ( org . xwiki . component . internal . ContextComponentManagerTest . Role . class ) ) ; state . become ( "otherspace" ) ; getMockery ( ) . checking ( new org . jmock . Expectations ( ) { { exactly ( 1 ) . of ( mockDocumentAccessBridge ) . getCurrentUserReference ( ) ; will ( returnValue ( new org . xwiki . model . reference . DocumentReference ( "space2" 2 , "XWiki" , "user" ) ) ) ; allowing ( mockWikiDescriptorManager ) . getCurrentWikiId ( ) ; will ( returnValue ( "space2" 0 ) ) ; allowing ( mockCurrentSpaceReferenceProvider ) . get ( ) ; will ( returnValue ( new org . xwiki . model . reference . SpaceReference ( "space2" , new org . xwiki . model . reference . WikiReference ( "space2" 0 ) ) ) ) ; allowing ( mockCurrentDocumentReferenceProvider ) . get ( ) ; will ( returnValue ( new org . xwiki . model . reference . DocumentReference ( "space2" 0 , "space2" , "document2" ) ) ) ; } } }
public class aTest{ @Test public void parseWithUnixLineEndingsInBody ( ) { java . lang . String body = ( ( ( ( ( ( "This<sp>is<sp>the<sp>body<sp>we<sp>need<sp>to<sp>parse.<sp>The<sp>line<sp>spaces<sp>in<sp>the<sp>body<sp>" + ( org . apache . olingo . odata2 . core . batch . BatchResponseParserTest . LF ) ) + ( org . apache . olingo . odata2 . core . batch . BatchResponseParserTest . LF ) ) + ( org . apache . olingo . odata2 . core . batch . BatchResponseParserTest . LF ) ) + "are<sp>" ) + ( org . apache . olingo . odata2 . core . batch . BatchResponseParserTest . LF ) ) + ( org . apache . olingo . odata2 . core . batch . BatchResponseParserTest . LF ) ) + "part<sp>of<sp>the<sp>body<sp>and<sp>must<sp>not<sp>be<sp>ignored<sp>or<sp>filtered." ; java . lang . String responseString = ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "--batch_123" + ( org . apache . olingo . odata2 . core . batch . BatchResponseParserTest . CRLF ) ) + "Content-Type:<sp>application/http" ) + ( org . apache . olingo . odata2 . core . batch . BatchResponseParserTest . CRLF ) ) + "Content-Length:<sp>234" ) + ( org . apache . olingo . odata2 . core . batch . BatchResponseParserTest . CRLF ) ) + "Content-Length:<sp>234" 0 ) + ( org . apache . olingo . odata2 . core . batch . BatchResponseParserTest . CRLF ) ) + ( org . apache . olingo . odata2 . core . batch . BatchResponseParserTest . CRLF ) ) + "Content-Length:<sp>234" 1 ) + ( org . apache . olingo . odata2 . core . batch . BatchResponseParserTest . CRLF ) ) + "Content-Type:<sp>application/xml;charset=utf-8" ) + ( org . apache . olingo . odata2 . core . batch . BatchResponseParserTest . CRLF ) ) + "Content-Length:<sp>125" ) + ( org . apache . olingo . odata2 . core . batch . BatchResponseParserTest . CRLF ) ) + ( org . apache . olingo . odata2 . core . batch . BatchResponseParserTest . CRLF ) ) + body ) + ( org . apache . olingo . odata2 . core . batch . BatchResponseParserTest . CRLF ) ) + "--batch_123--" ; java . io . InputStream stream = new java . io . ByteArrayInputStream ( responseString . getBytes ( ) ) ; org . apache . olingo . odata2 . api . client . batch . BatchSingleResponse response = org . apache . olingo . odata2 . api . ep . EntityProvider . parseBatchResponse ( stream , "multipart/mixed;boundary=batch_123" ) . get ( 0 ) ; org . junit . Assert . assertEquals ( body , response . getBody ( ) ) ; } }
public class aTest{ @Test public void setMqttCallbackSucceeds ( tests . unit . com . microsoft . azure . sdk . iot . device . transport . mqtt . MqttCallback ) { final com . microsoft . azure . sdk . iot . device . transport . mqtt . MqttConnection mqttConnection = mockit . Deencapsulation . newInstance ( com . microsoft . azure . sdk . iot . device . transport . mqtt . MqttConnection . class , new java . lang . Class [ ] { java . lang . String . class , java . lang . String . class , java . lang . String . class , java . lang . String . class , javax . net . ssl . SSLContext . class } , tests . unit . com . microsoft . azure . sdk . iot . device . transport . mqtt . MqttConnectionTest . SERVER_URI , tests . unit . com . microsoft . azure . sdk . iot . device . transport . mqtt . MqttConnectionTest . CLIENT_ID , tests . unit . com . microsoft . azure . sdk . iot . device . transport . mqtt . MqttConnectionTest . USER_NAME , tests . unit . com . microsoft . azure . sdk . iot . device . transport . mqtt . MqttConnectionTest . PWORD , mockIotHubSSLContext ) ; tests . unit . com . microsoft . azure . sdk . iot . device . transport . mqtt . MqttCallback testMqttCallback = mockedMqttCallback ; mockit . Deencapsulation . invoke ( mqttConnection , "setMqttCallback" , new java . lang . Class [ ] { tests . unit . com . microsoft . azure . sdk . iot . device . transport . mqtt . MqttCallback . class } , testMqttCallback ) ; tests . unit . com . microsoft . azure . sdk . iot . device . transport . mqtt . MqttCallback actualMqttCallback = mockit . Deencapsulation . getField ( mqttConnection , "mqttCallback" ) ; org . junit . Assert . assertEquals ( actualMqttCallback , testMqttCallback ) ; } }
public class aTest{ @Test public void testOAuth2ResourceOwnerWithClientID ( ) { com . streamsets . pipeline . stage . origin . http . HttpClientSourceIT . tokenGetCount = 0 ; try { com . streamsets . pipeline . config . DataFormat dataFormat = com . streamsets . pipeline . config . DataFormat . JSON ; com . streamsets . pipeline . stage . origin . http . HttpClientConfigBean conf = new com . streamsets . pipeline . stage . origin . http . HttpClientConfigBean ( ) ; conf . client . authType = com . streamsets . pipeline . lib . http . AuthenticationType . NONE ; conf . client . useOAuth2 = true ; conf . client . oauth2 . credentialsGrantType = RESOURCE_OWNER ; conf . client . oauth2 . username = ( ) -> com . streamsets . pipeline . stage . origin . http . HttpClientSourceIT . USERNAME ; conf . client . oauth2 . password = ( ) -> com . streamsets . pipeline . stage . origin . http . HttpClientSourceIT . PASSWORD ; conf . client . oauth2 . resourceOwnerClientId = ( ) -> com . streamsets . pipeline . stage . origin . http . HttpClientSourceIT . CLIENT_ID ; conf . client . oauth2 . resourceOwnerClientSecret = ( ) -> com . streamsets . pipeline . stage . origin . http . HttpClientSourceIT . CLIENT_SECRET ; conf . client . oauth2 . tokenUrl = ( getBaseUri ( ) ) + "resourceToken" ; conf . httpMode = HttpClientMode . STREAMING ; conf . resourceUrl = ( getBaseUri ( ) ) + "stream" ; conf . client . readTimeoutMillis = 1000 ; conf . basic . maxBatchSize = 100 ; conf . basic . maxWaitTime = 1000 ; conf . pollingInterval = 1000 ; conf . httpMethod = com . streamsets . pipeline . lib . http . HttpMethod . GET ; conf . dataFormat = dataFormat ; conf . dataFormatConfig . jsonContent = com . streamsets . pipeline . config . JsonMode . MULTIPLE_OBJECTS ; runBatchAndAssertNames ( dataFormat , conf ) ; org . junit . Assert . assertEquals ( 1 , com . streamsets . pipeline . stage . origin . http . HttpClientSourceIT . tokenGetCount ) ; } }
public class aTest{ @Test public void testVerifyAndRecoverFromOld ( ) { byte [ ] old = new byte [ ] { 1 , 2 , 3 } ; java . io . File file = new java . io . File ( createTestDir ( ) , "x" ) ; createFile ( new java . io . File ( ( ( file . getAbsolutePath ( ) ) + "-old" ) ) , old ) ; com . streamsets . datacollector . io . DataStore ds = new com . streamsets . datacollector . io . DataStore ( file ) ; try { byte [ ] arr = readViaStore ( ds ) ; org . junit . Assert . assertArrayEquals ( old , arr ) ; ds . getInputStream ( ) . close ( ) ; } }
public class aTest{ @Test public void testMerge ( ) { com . ctrip . platform . dal . dao . task . ShardedIntArrayResultMerger test = new com . ctrip . platform . dal . dao . task . ShardedIntArrayResultMerger ( ) ; try { test . recordPartial ( "1" , new java . lang . Integer [ ] { 0 , 1 , 2 , 3 } ) ; test . addPartial ( "1" , new int [ ] { 0 , 1 , 2 , 3 } ) ; test . recordPartial ( "2" , new java . lang . Integer [ ] { 4 , 5 , 6 } ) ; test . addPartial ( "2" , new int [ ] { 4 , 5 , 6 } ) ; test . recordPartial ( "3" , new java . lang . Integer [ ] { 7 , 8 , 9 } ) ; test . addPartial ( "3" , new int [ ] { 7 , 8 , 9 } ) ; int [ ] result = test . merge ( ) ; org . junit . Assert . assertArrayEquals ( new int [ ] { 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 } , result ) ; } }
public class aTest{ @Test public void testNullData ( ) { java . lang . String path = "/SIZE" ; org . apache . zookeeper . ZooKeeper zk = null ; zk = createClient ( ) ; try { zk . create ( path , null , Ids . OPEN_ACL_UNSAFE , CreateMode . PERSISTENT ) ; zk . exists ( path , false ) ; zk . exists ( path , false , this , null ) ; cn . await ( 10 , TimeUnit . SECONDS ) ; org . junit . Assert . assertSame ( 0L , cn . getCount ( ) ) ; } }
public class aTest{ @Test public void testSubstringNotEqual ( ) { long ts = nextTimestamp ( ) ; java . lang . String tenantId = getOrganizationId ( ) ; java . lang . String query = "SELECT<sp>organization_id,<sp>date,<sp>feature<sp>FROM<sp>PRODUCT_METRICS<sp>WHERE<sp>organization_id=?<sp>AND<sp>date<sp>><sp>to_date(?)" ; java . lang . String url = ( ( ( ( getUrl ( ) ) + ";" ) + ( org . apache . phoenix . util . PhoenixRuntime . CURRENT_SCN_ATTRIB ) ) + "=" ) + ( ts + 5 ) ; java . util . Properties props = org . apache . phoenix . util . PropertiesUtil . deepCopy ( org . apache . phoenix . end2end . TEST_PROPERTIES ) ; java . sql . Connection conn = java . sql . DriverManager . getConnection ( url , props ) ; try { org . apache . phoenix . end2end . ProductMetricsIT . initTableValues ( tenantId , org . apache . phoenix . end2end . ProductMetricsIT . getSplits ( tenantId ) , ts ) ; java . sql . PreparedStatement statement = conn . prepareStatement ( query ) ; statement . setString ( 1 , tenantId . substring ( 0 , 3 ) ) ; statement . setString ( 2 , org . apache . phoenix . end2end . ProductMetricsIT . DS4 ) ; java . sql . ResultSet rs = statement . executeQuery ( ) ; org . junit . Assert . assertFalse ( rs . next ( ) ) ; } }
public class aTest{ @Test public void testShouldValidationFailWhenNoHostGroupConfigurationProvidedAndRequiredPropertiesAreMissing ( ) { topologyConfigurationMap . put ( "kerberos-env" , new java . util . HashMap ( ) ) ; topologyConfigurationMap . get ( "kerberos-env" ) . put ( "realm" , "etwas" ) ; topologyConfigurationMap . put ( "krb5-conf" , new java . util . HashMap ( ) ) ; topologyConfigurationMap . get ( "krb5-conf" ) . put ( "master" 1 , "smthg" ) ; missingProps . put ( "master" , java . util . Collections . singletonList ( "kdc_type" ) ) ; missingProps . put ( "slave" , java . util . Collections . singletonList ( "kdc_type" ) ) ; replayAll ( ) ; java . lang . String expectedMsg = java . lang . String . format ( "Missing<sp>required<sp>properties.<sp>Specify<sp>a<sp>value<sp>for<sp>these<sp>properties<sp>in<sp>the<sp>blueprint<sp>or<sp>cluster<sp>creation<sp>template<sp>configuration.<sp>%s" , missingProps ) ; java . lang . String actualMsg = "" ; try { testSubject . validate ( clusterTopologyMock ) ; } catch ( org . apache . ambari . server . topology . InvalidTopologyException e ) { actualMsg = e . getMessage ( ) ; } org . junit . Assert . assertEquals ( "master" 0 , expectedMsg , actualMsg ) ; } }
public class aTest{ @Test public void testTraverse ( ) { System . out . println ( "test:traverse" ) ; org . glassfish . flashlight . datatree . TreeNode server = setupComplexTree ( ) ; java . util . List < org . glassfish . flashlight . datatree . TreeNode > list = server . traverse ( false ) ; java . lang . String [ ] expected = new java . lang . String [ 7 ] ; expected [ 0 ] = "server" ; expected [ 1 ] = "server.wto" ; expected [ 2 ] = "server" 0 ; expected [ 3 ] = "server.wto.wtoson.wtosonsdaughter" ; expected [ 4 ] = "server.wto.wtoson.wtosonsson" ; expected [ 5 ] = "server.wto.wtodaughter" ; expected [ 6 ] = "server.wto.wtodaughter.wtodaughtersdaughter" ; java . lang . String [ ] actual = new java . lang . String [ 7 ] ; org . junit . Assert . assertEquals ( expected . length , list . size ( ) ) ; } }
public class aTest{ @Test public void testVaccineSets ( ) { java . lang . String code = "" ; try { code = _setupTestVaccine ( true ) ; _checkVaccineIntoDb ( code ) ; } catch ( java . lang . Exception e ) { e . printStackTrace ( ) ; org . junit . Assert . assertEquals ( true , false ) ; } }
public class aTest{ @Test public void testTagBasedPolicyForTable ( ) { java . lang . String url = "jdbc:hive2://localhost:" + ( org . apache . ranger . services . hive . HIVERangerAuthorizerTest . port ) ; java . sql . Connection connection = java . sql . DriverManager . getConnection ( url , "admin" , "admin" ) ; java . sql . Statement statement = connection . createStatement ( ) ; statement . execute ( "SELECT<sp>*<sp>FROM<sp>words" 4 ) ; statement . close ( ) ; connection . close ( ) ; final java . lang . String tableUrl = ( "jdbc:hive2://localhost:" + ( org . apache . ranger . services . hive . HIVERangerAuthorizerTest . port ) ) + "SELECT<sp>*<sp>FROM<sp>words" 7 ; connection = java . sql . DriverManager . getConnection ( tableUrl , "admin" , "admin" ) ; statement = connection . createStatement ( ) ; statement . execute ( "CREATE<sp>TABLE<sp>WORDS<sp>(word<sp>STRING,<sp>count<sp>INT)" ) ; statement . execute ( "CREATE<sp>TABLE<sp>WORDS2<sp>(word<sp>STRING,<sp>count<sp>INT)" ) ; statement . close ( ) ; connection . close ( ) ; org . apache . hadoop . security . UserGroupInformation ugi = org . apache . hadoop . security . UserGroupInformation . createUserForTesting ( "alice" , new java . lang . String [ ] { "SELECT<sp>*<sp>FROM<sp>words" 1 } ) ; ugi . doAs ( new java . security . PrivilegedExceptionAction < java . lang . Void > ( ) { public org . apache . ranger . services . hive . Void run ( ) throws org . apache . ranger . services . hive . Exception { java . sql . Connection connection = java . sql . DriverManager . getConnection ( tableUrl , "alice" , "alice" ) ; java . sql . Statement statement = connection . createStatement ( ) ; java . sql . ResultSet resultSet = statement . executeQuery ( "SELECT<sp>*<sp>FROM<sp>words" ) ; org . junit . Assert . assertNotNull ( resultSet ) ; statement . close ( ) ; statement = connection . createStatement ( ) ; try { statement . executeQuery ( "SELECT<sp>*<sp>FROM<sp>words" 0 ) ; org . junit . Assert . fail ( "SELECT<sp>*<sp>FROM<sp>words" 2 ) ; } }
public class aTest{ @Test public void testCheckMagicNumberReadOnly ( ) { com . orientechnologies . orient . core . db . OrientDBConfig config = com . orientechnologies . orient . core . db . OrientDBConfig . builder ( ) . addConfig ( OGlobalConfiguration . STORAGE_CHECKSUM_MODE , OChecksumMode . StoreAndSwitchReadOnlyMode ) . addAttribute ( ODatabase . ATTRIBUTES . MINIMUMCLUSTERS , 1 ) . build ( ) ; orientDB = new com . orientechnologies . orient . core . db . OrientDB ( ( "embedded:" + ( com . orientechnologies . orient . core . storage . OStorageTestIT . buildPath . toFile ( ) . getAbsolutePath ( ) ) ) , config ) ; orientDB . create ( com . orientechnologies . orient . core . storage . OStorageTestIT . class . getSimpleName ( ) , ODatabaseType . PLOCAL , config ) ; com . orientechnologies . orient . core . db . ODatabaseSession session = orientDB . open ( com . orientechnologies . orient . core . storage . OStorageTestIT . class . getSimpleName ( ) , "admin" , "admin" , config ) ; com . orientechnologies . orient . core . metadata . OMetadata metadata = session . getMetadata ( ) ; com . orientechnologies . orient . core . metadata . schema . OSchema schema = metadata . getSchema ( ) ; schema . createClass ( "PageBreak" ) ; for ( int i = 0 ; i < 10 ; i ++ ) { com . orientechnologies . orient . core . record . impl . ODocument document = new com . orientechnologies . orient . core . record . impl . ODocument ( "PageBreak" ) ; document . field ( "value" , "value" ) ; document . save ( ) ; } com . orientechnologies . orient . core . storage . disk . OLocalPaginatedStorage storage = ( ( com . orientechnologies . orient . core . storage . disk . OLocalPaginatedStorage ) ( ( ( com . orientechnologies . orient . core . db . ODatabaseDocumentInternal ) ( session ) ) . getStorage ( ) ) ) ; com . orientechnologies . orient . core . storage . cache . OWriteCache wowCache = storage . getWriteCache ( ) ; com . orientechnologies . orient . core . db . OSharedContext ctx = ( ( com . orientechnologies . orient . core . db . ODatabaseDocumentInternal ) ( session ) ) . getSharedContext ( ) ; session . close ( ) ; final java . nio . file . Path storagePath = storage . getStoragePath ( ) ; long fileId = wowCache . fileIdByName ( "pagebreak.pcl" ) ; java . lang . String nativeFileName = wowCache . nativeFileNameById ( fileId ) ; storage . close ( true , false ) ; ctx . close ( ) ; int position = ( com . orientechnologies . orient . core . storage . fs . OFileClassic . HEADER_SIZE ) + ( com . orientechnologies . orient . core . storage . impl . local . paginated . base . ODurablePage . MAGIC_NUMBER_OFFSET ) ; java . io . RandomAccessFile file = new java . io . RandomAccessFile ( storagePath . resolve ( nativeFileName ) . toFile ( ) , "rw" ) ; file . seek ( position ) ; file . write ( 1 ) ; file . close ( ) ; session = orientDB . open ( com . orientechnologies . orient . core . storage . OStorageTestIT . class . getSimpleName ( ) , "admin" , "admin" ) ; session . query ( "select<sp>from<sp>PageBreak" ) . close ( ) ; java . lang . Thread . sleep ( 100 ) ; com . orientechnologies . orient . core . record . impl . ODocument document = new com . orientechnologies . orient . core . record . impl . ODocument ( "PageBreak" ) ; document . field ( "value" , "value" ) ; try { document . save ( ) ; org . junit . Assert . fail ( ) ; } catch ( com . orientechnologies . orient . core . exception . OPageIsBrokenException e ) { org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testLargeEmptinessSAT ( ) { int sizeTot = 4 ; for ( int size = 2 ; size < sizeTot ; size ++ ) { System . out . println ( size ) ; theory . sat . SATBooleanAlgebra ba = new theory . sat . SATBooleanAlgebra ( ( size + 1 ) ) ; logic . ltl . LTLFormula < java . lang . Integer , boolean [ ] > tot = new logic . ltl . True ( ) ; java . util . List < logic . ltl . LTLFormula < java . lang . Integer , boolean [ ] > > conjuncts = new java . util . LinkedList ( ) ; for ( int i = 1 ; i < size ; i ++ ) { conjuncts . add ( new logic . ltl . Eventually ( new logic . ltl . Predicate < java . lang . Integer , boolean [ ] > ( i ) ) ) ; } tot = new logic . ltl . And ( conjuncts ) ; long startTime = java . lang . System . currentTimeMillis ( ) ; automata . safa . SAFA < java . lang . Integer , boolean [ ] > safa1 = tot . getSAFA ( ba ) ; long stopTime = java . lang . System . currentTimeMillis ( ) ; startTime = java . lang . System . currentTimeMillis ( ) ; boolean b = true ; try { b = automata . safa . SAFA . isEmpty ( safa1 , ba ) ; org . junit . Assert . assertFalse ( b ) ; } }
public class aTest{ @Test public void testCreateTwoSubscriptionsBasedOnTwoTechnicalServices_OnBehalf ( ) { java . lang . String subId1 = java . lang . Long . toString ( java . lang . System . currentTimeMillis ( ) ) ; java . lang . String subId2 = java . lang . Long . toString ( ( ( java . lang . System . currentTimeMillis ( ) ) + 1 ) ) ; org . oscm . internal . vo . VOSubscription sub1 = subMgmt . subscribeToService ( org . oscm . test . data . Subscriptions . createVOSubscription ( subId1 ) , org . oscm . serviceprovisioningservice . assembler . ProductAssembler . toVOProduct ( testPrdOnbehalf , new org . oscm . i18nservice . bean . LocalizerFacade ( localizer , "en" ) ) , null , null , null , new java . util . ArrayList < org . oscm . internal . vo . VOUda > ( ) ) ; final long sub1Key = sub1 . getKey ( ) ; subMgmt . subscribeToService ( org . oscm . test . data . Subscriptions . createVOSubscription ( subId2 ) , org . oscm . serviceprovisioningservice . assembler . ProductAssembler . toVOProduct ( testPrd2Onbehalf , new org . oscm . i18nservice . bean . LocalizerFacade ( localizer , "en" ) ) , null , null , null , new java . util . ArrayList < org . oscm . internal . vo . VOUda > ( ) ) ; runTX ( new java . util . concurrent . Callable < java . lang . Void > ( ) { @ org . oscm . subscriptionservice . bean . Override public org . oscm . subscriptionservice . bean . Void call ( ) throws org . oscm . subscriptionservice . bean . Exception { org . oscm . domobjects . Subscription subscription = mgr . find ( org . oscm . domobjects . Subscription . class , sub1Key ) ; org . oscm . domobjects . Organization sourceOrganization = subscription . getProduct ( ) . getTechnicalProduct ( ) . getOrganization ( ) ; org . oscm . domobjects . Organization targetOrganization = subscription . getOrganization ( ) ; org . junit . Assert . assertNotNull ( getOrganizationReference ( sourceOrganization , targetOrganization ) ) ; return null ; } } }
public class aTest{ @Test public void testIncludeMacroWhenInvalidSectionSpecified ( ) { org . xwiki . rendering . macro . include . IncludeMacroParameters parameters = new org . xwiki . rendering . macro . include . IncludeMacroParameters ( ) ; parameters . setReference ( "document" ) ; parameters . setSection ( "unknown" ) ; final org . xwiki . rendering . transformation . MacroTransformationContext macroContext = createMacroTransformationContext ( "whatever" , false ) ; final org . xwiki . model . reference . DocumentReference resolvedReference = new org . xwiki . model . reference . DocumentReference ( "wiki" , "space" , "document" ) ; final org . xwiki . bridge . DocumentModelBridge mockDocument = getMockery ( ) . mock ( org . xwiki . bridge . DocumentModelBridge . class ) ; getMockery ( ) . checking ( new org . jmock . Expectations ( ) { { oneOf ( mockEntityReferenceResolver ) . resolve ( "document" , EntityType . DOCUMENT , macroContext . getCurrentMacroBlock ( ) ) ; will ( returnValue ( resolvedReference ) ) ; oneOf ( mockContextualAuthorization ) . hasAccess ( with ( Right . VIEW ) , with ( resolvedReference ) ) ; will ( returnValue ( true ) ) ; oneOf ( mockSetup . bridge ) . getDocumentInstance ( ( ( org . xwiki . model . reference . EntityReference ) ( resolvedReference ) ) ) ; will ( returnValue ( mockDocument ) ) ; allowing ( mockSetup . bridge ) . getTranslatedDocumentInstance ( resolvedReference ) ; will ( returnValue ( mockDocument ) ) ; oneOf ( mockSetup . bridge ) . getCurrentDocumentReference ( ) ; will ( returnValue ( new org . xwiki . model . reference . DocumentReference ( "wiki" , "Space" , "IncludingPage" ) ) ) ; oneOf ( mockDocument ) . getSyntax ( ) ; will ( returnValue ( Syntax . XWIKI_2_0 ) ) ; oneOf ( mockDocument ) . getXDOM ( ) ; will ( returnValue ( getXDOM ( "content" ) ) ) ; allowing ( mockDocument ) . getDocumentReference ( ) ; will ( returnValue ( resolvedReference ) ) ; allowing ( mockDocument ) . getRealLanguage ( ) ; will ( returnValue ( "" ) ) ; } } ) ; try { this . includeMacro . execute ( parameters , null , macroContext ) ; org . junit . Assert . fail ( "Should<sp>have<sp>raised<sp>an<sp>exception" ) ; } catch ( org . xwiki . rendering . macro . MacroExecutionException expected ) { org . junit . Assert . assertEquals ( "document" 0 , expected . getMessage ( ) ) ; } } }
public class aTest{ @Test public void tooSmallAWindowWithOverflowHiddenIsNotAProblem ( ) { org . openqa . selenium . WebDriver . Window window = driver . manage ( ) . window ( ) ; org . openqa . selenium . Dimension originalSize = window . getSize ( ) ; try { window . setSize ( new org . openqa . selenium . Dimension ( 1024 , 500 ) ) ; java . lang . String url = appServer . whereIs ( "overflow-body.html" ) ; driver . get ( url ) ; org . openqa . selenium . WebElement element = driver . findElement ( org . openqa . selenium . By . name ( "resultsFrame" ) ) ; org . junit . Assert . assertTrue ( element . isDisplayed ( ) ) ; } }
public class aTest{ @Test public void testRegister ( ) { javax . management . ObjectName objectName = null ; try { counter = 23 ; objectName = org . apache . hadoop . metrics2 . util . MBeans . register ( "UnitTest" , "RegisterTest" , this ) ; javax . management . MBeanServer platformMBeanServer = java . lang . management . ManagementFactory . getPlatformMBeanServer ( ) ; int jmxCounter = ( ( int ) ( platformMBeanServer . getAttribute ( objectName , "Counter" ) ) ) ; org . junit . Assert . assertEquals ( counter , jmxCounter ) ; } }
public class aTest{ @Test public void getUrlModuleGoodCase ( ) { final java . lang . String deviceId = "HostName=" 2 ; final java . lang . String moduleId = "somemodule" ; final java . lang . String iotHubName = "b.c.d" ; final java . lang . String hostName = "HostName=" 0 + iotHubName ; final java . lang . String sharedAccessKeyName = "ACCESSKEYNAME" ; final java . lang . String policyName = "SharedAccessKey" ; final java . lang . String sharedAccessKey = "1234567890abcdefghijklmnopqrstvwxyz=" ; final java . lang . String connectionString = ( ( ( ( ( ( "HostName=" + hostName ) + ";SharedAccessKeyName=" ) + sharedAccessKeyName ) + ";" ) + policyName ) + "=" ) + sharedAccessKey ; final com . microsoft . azure . sdk . iot . service . IotHubConnectionString iotHubConnectionString = com . microsoft . azure . sdk . iot . service . IotHubConnectionStringBuilder . createConnectionString ( connectionString ) ; final java . lang . String expected = "https://HOSTNAME.b.c.d/devices/xxx-device/modules/somemodule?" + ( tests . unit . com . microsoft . azure . sdk . iot . service . IotHubConnectionStringTest . URL_API_VERSION ) ; java . lang . String actual = iotHubConnectionString . getUrlModule ( deviceId , moduleId ) . toString ( ) ; org . junit . Assert . assertEquals ( "HostName=" 1 , expected , actual ) ; } }
public class aTest{ @Test public void testSelectAvgGroupbyBaseTable ( ) { org . verdictdb . core . sqlobject . BaseTable base = new org . verdictdb . core . sqlobject . BaseTable ( "myschema" , "mytable" , "t" ) ; java . lang . String aliasName = ",<sp>" 4 ; org . verdictdb . core . sqlobject . SelectQuery relation = org . verdictdb . core . sqlobject . SelectQuery . create ( java . util . Arrays . < org . verdictdb . core . sqlobject . SelectItem > asList ( new org . verdictdb . core . sqlobject . AliasedColumn ( new org . verdictdb . core . sqlobject . BaseColumn ( "t" , "mygroup" ) , "mygroup" ) , new org . verdictdb . core . sqlobject . AliasedColumn ( new org . verdictdb . core . sqlobject . ColumnOp ( "mygroup" 3 , new org . verdictdb . core . sqlobject . BaseColumn ( "t" , "mycolumn1" ) ) , aliasName ) ) , base ) ; relation . addGroupby ( new org . verdictdb . core . sqlobject . BaseColumn ( "t" , "mygroup" ) ) ; org . verdictdb . core . scrambling . ScrambleMetaSet meta = generateTestScrambleMeta ( ) ; org . verdictdb . core . rewriter . query . AggQueryRewriter rewriter = new org . verdictdb . core . rewriter . query . AggQueryRewriter ( meta ) ; java . util . List < org . apache . commons . lang3 . tuple . Pair < org . verdictdb . core . sqlobject . AbstractRelation , org . verdictdb . core . rewriter . query . AggblockMeta > > rewritten = rewriter . rewrite ( relation ) ; java . lang . String aliasForSumEstimate = org . verdictdb . core . rewriter . AliasRenamingRules . sumEstimateAliasName ( aliasName ) ; java . lang . String aliasForSumScaledSubsum = org . verdictdb . core . rewriter . AliasRenamingRules . sumScaledSumAliasName ( aliasName ) ; java . lang . String aliasForSumSquaredScaledSubsum = org . verdictdb . core . rewriter . AliasRenamingRules . sumSquaredScaledSumAliasName ( aliasName ) ; java . lang . String aliasForCountEstimate = org . verdictdb . core . rewriter . AliasRenamingRules . countEstimateAliasName ( aliasName ) ; java . lang . String aliasForSumScaledSubcount = org . verdictdb . core . rewriter . AliasRenamingRules . sumScaledCountAliasName ( aliasName ) ; java . lang . String aliasForSumSquaredScaledSubcount = org . verdictdb . core . rewriter . AliasRenamingRules . sumSquaredScaledCountAliasName ( aliasName ) ; java . lang . String aliasForCountSubsample = org . verdictdb . core . rewriter . AliasRenamingRules . countSubsampleAliasName ( ) ; java . lang . String aliasForSumSubsampleSize = org . verdictdb . core . rewriter . AliasRenamingRules . sumSubsampleSizeAliasName ( ) ; for ( int k = 0 ; k < ( aggblockCount ) ; k ++ ) { java . lang . String expected = ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "select<sp>verdictdbalias4.`verdictdbalias5`<sp>as<sp>`verdictdb:tier`,<sp>" + ( "verdictdbalias4.`verdictdbalias6`<sp>as<sp>`mygroup`,<sp>" + "mygroup" 0 ) ) + ( quoteAlias ( aliasForSumEstimate ) ) ) + ",<sp>" ) + ",<sp>" 3 ) + ( quoteAlias ( aliasForCountEstimate ) ) ) + ",<sp>" ) + ",<sp>" 5 ) + "mygroup" 5 ) + ( quoteAlias ( aliasForSumScaledSubsum ) ) ) + ",<sp>" ) + "verdictdbalias4.`verdictdbalias6`<sp>as<sp>`mygroup`,<sp>" 0 ) + "mygroup" 5 ) + ( quoteAlias ( aliasForSumSquaredScaledSubsum ) ) ) + ",<sp>" ) + "mygroup" 9 ) + "mygroup" 5 ) + ( quoteAlias ( aliasForSumScaledSubcount ) ) ) + ",<sp>" ) + "sum(pow(verdictdbalias4.`verdictdbalias8`,<sp>3))<sp>as<sp>" ) + ( quoteAlias ( aliasForSumSquaredScaledSubcount ) ) ) + ",<sp>" ) + "count(*)<sp>as<sp>" ) + ( quoteAlias ( aliasForCountSubsample ) ) ) + ",<sp>" ) + ",<sp>" 3 ) + ( quoteAlias ( aliasForSumSubsampleSize ) ) ) + ",<sp>" 7 ) + "from<sp>(select<sp>" ) + "verdictdbalias1.`verdictdbalias3`<sp>as<sp>`verdictdbalias5`,<sp>" ) + "verdictdbalias4.`verdictdbalias6`<sp>as<sp>`mygroup`,<sp>" 1 ) + "mygroup" 4 ) + ",<sp>" 8 ) + "mygroup" 2 ) + "t.`verdictdbsid`<sp>as<sp>`verdictdbalias2`,<sp>" ) + ",<sp>" 1 ) + "from<sp>`myschema`.`mytable`<sp>as<sp>t<sp>" ) + ",<sp>" 6 ) + k ) + "mygroup" 7 ) + ",<sp>" 9 ) + "mygroup" 6 ; org . verdictdb . sqlwriter . SelectQueryToSql relToSql = new org . verdictdb . sqlwriter . SelectQueryToSql ( new org . verdictdb . sqlsyntax . HiveSyntax ( ) ) ; java . lang . String actual = relToSql . toSql ( rewritten . get ( k ) . getLeft ( ) ) ; org . junit . Assert . assertEquals ( expected , actual ) ; } } }
public class aTest{ @Test public void connectionOrder ( ) { org . junit . Assume . assumeTrue ( ( ! ( initialGaleraUrl . contains ( "failover" ) ) ) ) ; org . mariadb . jdbc . UrlParser urlParser = org . mariadb . jdbc . UrlParser . parse ( initialGaleraUrl ) ; for ( int i = 0 ; i < ( urlParser . getHostAddresses ( ) . size ( ) ) ; i ++ ) { int serverNb ; try ( java . sql . Connection connection = getNewConnection ( true ) ) { serverNb = getServerId ( connection ) ; org . junit . Assert . assertTrue ( ( serverNb == ( i + 1 ) ) ) ; } }
public class aTest{ @Test public void testCreateXPageSessionExt ( ) { java . lang . String userName = org . openntf . domino . utils . Factory . getLocalServerName ( ) ; final long userHandle = com . ibm . domino . napi . c . NotesUtil . createUserNameList ( userName ) ; try { lotus . domino . Session xs = com . ibm . domino . napi . c . xsp . XSPNative . createXPageSessionExt ( userName , userHandle , false , true , false ) ; org . junit . Assert . assertEquals ( userName , xs . getEffectiveUserName ( ) ) ; xs . recycle ( ) ; } }
public class aTest{ @Test public void testGroupingFunctionInOverClauseOrderBy ( ) { java . lang . String sqlText = format ( ( "select<sp>a1,<sp>b1,<sp>grouping(a1),<sp>grouping(b1),<sp>row_number()<sp>over<sp>(partition<sp>by<sp>a1<sp>order<sp>by<sp>grouping(a1)+grouping(b1),b1)<sp>from<sp>t1<sp>--splice-properties<sp>useSpark=%s\n" + "group<sp>by<sp>rollup(a1,b1)<sp>order<sp>by<sp>1,5" ) , this . useSparkString ) ; java . sql . ResultSet rs = methodWatcher . executeQuery ( sqlText ) ; java . lang . String expected = "<sp>1<sp>|<sp>1<sp>|<sp>0<sp>|<sp>0<sp>|<sp>1<sp>|\n" 5 + ( ( ( ( ( ( ( ( ( ( ( "------------------------\n" + "<sp>1<sp>|<sp>1<sp>|<sp>0<sp>|<sp>0<sp>|<sp>1<sp>|\n" ) + "<sp>1<sp>|<sp>2<sp>|<sp>0<sp>|<sp>0<sp>|<sp>2<sp>|\n" ) + "<sp>1<sp>|<sp>1<sp>|<sp>0<sp>|<sp>0<sp>|<sp>1<sp>|\n" 1 ) + "<sp>1<sp>|NULL<sp>|<sp>0<sp>|<sp>1<sp>|<sp>4<sp>|\n" ) + "<sp>2<sp>|<sp>1<sp>|<sp>0<sp>|<sp>0<sp>|<sp>1<sp>|\n" ) + "<sp>1<sp>|<sp>1<sp>|<sp>0<sp>|<sp>0<sp>|<sp>1<sp>|\n" 3 ) + "<sp>2<sp>|NULL<sp>|<sp>0<sp>|<sp>1<sp>|<sp>3<sp>|\n" ) + "<sp>3<sp>|<sp>1<sp>|<sp>0<sp>|<sp>0<sp>|<sp>1<sp>|\n" ) + "<sp>1<sp>|<sp>1<sp>|<sp>0<sp>|<sp>0<sp>|<sp>1<sp>|\n" 4 ) + "<sp>1<sp>|<sp>1<sp>|<sp>0<sp>|<sp>0<sp>|<sp>1<sp>|\n" 2 ) + "NULL<sp>|NULL<sp>|<sp>1<sp>|<sp>1<sp>|<sp>1<sp>|" ) ; org . junit . Assert . assertEquals ( ( ( "<sp>1<sp>|<sp>1<sp>|<sp>0<sp>|<sp>0<sp>|<sp>1<sp>|\n" 0 + sqlText ) + "<sp>1<sp>|<sp>1<sp>|<sp>0<sp>|<sp>0<sp>|<sp>1<sp>|\n" 0 ) , expected , TestUtils . FormattedResult . ResultFactory . toStringUnsorted ( rs ) ) ; rs . close ( ) ; } }
public class aTest{ @Test public void testRenderTemplateForUpdateFileHeader ( ) { org . codehaus . mojo . license . api . FreeMarkerHelper helper = org . codehaus . mojo . license . api . FreeMarkerHelper . newDefaultHelper ( ) ; org . apache . maven . project . MavenProject project = new org . apache . maven . project . MavenProject ( ) ; project . setArtifact ( new org . apache . maven . artifact . DefaultArtifact ( "groupId" , "artifactId" , org . apache . maven . artifact . versioning . VersionRange . createFromVersionSpec ( "0" ) , "compile" , "projectName" 2 , "projectName" 0 , null ) ) ; project . setGroupId ( "groupId" ) ; project . setArtifactId ( "artifactId" ) ; project . setVersion ( "version" ) ; java . util . Map < java . lang . String , java . lang . Object > properties = new java . util . HashMap ( ) ; properties . put ( "project" , project ) ; properties . put ( "projectName" , "projectName" ) ; properties . put ( "projectName" 1 , "projectName" 1 ) ; properties . put ( "organizationName" , "organizationName" ) ; properties . put ( "addSvnKeyWords" , true ) ; java . lang . String s = helper . renderTemplate ( "/org/codehaus/mojo/license/default-file-header-description.ftl" , properties ) ; org . junit . Assert . assertEquals ( "projectName" 3 , s ) ; if ( org . codehaus . mojo . license . api . FreeMarkerHelperTest . log . isInfoEnabled ( ) ) { org . codehaus . mojo . license . api . FreeMarkerHelperTest . log . info ( s ) ; } } }
public class aTest{ @Test public void testComplexGet ( ) { java . lang . String rootDir = new java . io . File ( java . lang . System . getProperty ( "test.build.data" , "temp.txt" 0 ) ) . getAbsolutePath ( ) ; java . io . File testFile = null ; java . lang . String processIdInFile = ( org . apache . hadoop . util . Shell . WINDOWS ) ? "<sp>container_1353742680940_0002_01_000001<sp>" : "<sp>23<sp>" ; java . lang . String expectedProcessId = processIdInFile . trim ( ) ; try { testFile = new java . io . File ( rootDir , "temp.txt" ) ; java . io . PrintWriter fileWriter = new java . io . PrintWriter ( testFile ) ; fileWriter . println ( "<sp>" ) ; fileWriter . println ( "" ) ; fileWriter . println ( "abc" ) ; fileWriter . println ( "-123" ) ; fileWriter . println ( "-123<sp>" ) ; fileWriter . println ( processIdInFile ) ; fileWriter . println ( "6236" ) ; fileWriter . close ( ) ; java . lang . String processId = null ; processId = org . apache . hadoop . yarn . server . nodemanager . util . ProcessIdFileReader . getProcessId ( new org . apache . hadoop . fs . Path ( ( ( rootDir + ( org . apache . hadoop . fs . Path . SEPARATOR ) ) + "temp.txt" ) ) ) ; org . junit . Assert . assertEquals ( expectedProcessId , processId ) ; } }
public class aTest{ @Test public void testEmailUsingAuthWithNoUsernamePassword ( ) { com . emc . vipr . model . sys . eventhandler . ConnectEmcEmail email = new com . emc . vipr . model . sys . eventhandler . ConnectEmcEmail ( ) ; email . setEmailSender ( "DONOTREPLY@customer.com" ) ; email . setEmailServer ( "mailhub.lss.emc.com" ) ; email . setNotifyEmailAddress ( "joe.customer@customer.com" ) ; email . setPrimaryEmailAddress ( "emailalertesg@emc.com" ) ; email . setSafeEncryption ( "no" ) ; email . setSmtpAuthType ( "login" ) ; com . emc . storageos . model . property . PropertyInfoUpdate propInfo = null ; try { propInfo = ConfigService . ConfigureConnectEmc . configureEmail ( email ) ; } catch ( java . lang . Exception e ) { org . junit . Assert . assertNull ( propInfo ) ; return ; } }
public class aTest{ @Test public void testBadUsername_PreHashedClient ( ) { java . util . Map < java . lang . String , java . lang . Object > serverProps = new java . util . HashMap < java . lang . String , java . lang . Object > ( ) ; serverProps . put ( org . wildfly . security . sasl . digest . DigestTest . REALM_PROPERTY , "TestRealm" ) ; javax . security . sasl . SaslServer server = new org . wildfly . security . sasl . test . SaslServerBuilder ( org . wildfly . security . sasl . digest . DigestServerFactory . class , org . wildfly . security . sasl . digest . DigestTest . DIGEST ) . setUserName ( "George" ) . setPassword ( DigestPassword . ALGORITHM_DIGEST_MD5 , getDigestKeySpec ( "George" , "gpwd" , "TestRealm" ) ) . setProperties ( serverProps ) . setProtocol ( "TestProtocol" ) . setServerName ( "TestServer" ) . build ( ) ; javax . security . auth . callback . CallbackHandler clientCallback = org . wildfly . security . sasl . digest . DigestCallbackHandlerUtils . createDigestPwdClientCallbackHandler ( "Borris" , "gpwd" , "TestRealm" , null , "George" ) ; java . util . Map < java . lang . String , java . lang . Object > clientProps = new java . util . HashMap < java . lang . String , java . lang . Object > ( ) ; clientProps . put ( org . wildfly . security . sasl . digest . DigestTest . PRE_DIGESTED_PROPERTY , "true" ) ; javax . security . sasl . SaslClient client = javax . security . sasl . Sasl . createSaslClient ( new java . lang . String [ ] { org . wildfly . security . sasl . digest . DigestTest . DIGEST } , "George" , "TestProtocol" , "TestServer" , clientProps , clientCallback ) ; org . junit . Assert . assertFalse ( client . hasInitialResponse ( ) ) ; byte [ ] message = server . evaluateResponse ( new byte [ 0 ] ) ; try { client . evaluateChallenge ( message ) ; org . junit . Assert . fail ( "Expection<sp>exception<sp>not<sp>thrown." ) ; } }
public class aTest{ @Test public void testMvn1 ( ) { final org . eclipse . packagedrone . testing . server . channel . ChannelTester ct = org . eclipse . packagedrone . testing . server . channel . ChannelTester . create ( getWebContext ( ) , "deploy" 3 ) ; ct . addAspect ( "mvn" ) ; ct . addAspect ( "maven.repo" ) ; ct . assignDeployGroup ( "deploy" 3 ) ; final java . lang . String key = ct . getDeployKeys ( ) . iterator ( ) . next ( ) ; org . junit . Assert . assertNotNull ( key ) ; final org . eclipse . aether . RepositorySystem system = org . eclipse . packagedrone . testing . server . maven . MavenUtil . newRepositorySystem ( ) ; final org . eclipse . aether . RepositorySystemSession session = org . eclipse . packagedrone . testing . server . maven . MavenUtil . newRepositorySystemSession ( system ) ; org . eclipse . aether . artifact . Artifact jarArtifact = new org . eclipse . aether . artifact . DefaultArtifact ( "org.eclipse.packagedrone.testing" , "deploy" 2 , "" , "deploy" 1 , "0.0.1-SNAPSHOT" ) ; jarArtifact = jarArtifact . setFile ( new java . io . File ( org . eclipse . packagedrone . testing . server . channel . MavenTest . TEST_1_JAR ) ) ; org . eclipse . aether . artifact . Artifact pomArtifact = new org . eclipse . aether . util . artifact . SubArtifact ( jarArtifact , "" , "pom" ) ; pomArtifact = pomArtifact . setFile ( new java . io . File ( org . eclipse . packagedrone . testing . server . channel . MavenTest . TEST_1_POM ) ) ; org . eclipse . aether . artifact . Artifact srcArtifact = new org . eclipse . aether . util . artifact . SubArtifact ( jarArtifact , "sources" , "deploy" 1 ) ; srcArtifact = srcArtifact . setFile ( new java . io . File ( org . eclipse . packagedrone . testing . server . channel . MavenTest . TEST_1_SOURCES_JAR ) ) ; final org . eclipse . aether . util . repository . AuthenticationBuilder ab = new org . eclipse . aether . util . repository . AuthenticationBuilder ( ) ; ab . addUsername ( "deploy" ) ; ab . addPassword ( key ) ; final org . eclipse . aether . repository . Authentication auth = ab . build ( ) ; final org . eclipse . aether . repository . RemoteRepository distRepo = new org . eclipse . aether . repository . RemoteRepository . Builder ( "test" , "default" , resolve ( java . lang . String . format ( "deploy" 0 , ct . getId ( ) ) ) ) . setAuthentication ( auth ) . build ( ) ; final org . eclipse . aether . deployment . DeployRequest deployRequest = new org . eclipse . aether . deployment . DeployRequest ( ) ; deployRequest . addArtifact ( jarArtifact ) . addArtifact ( pomArtifact ) . addArtifact ( srcArtifact ) ; deployRequest . setRepository ( distRepo ) ; system . deploy ( session , deployRequest ) ; testUrl ( java . lang . String . format ( "deploy" 0 , ct . getId ( ) ) ) ; } }
public class aTest{ @Test public void bodyShouldBeSameWithThatOfHttpContent ( ) { byte [ ] body = new byte [ ] { 'h' , 'e' , 'l' , 'l' , 'o' } ; httpContent . setBody ( body ) ; com . android . volley . Request < java . lang . String > request = new com . navercorp . volleyextensions . volleyer . request . VolleyerRequest < java . lang . String > ( httpContent , clazz , responseParser , listener , errorListener ) ; try { org . junit . Assert . assertEquals ( httpContent . getBody ( ) , request . getBody ( ) ) ; } }
public class aTest{ @Test public void testGetStorageUnitEntityByBusinessObjectDataAndStorageStorageUnitNoExists ( ) { org . finra . herd . model . jpa . BusinessObjectDataEntity businessObjectDataEntity = new org . finra . herd . model . jpa . BusinessObjectDataEntity ( ) ; org . finra . herd . model . jpa . StorageEntity storageEntity = new org . finra . herd . model . jpa . StorageEntity ( ) ; storageEntity . setName ( org . finra . herd . service . helper . STORAGE_NAME ) ; when ( storageUnitDao . getStorageUnitByBusinessObjectDataAndStorage ( businessObjectDataEntity , storageEntity ) ) . thenReturn ( null ) ; when ( businessObjectDataHelper . businessObjectDataEntityAltKeyToString ( businessObjectDataEntity ) ) . thenReturn ( org . finra . herd . service . AbstractServiceTest . BUSINESS_OBJECT_DATA_KEY_AS_STRING ) ; try { storageUnitDaoHelper . getStorageUnitEntityByBusinessObjectDataAndStorage ( businessObjectDataEntity , storageEntity ) ; org . junit . Assert . fail ( ) ; } catch ( org . finra . herd . model . ObjectNotFoundException e ) { org . junit . Assert . assertEquals ( java . lang . String . format ( "Could<sp>not<sp>find<sp>storage<sp>unit<sp>in<sp>\"%s\"<sp>storage<sp>for<sp>the<sp>business<sp>object<sp>data<sp>{%s}." , org . finra . herd . service . helper . STORAGE_NAME , org . finra . herd . service . AbstractServiceTest . BUSINESS_OBJECT_DATA_KEY_AS_STRING ) , e . getMessage ( ) ) ; } }
public class aTest{ @Test public void testGetNominalLabelByCode ( ) { try { qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestNominalLabelResourceFacadeImp . entityManager . getTransaction ( ) . begin ( ) ; qa . qcri . aidr . dbmanager . dto . NominalLabelDTO result = qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestNominalLabelResourceFacadeImp . nominalLabelResourceFacadeImp . getNominalLabelByCode ( qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestNominalLabelResourceFacadeImp . nominalLabel . getNominalLabelCode ( ) ) ; qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestNominalLabelResourceFacadeImp . entityManager . getTransaction ( ) . commit ( ) ; org . junit . Assert . assertEquals ( qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestNominalLabelResourceFacadeImp . nominalLabel . getName ( ) , result . getName ( ) ) ; } }
public class aTest{ @Test public void testSetFocus ( ) { java . lang . String str = "" ; str += "package<sp>org.kie.test\n" ; str += "global<sp>java.util.List<sp>list\n" ; str += "rule<sp>rule1\n" ; str += "agenda-group<sp>\"badfocus\"" ; str += "when\n" ; str += "<sp>Integer(intValue<sp>><sp>0)\n" ; str += "then\n" ; str += "<sp>list.add(<sp>1<sp>);\n" ; str += "rule<sp>rule1\n" 3 ; str += "\n" ; org . kie . api . KieServices ks = KieServices . Factory . get ( ) ; org . kie . api . builder . KieFileSystem kfs = ks . newKieFileSystem ( ) . write ( "rule<sp>rule1\n" 1 , str ) ; ks . newKieBuilder ( kfs ) . buildAll ( ) ; org . kie . api . KieBase kbase = ks . newKieContainer ( ks . getRepository ( ) . getDefaultReleaseId ( ) ) . getKieBase ( ) ; org . kie . api . runtime . KieSession ksession = ks . getStoreServices ( ) . newKieSession ( kbase , null , env ) ; java . util . List < ? > list = new java . util . ArrayList < java . lang . Object > ( ) ; ksession . setGlobal ( "rule<sp>rule1\n" 0 , list ) ; ksession . insert ( 1 ) ; ksession . insert ( 2 ) ; ksession . insert ( 3 ) ; ksession . getAgenda ( ) . getAgendaGroup ( "rule<sp>rule1\n" 2 ) . setFocus ( ) ; ksession . fireAllRules ( ) ; org . junit . Assert . assertEquals ( 3 , list . size ( ) ) ; } }
public class aTest{ @Test public void testInvalidElementNames ( ) { java . util . stream . Stream . of ( "" , "foo" , "annotation-xml" , "foo-/" 0 , "-foo" , "foo-$" , "foo-/" , "FOO-BAR" , "foo/" , "l-unicorn" , "foo-" ) . forEach ( ( name ) -> org . junit . Assert . assertFalse ( java . lang . String . format ( "foo-/" 1 , name ) , com . vaadin . flow . internal . CustomElementNameValidator . isCustomElementName ( name ) ) ) ; } }
public class aTest{ @Test public void shouldFindPostsWithAuthorIdUsingDynamicSql ( ) { org . apache . ibatis . session . SqlSession session = org . apache . ibatis . session . SqlSessionTest . sqlMapper . openSession ( ) ; try { java . util . List < org . apache . ibatis . domain . blog . Post > posts = session . selectList ( "org.apache.ibatis.domain.blog.mappers.PostMapper.findPost" , new java . util . HashMap < java . lang . String , java . lang . Integer > ( ) { { put ( "author_id" , 101 ) ; } } ) ; org . junit . Assert . assertEquals ( 3 , posts . size ( ) ) ; } }
public class aTest{ @Test public void shouldAllowAddingAndRemovingMixinsOnlyIfNodeIsEmpty ( ) { javax . jcr . Node collection = session . getRootNode ( ) . addNode ( "collection" ) ; collection . addNode ( "child" ) ; session . save ( ) ; java . lang . String mixin = ModeShapeLexicon . LARGE_UNORDERED_COLLECTION . getString ( ) ; try { collection . addMixin ( mixin ) ; org . junit . Assert . fail ( "Unordered<sp>collection<sp>mixin<sp>should<sp>not<sp>be<sp>allowed<sp>if<sp>node<sp>has<sp>children" ) ; } catch ( javax . jcr . nodetype . ConstraintViolationException e ) { } session . getNode ( "/collection/child" ) . remove ( ) ; session . save ( ) ; collection . addMixin ( mixin ) ; collection . addNode ( "child1" ) ; collection . addNode ( "child2" ) ; session . save ( ) ; javax . jcr . NodeIterator children = collection . getNodes ( ) ; org . junit . Assert . assertEquals ( 2 , children . getSize ( ) ) ; try { collection . removeMixin ( mixin ) ; org . junit . Assert . fail ( "Unordered<sp>collection<sp>mixin<sp>should<sp>not<sp>be<sp>removed<sp>if<sp>node<sp>has<sp>children" ) ; } }
public class aTest{ @Test public void testRecursiveParam ( ) { final org . apache . hadoop . hdfs . web . resources . RecursiveParam p = new org . apache . hadoop . hdfs . web . resources . RecursiveParam ( RecursiveParam . DEFAULT ) ; org . junit . Assert . assertEquals ( false , p . getValue ( ) ) ; new org . apache . hadoop . hdfs . web . resources . RecursiveParam ( "falSe" ) ; try { new org . apache . hadoop . hdfs . web . resources . RecursiveParam ( "abc" ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void testExpression2 ( ) { java . lang . String expression = oldExpressions [ 2 ] ; try { java . util . List list = org . eclipse . birt . data . engine . expression . ExpressionCompilerUtilTest . extractColumnExpression ( new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( expression ) ) ; org . junit . Assert . assertTrue ( ( ( list . size ( ) ) == 1 ) ) ; } }
public class aTest{ @Test public void CellACLprivilegePrivilegetrue ( ) { java . lang . String aclString = "<D:acl<sp>xmlns:D='DAV:'<sp>xml:base='https://fqdn/aclTest/__role/__/'>" + ( ( ( ( ( ( ( ( ( ( "<D:ace>" + "<D:principal>" ) + "<D:all/>" ) + "</D:principal>" ) + "<D:privilege>" 1 ) + "<D:privilege>" ) + "<D:auth-read/>" ) + "</D:privilege>" ) + "<D:privilege>" 0 ) + "</D:ace>" ) + "</D:acl>" ) ; com . fujitsu . dc . core . model . jaxb . Acl aclToSet = null ; java . io . Reader reader = new java . io . StringReader ( aclString ) ; aclToSet = com . fujitsu . dc . core . model . jaxb . ObjectIo . unmarshal ( reader , com . fujitsu . dc . core . model . jaxb . Acl . class ) ; org . junit . Assert . assertTrue ( aclToSet . validateAcl ( true ) ) ; } }
public class aTest{ @Test public void testSaveClashingGAVForced ( ) { when ( pathToPom . toURI ( ) ) . thenReturn ( "default://p0/pom.xml" ) ; final org . kie . workbench . common . screens . projecteditor . model . ProjectScreenModel model = new org . kie . workbench . common . screens . projecteditor . model . ProjectScreenModel ( ) ; model . setPOM ( pom ) ; model . setPOMMetaData ( pomMetaData ) ; model . setPathToPOM ( pathToPom ) ; model . setKModule ( kmodule ) ; model . setKModuleMetaData ( kmoduleMetaData ) ; model . setPathToKModule ( pathToKieModule ) ; model . setProjectImports ( projectImports ) ; model . setProjectImportsMetaData ( projectImportsMetaData ) ; model . setPathToImports ( pathToModuleImports ) ; model . setRepositories ( moduleRepositories ) ; model . setPathToRepositories ( pathToModuleRepositories ) ; final org . guvnor . common . services . project . model . MavenRepositoryMetadata repositoryMetadata = new org . guvnor . common . services . project . model . MavenRepositoryMetadata ( "id" , "url" , org . guvnor . common . services . project . model . MavenRepositorySource . LOCAL ) ; moduleRepositories . getRepositories ( ) . add ( new org . guvnor . common . services . project . model . ModuleRepositories . ModuleRepository ( true , repositoryMetadata ) ) ; when ( repositoryResolver . getRepositoriesResolvingArtifact ( eq ( gav ) , eq ( module ) , eq ( repositoryMetadata ) ) ) . thenReturn ( new java . util . HashSet < org . guvnor . common . services . project . model . MavenRepositoryMetadata > ( ) { { add ( repositoryMetadata ) ; } } ) ; final java . lang . String comment = "comment" ; final org . guvnor . common . services . project . model . WorkspaceProject projectToBeReturned = new org . guvnor . common . services . project . model . WorkspaceProject ( ) ; doReturn ( projectToBeReturned ) . when ( projectService ) . resolveProject ( pathToPom ) ; try { final org . guvnor . common . services . project . model . WorkspaceProject project = service . save ( pathToPom , model , comment , DeploymentMode . FORCED ) ; org . junit . Assert . assertEquals ( projectToBeReturned , project ) ; } }
public class aTest{ @Test public void createTask ( ) { org . searchisko . api . tasker . TaskManager tested = getTested ( ) ; java . util . Map < java . lang . String , java . lang . Object > taskConfig = new java . util . HashMap < java . lang . String , java . lang . Object > ( ) ; org . mockito . Mockito . when ( tested . taskFactory . createTask ( "tasktype" , taskConfig ) ) . thenReturn ( org . mockito . Mockito . mock ( org . searchisko . api . tasker . Task . class ) ) ; org . mockito . Mockito . when ( tested . taskPersister . createTask ( "tasktype" , taskConfig ) ) . thenReturn ( "myid" ) ; org . junit . Assert . assertEquals ( "myid" , tested . createTask ( "tasktype" , taskConfig ) ) ; org . mockito . Mockito . verify ( tested . taskRunner ) . notifyNewTaskAvailableForRun ( ) ; org . mockito . Mockito . reset ( tested . taskFactory , tested . taskPersister , tested . taskRunner ) ; org . mockito . Mockito . when ( tested . taskFactory . createTask ( "tasktype" , taskConfig ) ) . thenThrow ( new org . searchisko . api . tasker . UnsupportedTaskException ( "" ) ) ; try { tested . createTask ( "tasktype" , taskConfig ) ; org . junit . Assert . fail ( "UnsupportedTaskException<sp>expected" ) ; } }
public class aTest{ @Test public void testArrayNotPresent ( ) { com . couchbase . jdbc . JDBCTestUtils . setConnection ( null ) ; java . lang . String drop_primary_index = "drop<sp>primary<sp>index<sp>on<sp>default" ; com . couchbase . jdbc . JDBCTestUtils . createPrimaryIndexes ( TestUtil . clusterInfo . bucketInformation . keySet ( ) ) ; org . json . simple . JSONObject obj = new org . json . simple . JSONObject ( ) ; java . lang . String deleteData = "delete<sp>from<sp>default" ; com . couchbase . jdbc . JDBCTestUtils . runQueryWithoutResult ( deleteData ) ; java . util . HashMap < java . lang . String , java . lang . Object > map = new java . util . HashMap < java . lang . String , java . lang . Object > ( ) ; map . put ( "name" , "crap" ) ; obj . putAll ( map ) ; org . json . simple . JSONArray expectedArray = new org . json . simple . JSONArray ( ) ; java . util . HashMap < java . lang . String , org . json . simple . JSONObject > objMap = new java . util . HashMap < java . lang . String , org . json . simple . JSONObject > ( ) ; objMap . put ( "1" , obj ) ; expectedArray . add ( obj ) ; com . couchbase . jdbc . JDBCTestUtils . insertData ( objMap , "default" ) ; java . lang . Thread . sleep ( 1000 ) ; java . lang . String query = "select<sp>name<sp>from<sp>default" ; com . couchbase . jdbc . JDBCTestUtils . setConnection ( null ) ; try ( java . sql . Statement stmt = JDBCTestUtils . con . createStatement ( ) ) { try ( java . sql . ResultSet rs = stmt . executeQuery ( query ) ) { com . couchbase . jdbc . CBResultSet cbrs = ( ( com . couchbase . jdbc . CBResultSet ) ( rs ) ) ; while ( cbrs . next ( ) ) { java . sql . ResultSetMetaData meta = cbrs . getMetaData ( ) ; com . couchbase . json . SQLJSON jsonVal = cbrs . getSQLJSON ( 1 ) ; try { jsonVal . getArray ( ) ; } catch ( java . sql . SQLException e ) { java . lang . String expectatedMessage = "value<sp>crap<sp>not<sp>a<sp>array" ; org . junit . Assert . assertEquals ( expectatedMessage . trim ( ) , e . getMessage ( ) . trim ( ) ) ; } } } } }
public class aTest{ @Test public void testCharAt$1_byte ( ) { System . out . println ( "testCharAt$1_byte" ) ; int index = 0 ; java . nio . charset . Charset cs = java . nio . charset . Charset . forName ( org . netbeans . modules . search . matcher . BufferedCharSequenceTest . UTF_8 ) ; for ( org . netbeans . modules . search . matcher . BufferedCharSequenceTest . TypeOfStream stype : org . netbeans . modules . search . matcher . BufferedCharSequenceTest . TypeOfStream . values ( ) ) { try { java . io . InputStream stream = getInputStream ( stype , org . netbeans . modules . search . matcher . BufferedCharSequenceTest . TypeOfContent . BYTE_1 , cs ) ; org . netbeans . modules . search . matcher . BufferedCharSequence instance = new org . netbeans . modules . search . matcher . BufferedCharSequence ( stream , cs . newDecoder ( ) , 1 ) ; char expResult = 'a' ; char result = instance . charAt ( index ) ; org . junit . Assert . assertEquals ( expResult , result ) ; } }
public class aTest{ @Test public void shouldCommitSuccessSuccess ( ) { try { wrapperService . composeSuccessThenSuccess ( ) ; org . junit . Assert . assertEquals ( 2 , countNodes ( ) ) ; } }
public class aTest{ @Test public void testCsvColumnSelectionCommasInsideQuotes ( ) { java . util . List < org . apache . drill . exec . rpc . user . QueryDataBatch > actualResults = testSqlWithResults ( ( "SELECT<sp>columns[0]<sp>as<sp>col1,<sp>columns[1]<sp>as<sp>col2,<sp>columns[2]<sp>as<sp>col3," + "columns[3]<sp>as<sp>col4<sp>from<sp>cp.`store/text/data/letters.csv`" ) ) ; final org . apache . drill . exec . store . text . TestResultSet expectedResultSet = new org . apache . drill . exec . store . text . TestResultSet ( ) ; expectedResultSet . addRow ( "c" 0 , "c" , "d,,<sp>\\n<sp>e" , "f\\\"g" ) ; expectedResultSet . addRow ( "d,<sp>e," , "f" , "g,,<sp>\\n<sp>h" , "i\\\"c" 1 ) ; expectedResultSet . addRow ( "g,<sp>h," , "i" , "j,,<sp>\\n<sp>k" , "l\\\"m" ) ; org . apache . drill . exec . store . text . TestResultSet actualResultSet = new org . apache . drill . exec . store . text . TestResultSet ( actualResults ) ; org . junit . Assert . assertEquals ( expectedResultSet , actualResultSet ) ; } }
public class aTest{ @Test public void testConfusionMatrixString ( ) { org . nd4j . evaluation . classification . Evaluation e = new org . nd4j . evaluation . classification . Evaluation ( org . nd4j . evaluation . Arrays . asList ( "a" , "b" , "c" ) ) ; org . nd4j . linalg . api . ndarray . INDArray class0 = org . nd4j . linalg . factory . Nd4j . create ( new double [ ] { 1 , 0 , 0 } , new long [ ] { 1 , 3 } ) ; org . nd4j . linalg . api . ndarray . INDArray class1 = org . nd4j . linalg . factory . Nd4j . create ( new double [ ] { 0 , 1 , 0 } , new long [ ] { 1 , 3 } ) ; org . nd4j . linalg . api . ndarray . INDArray class2 = org . nd4j . linalg . factory . Nd4j . create ( new double [ ] { 0 , 0 , 1 } , new long [ ] { 1 , 3 } ) ; e . eval ( class0 , class1 ) ; e . eval ( class0 , class1 ) ; e . eval ( class2 , class2 ) ; e . eval ( class2 , class2 ) ; e . eval ( class2 , class2 ) ; java . lang . String s = e . confusionMatrix ( ) ; java . lang . String exp = "<sp>0<sp>1<sp>2\n" + ( ( ( ( "a" 0 + "<sp>0<sp>2<sp>0<sp>|<sp>0<sp>=<sp>a\n" ) + "<sp>0<sp>0<sp>0<sp>|<sp>1<sp>=<sp>b\n" ) + "<sp>0<sp>0<sp>3<sp>|<sp>2<sp>=<sp>c\n" ) + "\nConfusion<sp>matrix<sp>format:<sp>Actual<sp>(rowClass)<sp>predicted<sp>as<sp>(columnClass)<sp>N<sp>times" ) ; org . junit . Assert . assertEquals ( exp , s ) ; System . out . println ( "============================" ) ; System . out . println ( e . stats ( ) ) ; System . out . println ( "\n\n\n\n" ) ; e = new org . nd4j . evaluation . classification . Evaluation ( ) ; class0 = org . nd4j . linalg . factory . Nd4j . create ( 1 , 31 ) ; class0 . putScalar ( 0 , 1 ) ; e . eval ( class0 , class0 ) ; System . out . println ( e . stats ( ) ) ; System . out . println ( "\n\n\n\n" ) ; System . out . println ( e . stats ( false , true ) ) ; } }
public class aTest{ @Test public void testConfigSerializationCustomMinimal ( ) { java . lang . String [ ] params = new java . lang . String [ ] { ( "/tika-config2.xml" 0 + ( testDataFile . toString ( ) ) ) + "/tika-config2.xml" , "--dump-minimal-config" } ; org . apache . tika . cli . TikaCLI . main ( params ) ; java . lang . String content = outContent . toString ( java . nio . charset . StandardCharsets . UTF_8 . name ( ) ) . replaceAll ( "[\r\n\t<sp>]+" , "<sp>" ) ; java . lang . String expected = "<parser<sp>class=\"org.apache.tika.parser.DefaultParser\"/tika-config2.xml" 1 + ( ( ( ( ( "<sp><mime-exclude>application/pdf</mime-exclude>" + "<sp><mime-exclude>image/jpeg</mime-exclude><sp>" ) + "</parser><sp>" ) + "<parser<sp>class=\"org.apache.tika.parser.EmptyParser\"/tika-config2.xml" 1 ) + "<sp><mime>application/pdf</mime><sp>" ) + "</parser>" ) ; org . junit . Assert . assertTrue ( content . contains ( expected ) ) ; } }
public class aTest{ @Test public void connectSucceedsWithSslv3 ( ) { org . kaazing . gateway . server . test . Gateway gateway = new org . kaazing . gateway . server . test . Gateway ( ) ; java . net . Socket socket = null ; try { org . kaazing . gateway . server . test . config . GatewayConfiguration configuration = new org . kaazing . gateway . server . test . config . builder . GatewayConfigurationBuilder ( ) . service ( ) . accept ( "tcp://localhost:8557" ) . connect ( "ssl://localhost:8558" ) . type ( "proxy" ) . connectOption ( "ssl.protocols" , "SSLv3" ) . done ( ) . service ( ) . accept ( "ssl://localhost:8558" ) . type ( "echo" ) . acceptOption ( "ssl.protocols" , "SSLv3" ) . done ( ) . security ( ) . keyStore ( keyStore ) . keyStorePassword ( password ) . trustStore ( trustStore ) . done ( ) . done ( ) ; gateway . start ( configuration ) ; socket = javax . net . SocketFactory . getDefault ( ) . createSocket ( "localhost" , 8557 ) ; java . io . BufferedWriter w = new java . io . BufferedWriter ( new java . io . OutputStreamWriter ( socket . getOutputStream ( ) ) ) ; java . io . BufferedReader r = new java . io . BufferedReader ( new java . io . InputStreamReader ( socket . getInputStream ( ) ) ) ; java . lang . String expected = "Hello<sp>World!" ; w . write ( expected ) ; w . newLine ( ) ; w . flush ( ) ; java . lang . String got = r . readLine ( ) ; org . junit . Assert . assertEquals ( expected , got ) ; w . close ( ) ; r . close ( ) ; } }
public class aTest{ @Test public void testDeleteObjectOrCancelCheckOut ( ) { org . apache . chemistry . opencmis . client . api . CmisObject ob = session . getObjectByPath ( "/testfolder1/testfile1" ) ; ( ( org . apache . chemistry . opencmis . client . api . Document ) ( ob ) ) . checkIn ( true , null , null , "comment" ) ; ( ( org . apache . chemistry . opencmis . client . api . Document ) ( ob ) ) . checkOut ( ) ; java . util . Map < java . lang . String , java . lang . Object > map = new java . util . HashMap ( ) ; map . put ( "dc:title" , "new<sp>title" ) ; map . put ( "dc:subjects" , java . util . Arrays . asList ( "a" , "b" , "c" ) ) ; ob . updateProperties ( map ) ; waitForAsyncCompletion ( ) ; ( ( org . apache . chemistry . opencmis . client . api . Document ) ( ob ) ) . cancelCheckOut ( ) ; session . clear ( ) ; ob = session . getObjectByPath ( "/testfolder1/testfile1" ) ; org . junit . Assert . assertFalse ( "new<sp>title" . equals ( ob . getPropertyValue ( "dc:title" ) ) ) ; ob = session . getObjectByPath ( "/testfolder1/testfile2" ) ; map = new java . util . HashMap ( ) ; map . put ( "dc:title" , "new<sp>title" ) ; map . put ( "dc:subjects" , java . util . Arrays . asList ( "a" , "b" , "c" ) ) ; ob . updateProperties ( map ) ; waitForAsyncCompletion ( ) ; ( ( org . apache . chemistry . opencmis . client . api . Document ) ( ob ) ) . cancelCheckOut ( ) ; session . clear ( ) ; try { ob = session . getObjectByPath ( "/testfolder1/testfile2" ) ; org . junit . Assert . fail ( "Document<sp>should<sp>be<sp>deleted" ) ; } }
public class aTest{ @Test public void testColumnCountGetFilter ( ) { java . lang . String family = "Family" ; org . apache . hadoop . hbase . HTableDescriptor htd = new org . apache . hadoop . hbase . HTableDescriptor ( "testColumnCountGetFilter" ) ; htd . addFamily ( new org . apache . hadoop . hbase . HColumnDescriptor ( family ) ) ; org . apache . hadoop . hbase . HRegionInfo info = new org . apache . hadoop . hbase . HRegionInfo ( htd . getName ( ) , null , null , false ) ; org . apache . hadoop . hbase . regionserver . HRegion region = org . apache . hadoop . hbase . regionserver . HRegion . createHRegion ( info , org . apache . hadoop . hbase . filter . TestColumnCountGetFilter . TEST_UTIL . getDataTestDir ( ) , org . apache . hadoop . hbase . filter . TestColumnCountGetFilter . TEST_UTIL . getConfiguration ( ) , htd ) ; try { java . lang . String valueString = "ValueString" ; java . lang . String row = "row-1" ; java . util . List < java . lang . String > columns = generateRandomWords ( 10000 , "column" ) ; org . apache . hadoop . hbase . client . Put p = new org . apache . hadoop . hbase . client . Put ( org . apache . hadoop . hbase . util . Bytes . toBytes ( row ) ) ; p . setWriteToWAL ( false ) ; for ( java . lang . String column : columns ) { org . apache . hadoop . hbase . KeyValue kv = org . apache . hadoop . hbase . KeyValueTestUtil . create ( row , family , column , 0 , valueString ) ; p . add ( kv ) ; } region . put ( p ) ; org . apache . hadoop . hbase . client . Get get = new org . apache . hadoop . hbase . client . Get ( row . getBytes ( ) ) ; org . apache . hadoop . hbase . filter . Filter filter = new org . apache . hadoop . hbase . filter . ColumnCountGetFilter ( 100 ) ; get . setFilter ( filter ) ; org . apache . hadoop . hbase . client . Scan scan = new org . apache . hadoop . hbase . client . Scan ( get ) ; org . apache . hadoop . hbase . regionserver . InternalScanner scanner = region . getScanner ( scan ) ; java . util . List < org . apache . hadoop . hbase . KeyValue > results = new java . util . ArrayList < org . apache . hadoop . hbase . KeyValue > ( ) ; scanner . next ( results ) ; org . junit . Assert . assertEquals ( 100 , results . size ( ) ) ; } }
public class aTest{ @Test public void testSingleQueryZeroSamplingRate ( ) { java . util . Properties props = org . apache . phoenix . util . PropertiesUtil . deepCopy ( TestUtil . TEST_PROPERTIES ) ; java . sql . Connection conn = java . sql . DriverManager . getConnection ( getUrl ( ) , props ) ; try { prepareTableWithValues ( conn , 100 ) ; java . lang . String query = ( "SELECT<sp>i1,<sp>i2<sp>FROM<sp>" + ( tableName ) ) + "<sp>tablesample<sp>(0)<sp>" ; java . sql . ResultSet rs = conn . createStatement ( ) . executeQuery ( query ) ; org . junit . Assert . assertFalse ( rs . next ( ) ) ; } }
public class aTest{ @Test public void testProcessorEvents4 ( ) { java . lang . String events = recordRichStringProcessorEvents ( ( "\t\'\'\'\n" + ( ( ( "acceptTemplateText(\t)" 1 + "\t\n" ) + "acceptTemplateText(\t)" 0 ) + "acceptTemplateText(\t)" 2 ) ) ) ; java . lang . String expected = "announceNextLiteral()\n" + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "acceptTemplateText()\n" + "acceptTemplateText(\t)" 3 ) + "acceptTemplateText(\t)\n" ) + "acceptIfCondition()\n" ) + "announceNextLiteral()\n" ) + "acceptTemplateText()\n" ) + "acceptTemplateText(\t)" 3 ) + "acceptTemplateText(\t)\n" ) + "acceptSemanticText()\n" ) + "acceptTemplateText()\n" ) + "acceptSemanticText()\n" ) + "acceptSemanticText()\n" ) + "acceptSemanticLineBreak()\n" ) + "acceptTemplateText(\t)\n" ) + "acceptEndIf()\n" ) + "announceNextLiteral()\n" ) + "acceptTemplateText()\n" ) + "acceptTemplateText(\t)" 3 ) + "acceptTemplateText(\t)" ) ; org . junit . Assert . assertEquals ( expected , events ) ; } }
public class aTest{ @Test public void testBug449185 ( ) { try { org . eclipse . xtend2 . lib . StringConcatenation _builder = new org . eclipse . xtend2 . lib . StringConcatenation ( ) ; _builder . append ( "import<sp>org.eclipse.xtend.lib.annotations.Data" ) ; _builder . newLine ( ) ; _builder . append ( "@Data<sp>class<sp>A<sp>{" ) ; _builder . newLine ( ) ; _builder . append ( "}" ) ; _builder . newLine ( ) ; _builder . newLine ( ) ; _builder . append ( "@Data<sp>class<sp>C<sp>extends<sp>A<sp>{" ) ; _builder . newLine ( ) ; _builder . append ( "\t" ) ; _builder . append ( "val<sp>int<sp>c" ) ; _builder . newLine ( ) ; _builder . append ( "}" ) ; _builder . newLine ( ) ; _builder . newLine ( ) ; _builder . append ( "@Data<sp>class<sp>B<sp>{" ) ; _builder . newLine ( ) ; _builder . append ( "\t" ) ; _builder . append ( "val<sp>int<sp>b" ) ; _builder . newLine ( ) ; _builder . append ( "}" ) ; _builder . newLine ( ) ; _builder . newLine ( ) ; _builder . append ( "@Data<sp>class<sp>C<sp>extends<sp>A<sp>{" 0 ) ; _builder . newLine ( ) ; _builder . append ( "\t" ) ; _builder . append ( "val<sp>double<sp>d" ) ; _builder . newLine ( ) ; _builder . append ( "}" ) ; _builder . newLine ( ) ; final org . eclipse . xtext . util . IAcceptor < org . eclipse . xtext . xbase . testing . CompilationTestHelper . Result > _function = ( org . eclipse . xtext . xbase . testing . CompilationTestHelper . Result it ) -> { final Class < ? > d = it . getCompiledClass ( "D" ) ; final Class < ? > [ ] parameterTypes = org . eclipse . xtext . xbase . lib . IterableExtensions . < Constructor < ? > > head ( ( ( Iterable < Constructor < ? > > ) ( org . eclipse . xtext . xbase . lib . Conversions . doWrapArray ( d . getDeclaredConstructors ( ) ) ) ) ) . getParameterTypes ( ) ; org . junit . Assert . assertArrayEquals ( new java . lang . Object [ ] { . class , . class } , parameterTypes ) ; } }
public class aTest{ @Test public void testAnalyzeTgz ( ) { org . owasp . dependencycheck . analyzer . ArchiveAnalyzer instance = new org . owasp . dependencycheck . analyzer . ArchiveAnalyzer ( ) ; instance . initialize ( getSettings ( ) ) ; instance . accept ( new java . io . File ( "zip" ) ) ; try ( org . owasp . dependencycheck . Engine engine = new org . owasp . dependencycheck . Engine ( getSettings ( ) ) ) { instance . prepare ( null ) ; java . io . File file = org . owasp . dependencycheck . BaseTest . getResourceAsFile ( this , "file.tgz" ) ; getSettings ( ) . setBoolean ( Settings . KEYS . AUTO_UPDATE , false ) ; getSettings ( ) . setBoolean ( Settings . KEYS . ANALYZER_NEXUS_ENABLED , false ) ; getSettings ( ) . setBoolean ( Settings . KEYS . ANALYZER_CENTRAL_ENABLED , false ) ; int initial_size = engine . getDependencies ( ) . length ; engine . scan ( file ) ; engine . analyzeDependencies ( ) ; int ending_size = engine . getDependencies ( ) . length ; org . junit . Assert . assertTrue ( ( initial_size < ending_size ) ) ; } }
public class aTest{ @Test public void testStartupRegistersAndRemovesOlapConnections ( ) { final org . pentaho . platform . plugin . action . olap . IOlapService mockOlapService = org . mockito . Mockito . mock ( org . pentaho . platform . plugin . action . olap . IOlapService . class ) ; final org . pentaho . platform . api . engine . IPentahoSession mockSession = org . mockito . Mockito . mock ( org . pentaho . platform . api . engine . IPentahoSession . class ) ; org . pentaho . platform . plugin . action . olap . Olap4jSystemListener listener = new org . pentaho . platform . plugin . action . olap . Olap4jSystemListener ( ) { @ org . pentaho . platform . plugin . action . olap . Override org . pentaho . platform . plugin . action . olap . IOlapService getOlapService ( org . pentaho . platform . api . engine . IPentahoSession session ) { org . junit . Assert . assertSame ( mockSession , session ) ; return mockOlapService ; } }
public class aTest{ @Test public void testConcurrentIteratorWithAdditionsEqualHashCodes ( ) { com . sun . sgs . test . app . util . TestScalableHashMap . txnScheduler . runTask ( new com . sun . sgs . test . util . TestAbstractKernelRunnable ( ) { public void run ( ) throws com . sun . sgs . test . app . util . Exception { com . sun . sgs . app . util . ScalableHashMap < com . sun . sgs . test . app . util . TestScalableHashMap . Equals , java . lang . Integer > test = new com . sun . sgs . app . util . ScalableHashMap < com . sun . sgs . test . app . util . TestScalableHashMap . Equals , java . lang . Integer > ( 16 ) ; java . util . Map < com . sun . sgs . test . app . util . TestScalableHashMap . Equals , java . lang . Integer > control = new java . util . HashMap < com . sun . sgs . test . app . util . TestScalableHashMap . Equals , java . lang . Integer > ( ) ; java . util . Iterator < Map . Entry < com . sun . sgs . test . app . util . TestScalableHashMap . Equals , java . lang . Integer > > it = test . entrySet ( ) . iterator ( ) ; java . io . ByteArrayOutputStream baos = new java . io . ByteArrayOutputStream ( ) ; java . io . ObjectOutputStream oos = new java . io . ObjectOutputStream ( baos ) ; oos . writeObject ( it ) ; int [ ] a = new int [ 128 ] ; for ( int i = 0 ; i < ( a . length ) ; i ++ ) { int j = com . sun . sgs . test . app . util . TestScalableHashMap . RANDOM . nextInt ( ) ; test . put ( new com . sun . sgs . test . app . util . TestScalableHashMap . Equals ( j ) , j ) ; control . put ( new com . sun . sgs . test . app . util . TestScalableHashMap . Equals ( j ) , j ) ; a [ i ] = j ; } int entries = 0 ; byte [ ] serializedForm = baos . toByteArray ( ) ; java . io . ByteArrayInputStream bais = new java . io . ByteArrayInputStream ( serializedForm ) ; java . io . ObjectInputStream ois = new java . io . ObjectInputStream ( bais ) ; it = ( ( java . util . Iterator < Map . Entry < com . sun . sgs . test . app . util . TestScalableHashMap . Equals , java . lang . Integer > > ) ( ois . readObject ( ) ) ) ; while ( it . hasNext ( ) ) { Map . Entry < com . sun . sgs . test . app . util . TestScalableHashMap . Equals , java . lang . Integer > e = it . next ( ) ; control . remove ( e . getKey ( ) ) ; } org . junit . Assert . assertEquals ( 0 , control . size ( ) ) ; } } }
public class aTest{ @Test public void testOAuth2Jwt ( ) { com . streamsets . pipeline . stage . origin . http . HttpClientSourceIT . tokenGetCount = 0 ; com . streamsets . pipeline . stage . origin . http . HttpClientSourceIT . keyPair = java . security . KeyPairGenerator . getInstance ( "RSA" ) . generateKeyPair ( ) ; try { com . streamsets . pipeline . config . DataFormat dataFormat = com . streamsets . pipeline . config . DataFormat . JSON ; com . streamsets . pipeline . stage . origin . http . HttpClientConfigBean conf = new com . streamsets . pipeline . stage . origin . http . HttpClientConfigBean ( ) ; conf . client . authType = com . streamsets . pipeline . lib . http . AuthenticationType . NONE ; conf . client . useOAuth2 = true ; conf . client . oauth2 . credentialsGrantType = com . streamsets . pipeline . lib . http . oauth2 . OAuth2GrantTypes . JWT ; conf . client . oauth2 . algorithm = com . streamsets . pipeline . lib . http . oauth2 . SigningAlgorithms . RS256 ; conf . client . oauth2 . jwtClaims = com . streamsets . pipeline . stage . origin . http . HttpClientSourceIT . JWT ; conf . client . oauth2 . key = ( ) -> org . apache . commons . codec . binary . Base64 . encodeBase64String ( com . streamsets . pipeline . stage . origin . http . HttpClientSourceIT . keyPair . getPrivate ( ) . getEncoded ( ) ) ; conf . client . oauth2 . tokenUrl = ( getBaseUri ( ) ) + "jwtToken" ; conf . httpMode = HttpClientMode . STREAMING ; conf . resourceUrl = ( getBaseUri ( ) ) + "stream" ; conf . client . readTimeoutMillis = 1000 ; conf . basic . maxBatchSize = 100 ; conf . basic . maxWaitTime = 1000 ; conf . pollingInterval = 1000 ; conf . httpMethod = com . streamsets . pipeline . lib . http . HttpMethod . GET ; conf . dataFormat = dataFormat ; conf . dataFormatConfig . jsonContent = com . streamsets . pipeline . config . JsonMode . MULTIPLE_OBJECTS ; runBatchAndAssertNames ( dataFormat , conf ) ; org . junit . Assert . assertEquals ( 1 , com . streamsets . pipeline . stage . origin . http . HttpClientSourceIT . tokenGetCount ) ; } }
public class aTest{ @Test public void testClusterAlbPathPattern ( ) { final java . lang . String vpcStackName = "vpc-2azs-" + ( this . random8String ( ) ) ; final java . lang . String clusterStackName = "ecs-cluster-" + ( this . random8String ( ) ) ; final java . lang . String stackName = "ClassB" 6 + ( this . random8String ( ) ) ; final java . lang . String classB = "ClassB" 5 ; final java . lang . String keyName = "key-" + ( this . random8String ( ) ) ; try { this . createKey ( keyName ) ; try { this . createStack ( vpcStackName , "vpc/vpc-2azs.yaml" , new com . amazonaws . services . cloudformation . model . Parameter ( ) . withParameterKey ( "ClassB" ) . withParameterValue ( classB ) ) ; try { this . createStack ( clusterStackName , "ecs/cluster.yaml" , new com . amazonaws . services . cloudformation . model . Parameter ( ) . withParameterKey ( "ParentVPCStack" ) . withParameterValue ( vpcStackName ) , new com . amazonaws . services . cloudformation . model . Parameter ( ) . withParameterKey ( "ClassB" 0 ) . withParameterValue ( keyName ) ) ; final java . lang . String cluster = this . getStackOutputValue ( clusterStackName , "Cluster" ) ; try { this . createStack ( stackName , "ecs/service-cluster-alb.yaml" , new com . amazonaws . services . cloudformation . model . Parameter ( ) . withParameterKey ( "ParentClusterStack" ) . withParameterValue ( clusterStackName ) , new com . amazonaws . services . cloudformation . model . Parameter ( ) . withParameterKey ( "ClassB" 3 ) . withParameterValue ( "ClassB" 4 ) ) ; final java . lang . String url = this . getStackOutputValue ( stackName , "ClassB" 1 ) ; final java . util . concurrent . Callable < java . lang . Boolean > callable = ( ) -> { final org . apache . http . HttpResponse response = de . taimos . httputils . WS . url ( url ) . timeout ( 10000 ) . get ( ) ; if ( ( de . taimos . httputils . WS . getStatus ( response ) ) != 404 ) { throw new java . lang . RuntimeException ( ( "ClassB" 2 + ( de . taimos . httputils . WS . getStatus ( response ) ) ) ) ; } return true ; } ; org . junit . Assert . assertTrue ( this . retry ( callable ) ) ; } }
public class aTest{ @Test public void testSchemaDerivationWithRowtime ( ) { final java . util . Map < java . lang . String , java . lang . String > properties = new java . util . HashMap ( ) ; properties . put ( "schema.1.type" 3 , "otherField" ) ; properties . put ( "schema.1.type" 1 , "VARCHAR" ) ; properties . put ( "schema.0.from" , "csvField" ) ; properties . put ( "p" 6 , "p" 2 ) ; properties . put ( "schema.1.type" , "VARCHAR" ) ; properties . put ( "schema.2.name" , "p" ) ; properties . put ( "schema.2.type" , "p" 3 ) ; properties . put ( "p" 4 , "p" 0 ) ; properties . put ( "p" 7 , "r" ) ; properties . put ( "p" 9 , "p" 3 ) ; properties . put ( "schema.3.rowtime.timestamps.type" , "schema.1.type" 2 ) ; properties . put ( "p" 8 , "p" 1 ) ; properties . put ( "schema.1.type" 0 , "p" 5 ) ; final org . apache . flink . table . api . TableSchema actualSchema = org . apache . flink . table . factories . TableFormatFactoryBase . deriveSchema ( properties ) ; final org . apache . flink . table . api . TableSchema expectedSchema = org . apache . flink . table . api . TableSchema . builder ( ) . field ( "csvField" , Types . STRING ) . field ( "p" 2 , Types . STRING ) . field ( "p" 1 , Types . SQL_TIMESTAMP ) . build ( ) ; org . junit . Assert . assertEquals ( expectedSchema , actualSchema ) ; } }
public class aTest{ @Test public void testGetCompatibleProductsListForPartnerTemplate ( ) { runTX ( new java . util . concurrent . Callable < java . lang . Void > ( ) { @ org . oscm . domobjects . Override public org . oscm . domobjects . Void call ( ) throws org . oscm . domobjects . Exception { doTestAdd ( ) ; return null ; } } ) ; runTX ( new java . util . concurrent . Callable < java . lang . Void > ( ) { @ org . oscm . domobjects . Override public org . oscm . domobjects . Void call ( ) throws org . oscm . domobjects . Exception { domObjects . set ( 0 , mgr . find ( org . oscm . domobjects . Product . class , domObjects . get ( 0 ) . getKey ( ) ) ) ; domObjects . set ( 1 , mgr . find ( org . oscm . domobjects . Product . class , domObjects . get ( 1 ) . getKey ( ) ) ) ; domObjects . set ( 2 , mgr . find ( org . oscm . domobjects . Product . class , domObjects . get ( 2 ) . getKey ( ) ) ) ; final org . oscm . domobjects . Product prod1 = ( ( org . oscm . domobjects . Product ) ( domObjects . get ( 0 ) ) ) ; final org . oscm . domobjects . Product prod2 = ( ( org . oscm . domobjects . Product ) ( domObjects . get ( 1 ) ) ) ; org . oscm . domobjects . Product child = ( ( org . oscm . domobjects . Product ) ( domObjects . get ( 2 ) ) ) ; org . oscm . domobjects . ProductReference reference = new org . oscm . domobjects . ProductReference ( prod1 , prod2 ) ; mgr . persist ( reference ) ; child . setProductId ( "Prod1Child" ) ; child . setTemplate ( prod1 ) ; child . setType ( ServiceType . PARTNER_TEMPLATE ) ; return null ; } } ) ; runTX ( new java . util . concurrent . Callable < java . lang . Void > ( ) { @ org . oscm . domobjects . Override public org . oscm . domobjects . Void call ( ) { domObjects . set ( 0 , mgr . find ( org . oscm . domobjects . Product . class , domObjects . get ( 0 ) . getKey ( ) ) ) ; domObjects . set ( 1 , mgr . find ( org . oscm . domobjects . Product . class , domObjects . get ( 1 ) . getKey ( ) ) ) ; domObjects . set ( 2 , mgr . find ( org . oscm . domobjects . Product . class , domObjects . get ( 2 ) . getKey ( ) ) ) ; org . oscm . domobjects . Product child = ( ( org . oscm . domobjects . Product ) ( domObjects . get ( 2 ) ) ) ; java . util . List < org . oscm . domobjects . Product > products = child . getCompatibleProductsList ( ) ; org . junit . Assert . assertEquals ( 0 , products . size ( ) ) ; return null ; } } }
public class aTest{ @Test public void variableContext ( ) { final org . apache . rya . api . function . sp . StatementPatternMatcher matcher = new org . apache . rya . api . function . sp . StatementPatternMatcher ( org . apache . rya . api . function . sp . StatementPatternMatcherTest . getSp ( ( "urn:talksTo" 1 + ( ( ( "GRAPH<sp>?c<sp>{" + "?s<sp>?p<sp>?o<sp>." ) + "}" ) + "}" ) ) ) ) ; final org . eclipse . rdf4j . model . ValueFactory vf = org . eclipse . rdf4j . model . impl . SimpleValueFactory . getInstance ( ) ; final org . eclipse . rdf4j . model . Statement statement = vf . createStatement ( vf . createIRI ( "urn:Alice" ) , vf . createIRI ( "urn:talksTo" ) , vf . createIRI ( "urn:Bob" ) , vf . createIRI ( "urn:talksTo" 0 ) ) ; final org . eclipse . rdf4j . query . algebra . evaluation . QueryBindingSet expected = new org . eclipse . rdf4j . query . algebra . evaluation . QueryBindingSet ( ) ; expected . addBinding ( "s" , vf . createIRI ( "urn:Alice" ) ) ; expected . addBinding ( "p" , vf . createIRI ( "urn:talksTo" ) ) ; expected . addBinding ( "o" , vf . createIRI ( "urn:Bob" ) ) ; expected . addBinding ( "c" , vf . createIRI ( "urn:talksTo" 0 ) ) ; final java . util . Optional < org . eclipse . rdf4j . query . BindingSet > bs = matcher . match ( statement ) ; org . junit . Assert . assertEquals ( expected , bs . get ( ) ) ; } }
public class aTest{ @Test public void testGetPropertyCharacterArrayNull ( ) { try { java . lang . reflect . Method method = org . apache . olingo . odata2 . jpa . processor . core . access . data . JPAEntityParserForStaticMethodTest . class . getMethod ( "getCharacterArrayNull" , ( ( java . lang . Class < ? > [ ] ) ( null ) ) ) ; java . lang . String output = ( ( java . lang . String ) ( org . apache . olingo . odata2 . jpa . processor . core . access . data . JPAEntityParser . getPropertyValue ( method , this , "" ) ) ) ; org . junit . Assert . assertNull ( output ) ; } }
public class aTest{ @Test public void testWithWrongPackage2 ( ) { com . google . inject . Injector injector = com . google . inject . Guice . createInjector ( de . devsurf . injection . guice . scanner . StartupModule . create ( de . devsurf . injection . guice . scanner . sonatype . SonatypeScanner . class , de . devsurf . injection . guice . scanner . PackageFilter . create ( "java" ) ) ) ; org . junit . Assert . assertNotNull ( injector ) ; try { de . devsurf . injection . guice . scanner . sonatype . tests . autobind . multiple . MultibindTests . SecondContainer container = injector . getInstance ( de . devsurf . injection . guice . scanner . sonatype . tests . autobind . multiple . MultibindTests . SecondContainer . class ) ; org . junit . Assert . fail ( ( "The<sp>Scanner<sp>scanned<sp>the<sp>wrong<sp>package,<sp>so<sp>no<sp>Implementation<sp>should<sp>be<sp>bound<sp>to<sp>this<sp>Interface.<sp>Instance<sp>null?<sp>" + ( container == null ) ) ) ; } }
public class aTest{ @Test public void enrichmentTestGO ( ) { owltools . io . ParserWrapper pw = new owltools . io . ParserWrapper ( ) ; sourceOntol = pw . parseOBO ( getResourceIRIString ( "go-subset-t1.obo" ) ) ; org . semanticweb . owlapi . model . OWLObjectProperty INVOLVED_IN = OBOUpperVocabulary . RO_involved_in . getObjectProperty ( sourceOntol ) ; java . util . Map < org . semanticweb . owlapi . model . OWLClass , org . semanticweb . owlapi . model . OWLClass > qmap = owltools . mooncat . TransformationUtils . createObjectPropertyView ( sourceOntol , sourceOntol , INVOLVED_IN , null , true ) ; LOG . info ( ( "VIEW<sp>SIZE" + ( qmap . size ( ) ) ) ) ; sourceOntol . getOWLOntologyManager ( ) . saveOntology ( sourceOntol , org . semanticweb . owlapi . model . IRI . create ( new java . io . File ( "<sp>" 1 ) ) ) ; g = new owltools . graph . OWLGraphWrapper ( sourceOntol ) ; owltools . io . TableToAxiomConverter ttac = new owltools . io . TableToAxiomConverter ( g ) ; ttac . config . axiomType = org . semanticweb . owlapi . model . AxiomType . CLASS_ASSERTION ; ttac . config . property = INVOLVED_IN . getIRI ( ) ; ttac . config . isSwitchSubjectObject = true ; ttac . parse ( "<sp>" 2 ) ; g . getManager ( ) . removeAxioms ( sourceOntol , sourceOntol . getAxioms ( AxiomType . DISJOINT_CLASSES ) ) ; try { owltools . io . OWLPrettyPrinter pp = new owltools . io . OWLPrettyPrinter ( g ) ; createOwlSim ( ) ; LOG . info ( ( "ont<sp>=<sp>" + ( owlsim . getSourceOntology ( ) ) ) ) ; LOG . info ( ( "r<sp>=<sp>" + ( owlsim . getReasoner ( ) ) ) ) ; owlsim . createElementAttributeMapFromOntology ( ) ; for ( org . semanticweb . owlapi . model . OWLNamedIndividual ind : sourceOntol . getIndividualsInSignature ( ) ) { LOG . debug ( ind ) ; for ( org . semanticweb . owlapi . model . OWLClass c : owlsim . getReasoner ( ) . getTypes ( ind , true ) . getFlattened ( ) ) { LOG . debug ( ( "<sp>T:" + c ) ) ; } for ( org . semanticweb . owlapi . model . OWLClassExpression c : owltools . util . OwlHelper . getTypes ( ind , sourceOntol ) ) { LOG . debug ( ( "<sp>" 0 + c ) ) ; } } org . semanticweb . owlapi . model . OWLClass rc1 = get ( "<sp>" 3 ) ; org . semanticweb . owlapi . model . OWLClass rc2 = get ( "cellular_component" ) ; org . semanticweb . owlapi . model . OWLClass pc = g . getDataFactory ( ) . getOWLThing ( ) ; owltools . sim2 . EnrichmentConfig ec = new owltools . sim2 . EnrichmentConfig ( ) ; ec . pValueCorrectedCutoff = 0.05 ; ec . attributeInformationContentCutoff = 3.0 ; owlsim . setEnrichmentConfig ( ec ) ; org . semanticweb . owlapi . model . OWLClass vc1 = qmap . get ( rc1 ) ; org . semanticweb . owlapi . model . OWLClass vc2 = qmap . get ( rc2 ) ; int n = 0 ; java . util . List < owltools . sim2 . EnrichmentResult > results = owlsim . calculateAllByAllEnrichment ( pc , vc1 , vc2 ) ; LOG . debug ( ( ( ( "Results:<sp>" + rc1 ) + "<sp>" ) + rc2 ) ) ; for ( owltools . sim2 . EnrichmentResult result : results ) { LOG . debug ( ( "R=" + ( render ( result , pp ) ) ) ) ; n ++ ; } org . junit . Assert . assertTrue ( ( n > 0 ) ) ; owlsim . showTimings ( ) ; } }
public class aTest{ @Test public void testBarcelonaQueryJB3 ( ) { javax . jdo . PersistenceManager pm = org . zoodb . test . testutil . TestTools . openPM ( ) ; pm . currentTransaction ( ) . begin ( ) ; java . lang . String filter = "this.i2<sp>==<sp>param" ; for ( int i = 1 ; i <= ( org . zoodb . test . jdo . Test_072_PolePosBarcelonaQuery . COUNT ) ; i ++ ) { javax . jdo . Query query = pm . newQuery ( org . zoodb . test . jdo . classes . TC3 . class , filter ) ; query . declareParameters ( "int<sp>param" ) ; org . junit . Assert . assertEquals ( 2 , doQuery ( query , i ) ) ; } }
public class aTest{ @Test public void testNodeDecomissionRespectsRackPolicy ( ) { org . apache . hadoop . conf . Configuration conf = getConf ( ) ; short REPLICATION_FACTOR = 2 ; final org . apache . hadoop . fs . Path filePath = new org . apache . hadoop . fs . Path ( "/testFile" ) ; org . apache . hadoop . fs . FileSystem localFileSys = org . apache . hadoop . fs . FileSystem . getLocal ( conf ) ; org . apache . hadoop . fs . Path workingDir = localFileSys . getWorkingDirectory ( ) ; org . apache . hadoop . fs . Path dir = new org . apache . hadoop . fs . Path ( workingDir , "build/test/data/temp/decommission" ) ; org . apache . hadoop . fs . Path excludeFile = new org . apache . hadoop . fs . Path ( dir , "exclude" ) ; org . apache . hadoop . fs . Path includeFile = new org . apache . hadoop . fs . Path ( dir , "include" ) ; org . junit . Assert . assertTrue ( localFileSys . mkdirs ( dir ) ) ; org . apache . hadoop . hdfs . DFSTestUtil . writeFile ( localFileSys , excludeFile , "" ) ; org . apache . hadoop . hdfs . DFSTestUtil . writeFile ( localFileSys , includeFile , "" ) ; conf . set ( DFSConfigKeys . DFS_HOSTS_EXCLUDE , excludeFile . toUri ( ) . getPath ( ) ) ; conf . set ( DFSConfigKeys . DFS_HOSTS , includeFile . toUri ( ) . getPath ( ) ) ; java . lang . String [ ] racks = new java . lang . String [ ] { "/rack1" , "/rack1" , "/rack2" , "/rack2" } ; org . apache . hadoop . hdfs . MiniDFSCluster cluster = new org . apache . hadoop . hdfs . MiniDFSCluster . Builder ( conf ) . numDataNodes ( racks . length ) . racks ( racks ) . build ( ) ; final org . apache . hadoop . hdfs . server . namenode . FSNamesystem ns = cluster . getNameNode ( ) . getNamesystem ( ) ; try { final org . apache . hadoop . fs . FileSystem fs = cluster . getFileSystem ( ) ; org . apache . hadoop . hdfs . DFSTestUtil . createFile ( fs , filePath , 1L , REPLICATION_FACTOR , 1L ) ; org . apache . hadoop . hdfs . protocol . ExtendedBlock b = org . apache . hadoop . hdfs . DFSTestUtil . getFirstBlock ( fs , filePath ) ; org . apache . hadoop . hdfs . DFSTestUtil . waitForReplication ( cluster , b , 2 , REPLICATION_FACTOR , 0 ) ; org . apache . hadoop . fs . BlockLocation [ ] locs = fs . getFileBlockLocations ( fs . getFileStatus ( filePath ) , 0 , Long . MAX_VALUE ) ; java . lang . String name = locs [ 0 ] . getNames ( ) [ 0 ] ; org . apache . hadoop . hdfs . DFSTestUtil . writeFile ( localFileSys , excludeFile , name ) ; ns . getBlockManager ( ) . getDatanodeManager ( ) . refreshNodes ( conf ) ; org . apache . hadoop . hdfs . DFSTestUtil . waitForDecommission ( fs , name ) ; org . apache . hadoop . hdfs . DFSTestUtil . waitForReplication ( cluster , b , 2 , REPLICATION_FACTOR , 0 ) ; } }
public class aTest{ @Test public void testEvaluateTwoIndexThreeVarOrder2 ( ) { final org . eclipse . rdf4j . model . IRI superclass = org . apache . rya . indexing . external . AccumuloPcjIT . VF . createIRI ( "o" 2 ) ; final org . eclipse . rdf4j . model . IRI superclass2 = org . apache . rya . indexing . external . AccumuloPcjIT . VF . createIRI ( "uri:superclass2" ) ; conn . add ( subclass , RDF . TYPE , superclass ) ; conn . add ( subclass2 , RDF . TYPE , superclass2 ) ; conn . add ( obj , RDFS . LABEL , org . apache . rya . indexing . external . AccumuloPcjIT . VF . createLiteral ( "label" ) ) ; conn . add ( obj2 , RDFS . LABEL , org . apache . rya . indexing . external . AccumuloPcjIT . VF . createLiteral ( "label2" ) ) ; accCon . tableOperations ( ) . create ( "table2" ) ; conn . add ( obj , RDFS . LABEL , org . apache . rya . indexing . external . AccumuloPcjIT . VF . createLiteral ( "label" ) ) ; conn . add ( obj2 , RDFS . LABEL , org . apache . rya . indexing . external . AccumuloPcjIT . VF . createLiteral ( "label2" ) ) ; final java . lang . String indexSparqlString = "" + ( ( ( ( "o" 3 + "o" 6 ) + "o" 7 ) + "o" 1 ) + "o" 9 ) ; final java . lang . String indexSparqlString2 = "" + ( ( ( ( ( "o" 8 + "o" 6 ) + "<sp>?e<sp><uri:talksTo><sp>?o<sp>.<sp>" ) + "<sp>?o<sp><http://www.w3.org/2000/01/rdf-schema#label><sp>?l.<sp>" ) + "<sp>?c<sp>a<sp>?f<sp>.<sp>" ) + "o" 9 ) ; final java . lang . String queryString = "" + ( ( ( ( ( ( ( "o" 4 + "o" 6 ) + "o" 7 ) + "<sp>?e<sp><http://www.w3.org/2000/01/rdf-schema#label><sp>?l.<sp>" ) + "<sp>?e<sp><uri:talksTo><sp>?o<sp>.<sp>" ) + "<sp>?o<sp><http://www.w3.org/2000/01/rdf-schema#label><sp>?l.<sp>" ) + "<sp>?c<sp>a<sp>?f<sp>.<sp>" ) + "o" 9 ) ; final org . apache . rya . indexing . external . AccumuloPcjIT . CountingResultHandler crh2 = new org . apache . rya . indexing . external . AccumuloPcjIT . CountingResultHandler ( ) ; org . apache . rya . indexing . external . PcjIntegrationTestingUtil . createAndPopulatePcj ( conn , accCon , ( ( tablename ) + 1 ) , indexSparqlString , new java . lang . String [ ] { "o" 5 , "label2" 0 , "o" 0 } , com . google . common . base . Optional . absent ( ) ) ; org . apache . rya . indexing . external . PcjIntegrationTestingUtil . createAndPopulatePcj ( conn , accCon , ( ( tablename ) + 2 ) , indexSparqlString2 , new java . lang . String [ ] { "o" , "label2" 1 , "label2" 0 , "o" 5 , "o" 0 } , com . google . common . base . Optional . absent ( ) ) ; org . apache . rya . indexing . external . PcjIntegrationTestingUtil . deleteCoreRyaTables ( accCon , prefix ) ; pcjConn . prepareTupleQuery ( QueryLanguage . SPARQL , queryString ) . evaluate ( crh2 ) ; org . junit . Assert . assertEquals ( 2 , crh2 . getCount ( ) ) ; } }
public class aTest{ @Test public void testLogging ( ) { net . xenqtt . Log . trace ( "debug" 1 , "trace" ) ; net . xenqtt . Log . debug ( "debug" 1 , "debug" ) ; net . xenqtt . Log . info ( "debug" 2 , "info" ) ; net . xenqtt . Log . warn ( "debug" 2 , "debug" 3 ) ; net . xenqtt . Log . warn ( new java . lang . RuntimeException ( "debug" 4 ) , "debug" 2 , "debug" 3 ) ; net . xenqtt . Log . error ( "debug" 2 , "error" ) ; net . xenqtt . Log . error ( new java . lang . RuntimeException ( "Exception:<sp>error" ) , "debug" 2 , "error" ) ; net . xenqtt . Log . fatal ( "debug" 2 , "fatal" ) ; net . xenqtt . Log . fatal ( new java . lang . RuntimeException ( "Exception:<sp>fatal" ) , "debug" 2 , "fatal" ) ; java . lang . Thread . sleep ( 1000 ) ; java . util . Map < java . lang . String , java . lang . Integer > counts = new java . util . HashMap < java . lang . String , java . lang . Integer > ( ) ; counts . put ( "Should<sp>appear:<sp>info" , 1 ) ; counts . put ( "debug" 5 , 2 ) ; counts . put ( "debug" 4 , 1 ) ; counts . put ( "Should<sp>appear:<sp>error" , 2 ) ; counts . put ( "Exception:<sp>error" , 1 ) ; counts . put ( "Should<sp>appear:<sp>fatal" , 2 ) ; counts . put ( "Exception:<sp>fatal" , 1 ) ; for ( java . lang . String entry : net . xenqtt . LogTest . entries ) { java . lang . String formattedEntry = entry . replaceAll ( "debug" 6 , "debug" 0 ) ; java . lang . Integer count = counts . get ( formattedEntry ) ; if ( count == null ) { continue ; } counts . put ( formattedEntry , new java . lang . Integer ( ( ( count . intValue ( ) ) - 1 ) ) ) ; } for ( java . util . Map . Entry < java . lang . String , java . lang . Integer > count : counts . entrySet ( ) ) { org . junit . Assert . assertEquals ( count . getKey ( ) , 0 , count . getValue ( ) . intValue ( ) ) ; } } }
public class aTest{ @Test public void testDictionaryBehaviorNoNewlineTemplate ( ) { java . lang . String templates = "]\n" 0 + ( ( ( ( ( "\t\"x\"<sp>:<sp><%hi%>\n" + "]\n" ) + "\n" ) + "t()<sp>::=<sp><<\n" ) + "<d.x>\n" ) + ">>\n" ) ; writeFile ( tmpdir , "t.stg" , templates ) ; org . stringtemplate . v4 . STGroup group = new org . stringtemplate . v4 . STGroupFile ( ( ( ( tmpdir ) + ( java . io . File . separatorChar ) ) + "t.stg" ) ) ; org . stringtemplate . v4 . ST st = group . getInstanceOf ( "t" ) ; java . lang . String expected = "hi" ; java . lang . String result = st . render ( ) ; org . junit . Assert . assertEquals ( expected , result ) ; } }
public class aTest{ @Test public void testPush ( ) { com . sun . sgs . test . app . util . TestScalableDeque . txnScheduler . runTask ( new com . sun . sgs . test . util . TestAbstractKernelRunnable ( ) { public void run ( ) throws com . sun . sgs . test . app . util . Exception { com . sun . sgs . app . util . ScalableDeque < java . lang . Integer > d = new com . sun . sgs . app . util . ScalableDeque < java . lang . Integer > ( ) ; d . push ( 1 ) ; org . junit . Assert . assertEquals ( 1 , ( ( int ) ( d . remove ( ) ) ) ) ; } } }
public class aTest{ @Test public void testRollingProcessLogMultiThreadedMultiFile ( ) { cleanLogs ( "processLog.test-multi-thread-multi-file" ) ; com . serotonin . m2m2 . util . log . LogLevel level = LogLevel . TRACE ; try ( com . serotonin . m2m2 . util . log . ProcessLog log = new com . serotonin . m2m2 . util . log . ProcessLog ( "processLog." , "test-multi-thread-multi-file" , level , true , 10000 , 100 ) ) { java . util . concurrent . atomic . AtomicInteger count = new java . util . concurrent . atomic . AtomicInteger ( ) ; java . util . concurrent . atomic . AtomicBoolean running = new java . util . concurrent . atomic . AtomicBoolean ( true ) ; java . util . concurrent . atomic . AtomicInteger active = new java . util . concurrent . atomic . AtomicInteger ( ) ; int threadCount = 10 ; for ( int i = 0 ; i < threadCount ; i ++ ) { new java . lang . Thread ( ) { @ com . serotonin . m2m2 . util . log . Override public void run ( ) { active . incrementAndGet ( ) ; while ( running . get ( ) ) { log . info ( ( "Writing<sp>message<sp>" + ( count . getAndIncrement ( ) ) ) ) ; try { java . lang . Thread . sleep ( 10 ) ; } catch ( java . lang . InterruptedException e ) { } } active . decrementAndGet ( ) ; } } . start ( ) ; } java . lang . Thread . sleep ( 2000 ) ; running . set ( false ) ; while ( ( active . get ( ) ) > 0 ) { java . lang . Thread . sleep ( 100 ) ; } java . io . File [ ] files = log . getFiles ( ) ; int messageCount = 0 ; for ( int i = 0 ; i < ( files . length ) ; i ++ ) { java . lang . String result = getLogContents ( files [ i ] ) ; messageCount += result . split ( "\\n" ) . length ; } org . junit . Assert . assertEquals ( count . get ( ) , messageCount ) ; } }
public class aTest{ @Test public void testTablesWithSnapshots ( ) { final org . apache . hadoop . hbase . client . Connection conn = org . apache . hadoop . hbase . quotas . TestSpaceQuotasWithSnapshots . TEST_UTIL . getConnection ( ) ; final org . apache . hadoop . hbase . quotas . SpaceViolationPolicy policy = SpaceViolationPolicy . NO_INSERTS ; final org . apache . hadoop . hbase . TableName tn = helper . createTableWithRegions ( 10 ) ; final long tableLimit = 3L * ( SpaceQuotaHelperForTests . ONE_MEGABYTE ) ; org . apache . hadoop . hbase . quotas . TestSpaceQuotasWithSnapshots . TEST_UTIL . getAdmin ( ) . setQuota ( org . apache . hadoop . hbase . quotas . QuotaSettingsFactory . limitTableSpace ( tn , tableLimit , policy ) ) ; org . apache . hadoop . hbase . quotas . TestSpaceQuotasWithSnapshots . LOG . info ( "Writing<sp>first<sp>data<sp>set" ) ; helper . writeData ( tn , ( 1L * ( SpaceQuotaHelperForTests . ONE_MEGABYTE ) ) , "q1" ) ; org . apache . hadoop . hbase . quotas . TestSpaceQuotasWithSnapshots . LOG . info ( "Creating<sp>snapshot" ) ; org . apache . hadoop . hbase . quotas . TestSpaceQuotasWithSnapshots . TEST_UTIL . getAdmin ( ) . snapshot ( ( ( tn . toString ( ) ) + "snap1" ) , tn , SnapshotType . FLUSH ) ; org . apache . hadoop . hbase . quotas . TestSpaceQuotasWithSnapshots . LOG . info ( "Writing<sp>second<sp>data<sp>set" ) ; helper . writeData ( tn , ( 1L * ( SpaceQuotaHelperForTests . ONE_MEGABYTE ) ) , "q2" ) ; org . apache . hadoop . hbase . quotas . TestSpaceQuotasWithSnapshots . LOG . info ( "Flushing<sp>and<sp>major<sp>compacting<sp>table" ) ; org . apache . hadoop . hbase . quotas . TestSpaceQuotasWithSnapshots . TEST_UTIL . getAdmin ( ) . flush ( tn ) ; org . apache . hadoop . hbase . quotas . TestSpaceQuotasWithSnapshots . TEST_UTIL . compact ( tn , true ) ; org . apache . hadoop . hbase . quotas . TestSpaceQuotasWithSnapshots . LOG . info ( "Checking<sp>for<sp>quota<sp>violation" ) ; org . apache . hadoop . hbase . quotas . TestSpaceQuotasWithSnapshots . TEST_UTIL . waitFor ( 60000 , 1000 , new org . apache . hadoop . hbase . Waiter . Predicate < java . lang . Exception > ( ) { @ org . apache . hadoop . hbase . quotas . Override public boolean evaluate ( ) throws org . apache . hadoop . hbase . quotas . Exception { org . apache . hadoop . hbase . client . Scan s = org . apache . hadoop . hbase . quotas . QuotaTableUtil . makeQuotaSnapshotScanForTable ( tn ) ; try ( org . apache . hadoop . hbase . client . Table t = conn . getTable ( QuotaTableUtil . QUOTA_TABLE_NAME ) ) { org . apache . hadoop . hbase . client . ResultScanner rs = t . getScanner ( s ) ; try { org . apache . hadoop . hbase . client . Result r = org . apache . hbase . thirdparty . com . google . common . collect . Iterables . getOnlyElement ( rs ) ; org . apache . hadoop . hbase . CellScanner cs = r . cellScanner ( ) ; org . junit . Assert . assertTrue ( cs . advance ( ) ) ; org . apache . hadoop . hbase . Cell c = cs . current ( ) ; org . apache . hadoop . hbase . quotas . SpaceQuotaSnapshot snapshot = org . apache . hadoop . hbase . quotas . SpaceQuotaSnapshot . toSpaceQuotaSnapshot ( QuotaProtos . SpaceQuotaSnapshot . parseFrom ( org . apache . hbase . thirdparty . com . google . protobuf . UnsafeByteOperations . unsafeWrap ( c . getValueArray ( ) , c . getValueOffset ( ) , c . getValueLength ( ) ) ) ) ; org . apache . hadoop . hbase . quotas . TestSpaceQuotasWithSnapshots . LOG . info ( ( ( ( ( ( snapshot . getUsage ( ) ) + "/" ) + ( snapshot . getLimit ( ) ) ) + "<sp>" ) + ( snapshot . getQuotaStatus ( ) ) ) ) ; return snapshot . getQuotaStatus ( ) . isInViolation ( ) ; } }
public class aTest{ @Test public void testEncodeMongoDBDatabaseNums ( ) { System . out . println ( ( ( com . telefonica . iot . cygnus . utils . CommonUtilsForTests . getTestTraceHead ( "[NGSICharsets.encodeMongoDBDatabase]" ) ) + "--------<sp>Numbers<sp>are<sp>not<sp>encoded" ) ) ; java . lang . String in = "0123456789" ; java . lang . String expected = "0123456789" ; java . lang . String out = com . telefonica . iot . cygnus . utils . NGSICharsets . encodeMongoDBDatabase ( in ) ; try { org . junit . Assert . assertEquals ( expected , out ) ; System . out . println ( ( ( ( ( com . telefonica . iot . cygnus . utils . CommonUtilsForTests . getTestTraceHead ( "[NGSICharsets.encodeMongoDBDatabase]" ) ) + "-<sp>OK<sp>-<sp>'" ) + in ) + "'<sp>has<sp>not<sp>been<sp>encoded" ) ) ; } }
public class aTest{ @Test public void testAdd ( ) { com . orsoncharts . data . xyz . XYZSeriesCollection < java . lang . String > dataset = new com . orsoncharts . data . xyz . XYZSeriesCollection < java . lang . String > ( ) ; com . orsoncharts . data . xyz . XYZSeries < java . lang . String > s = new com . orsoncharts . data . xyz . XYZSeries < java . lang . String > ( "S1" ) ; dataset . add ( s ) ; org . junit . Assert . assertEquals ( 1 , dataset . getSeriesCount ( ) ) ; try { dataset . add ( new com . orsoncharts . data . xyz . XYZSeries < java . lang . String > ( "S1" ) ) ; org . junit . Assert . fail ( "Adding<sp>a<sp>series<sp>with<sp>the<sp>same<sp>name<sp>not<sp>permitted." ) ; } }
public class aTest{ @Test public void bigSkip ( ) { org . cojen . tupl . View ix = openIndex ( "skippy" ) ; for ( int i = 0 ; i < 1000000 ; i ++ ) { ix . store ( Transaction . BOGUS , key ( i ) , value ( 1 ) ) ; } mDb . checkpoint ( ) ; org . cojen . tupl . Cursor c = ix . newCursor ( null ) ; c . first ( ) ; c . skip ( 10000 ) ; fastAssertArrayEquals ( key ( 10000 ) , c . key ( ) ) ; mDb . checkpoint ( ) ; c . first ( ) ; c . skip ( 10000 ) ; fastAssertArrayEquals ( key ( 10000 ) , c . key ( ) ) ; c . skip ( 900000 ) ; fastAssertArrayEquals ( key ( 910000 ) , c . key ( ) ) ; mDb . checkpoint ( ) ; c . last ( ) ; c . skip ( ( - 99 ) ) ; fastAssertArrayEquals ( key ( 999900 ) , c . key ( ) ) ; c . last ( ) ; c . skip ( ( - 9999 ) ) ; fastAssertArrayEquals ( key ( 990000 ) , c . key ( ) ) ; c . skip ( ( - 980000 ) ) ; fastAssertArrayEquals ( key ( 10000 ) , c . key ( ) ) ; c . skip ( Long . MIN_VALUE ) ; org . junit . Assert . assertNull ( c . key ( ) ) ; try { c . skip ( Long . MAX_VALUE ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void testNewCoordinatorCompletedExchange ( ) { spiFactory = TestRecordingCommunicationSpi :: new ; org . apache . ignite . internal . IgniteEx crd = ( ( org . apache . ignite . internal . IgniteEx ) ( startGrid ( "crd" ) ) ) ; org . apache . ignite . internal . IgniteEx newCrd = startGrid ( 1 ) ; crd . cluster ( ) . active ( true ) ; org . apache . ignite . internal . processors . affinity . AffinityTopologyVersion joinThirdNodeVer = new org . apache . ignite . internal . processors . affinity . AffinityTopologyVersion ( 3 , 0 ) ; org . apache . ignite . internal . processors . affinity . AffinityTopologyVersion joinFourNodeVer = new org . apache . ignite . internal . processors . affinity . AffinityTopologyVersion ( 4 , 0 ) ; org . apache . ignite . internal . TestRecordingCommunicationSpi spi = org . apache . ignite . internal . TestRecordingCommunicationSpi . spi ( crd ) ; final java . util . concurrent . CountDownLatch sendFullMsgLatch = new java . util . concurrent . CountDownLatch ( 1 ) ; spi . blockMessages ( ( node , msg ) -> { if ( ( msg instanceof org . apache . ignite . internal . processors . cache . distributed . dht . preloader . GridDhtPartitionsFullMessage ) && ( ( node . order ( ) ) > 2 ) ) { try { sendFullMsgLatch . await ( ) ; } catch ( ignored ) { } return true ; } return false ; } ) ; org . apache . ignite . internal . IgniteInternalFuture joinTwoNodesFut = org . apache . ignite . testframework . GridTestUtils . runAsync ( ( ) -> startGridsMultiThreaded ( 2 , 2 ) ) ; org . apache . ignite . internal . processors . cache . GridCachePartitionExchangeManager exchangeMgr = newCrd . context ( ) . cache ( ) . context ( ) . exchange ( ) ; org . apache . ignite . testframework . GridTestUtils . waitForCondition ( ( ) -> ( exchangeMgr . readyAffinityVersion ( ) . compareTo ( joinThirdNodeVer ) ) >= 0 , getTestTimeout ( ) ) ; org . apache . ignite . internal . IgniteInternalFuture startLastNodeFut = org . apache . ignite . testframework . GridTestUtils . runAsync ( ( ) -> startGrid ( 5 ) ) ; org . apache . ignite . testframework . GridTestUtils . waitForCondition ( ( ) -> ( exchangeMgr . lastTopologyFuture ( ) . initialVersion ( ) . compareTo ( joinFourNodeVer ) ) >= 0 , getTestTimeout ( ) ) ; org . apache . ignite . internal . IgniteInternalFuture stopCrdFut = org . apache . ignite . testframework . GridTestUtils . runAsync ( ( ) -> stopGrid ( "crd" , true , false ) ) ; org . apache . ignite . internal . util . typedef . internal . U . sleep ( 1000 ) ; sendFullMsgLatch . countDown ( ) ; stopCrdFut . get ( ) ; joinTwoNodesFut . get ( ) ; startLastNodeFut . get ( ) ; awaitPartitionMapExchange ( ) ; for ( org . apache . ignite . Ignite grid : org . apache . ignite . internal . util . typedef . G . allGrids ( ) ) { org . apache . ignite . IgniteCache cache = grid . cache ( org . apache . ignite . internal . processors . cache . PartitionsExchangeCoordinatorFailoverTest . CACHE_NAME ) ; org . junit . Assert . assertNotNull ( cache ) ; cache . put ( 0 , 0 ) ; } } }
public class aTest{ @Test public void testReadFromInputStream ( ) { org . apache . nifi . flowfile . FlowFile flowFile = session . create ( ) ; flowFile = session . write ( flowFile , new org . apache . nifi . processor . io . OutputStreamCallback ( ) { @ org . apache . nifi . controller . repository . Override public void process ( final java . io . OutputStream out ) throws java . io . IOException { out . write ( "hello,<sp>world" . getBytes ( ) ) ; } } ) ; try ( java . io . InputStream in = session . read ( flowFile ) ) { final byte [ ] buffer = new byte [ 12 ] ; org . apache . nifi . stream . io . StreamUtils . fillBuffer ( in , buffer ) ; org . junit . Assert . assertEquals ( "hello,<sp>world" , new java . lang . String ( buffer ) ) ; } }
public class aTest{ @Test public void testParseDate ( ) { java . lang . String [ ] listOfFalseStrings = new java . lang . String [ ] { null , "2017/12/12T11:22:10.999Z" , "2017:12:12T:12:12:12.000Z" , "2017_12_12T_12_12_12_000Z" , "2017;12;12T11;11;11;000Z" , "2017-12-12T12-12-12-000Z" , "2017:12:12T12/12/12/000Z" , "2017.12.12T12.12.12.000Z" , "2017/12/12T12:12:12:000X" } ; int sizeOfFalseStrings = listOfFalseStrings . length ; java . lang . String [ ] listOfPermittedStrings = new java . lang . String [ ] { "2017-12-12T12-12-12-000Z" 1 } ; int sizeOfPermittedStrings = listOfPermittedStrings . length ; for ( int i = 0 ; i < sizeOfFalseStrings ; i ++ ) { if ( ( listOfFalseStrings [ i ] ) == null ) { try { org . junit . Assert . assertNull ( org . eclipse . kapua . commons . util . KapuaDateUtils . parseDate ( listOfFalseStrings [ i ] ) ) ; } }
public class aTest{ @Test public void testNormalizeStagedTuplesInAccumulate ( ) { final java . lang . String drl = "global<sp>java.util.List<sp>list;\n" + ( ( ( ( ( ( ( ( ( ( "<sp>count($l)\n" 3 + "<sp>not(<sp>String()<sp>)\n" ) + "<sp>count($l)\n" 2 ) + "<sp>$l:<sp>Long();\n" ) + "<sp>count($l)\n" ) + "<sp>count($l)\n" 0 ) + "<sp>count($l)\n" 1 ) + "then\n" ) + "<sp>list.add(<sp>\"fired\"<sp>);<sp>\n" ) + "<sp>insert(new<sp>String());\n" ) + "<sp>count($l)\n" 4 ) ; final org . kie . api . KieBase kbase = org . drools . testcoverage . common . util . KieBaseUtil . getKieBaseFromKieModuleFromDrl ( "accumulate-test" , kieBaseTestConfiguration , drl ) ; final org . kie . api . runtime . KieSession ksession = kbase . newKieSession ( ) ; try { final java . util . List < java . lang . String > list = new java . util . ArrayList ( ) ; ksession . setGlobal ( "list" , list ) ; ksession . fireAllRules ( ) ; org . junit . Assert . assertEquals ( 1 , list . size ( ) ) ; } }
public class aTest{ @Test public void testTopNBySegmentResults ( ) { org . apache . druid . query . topn . TopNQuery query = new org . apache . druid . query . topn . TopNQueryBuilder ( ) . dataSource ( QueryRunnerTestHelper . dataSource ) . granularity ( QueryRunnerTestHelper . allGran ) . dimension ( QueryRunnerTestHelper . marketDimension ) . metric ( QueryRunnerTestHelper . dependentPostAggMetric ) . threshold ( 4 ) . intervals ( QueryRunnerTestHelper . fullOnIntervalSpec ) . aggregators ( com . google . common . collect . Lists . newArrayList ( com . google . common . collect . Iterables . concat ( commonAggregators , com . google . common . collect . Lists . newArrayList ( new org . apache . druid . query . aggregation . DoubleMaxAggregatorFactory ( "maxIndex" , "unused" 1 ) , new org . apache . druid . query . aggregation . DoubleMinAggregatorFactory ( "minIndex" , "unused" 1 ) ) ) ) ) . postAggregators ( java . util . Arrays . asList ( QueryRunnerTestHelper . addRowsIndexConstant , QueryRunnerTestHelper . dependentPostAgg ) ) . context ( com . google . common . collect . ImmutableMap . of ( "finalize" , true , "bySegment" , true ) ) . build ( ) ; org . apache . druid . query . topn . TopNResultValue topNResult = new org . apache . druid . query . topn . TopNResultValue ( java . util . Arrays . < java . util . Map < java . lang . String , java . lang . Object > > asList ( com . google . common . collect . ImmutableMap . < java . lang . String , java . lang . Object > builder ( ) . put ( QueryRunnerTestHelper . marketDimension , "unused" 2 ) . put ( "rows" , 186L ) . put ( "unused" 1 , 215679.82879638672 ) . put ( "addRowsIndexConstant" , 215866.82879638672 ) . put ( QueryRunnerTestHelper . dependentPostAggMetric , 216053.82879638672 ) . put ( "unused" 3 , QueryRunnerTestHelper . UNIQUES_2 ) . put ( "maxIndex" , 1743.92175 ) . put ( "minIndex" , 792.3260498046875 ) . build ( ) , com . google . common . collect . ImmutableMap . < java . lang . String , java . lang . Object > builder ( ) . put ( QueryRunnerTestHelper . marketDimension , "unused" 0 ) . put ( "rows" , 186L ) . put ( "unused" 1 , 192046.1060180664 ) . put ( "addRowsIndexConstant" , 192233.1060180664 ) . put ( QueryRunnerTestHelper . dependentPostAggMetric , 192420.1060180664 ) . put ( "unused" 3 , QueryRunnerTestHelper . UNIQUES_2 ) . put ( "maxIndex" , 1870.061029 ) . put ( "minIndex" , 545.9906005859375 ) . build ( ) , com . google . common . collect . ImmutableMap . < java . lang . String , java . lang . Object > builder ( ) . put ( QueryRunnerTestHelper . marketDimension , "spot" ) . put ( "rows" , 837L ) . put ( "unused" 1 , 95606.57232284546 ) . put ( "addRowsIndexConstant" , 96444.57232284546 ) . put ( QueryRunnerTestHelper . dependentPostAggMetric , 97282.57232284546 ) . put ( "unused" 3 , QueryRunnerTestHelper . UNIQUES_9 ) . put ( "maxIndex" , 277.273533 ) . put ( "minIndex" , 59.02102279663086 ) . build ( ) ) ) ; @ org . apache . druid . query . topn . SuppressWarnings ( "unused" ) java . util . List < org . apache . druid . query . Result < org . apache . druid . query . BySegmentResultValueClass < org . apache . druid . query . Result < org . apache . druid . query . topn . TopNResultValue > > > > expectedResults = java . util . Collections . singletonList ( new org . apache . druid . query . Result ( org . apache . druid . java . util . common . DateTimes . of ( "2011-01-12T00:00:00.000Z" ) , new org . apache . druid . query . BySegmentResultValueClass ( java . util . Collections . singletonList ( new org . apache . druid . query . Result ( org . apache . druid . java . util . common . DateTimes . of ( "2011-01-12T00:00:00.000Z" ) , topNResult ) ) , QueryRunnerTestHelper . segmentId . toString ( ) , org . apache . druid . java . util . common . Intervals . of ( "1970-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z" ) ) ) ) ; org . apache . druid . java . util . common . guava . Sequence < org . apache . druid . query . Result < org . apache . druid . query . topn . TopNResultValue > > results = runWithMerge ( query ) ; for ( org . apache . druid . query . Result < org . apache . druid . query . topn . TopNResultValue > result : results . toList ( ) ) { org . junit . Assert . assertEquals ( result . getValue ( ) , result . getValue ( ) ) ; } } }
public class aTest{ @Test public void testResultSetIteratorIsEmptyTrue ( ) { java . lang . String [ ] bindingNameFilter = new java . lang . String [ 1 ] ; bindingNameFilter [ 0 ] = "FILTER_AMOUNT" ; org . eclipse . birt . data . engine . api . IBaseExpression [ ] bindingExprFilter = new org . eclipse . birt . data . engine . api . IBaseExpression [ 1 ] ; bindingExprFilter [ 0 ] = new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( "dataSetRow.AMOUNT" ) ; org . eclipse . birt . data . engine . api . querydefn . FilterDefinition [ ] filterDefn = new org . eclipse . birt . data . engine . api . querydefn . FilterDefinition [ ] { new org . eclipse . birt . data . engine . api . querydefn . FilterDefinition ( new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( "row.FILTER_AMOUNT<sp><<sp>0" ) ) } ; java . lang . String [ ] bindingNameRow = new java . lang . String [ 6 ] ; bindingNameRow [ 0 ] = "row.FILTER_AMOUNT<sp><<sp>0" 2 ; bindingNameRow [ 1 ] = "ROW_rowPosition" ; bindingNameRow [ 2 ] = "ROW_COUNTRY" ; bindingNameRow [ 3 ] = "ROW_CITY" ; bindingNameRow [ 4 ] = "ROW_SALE_DATE" ; bindingNameRow [ 5 ] = "row.FILTER_AMOUNT<sp><<sp>0" 0 ; org . eclipse . birt . data . engine . api . IBaseExpression [ ] bindingExprRow = new org . eclipse . birt . data . engine . api . IBaseExpression [ 6 ] ; bindingExprRow [ 0 ] = new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( "dataSetRow[0]" ) ; bindingExprRow [ 1 ] = new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( "dataSetRow._rowPosition" ) ; bindingExprRow [ 2 ] = new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( "dataSetRow.COUNTRY" ) ; bindingExprRow [ 3 ] = new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( "row.FILTER_AMOUNT<sp><<sp>0" 3 ) ; bindingExprRow [ 4 ] = new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( "row.FILTER_AMOUNT<sp><<sp>0" 1 ) ; bindingExprRow [ 5 ] = new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( "dataSetRow.AMOUNT" ) ; org . eclipse . birt . data . engine . api . querydefn . QueryDefinition queryDefination = createQuery ( null , null , null , null , null , null , bindingNameFilter , bindingExprFilter , filterDefn , bindingNameRow , bindingExprRow ) ; org . eclipse . birt . data . engine . api . IResultIterator result = executeQuery ( queryDefination ) ; org . junit . Assert . assertTrue ( result . isEmpty ( ) ) ; } }
public class aTest{ @Test public void runPerformanceTest ( ) { try { long pojoTime = touchAllObjects ( com . insightfullogic . slab . performance . GameEventPerformanceTest . Accessor . POJO ) ; long slabTime = touchAllObjects ( com . insightfullogic . slab . performance . GameEventPerformanceTest . Accessor . SLAB ) ; org . junit . Assert . assertTrue ( ( slabTime < pojoTime ) ) ; } }
public class aTest{ @Test public void createOrUpdateRequestWithEtagSucceed ( tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . IndividualEnrollment , tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . IndividualEnrollment ) { final java . lang . String registrationId = "registrationId-1" ; final java . lang . String enrollmentPath = "enrollments/" + registrationId ; final java . lang . String enrollmentPayload = "validJson" ; final java . lang . String resultPayload = "validJson" ; final java . lang . String eTag = "validEtag" ; final java . util . Map < java . lang . String , java . lang . String > headerParameters = new java . util . HashMap < java . lang . String , java . lang . String > ( ) { { put ( "If-Match" , eTag ) ; } } ; tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . IndividualEnrollmentManager individualEnrollmentManager = createIndividualEnrollmentManager ( ) ; new tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . StrictExpectations ( ) { { mockedIndividualEnrollment . getRegistrationId ( ) ; result = registrationId ; mockedIndividualEnrollment . toJson ( ) ; result = enrollmentPayload ; mockedIndividualEnrollment . getEtag ( ) ; result = eTag ; times = 2 ; mockedContractApiHttp . request ( HttpMethod . PUT , enrollmentPath , headerParameters , enrollmentPayload ) ; result = mockedHttpResponse ; times = 1 ; mockedHttpResponse . getBody ( ) ; result = resultPayload . getBytes ( ) ; tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . Deencapsulation . newInstance ( tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . IndividualEnrollment . class , resultPayload ) ; result = mockedIndividualEnrollmentResponse ; } } ; tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . IndividualEnrollment response = tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . Deencapsulation . invoke ( individualEnrollmentManager , "createOrUpdate" , new java . lang . Class [ ] { tests . unit . com . microsoft . azure . sdk . iot . provisioning . service . IndividualEnrollment . class } , mockedIndividualEnrollment ) ; org . junit . Assert . assertNotNull ( response ) ; } }
public class aTest{ @Test public void testReloadAndSetInterpreter ( ) { org . apache . zeppelin . notebook . Note note = notebook . createNote ( "note1" , AuthenticationInfo . ANONYMOUS ) ; org . apache . zeppelin . notebook . Paragraph p1 = note . insertNewParagraph ( 0 , AuthenticationInfo . ANONYMOUS ) ; p1 . setText ( "%md<sp>hello<sp>world" ) ; notebook . reloadAllNotes ( anonymous ) ; org . junit . Assert . assertEquals ( 1 , notebook . getAllNotes ( ) . size ( ) ) ; note = notebook . getAllNotes ( ) . get ( 0 ) ; try { note . getParagraphs ( ) . get ( 0 ) . getBindedInterpreter ( ) ; org . junit . Assert . fail ( "Should<sp>throw<sp>InterpreterNotFoundException" ) ; } }
public class aTest{ @Test public void testAddMultiFiles ( ) { com . geekhua . filequeue . Config config = new com . geekhua . filequeue . Config ( ) ; config . setBaseDir ( com . geekhua . filequeue . FileQueueImplTest . baseDir . getAbsolutePath ( ) ) ; config . setMsgAvgLen ( 10 ) ; config . setName ( "test" ) ; config . setFileSiz ( 1024 ) ; com . geekhua . filequeue . FileQueue < java . lang . Integer > fq = new com . geekhua . filequeue . FileQueueImpl < java . lang . Integer > ( config ) ; int times = 1000 ; for ( int i = 0 ; i < times ; i ++ ) { fq . add ( i ) ; } for ( int i = 0 ; i < times ; i ++ ) { org . junit . Assert . assertEquals ( java . lang . Integer . valueOf ( i ) , fq . get ( ) ) ; } }
public class aTest{ @Test public void testAllFiles ( ) { java . io . File dir = new java . io . File ( "./src/test/testdata/fwcommon/objectmanager" ) ; java . io . File [ ] files = dir . listFiles ( ) ; org . junit . Assert . assertTrue ( ( ( files . length ) > 0 ) ) ; long start = java . lang . System . currentTimeMillis ( ) ; for ( java . io . File f : files ) { if ( f . getName ( ) . endsWith ( ".xml" ) ) { new com . gs . fw . common . freyaxml . test . fwcommon . objectmanager . FWCommonObjectManagerUnmarshaller ( ) . parse ( f . getCanonicalPath ( ) ) ; } } }
public class aTest{ @Test public void testServicesAnnotatedNoneSetButImplements ( ) { @ com . liferay . portal . kernel . spring . osgi . OSGiBeanProperties class C implements java . io . Serializable { } java . util . Set < java . lang . String > interfaceNames = OSGiBeanProperties . Service . interfaceNames ( new C ( ) , C . class . getAnnotation ( com . liferay . portal . kernel . spring . osgi . OSGiBeanProperties . class ) , StringPool . EMPTY_ARRAY ) ; org . junit . Assert . assertEquals ( interfaceNames . toString ( ) , 2 , interfaceNames . size ( ) ) ; } }
public class aTest{ @Test public void testCleanLast ( ) { java . io . File tablefile = new java . io . File ( java . lang . System . getProperty ( "java.io.tmpdir" ) , "test2.stack" ) ; byte [ ] b = net . yacy . cora . document . encoding . ASCII . getBytes ( "testdata" ) ; net . yacy . kelondro . io . Records rec = new net . yacy . kelondro . io . Records ( tablefile , b . length ) ; try { rec . add ( b , 0 ) ; for ( int i = 0 ; i < 5 ; i ++ ) { rec . cleanLast ( ) ; } org . junit . Assert . assertEquals ( 0 , rec . size ( ) ) ; } }
public class aTest{ @Test public void testRequestMemorySegmentsInterruptable ( ) { final int numBuffers = 10 ; org . apache . flink . runtime . io . network . buffer . NetworkBufferPool globalPool = new org . apache . flink . runtime . io . network . buffer . NetworkBufferPool ( numBuffers , 128 ) ; org . apache . flink . core . memory . MemorySegment segment = globalPool . requestMemorySegment ( ) ; org . junit . Assert . assertNotNull ( segment ) ; final org . apache . flink . core . testutils . OneShotLatch isRunning = new org . apache . flink . core . testutils . OneShotLatch ( ) ; org . apache . flink . core . testutils . CheckedThread asyncRequest = new org . apache . flink . core . testutils . CheckedThread ( ) { @ org . apache . flink . runtime . io . network . buffer . Override public void go ( ) throws org . apache . flink . runtime . io . network . buffer . Exception { isRunning . trigger ( ) ; globalPool . requestMemorySegments ( 10 ) ; } } }
public class aTest{ @Test public void test ( ) { org . aksw . fox . tools . re . REToolsTest . LOG . info ( "REToolsTest<sp>start<sp>..." ) ; final org . aksw . fox . tools . ToolsGenerator toolsGenerator = new org . aksw . fox . tools . ToolsGenerator ( ) ; for ( final java . lang . String lang : org . aksw . fox . tools . ToolsGenerator . usedLang ) { final org . aksw . fox . tools . re . RETools reTools = toolsGenerator . getRETools ( lang ) ; final java . util . List < org . aksw . fox . tools . re . IRE > tools = reTools . getRETool ( lang ) ; org . junit . Assert . assertNotNull ( "test" , tools ) ; } }
public class aTest{ @Test public void convertArrayToStream ( mockit . ExpectationsUsingReturnTypeConversionTest$Java8Collaborator ) { org . junit . Assume . assumeTrue ( mockit . JAVA8 ) ; final java . lang . String [ ] values = new java . lang . String [ ] { "Test" , "<sp>abc<sp>" } ; new mockit . Expectations ( ) { { mock2 . getStream ( ) ; result = values ; } } ; java . lang . Object [ ] resultingValues = mock2 . getStream ( ) . toArray ( ) ; org . junit . Assert . assertArrayEquals ( values , resultingValues ) ; } }
public class aTest{ @Test public void shouldInstrumentES6ClassConstructor ( ) { java . lang . String source = "class<sp>Rectangle<sp>{\n" + ( ( ( ( "class<sp>Rectangle<sp>{\n" 0 + "<sp>this.height<sp>=<sp>height;\n" ) + "<sp>this.width<sp>=<sp>width;\n" ) + "<sp>}\n" ) + "}" ) ; java . lang . String instrumentedSource = sourceProcessor . instrumentSource ( source ) ; java . lang . String expectedSource = "_$jscoverage[\'test.js\'].lineData[1]++;\n" + ( ( ( ( ( ( ( ( "class<sp>Rectangle<sp>{\n" + "class<sp>Rectangle<sp>{\n" 0 ) + "<sp>_$jscoverage[\'test.js\'].functionData[0]++;\n" ) + "<sp>_$jscoverage[\'test.js\'].lineData[3]++;\n" ) + "<sp>this.height<sp>=<sp>height;\n" ) + "<sp>_$jscoverage[\'test.js\'].lineData[4]++;\n" ) + "<sp>this.width<sp>=<sp>width;\n" ) + "<sp>}\n" ) + "}\n" ) ; org . junit . Assert . assertEquals ( expectedSource , instrumentedSource ) ; } }
public class aTest{ @Test public void testPreferences ( ) { java . net . URL location = getStorageLocation ( ) ; org . junit . Assert . assertNotNull ( location ) ; { org . eclipse . equinox . security . storage . ISecurePreferences preferences = newPreferences ( getStorageLocation ( ) , getOptions ( ) ) ; fill ( preferences ) ; preferences . flush ( ) ; closePreferences ( preferences ) ; } }
public class aTest{ @Test public void testFormatDate ( ) { java . util . Date falseDate = null ; java . util . Date permittedDate = new java . util . Date ( ) ; try { org . junit . Assert . assertNull ( org . eclipse . kapua . commons . util . KapuaDateUtils . formatDate ( falseDate ) ) ; } }
public class aTest{ @Test public void testLowSyncpoint ( ) { final org . apache . hadoop . conf . Configuration conf = new org . apache . hadoop . conf . Configuration ( ) ; final org . apache . hadoop . fs . FileSystem fs = org . apache . hadoop . fs . FileSystem . getLocal ( conf ) ; final org . apache . hadoop . fs . Path path = new org . apache . hadoop . fs . Path ( org . apache . hadoop . test . GenericTestUtils . getTempPath ( "sequencefile.sync.test" ) ) ; final org . apache . hadoop . io . IntWritable input = new org . apache . hadoop . io . IntWritable ( ) ; final org . apache . hadoop . io . Text val = new org . apache . hadoop . io . Text ( ) ; org . apache . hadoop . io . SequenceFile . Writer writer = new org . apache . hadoop . io . SequenceFile . Writer ( conf , SequenceFile . Writer . file ( path ) , SequenceFile . Writer . compression ( CompressionType . NONE ) , SequenceFile . Writer . keyClass ( org . apache . hadoop . io . IntWritable . class ) , SequenceFile . Writer . valueClass ( org . apache . hadoop . io . Text . class ) , SequenceFile . Writer . syncInterval ( ( 20 * 100 ) ) ) ; org . junit . Assert . assertEquals ( writer . syncInterval , ( 20 * 100 ) ) ; try { org . apache . hadoop . io . TestSequenceFileSync . writeSequenceFile ( writer , org . apache . hadoop . io . TestSequenceFileSync . NUMRECORDS ) ; for ( int i = 0 ; i < 5 ; i ++ ) { final org . apache . hadoop . io . SequenceFile . Reader reader ; if ( ( i % 2 ) == 0 ) { final int bufferSize = conf . getInt ( "io.file.buffer.size" , 4096 ) ; reader = new org . apache . hadoop . io . SequenceFile . Reader ( conf , SequenceFile . Reader . file ( path ) , SequenceFile . Reader . bufferSize ( bufferSize ) ) ; } }
public class aTest{ @Test public void testIfPresentOrElseWhenValuePresentAndEmptyActionNull ( ) { com . annimon . stream . Optional . of ( 10 ) . ifPresentOrElse ( new com . annimon . stream . function . Consumer < java . lang . Integer > ( ) { @ com . annimon . stream . Override public void accept ( java . lang . Integer value ) { org . junit . Assert . assertEquals ( 10 , ( ( int ) ( value ) ) ) ; } } }
public class aTest{ @Test public void testBlokkeringAntwoordFout ( ) { final nl . bzk . migratiebrp . bericht . model . sync . generated . BlokkeringVerzoekType blokkeringVerzoekType = new nl . bzk . migratiebrp . bericht . model . sync . generated . BlokkeringVerzoekType ( ) ; final nl . bzk . migratiebrp . bericht . model . sync . impl . BlokkeringVerzoekBericht blokkeringInfoVerzoek = new nl . bzk . migratiebrp . bericht . model . sync . impl . BlokkeringVerzoekBericht ( blokkeringVerzoekType ) ; try { blokkeringVerzoekService . verwerkBericht ( blokkeringInfoVerzoek ) ; org . junit . Assert . fail ( "Er<sp>zou<sp>een<sp>fout<sp>op<sp>moeten<sp>treden." ) ; } catch ( final java . lang . Exception e ) { org . junit . Assert . assertNotNull ( "Er<sp>zou<sp>een<sp>fout<sp>op<sp>moeten<sp>treden." , e ) ; } }
public class aTest{ @Test public void testPrevious ( ) { java . util . List [ ] results = org . teiid . jdbc . TestAllResultsImpl . exampleResults1 ( 5 ) ; org . teiid . jdbc . ResultSetImpl rs = new org . teiid . jdbc . ResultSetImpl ( exampleResultsMsg1 ( ) , statement ) ; while ( rs . next ( ) ) { } int i = ( results . length ) - 1 ; while ( rs . previous ( ) ) { java . util . List expected = new java . util . ArrayList ( 1 ) ; expected . add ( new java . lang . Integer ( ( i + 1 ) ) ) ; org . junit . Assert . assertEquals ( expected , rs . getCurrentRecord ( ) ) ; i -- ; } }
public class aTest{ @Test public void notifyFailureAfterDoAuthenticateThrowsAuthenticationException ( ) { org . apache . shiro . authc . AuthenticationListener mockListener = createMock ( org . apache . shiro . authc . AuthenticationListener . class ) ; org . apache . shiro . authc . AuthenticationToken token = newToken ( ) ; final org . apache . shiro . authc . AuthenticationException ae = new org . apache . shiro . authc . AuthenticationException ( "dummy<sp>exception<sp>to<sp>test<sp>notification" ) ; abstractAuthenticator = new org . apache . shiro . authc . AbstractAuthenticator ( ) { protected org . apache . shiro . authc . AuthenticationInfo doAuthenticate ( org . apache . shiro . authc . AuthenticationToken token ) throws org . apache . shiro . authc . AuthenticationException { throw ae ; } } ; abstractAuthenticator . getAuthenticationListeners ( ) . add ( mockListener ) ; mockListener . onFailure ( token , ae ) ; replay ( mockListener ) ; boolean exceptionThrown = false ; try { abstractAuthenticator . authenticate ( token ) ; } catch ( org . apache . shiro . authc . AuthenticationException e ) { exceptionThrown = true ; org . junit . Assert . assertEquals ( e , ae ) ; } }
public class aTest{ @Test public void testOutOfBoundCols ( ) { java . sql . Statement stmt = con . createStatement ( ) ; java . sql . ResultSet res = stmt . executeQuery ( ( "select<sp>*<sp>from<sp>" + ( org . apache . hive . jdbc . cbo_rp_TestJdbcDriver2 . tableName ) ) ) ; org . junit . Assert . assertTrue ( res . next ( ) ) ; try { res . getInt ( 200 ) ; } }
public class aTest{ @Test public void testGetBaseUrl ( ) { org . mockito . Mockito . when ( urlMock . openConnection ( ) ) . thenReturn ( httpURLConnectionMock ) ; org . mockito . Mockito . when ( urlMock . toString ( ) ) . thenReturn ( "headerKey2" 1 ) ; java . lang . String payload = "line1\nline2\nline3\nline4\n" ; java . io . InputStream stream = new java . io . ByteArrayInputStream ( payload . getBytes ( StandardCharsets . UTF_8 ) ) ; java . util . Map < java . lang . String , java . util . List < java . lang . String > > headers = new java . util . HashMap < java . lang . String , java . util . List < java . lang . String > > ( ) ; java . util . List < java . lang . String > header1 = new java . util . ArrayList < java . lang . String > ( ) ; java . util . List < java . lang . String > header2 = new java . util . ArrayList < java . lang . String > ( ) ; java . util . List < java . lang . String > header3 = new java . util . ArrayList < java . lang . String > ( ) ; header1 . add ( "headerKey2" 4 ) ; header1 . add ( "header12" ) ; header1 . add ( "headerKey2" 0 ) ; header2 . add ( "headerKey2" 2 ) ; header2 . add ( "headerKey2" 6 ) ; header2 . add ( "headerKey2" 7 ) ; header3 . add ( "header31" ) ; headers . put ( "headerKey2" 5 , header1 ) ; headers . put ( "headerKey2" , header2 ) ; headers . put ( "headerKey3" , header3 ) ; org . mockito . Mockito . when ( httpURLConnectionMock . getResponseCode ( ) ) . thenReturn ( 200 ) ; org . mockito . Mockito . when ( httpURLConnectionMock . getInputStream ( ) ) . thenReturn ( stream ) ; org . mockito . Mockito . when ( httpURLConnectionMock . getRequestMethod ( ) ) . thenReturn ( "GET" ) ; org . mockito . Mockito . when ( httpURLConnectionMock . getHeaderFields ( ) ) . thenReturn ( headers ) ; org . mockito . Mockito . when ( httpURLConnectionMock . getResponseMessage ( ) ) . thenReturn ( "OK" ) ; org . mockito . Mockito . when ( httpURLConnectionMock . getContentType ( ) ) . thenReturn ( "application/json;charset=UTF-8" ) ; com . blazemeter . jmeter . hls . logic . DataRequest dataRequestResult = p . getBaseUrl ( urlMock , sampleResultMock , true ) ; com . blazemeter . jmeter . hls . logic . DataRequest dataRequestExpected = new com . blazemeter . jmeter . hls . logic . DataRequest ( ) ; dataRequestExpected . setRequestHeaders ( "GET<sp>http://www.mock.com\n" ) ; dataRequestExpected . setHeaders ( headers ) ; dataRequestExpected . setResponse ( payload ) ; dataRequestExpected . setResponseCode ( "200" ) ; dataRequestExpected . setResponseMessage ( "OK" ) ; dataRequestExpected . setContentType ( "application/json;charset=UTF-8" ) ; dataRequestExpected . setSuccess ( true ) ; dataRequestExpected . setSentBytes ( payload . length ( ) ) ; dataRequestExpected . setContentEncoding ( "headerKey2" 3 ) ; org . junit . Assert . assertEquals ( dataRequestExpected , dataRequestResult ) ; } }
public class aTest{ @Test public void shouldFindTotalUsers ( ) { executeUpdate ( "CREATE<sp>(m:User<sp>{name:'Michal'})<-[:FRIEND_OF]-(a:User<sp>{name:'Adam'})" ) ; transactionTemplate . execute ( new org . springframework . transaction . support . TransactionCallbackWithoutResult ( ) { @ org . springframework . data . neo4j . queries . Override public void doInTransactionWithoutResult ( org . springframework . transaction . TransactionStatus status ) { int users = userRepository . findTotalUsers ( ) ; org . junit . Assert . assertEquals ( users , 2 ) ; } } }
public class aTest{ @Test public void testSubInheritanceHierarchyFunction ( ) { spoon . reflect . factory . Factory factory = spoon . testing . utils . ModelUtils . build ( new java . io . File ( "./src/test/java/spoon/test/refactoring/parameter/testclasses" ) ) ; java . util . List < java . lang . String > allSubtypes = factory . Class ( ) . get ( spoon . test . refactoring . parameter . testclasses . TypeA . class ) . map ( new spoon . reflect . visitor . filter . SubInheritanceHierarchyFunction ( ) ) . map ( ( spoon . reflect . declaration . CtType type ) -> type . getQualifiedName ( ) ) . list ( ) ; checkContainsOnly ( allSubtypes , "spoon.test.refactoring.parameter.testclasses.TypeB" , "spoon.test.refactoring.parameter.testclasses.TypeB$1" , "spoon.test.refactoring.parameter.testclasses.TypeC" ) ; allSubtypes = factory . Class ( ) . get ( spoon . test . refactoring . parameter . testclasses . TypeB . class ) . map ( new spoon . reflect . visitor . filter . SubInheritanceHierarchyFunction ( ) ) . map ( ( spoon . reflect . declaration . CtType type ) -> type . getQualifiedName ( ) ) . list ( ) ; checkContainsOnly ( allSubtypes , "spoon.test.refactoring.parameter.testclasses.TypeB$1" , "spoon.test.refactoring.parameter.testclasses.TypeC" ) ; allSubtypes = factory . Class ( ) . get ( spoon . test . refactoring . parameter . testclasses . TypeC . class ) . map ( new spoon . reflect . visitor . filter . SubInheritanceHierarchyFunction ( ) ) . map ( ( spoon . reflect . declaration . CtType type ) -> type . getQualifiedName ( ) ) . list ( ) ; org . junit . Assert . assertEquals ( 0 , allSubtypes . size ( ) ) ; allSubtypes = factory . Interface ( ) . get ( spoon . test . refactoring . parameter . testclasses . IFaceB . class ) . map ( new spoon . reflect . visitor . filter . SubInheritanceHierarchyFunction ( ) ) . map ( ( spoon . reflect . declaration . CtType type ) -> type . getQualifiedName ( ) ) . list ( ) ; checkContainsOnly ( allSubtypes , "spoon.test.refactoring.parameter.testclasses.TypeB" , "spoon.test.refactoring.parameter.testclasses.TypeB$1" , "spoon.test.refactoring.parameter.testclasses.TypeB$1Local" , "spoon.test.refactoring.parameter.testclasses.TypeM" 1 , "spoon.test.refactoring.parameter.testclasses.TypeC" , "spoon.test.refactoring.parameter.testclasses.IFaceL" , "spoon.test.refactoring.parameter.testclasses.TypeL" , "spoon.test.refactoring.parameter.testclasses.TypeM" ) ; allSubtypes = factory . Interface ( ) . get ( spoon . test . refactoring . parameter . testclasses . IFaceL . class ) . map ( new spoon . reflect . visitor . filter . SubInheritanceHierarchyFunction ( ) ) . map ( ( spoon . reflect . declaration . CtType type ) -> type . getQualifiedName ( ) ) . list ( ) ; checkContainsOnly ( allSubtypes , "spoon.test.refactoring.parameter.testclasses.TypeB$1Local" , "spoon.test.refactoring.parameter.testclasses.TypeL" , "spoon.test.refactoring.parameter.testclasses.TypeM" ) ; allSubtypes = factory . Interface ( ) . get ( spoon . test . refactoring . parameter . testclasses . IFaceK . class ) . map ( new spoon . reflect . visitor . filter . SubInheritanceHierarchyFunction ( ) ) . map ( ( spoon . reflect . declaration . CtType type ) -> type . getQualifiedName ( ) ) . list ( ) ; checkContainsOnly ( allSubtypes , "spoon.test.refactoring.parameter.testclasses.TypeB$1Local" , "spoon.test.refactoring.parameter.testclasses.TypeL" , "spoon.test.refactoring.parameter.testclasses.TypeM" , "spoon.test.refactoring.parameter.testclasses.TypeK" , "spoon.test.refactoring.parameter.testclasses.TypeM" 0 , "spoon.test.refactoring.parameter.testclasses.TypeS" ) ; } }
public class aTest{ @Test public void deleteObjectNotExistingTest ( ) { server . deleteObject ( org . talend . esb . auxiliary . storage . service . rest . AuxiliaryStorageRestServiceImplTest . KEY ) ; org . easymock . EasyMock . expectLastCall ( ) . andStubThrow ( new org . talend . esb . auxiliary . storage . common . exception . ObjectNotFoundException ( "Object<sp>is<sp>not<sp>found" ) ) ; replayAll ( ) ; try { restService . remove ( org . talend . esb . auxiliary . storage . service . rest . AuxiliaryStorageRestServiceImplTest . KEY ) ; } catch ( java . lang . Exception ex ) { org . junit . Assert . assertTrue ( ( ex instanceof org . talend . esb . auxiliary . storage . common . exception . ObjectNotFoundException ) ) ; } }
public class aTest{ @Test public void testNewPacket ( ) { try { org . pcap4j . packet . Dot11ProbeRequestPacket p = org . pcap4j . packet . Dot11ProbeRequestPacket . newPacket ( packet . getRawData ( ) , 0 , packet . getRawData ( ) . length ) ; org . junit . Assert . assertEquals ( packet , p ) ; } }
public class aTest{ @Test public void clientPartialServerWholeByteBuffer ( ) { org . glassfish . tyrus . server . Server server = startServer ( org . glassfish . tyrus . test . standard_config . MessageHandlersTest . WholeByteBuffer . class ) ; try { final javax . websocket . ClientEndpointConfig cec = ClientEndpointConfig . Builder . create ( ) . build ( ) ; messageLatch = new java . util . concurrent . CountDownLatch ( 1 ) ; org . glassfish . tyrus . client . ClientManager client = createClient ( ) ; client . connectToServer ( new javax . websocket . Endpoint ( ) { @ org . glassfish . tyrus . test . standard_config . Override public void onOpen ( javax . websocket . Session session , javax . websocket . EndpointConfig EndpointConfig ) { session . addMessageHandler ( new javax . websocket . MessageHandler . Whole < java . nio . ByteBuffer > ( ) { @ java . lang . Override public void onMessage ( java . nio . ByteBuffer message ) { if ( new java . lang . String ( message . array ( ) ) . equals ( "In<sp>my<sp>experience,<sp>there's<sp>no<sp>such<sp>thing<sp>as<sp>luck." ) ) { messageLatch . countDown ( ) ; } } } ) ; try { session . getBasicRemote ( ) . sendBinary ( java . nio . ByteBuffer . wrap ( "In<sp>my<sp>experience" . getBytes ( ) ) , false ) ; session . getBasicRemote ( ) . sendBinary ( java . nio . ByteBuffer . wrap ( ",<sp>there's<sp>no<sp>such<sp>" . getBytes ( ) ) , false ) ; session . getBasicRemote ( ) . sendBinary ( java . nio . ByteBuffer . wrap ( "thing<sp>as<sp>luck." . getBytes ( ) ) , true ) ; } catch ( java . io . IOException e ) { } } } , cec , getURI ( org . glassfish . tyrus . test . standard_config . MessageHandlersTest . WholeByteBuffer . class ) ) ; messageLatch . await ( 1 , TimeUnit . SECONDS ) ; org . junit . Assert . assertEquals ( 0 , messageLatch . getCount ( ) ) ; } }
public class aTest{ @Test public void testCreateIntrabandCustomizedImpl ( ) { java . lang . System . setProperty ( PropsKeys . INTRABAND_IMPL , com . liferay . portal . kernel . nio . intraband . nonblocking . SelectorIntraband . class . getName ( ) ) ; com . liferay . portal . kernel . nio . intraband . Intraband intraband = null ; try { intraband = com . liferay . portal . kernel . nio . intraband . IntrabandFactoryUtil . createIntraband ( ) ; org . junit . Assert . assertSame ( com . liferay . portal . kernel . nio . intraband . nonblocking . SelectorIntraband . class , intraband . getClass ( ) ) ; } }
public class aTest{ @Test public void NoPathTest ( ) { java . util . HashSet < hu . elte . txtuml . utils . Pair < hu . elte . txtuml . layout . visualizer . algorithms . links . graphsearchhelpers . Node , java . lang . Double > > startset = new java . util . HashSet < hu . elte . txtuml . utils . Pair < hu . elte . txtuml . layout . visualizer . algorithms . links . graphsearchhelpers . Node , java . lang . Double > > ( ) ; startset . addAll ( _startA ) ; java . util . HashSet < hu . elte . txtuml . utils . Pair < hu . elte . txtuml . layout . visualizer . algorithms . links . graphsearchhelpers . Node , java . lang . Double > > endset = new java . util . HashSet < hu . elte . txtuml . utils . Pair < hu . elte . txtuml . layout . visualizer . algorithms . links . graphsearchhelpers . Node , java . lang . Double > > ( ) ; endset . addAll ( _endB ) ; java . util . HashMap < hu . elte . txtuml . layout . visualizer . model . Point , hu . elte . txtuml . layout . visualizer . algorithms . links . graphsearchhelpers . Color > colors = new java . util . HashMap < hu . elte . txtuml . layout . visualizer . model . Point , hu . elte . txtuml . layout . visualizer . algorithms . links . graphsearchhelpers . Color > ( ) ; colors . putAll ( _colorsA ) ; colors . putAll ( _colorsB ) ; colors . put ( new hu . elte . txtuml . layout . visualizer . model . Point ( 0 , ( - 2 ) ) , Color . Red ) ; colors . put ( new hu . elte . txtuml . layout . visualizer . model . Point ( 0 , 2 ) , Color . Red ) ; colors . put ( new hu . elte . txtuml . layout . visualizer . model . Point ( ( - 2 ) , 0 ) , Color . Red ) ; colors . put ( new hu . elte . txtuml . layout . visualizer . model . Point ( 2 , 0 ) , Color . Red ) ; try { hu . elte . txtuml . layout . visualizer . algorithms . links . GraphSearch gs = new hu . elte . txtuml . layout . visualizer . algorithms . links . GraphSearch ( startset , endset , colors , _bounds ) ; gs . value ( ) ; org . junit . Assert . fail ( "Test<sp>failed!" ) ; } catch ( hu . elte . txtuml . layout . visualizer . exceptions . CannotFindAssociationRouteException | hu . elte . txtuml . layout . visualizer . exceptions . CannotStartAssociationRouteException e ) { org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testCompareLongGTDecimal2 ( ) { long ts = nextTimestamp ( ) ; org . apache . phoenix . end2end . CompareDecimalToLongIT . initTableValues ( null , ts ) ; java . lang . String query = "SELECT<sp>l<sp>FROM<sp>LongInKeyTest<sp>where<sp>l<sp>><sp>2.5" ; java . util . Properties props = new java . util . Properties ( ) ; props . setProperty ( PhoenixRuntime . CURRENT_SCN_ATTRIB , java . lang . Long . toString ( ( ts + 2 ) ) ) ; java . sql . Connection conn = java . sql . DriverManager . getConnection ( getUrl ( ) , props ) ; try { java . sql . PreparedStatement statement = conn . prepareStatement ( query ) ; java . sql . ResultSet rs = statement . executeQuery ( ) ; org . junit . Assert . assertFalse ( rs . next ( ) ) ; } }
public class aTest{ @Test public void testReadWorldName ( ) { System . out . println ( "readWorldName" ) ; java . lang . String worldName = "foobar" ; mudmap2 . backend . World world = new mudmap2 . backend . World ( worldName ) ; java . lang . String wfjFile = ( folder . getRoot ( ) ) + "/wfj" ; try { mudmap2 . backend . WorldFileReader . current . WorldFileJSON wfj = new mudmap2 . backend . WorldFileReader . current . WorldFileJSON ( wfjFile ) ; wfj . writeFile ( world ) ; } catch ( java . io . IOException ex ) { java . util . logging . Logger . getLogger ( mudmap2 . backend . WorldFileReader . current . WorldFileJSONTest . class . getName ( ) ) . log ( Level . SEVERE , null , ex ) ; org . junit . Assert . fail ( "Could<sp>not<sp>create<sp>files<sp>for<sp>test" ) ; } mudmap2 . backend . WorldFileReader . current . WorldFileJSON instance = new mudmap2 . backend . WorldFileReader . current . WorldFileJSON ( wfjFile ) ; try { java . lang . String result = instance . readWorldName ( ) ; org . junit . Assert . assertEquals ( worldName , result ) ; } }
public class aTest{ @Test public void clientWholeServerPartialByteArray ( ) { org . glassfish . tyrus . server . Server server = startServer ( org . glassfish . tyrus . test . standard_config . MessageHandlersTest . PartialByteArray . class ) ; try { final javax . websocket . ClientEndpointConfig cec = ClientEndpointConfig . Builder . create ( ) . build ( ) ; messageLatch = new java . util . concurrent . CountDownLatch ( 1 ) ; org . glassfish . tyrus . client . ClientManager client = createClient ( ) ; client . connectToServer ( new javax . websocket . Endpoint ( ) { @ org . glassfish . tyrus . test . standard_config . Override public void onOpen ( javax . websocket . Session session , javax . websocket . EndpointConfig EndpointConfig ) { session . addMessageHandler ( new javax . websocket . MessageHandler . Whole < byte [ ] > ( ) { @ java . lang . Override public void onMessage ( byte [ ] message ) { if ( new java . lang . String ( message ) . equals ( "In<sp>my<sp>experience,<sp>there's<sp>no<sp>such<sp>thing<sp>as<sp>luck." ) ) { messageLatch . countDown ( ) ; } } } ) ; try { session . getBasicRemote ( ) . sendBinary ( java . nio . ByteBuffer . wrap ( "In<sp>my<sp>experience,<sp>there's<sp>no<sp>such<sp>thing<sp>as<sp>luck." . getBytes ( ) ) ) ; } catch ( java . io . IOException e ) { } } } , cec , getURI ( org . glassfish . tyrus . test . standard_config . MessageHandlersTest . PartialByteArray . class ) ) ; messageLatch . await ( 1 , TimeUnit . SECONDS ) ; org . junit . Assert . assertEquals ( 0 , messageLatch . getCount ( ) ) ; } }
public class aTest{ @Test public void testWrongIndicatorWhileMultilineProcessing ( ) { java . lang . String line1 = "M:00,02,VgIMAQpXb2huemltbWVyCvMrAgtUb2lsZXR0ZSBFRwrenQMOVG9pbGV0dGUgMS4gT0cK3rgECkJhZGV6aW1tZXIK3qoFDFNjaGxhZnppbW1lcgresQYDSmFuD4lCBwlDaHJpc3RpbmEPiTYIBEZsdXIPiT0KEEJhZGV6aW1tZXIgMi4gT0cPiRwLBULDvHJvD4k/DAxHw6RzdGV6aW1tZXIPiRoJC1dhc2Noa8O8Y2hlD4lXNgQHOCtLRVEwMTg4NjczCFRlcnJhc3NlAQQHMblLRVEwMTg3MTkwCEZsdXJ0w7xyAQIK8ytLRVEwMzc5NTg3C1dhbmRoZWl6dW5nAQIK9P9LRVEwMzgwMDU1DkZlbnN0ZXJoZWl6dW5nAQQHMbtLRVEwMTg3MTg4CEZsdXJ0w7xyAgQHMuxLRVEwMTg2ODg0B0ZlbnN0ZXICAQrenUtFUTA0MDY5NjIHSGVpenVuZwIBCt64S0VRMDQwNjk4OQdIZWl6dW5nAwQIFGdLRVEwMTkwNTc3B0ZlbnN0ZXIDBAc2l0tFUTAxODU5NDUIRmx1cnTDvHIEAQreqktFUTA0MDY5NzUHSGVpenVuZwQBCt8JS0VRMDQwNzA3MA5IYW5kdHVjaGVpenVuZwQEBzhTS0VRMDE4ODcxMAdGZW5zdGVyBAQIFIxLRVEwMTkwNTQzFkZlbnN0ZXIgU3RyYcOfZSByZWNodHMFAQresUtFUTA0MDY5ODIHSGVpenVuZwUEBzHmS0VRMDE4NzE0NhVGZW5zdGVyIFN0cmHDn2UgbGlua3MFAxBXqUxFUTA5ODIxNTYOV2FuZHRoZXJtb3N0YXQBBA/u1ExFUTA3OTQ3NTIIRmx1cnTDvHIGBA/v6kxFUTA3OTQ0NzQNRmVuc3RlciBsaW5rcwYED/HnTEVRMDc5Mzk2NA5GZW5zdGVyIHJlY2h0cwYBD4lCTEVRMTAwNDYwMAdIZWl6dW5nBgQP9BVMRVEwNzkzNDA2CEZsdXJ0w7xyBwQP79FMRVEwNzk0NDk5B0ZlbnN0ZXIHAQ+JNkxFUTEwMDQ1ODgHSGVpenVuZwcBD4k9TEVRMTAwNDU5NQ1IZWl6dW5nIHVudGVuCAEPiRxMRVExMDA0NTYyB0hlaXp1bmcKBA/yTUxFUTA3OTM4NjIHRmVuc3RlcgoED/F+TEVRMDc5NDA2OQhGbHVydMO8cgoBD4k/TEVRMTAwNDU5NwdIZWl6dW5nCwQP8YdMRVEwNzk0MDYwB0ZlbnN0ZXILBA/xSExFUTA3OTQxMjQIRmx1cnTDvHILBA/yVkxFUTA3OTM4NTMURmVuc3RlciBHYXJ0ZW4gbGlua3MMBA/yI0xFUTA3OTM5MDQVRmVuc3RlciBHYXJ0ZW4gcmVjaHRzDAEPiRpMRVExMDA0NTYwB0hlaXp1bmcMBA/vj0xFUTA3OTQ1NjUPRmVuc3RlciBTdHJhw59lDAQP8CtMRVEwNzk0NDA5BFTDvHIDBAgUa0tFUTAxODcwNjkNRmVuc3RlciBTZWl0ZQUEBzagS0VRMDE4NTkzNhVGZW5zdGVyIFN0cmHDn2UgbGlua3MBBA/wI0xFUTA3OTQ0MTYORmVuc3RlciBLw7xjaGUBAxBV50xFUTA5ODI2NzYOV2FuZHRoZXJtb3N0YXQFAxBW2kxFUTA5ODIzNjgOV2FuZHRoZXJtb3N0YXQEAxBV4kxFUTA5ODI2NzEOV2FuZHRoZXJtb3N0YXQHAxBZWExFUTA5ODE3MjkOV2FuZHRoZXJtb3N0YXQMAxBV6ExFUTA5ODI2NzcOV2FuZHRoZXJtb3N0YXQGAxBV40xFUTA5ODI2NzIOV2FuZHRoZXJtb3N0YXQKBAcxoEtFUTAxODcyMTYLV2FzY2hrw7xjaGUF" ; java . lang . String line2 = "H:KHA0007199,081dd4,0113,00000000,0d524351,10,30,0f0407,1130,03,0000" ; try { org . junit . Assert . assertFalse ( this . processor . addReceivedLine ( line1 ) ) ; this . processor . addReceivedLine ( line2 ) ; org . junit . Assert . fail ( "Expected<sp>exception<sp>was<sp>not<sp>thrown." ) ; } }
public class aTest{ @Test public void testDeclaredSlidingWindowWith2Arguments ( ) { java . lang . String str = "declare<sp>String\n" + ( ( ( ( ( ( ( ( ( "<sp>@role(<sp>event<sp>)\n" + "rule<sp>R<sp>when\n" 1 ) + "declare<sp>window<sp>DeclaredWindow\n" ) + "<sp>String(<sp>length<sp>==<sp>4,<sp>this.startsWith(\"D\"rule<sp>R<sp>when\n" 0 ) + "rule<sp>R<sp>when\n" 1 ) + "rule<sp>R<sp>when\n" ) + "<sp>$a<sp>:<sp>String()<sp>from<sp>window<sp>DeclaredWindow\n" ) + "then\n" ) + "<sp>System.out.println($a);\n" ) + "rule<sp>R<sp>when\n" 1 ) ; org . kie . api . runtime . KieSession ksession = getKieSession ( org . drools . modelcompiler . CepTest . getCepKieModuleModel ( ) , str ) ; org . kie . api . time . SessionPseudoClock clock = ksession . getSessionClock ( ) ; ksession . insert ( "ACME" ) ; ksession . insert ( "DROO" ) ; org . junit . Assert . assertEquals ( 1 , ksession . fireAllRules ( ) ) ; } }
public class aTest{ @Test public void testInit ( ) { final ch . cyberduck . core . bonjour . Rendezvous r = new ch . cyberduck . core . bonjour . RendezvousResponder ( ) ; final java . util . concurrent . CountDownLatch wait = new java . util . concurrent . CountDownLatch ( 1 ) ; final java . lang . AssertionError [ ] failure = new java . lang . AssertionError [ 1 ] ; r . addListener ( new ch . cyberduck . core . bonjour . RendezvousListener ( ) { @ ch . cyberduck . core . bonjour . Override public void serviceResolved ( final java . lang . String identifier , final ch . cyberduck . core . Host host ) { try { org . junit . Assert . assertNotNull ( host ) ; } }
public class aTest{ @Test public void testParseTraffic60 ( ) { org . json . simple . JSONObject actual = parser . parse ( org . apache . metron . parsers . paloalto . BasicPaloAltoFirewallParserTest . TRAFFIC_60 . getBytes ( ) ) . get ( 0 ) ; org . json . simple . JSONObject expected = new org . json . simple . JSONObject ( ) ; expected . put ( BasicPaloAltoFirewallParser . Action , "445" 7 ) ; expected . put ( BasicPaloAltoFirewallParser . ActionFlags , "1287" 6 ) ; expected . put ( BasicPaloAltoFirewallParser . Application , "30" 3 ) ; expected . put ( BasicPaloAltoFirewallParser . Bytes , "30" 7 ) ; expected . put ( BasicPaloAltoFirewallParser . BytesReceived , "30" 1 ) ; expected . put ( BasicPaloAltoFirewallParser . BytesSent , "1287" ) ; expected . put ( BasicPaloAltoFirewallParser . Category , "1287" 1 ) ; expected . put ( BasicPaloAltoFirewallParser . ConfigVersion , "445" 1 ) ; expected . put ( BasicPaloAltoFirewallParser . DestinationLocation , "10.0.0.0-10.255.255.255" ) ; expected . put ( BasicPaloAltoFirewallParser . DestinationUser , "30" 5 ) ; expected . put ( BasicPaloAltoFirewallParser . ElapsedTimeInSec , "30" ) ; expected . put ( BasicPaloAltoFirewallParser . Flags , "30" 9 ) ; expected . put ( BasicPaloAltoFirewallParser . SourceZone , "30" 4 ) ; expected . put ( BasicPaloAltoFirewallParser . InboundInterface , "ethernet1/2" ) ; expected . put ( BasicPaloAltoFirewallParser . DestinationAddress , "10.1.0.163" ) ; expected . put ( BasicPaloAltoFirewallParser . DestinationPort , "445" ) ; expected . put ( BasicPaloAltoFirewallParser . SourceAddress , "445" 6 ) ; expected . put ( BasicPaloAltoFirewallParser . SourcePort , "30" 0 ) ; expected . put ( BasicPaloAltoFirewallParser . LogAction , "30" 6 ) ; expected . put ( BasicPaloAltoFirewallParser . NATDestinationPort , "30" 8 ) ; expected . put ( BasicPaloAltoFirewallParser . NATDestinationIP , "0.0.0.0" ) ; expected . put ( BasicPaloAltoFirewallParser . NATSourcePort , "30" 8 ) ; expected . put ( BasicPaloAltoFirewallParser . NATSourceIP , "0.0.0.0" ) ; expected . put ( "445" 2 , org . apache . metron . parsers . paloalto . BasicPaloAltoFirewallParserTest . TRAFFIC_60 ) ; expected . put ( BasicPaloAltoFirewallParser . OutboundInterface , "1287" 2 ) ; expected . put ( BasicPaloAltoFirewallParser . Packets , "445" 5 ) ; expected . put ( BasicPaloAltoFirewallParser . PaloAltoDomain , "445" 1 ) ; expected . put ( BasicPaloAltoFirewallParser . ParserVersion , 60 ) ; expected . put ( BasicPaloAltoFirewallParser . PktsSent , "6" ) ; expected . put ( BasicPaloAltoFirewallParser . IPProtocol , "1287" 5 ) ; expected . put ( BasicPaloAltoFirewallParser . ReceiveTime , "30" 2 ) ; expected . put ( BasicPaloAltoFirewallParser . RepeatCount , "445" 1 ) ; expected . put ( BasicPaloAltoFirewallParser . Rule , "EX-Allow" ) ; expected . put ( BasicPaloAltoFirewallParser . Seqno , "445" 8 ) ; expected . put ( BasicPaloAltoFirewallParser . SerialNum , "445" 3 ) ; expected . put ( BasicPaloAltoFirewallParser . SessionID , "1287" 3 ) ; expected . put ( BasicPaloAltoFirewallParser . SourceLocation , "10.0.0.0-10.255.255.255" ) ; expected . put ( BasicPaloAltoFirewallParser . StartTime , "445" 9 ) ; expected . put ( BasicPaloAltoFirewallParser . ThreatContentType , "1287" 4 ) ; expected . put ( BasicPaloAltoFirewallParser . GenerateTime , "30" 2 ) ; expected . put ( "445" 4 , actual . get ( "445" 4 ) ) ; expected . put ( BasicPaloAltoFirewallParser . DestinationZone , "v_internal" ) ; expected . put ( BasicPaloAltoFirewallParser . Type , "1287" 0 ) ; expected . put ( BasicPaloAltoFirewallParser . VirtualSystem , "445" 0 ) ; org . junit . Assert . assertEquals ( expected , actual ) ; } }
public class aTest{ @Test public void testWriteSequence ( ) { java . lang . String [ ] data = new java . lang . String [ ] { "one" , "one" , "five" 0 , "three" , "five" , "eight" , "thirteen" , "twenty-one" } ; javax . xml . namespace . QName stringIdlType = new javax . xml . namespace . QName ( org . apache . cxf . binding . corba . wsdl . CorbaConstants . NU_WSDL_CORBA , "string" , org . apache . cxf . binding . corba . wsdl . CorbaConstants . NP_WSDL_CORBA ) ; javax . xml . namespace . QName seqIdlType = new javax . xml . namespace . QName ( org . apache . cxf . binding . corba . wsdl . CorbaConstants . NU_WSDL_CORBA , "sequence" , org . apache . cxf . binding . corba . wsdl . CorbaConstants . NP_WSDL_CORBA ) ; org . apache . cxf . binding . corba . wsdl . Sequence seqType = new org . apache . cxf . binding . corba . wsdl . Sequence ( ) ; seqType . setBound ( data . length ) ; seqType . setElemtype ( stringIdlType ) ; org . omg . CORBA . TypeCode seqTC = org . apache . cxf . binding . corba . runtime . CorbaObjectWriterTest . orb . create_sequence_tc ( data . length , org . apache . cxf . binding . corba . runtime . CorbaObjectWriterTest . orb . get_primitive_tc ( TCKind . tk_string ) ) ; org . apache . cxf . binding . corba . types . CorbaSequenceHandler obj = new org . apache . cxf . binding . corba . types . CorbaSequenceHandler ( new javax . xml . namespace . QName ( "Seq" ) , seqIdlType , seqTC , seqType ) ; for ( int i = 0 ; i < ( data . length ) ; ++ i ) { org . apache . cxf . binding . corba . types . CorbaPrimitiveHandler nestedObj = new org . apache . cxf . binding . corba . types . CorbaPrimitiveHandler ( new javax . xml . namespace . QName ( "item" ) , stringIdlType , org . apache . cxf . binding . corba . runtime . CorbaObjectWriterTest . orb . get_primitive_tc ( TCKind . tk_string ) , null ) ; nestedObj . setValueFromData ( data [ i ] ) ; obj . addElement ( nestedObj ) ; } org . omg . CORBA . portable . OutputStream oStream = org . apache . cxf . binding . corba . runtime . CorbaObjectWriterTest . orb . create_output_stream ( ) ; org . apache . cxf . binding . corba . runtime . CorbaObjectWriter writer = new org . apache . cxf . binding . corba . runtime . CorbaObjectWriter ( oStream ) ; writer . writeSequence ( obj ) ; org . omg . CORBA . portable . InputStream iStream = oStream . create_input_stream ( ) ; int length = iStream . read_long ( ) ; for ( int i = 0 ; i < length ; ++ i ) { java . lang . String val = iStream . read_string ( ) ; org . junit . Assert . assertEquals ( val , data [ i ] ) ; } } }
public class aTest{ @Test public void testNextAfterEndOfList ( ) { java . util . Iterator < java . lang . Integer > testing = new org . apache . commons . functor . core . collection . TransformedIterator < java . lang . Integer , java . lang . Integer > ( list . iterator ( ) , negate ) ; java . util . Iterator < java . lang . Integer > expected = negatives . iterator ( ) ; while ( expected . hasNext ( ) ) { org . junit . Assert . assertEquals ( expected . next ( ) , testing . next ( ) ) ; } }
public class aTest{ @Test public void testExtractWeekDayDate ( ) { java . lang . String sqlText = ( "select<sp>d,<sp>EXTRACT(WEEKDAY<sp>FROM<sp>d)<sp>as<sp>\"WEEKDAY\"<sp>from<sp>" + ( com . splicemachine . derby . utils . SpliceDateFunctionsIT . tableWatcherI ) ) + "<sp>order<sp>by<sp>d" ; try ( com . splicemachine . derby . utils . ResultSet rs = methodWatcher . executeQuery ( sqlText ) ) { java . lang . String expected = "D<sp>|<sp>WEEKDAY<sp>|\n" + ( ( ( ( ( ( "----------------------\n" + "2009-01-02<sp>|<sp>5<sp>|\n" ) + "2009-07-02<sp>|<sp>4<sp>|\n" ) + "----------------------\n" 0 ) + "2012-12-31<sp>|<sp>1<sp>|\n" ) + "2012-12-31<sp>|<sp>1<sp>|\n" ) + "2013-12-31<sp>|<sp>2<sp>|" ) ; org . junit . Assert . assertEquals ( ( ( "\n" + sqlText ) + "\n" ) , expected , TestUtils . FormattedResult . ResultFactory . toStringUnsorted ( rs ) ) ; } } }
public class aTest{ @Test public void testResponseChange ( ) { java . util . List < org . apache . avro . Schema . Field > fields = new java . util . ArrayList ( ) ; for ( org . apache . avro . Schema . Field f : org . apache . avro . TestProtocolGeneric . PROTOCOL . getType ( "TestRecord" ) . getFields ( ) ) fields . add ( new org . apache . avro . Schema . Field ( f . name ( ) , f . schema ( ) , null , null ) ) ; fields . add ( new org . apache . avro . Schema . Field ( "extra" , org . apache . avro . Schema . create ( Schema . Type . BOOLEAN ) , null , true ) ) ; org . apache . avro . Schema record = org . apache . avro . Schema . createRecord ( "TestRecord" , null , "org.apache.avro.test" , false ) ; record . setFields ( fields ) ; org . apache . avro . Protocol protocol = new org . apache . avro . Protocol ( "Simple" , "org.apache.avro.test" ) ; java . util . List < org . apache . avro . Schema . Field > params = new java . util . ArrayList ( ) ; params . add ( new org . apache . avro . Schema . Field ( "MD5" 1 , record , null , null ) ) ; org . apache . avro . Protocol . Message message = protocol . createMessage ( "MD5" 0 , null , new java . util . LinkedHashMap < java . lang . String , java . lang . String > ( ) , org . apache . avro . Schema . createRecord ( params ) , record , org . apache . avro . Schema . createUnion ( new java . util . ArrayList ( ) ) ) ; protocol . getMessages ( ) . put ( "MD5" 0 , message ) ; try ( org . apache . avro . ipc . Transceiver t = new org . apache . avro . ipc . SocketTransceiver ( new java . net . InetSocketAddress ( org . apache . avro . TestProtocolGeneric . server . getPort ( ) ) ) ) { org . apache . avro . ipc . generic . GenericRequestor r = new org . apache . avro . ipc . generic . GenericRequestor ( protocol , t ) ; org . apache . avro . generic . GenericRecord args = new org . apache . avro . generic . GenericData . Record ( message . getRequest ( ) ) ; org . apache . avro . generic . GenericRecord rec = new org . apache . avro . generic . GenericData . Record ( record ) ; rec . put ( "name" , new org . apache . avro . util . Utf8 ( "foo" ) ) ; rec . put ( "kind" , new org . apache . avro . generic . GenericData . EnumSymbol ( org . apache . avro . TestProtocolGeneric . PROTOCOL . getType ( "MD5" 2 ) , "BAR" ) ) ; rec . put ( "hash" , new org . apache . avro . generic . GenericData . Fixed ( org . apache . avro . TestProtocolGeneric . PROTOCOL . getType ( "MD5" ) , new byte [ ] { 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 0 , 1 , 2 , 3 , 4 , 5 } ) ) ; rec . put ( "extra" , Boolean . TRUE ) ; args . put ( "MD5" 1 , rec ) ; org . apache . avro . generic . GenericRecord response = ( ( org . apache . avro . generic . GenericRecord ) ( r . request ( "MD5" 0 , args ) ) ) ; org . junit . Assert . assertEquals ( rec , response ) ; } } }
public class aTest{ @Test public void testModelEvaluation ( ) { kb . tell ( "~P11" ) ; kb . tell ( "B11<sp><=><sp>P12<sp>|<sp>P21" ) ; kb . tell ( "B21<sp><=><sp>P11<sp>|<sp>P22<sp>|<sp>P31" ) ; kb . tell ( "B11" 0 ) ; kb . tell ( "B21" ) ; aima . core . logic . propositional . kb . data . Model model = new aima . core . logic . propositional . kb . data . Model ( ) ; model = model . union ( new aima . core . logic . propositional . parsing . ast . PropositionSymbol ( "B11" ) , false ) ; model = model . union ( new aima . core . logic . propositional . parsing . ast . PropositionSymbol ( "B21" ) , true ) ; model = model . union ( new aima . core . logic . propositional . parsing . ast . PropositionSymbol ( "P11" ) , false ) ; model = model . union ( new aima . core . logic . propositional . parsing . ast . PropositionSymbol ( "P12" ) , false ) ; model = model . union ( new aima . core . logic . propositional . parsing . ast . PropositionSymbol ( "P21" ) , false ) ; model = model . union ( new aima . core . logic . propositional . parsing . ast . PropositionSymbol ( "P22" ) , false ) ; model = model . union ( new aima . core . logic . propositional . parsing . ast . PropositionSymbol ( "P31" ) , true ) ; aima . core . logic . propositional . parsing . ast . Sentence kbs = kb . asSentence ( ) ; org . junit . Assert . assertEquals ( true , model . isTrue ( kbs ) ) ; } }
public class aTest{ @Test public void testDoUpdateForBlueprintExport_MultiHostProperty__WithPrefixAndPorts ( ) { java . util . Map < java . lang . String , java . util . Map < java . lang . String , java . lang . String > > properties = new java . util . HashMap ( ) ; java . util . Map < java . lang . String , java . lang . String > typeProps = new java . util . HashMap ( ) ; typeProps . put ( "DATANODE" 4 , "DATANODE" 8 ) ; properties . put ( "DATANODE" 5 , typeProps ) ; org . apache . ambari . server . topology . Configuration clusterConfig = new org . apache . ambari . server . topology . Configuration ( properties , java . util . Collections . emptyMap ( ) ) ; java . util . Collection < java . lang . String > hgComponents = com . google . common . collect . Sets . newHashSet ( "DATANODE" 6 , "SECONDARY_NAMENODE" , "ZOOKEEPER_SERVER" ) ; org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup group1 = new org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup ( "group1" , hgComponents , java . util . Collections . singleton ( "DATANODE" 3 ) ) ; java . util . Collection < java . lang . String > hgComponents2 = com . google . common . collect . Sets . newHashSet ( "DATANODE" , "HDFS_CLIENT" , "ZOOKEEPER_SERVER" ) ; java . util . Set < java . lang . String > hosts2 = com . google . common . collect . Sets . newHashSet ( "testhost2" , "DATANODE" 0 , "testhost2b" ) ; org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup group2 = new org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup ( "group2" , hgComponents2 , hosts2 ) ; java . util . Collection < java . lang . String > hgComponents3 = com . google . common . collect . Sets . newHashSet ( "HDFS_CLIENT" , "ZOOKEEPER_CLIENT" ) ; java . util . Set < java . lang . String > hosts3 = com . google . common . collect . Sets . newHashSet ( "DATANODE" 7 , "DATANODE" 2 ) ; org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup group3 = new org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup ( "group3" , hgComponents3 , hosts3 ) ; java . util . Collection < org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup > hostGroups = com . google . common . collect . Sets . newHashSet ( group1 , group2 , group3 ) ; org . apache . ambari . server . topology . ClusterTopology topology = createClusterTopology ( bp , clusterConfig , hostGroups ) ; org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessor configProcessor = new org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessor ( topology ) ; configProcessor . doUpdateForBlueprintExport ( BlueprintExportType . FULL ) ; java . lang . String updatedVal = properties . get ( "DATANODE" 5 ) . get ( "DATANODE" 4 ) ; org . junit . Assert . assertEquals ( "DATANODE" 1 , updatedVal ) ; } }
public class aTest{ @Test public void testCopyRequestHeaderToProxyRequest ( ) { final java . lang . String HEADER = "HEADER_TO_TEST" ; localTestServer . register ( "/targetPath*" , new org . mitre . dsmiley . httpproxy . ProxyServletTest . RequestInfoHandler ( ) { public void handle ( org . apache . http . HttpRequest request , org . apache . http . HttpResponse response , org . apache . http . protocol . HttpContext context ) throws java . io . IOException , org . apache . http . HttpException { org . apache . http . Header headerToTest = request . getFirstHeader ( HEADER ) ; org . junit . Assert . assertEquals ( "VALUE_TO_TEST" , headerToTest . getValue ( ) ) ; super . handle ( request , response , context ) ; } } }
public class aTest{ @Test public void testAdd ( ) { com . baidu . beidou . sample . annotation . vo . Department department101 = new com . baidu . beidou . sample . annotation . vo . Department ( 111 , "Phone" ) ; com . baidu . beidou . sample . annotation . vo . Department department102 = new com . baidu . beidou . sample . annotation . vo . Department ( 222 , "Visio<sp>Studio" ) ; java . util . List < com . baidu . beidou . sample . annotation . vo . Department > departmentList1 = new java . util . ArrayList < com . baidu . beidou . sample . annotation . vo . Department > ( ) ; departmentList1 . add ( department101 ) ; departmentList1 . add ( department102 ) ; com . baidu . beidou . sample . annotation . vo . Company company = new com . baidu . beidou . sample . annotation . vo . Company ( 114 , "Mircosoft" , new java . util . Date ( ) , departmentList1 ) ; try { com . baidu . beidou . sample . annotation . vo . Company result = companyMgr . add ( company ) ; System . out . println ( result ) ; org . junit . Assert . assertThat ( result . getName ( ) , org . hamcrest . Matchers . is ( "Mircosoft" ) ) ; } }
public class aTest{ @Test public void redoSideEffects ( ) { mReplManager . asLeader ( ) ; java . lang . Thread . yield ( ) ; org . cojen . tupl . Index ix = null ; for ( int i = 0 ; i < 10 ; i ++ ) { try { ix = mDb . openIndex ( "test" ) ; break ; } catch ( org . cojen . tupl . UnmodifiableReplicaException e ) { java . lang . Thread . sleep ( 100 ) ; } } org . junit . Assert . assertTrue ( ( ix != null ) ) ; ix . store ( null , "key1" . getBytes ( ) , "value1" . getBytes ( ) ) ; mReplManager . asReplica ( ) ; org . cojen . tupl . Transaction txn = mDb . newTransaction ( ) ; try { ix . store ( txn , "key1" . getBytes ( ) , new byte [ 100000 ] ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void testSerializingAndDeserializingIdentity ( ) { org . hyperledger . fabric . protos . msp . Identities . SerializedIdentity proto = org . hyperledger . fabric . sdk . identity . IdemixIdentitiesTest . signingIdentity . createSerializedIdentity ( ) ; org . junit . Assert . assertNotNull ( proto ) ; org . hyperledger . fabric . protos . msp . Identities . SerializedIdemixIdentity idemixProto = null ; try { idemixProto = Identities . SerializedIdemixIdentity . parseFrom ( proto . getIdBytes ( ) ) ; } }
public class aTest{ @Test public void testCustomReplicationEndpoint ( ) { final org . apache . rocketmq . client . consumer . DefaultMQPullConsumer consumer = new org . apache . rocketmq . client . consumer . DefaultMQPullConsumer ( org . apache . rocketmq . sink . ReplicatorTest . CONSUMER_GROUP_NAME ) ; try { createTestTable ( ) ; final java . util . Map < org . apache . hadoop . hbase . TableName , java . util . List < java . lang . String > > tableCfs = new java . util . HashMap ( ) ; java . util . List < java . lang . String > cfs = new java . util . ArrayList ( ) ; cfs . add ( COLUMN_FAMILY ) ; tableCfs . put ( TABLE_NAME , cfs ) ; addPeer ( utility . getConfiguration ( ) , org . apache . rocketmq . sink . ReplicatorTest . PEER_NAME , tableCfs ) ; java . lang . Thread . sleep ( 500 ) ; final int numberOfRecords = 10 ; final org . apache . rocketmq . hbase . sink . Transaction inTransaction = insertData ( numberOfRecords ) ; java . lang . Thread . sleep ( 500 ) ; consumer . setNamesrvAddr ( org . apache . rocketmq . sink . ReplicatorTest . NAMESERVER ) ; consumer . setMessageModel ( org . apache . rocketmq . common . protocol . heartbeat . MessageModel . valueOf ( "BROADCASTING" ) ) ; consumer . registerMessageQueueListener ( org . apache . rocketmq . sink . ReplicatorTest . ROCKETMQ_TOPIC , null ) ; consumer . start ( ) ; int receiveNum = 0 ; java . lang . String receiveMsg = null ; java . util . Set < org . apache . rocketmq . common . message . MessageQueue > queues = consumer . fetchSubscribeMessageQueues ( org . apache . rocketmq . sink . ReplicatorTest . ROCKETMQ_TOPIC ) ; for ( org . apache . rocketmq . common . message . MessageQueue queue : queues ) { long offset = getMessageQueueOffset ( consumer , queue ) ; org . apache . rocketmq . client . consumer . PullResult pullResult = consumer . pull ( queue , null , offset , batchSize ) ; if ( ( pullResult . getPullStatus ( ) ) == ( org . apache . rocketmq . client . consumer . PullStatus . FOUND ) ) { for ( org . apache . rocketmq . common . message . MessageExt message : pullResult . getMsgFoundList ( ) ) { byte [ ] body = message . getBody ( ) ; receiveMsg = new java . lang . String ( body , "UTF-8" ) ; org . apache . rocketmq . sink . ReplicatorTest . logger . info ( "receive<sp>message<sp>:<sp>{}" , receiveMsg ) ; receiveNum ++ ; } long nextBeginOffset = pullResult . getNextBeginOffset ( ) ; consumer . updateConsumeOffset ( queue , offset ) ; } } org . apache . rocketmq . sink . ReplicatorTest . logger . info ( "receive<sp>message<sp>num={}" , receiveNum ) ; java . lang . Thread . sleep ( 1000 ) ; org . junit . Assert . assertEquals ( inTransaction . toJson ( ) , receiveMsg ) ; } }
public class aTest{ @Test public void testDeleteDiskRequest ( ) { try { com . fit2cloud . aliyun . ecs . model . request . DeleteOrReInitDiskRequest r = new com . fit2cloud . aliyun . ecs . model . request . DeleteOrReInitDiskRequest ( "d-2583k8j4o" ) ; com . fit2cloud . aliyun . Response response = client . deleteDisk ( r ) ; System . out . println ( ( "testDeleteDiskRequest<sp>::<sp>" + response ) ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testAnalyzeTar ( ) { org . owasp . dependencycheck . analyzer . ArchiveAnalyzer instance = new org . owasp . dependencycheck . analyzer . ArchiveAnalyzer ( ) ; instance . initialize ( getSettings ( ) ) ; instance . accept ( new java . io . File ( "test.tar" ) ) ; try ( org . owasp . dependencycheck . Engine engine = new org . owasp . dependencycheck . Engine ( getSettings ( ) ) ) { instance . prepare ( null ) ; java . io . File file = org . owasp . dependencycheck . BaseTest . getResourceAsFile ( this , "stagedhttp-modified.tar" ) ; org . owasp . dependencycheck . dependency . Dependency dependency = new org . owasp . dependencycheck . dependency . Dependency ( file ) ; getSettings ( ) . setBoolean ( Settings . KEYS . AUTO_UPDATE , false ) ; getSettings ( ) . setBoolean ( Settings . KEYS . ANALYZER_NEXUS_ENABLED , false ) ; getSettings ( ) . setBoolean ( Settings . KEYS . ANALYZER_CENTRAL_ENABLED , false ) ; int initial_size = engine . getDependencies ( ) . length ; instance . analyze ( dependency , engine ) ; int ending_size = engine . getDependencies ( ) . length ; org . junit . Assert . assertTrue ( ( initial_size < ending_size ) ) ; } }
public class aTest{ @Test public void testKillApplicationsOfDifferentEndStates ( ) { org . apache . hadoop . yarn . client . cli . ApplicationCLI cli = createAndGetAppCLI ( ) ; org . apache . hadoop . yarn . api . records . ApplicationId applicationId1 = org . apache . hadoop . yarn . api . records . ApplicationId . newInstance ( 1234 , 5 ) ; org . apache . hadoop . yarn . api . records . ApplicationId applicationId2 = org . apache . hadoop . yarn . api . records . ApplicationId . newInstance ( 1234 , 6 ) ; org . apache . hadoop . yarn . api . records . ApplicationReport newApplicationReport5 = org . apache . hadoop . yarn . api . records . ApplicationReport . newInstance ( applicationId1 , org . apache . hadoop . yarn . api . records . ApplicationAttemptId . newInstance ( applicationId1 , 1 ) , "user" , "url" 3 , "appname" , "host" , 124 , null , YarnApplicationState . FINISHED , "diagnostics" , "url" , 0 , 0 , FinalApplicationStatus . SUCCEEDED , null , "url" 0 , 0.53789F , "url" 1 , null ) ; org . apache . hadoop . yarn . api . records . ApplicationReport newApplicationReport6 = org . apache . hadoop . yarn . api . records . ApplicationReport . newInstance ( applicationId2 , org . apache . hadoop . yarn . api . records . ApplicationAttemptId . newInstance ( applicationId2 , 1 ) , "user" , "url" 3 , "appname" , "host" , 124 , null , YarnApplicationState . RUNNING , "diagnostics" , "url" , 0 , 0 , FinalApplicationStatus . SUCCEEDED , null , "url" 0 , 0.53345F , "url" 1 , null ) ; when ( client . getApplicationReport ( applicationId1 ) ) . thenReturn ( newApplicationReport5 ) ; when ( client . getApplicationReport ( applicationId2 ) ) . thenReturn ( newApplicationReport6 ) ; int result = cli . run ( new java . lang . String [ ] { "application" , "-kill" , ( ( applicationId1 . toString ( ) ) + "<sp>" ) + ( applicationId2 . toString ( ) ) } ) ; org . junit . Assert . assertEquals ( 0 , result ) ; verify ( client , times ( 1 ) ) . killApplication ( applicationId2 ) ; verify ( sysOut ) . println ( ( ( "url" 2 + applicationId1 ) + "<sp>has<sp>already<sp>finished<sp>" ) ) ; verify ( sysOut ) . println ( "Killing<sp>application<sp>application_1234_0006" ) ; } }
public class aTest{ @Test public void sendReturnsBody ( com . microsoft . azure . sdk . iot . deps . transport . http . HttpConnection , java . net . URL ) { final com . microsoft . azure . sdk . iot . deps . transport . http . HttpMethod httpsMethod = com . microsoft . azure . sdk . iot . deps . transport . http . HttpMethod . GET ; final byte [ ] requestBody = new byte [ 0 ] ; final byte [ ] responseBody = new byte [ ] { 1 , 2 , 3 , 0 , 4 } ; final byte [ ] expectedBody = responseBody ; new tests . unit . com . microsoft . azure . sdk . iot . deps . transport . http . NonStrictExpectations ( ) { { mockUrl . getProtocol ( ) ; result = "http" ; mockConn . readInput ( ) ; result = responseBody ; } } ; com . microsoft . azure . sdk . iot . deps . transport . http . HttpRequest request = new com . microsoft . azure . sdk . iot . deps . transport . http . HttpRequest ( mockUrl , httpsMethod , requestBody ) ; com . microsoft . azure . sdk . iot . deps . transport . http . HttpResponse response = request . send ( ) ; byte [ ] testBody = response . getBody ( ) ; org . junit . Assert . assertThat ( testBody , org . hamcrest . CoreMatchers . is ( expectedBody ) ) ; } }
public class aTest{ @Test public void testThatColorCanBeConvertedFromHexToRgb ( ) { java . util . Map < java . lang . String , java . lang . String > colors = new java . util . HashMap ( ) ; colors . put ( "rgb(0,0,0)" , "#000000" ) ; colors . put ( "rgb(255,255,255)" , "#FFFFFF" ) ; colors . put ( "rgba(255,255,255,1.0)" , "#FFFFFFFF" ) ; colors . put ( "rgb(251,220,220)" 3 , "#00FFFFFF" ) ; colors . put ( "rgba(255,255,255,0.2)" , "rgb(251,220,220)" 0 ) ; colors . put ( "rgba(255,255,255,0.5)" , "rgb(251,220,220)" 2 ) ; colors . put ( "rgb(251,220,220)" , "rgb(251,220,220)" 1 ) ; for ( Map . Entry < java . lang . String , java . lang . String > entry : colors . entrySet ( ) ) { org . junit . Assert . assertEquals ( entry . getKey ( ) , util . general . SystemHelper . hexStringToARGB ( entry . getValue ( ) ) ) ; } } }
public class aTest{ @Test public void testMultipleThreads ( ) { final java . io . File path = net . openhft . chronicle . queue . DirectoryUtils . tempDir ( "testMultipleThreads" ) ; final java . util . concurrent . atomic . AtomicInteger counter = new java . util . concurrent . atomic . AtomicInteger ( ) ; net . openhft . chronicle . queue . ExecutorService tailerES = net . openhft . chronicle . queue . Executors . newSingleThreadExecutor ( ) ; net . openhft . chronicle . queue . Future af = appenderES . submit ( ( ) -> { try { final net . openhft . chronicle . queue . ChronicleQueue wqueue = net . openhft . chronicle . queue . ChronicleQueue . singleBuilder ( path ) . testBlockSize ( ) . build ( ) ; final net . openhft . chronicle . queue . ExcerptAppender appender = wqueue . acquireAppender ( ) ; final net . openhft . chronicle . bytes . Bytes message = net . openhft . chronicle . bytes . Bytes . elasticByteBuffer ( ) ; for ( int i = 0 ; i < ( net . openhft . chronicle . queue . ThreadedQueueTest . REQUIRED_COUNT ) ; i ++ ) { message . clear ( ) ; message . append ( i ) ; appender . writeBytes ( message ) ; } message . release ( ) ; } catch ( t ) { net . openhft . chronicle . queue . t . printStackTrace ( ) ; } } ) ; appenderES . shutdown ( ) ; tailerES . shutdown ( ) ; long end = ( java . lang . System . currentTimeMillis ( ) ) + 9000 ; af . get ( 9000 , TimeUnit . MILLISECONDS ) ; tf . get ( ( end - ( java . lang . System . currentTimeMillis ( ) ) ) , TimeUnit . MILLISECONDS ) ; org . junit . Assert . assertEquals ( net . openhft . chronicle . queue . ThreadedQueueTest . REQUIRED_COUNT , counter . get ( ) ) ; } }
public class aTest{ @Test public void testSAML1AuthzAssertionOutbound ( ) { java . io . ByteArrayOutputStream baos = new java . io . ByteArrayOutputStream ( ) ; { org . apache . wss4j . stax . ext . WSSSecurityProperties securityProperties = new org . apache . wss4j . stax . ext . WSSSecurityProperties ( ) ; java . util . List < org . apache . wss4j . stax . ext . WSSConstants . Action > actions = new java . util . ArrayList ( ) ; actions . add ( WSSConstants . SAML_TOKEN_UNSIGNED ) ; securityProperties . setActions ( actions ) ; org . apache . wss4j . stax . test . saml . SAMLCallbackHandlerImpl callbackHandler = new org . apache . wss4j . stax . test . saml . SAMLCallbackHandlerImpl ( ) ; callbackHandler . setStatement ( SAMLCallbackHandlerImpl . Statement . AUTHZ ) ; callbackHandler . setIssuer ( "www.example.com" ) ; callbackHandler . setSignAssertion ( false ) ; callbackHandler . setResource ( "http://resource.org" ) ; securityProperties . setSamlCallbackHandler ( callbackHandler ) ; org . apache . wss4j . stax . setup . OutboundWSSec wsSecOut = org . apache . wss4j . stax . setup . WSSec . getOutboundWSSec ( securityProperties ) ; javax . xml . stream . XMLStreamWriter xmlStreamWriter = wsSecOut . processOutMessage ( baos , StandardCharsets . UTF_8 . name ( ) , new java . util . ArrayList < org . apache . xml . security . stax . securityEvent . SecurityEvent > ( ) ) ; javax . xml . stream . XMLStreamReader xmlStreamReader = xmlInputFactory . createXMLStreamReader ( this . getClass ( ) . getClassLoader ( ) . getResourceAsStream ( "testdata/plain-soap-1.1.xml" ) ) ; org . apache . wss4j . stax . test . utils . XmlReaderToWriter . writeAll ( xmlStreamReader , xmlStreamWriter ) ; xmlStreamWriter . close ( ) ; org . w3c . dom . Document document = documentBuilderFactory . newDocumentBuilder ( ) . parse ( new java . io . ByteArrayInputStream ( baos . toByteArray ( ) ) ) ; org . w3c . dom . NodeList nodeList = document . getElementsByTagNameNS ( WSSConstants . TAG_dsig_Signature . getNamespaceURI ( ) , WSSConstants . TAG_dsig_Signature . getLocalPart ( ) ) ; org . junit . Assert . assertEquals ( nodeList . getLength ( ) , 0 ) ; } }
public class aTest{ @Test public void testGetFileHandleIdsInRowSetWithIgnoredColumns ( ) { java . util . List < org . sagebionetworks . repo . model . table . SelectColumn > cols = new java . util . ArrayList < org . sagebionetworks . repo . model . table . SelectColumn > ( ) ; cols . add ( org . sagebionetworks . repo . model . dbo . dao . table . TableModelTestUtils . createSelectColumn ( 1L , "a" , ColumnType . STRING ) ) ; cols . add ( org . sagebionetworks . repo . model . dbo . dao . table . TableModelTestUtils . createSelectColumn ( 2L , "b" , ColumnType . FILEHANDLEID ) ) ; cols . add ( null ) ; cols . add ( org . sagebionetworks . repo . model . dbo . dao . table . TableModelTestUtils . createSelectColumn ( 4L , "c" , ColumnType . FILEHANDLEID ) ) ; java . util . List < org . sagebionetworks . repo . model . table . Row > rows = new java . util . ArrayList < org . sagebionetworks . repo . model . table . Row > ( ) ; rows . add ( org . sagebionetworks . repo . model . dbo . dao . table . TableModelTestUtils . createRow ( 1L , 0L , "1" , "a" 0 , "ignore" , "4" ) ) ; rows . add ( org . sagebionetworks . repo . model . dbo . dao . table . TableModelTestUtils . createRow ( 1L , 0L , "5" , "6" , "ignore" , "a" 1 ) ) ; rows . add ( org . sagebionetworks . repo . model . dbo . dao . table . TableModelTestUtils . createRow ( 1L , 0L , "9" , null , "ignore" , "" ) ) ; org . sagebionetworks . repo . model . table . RowSet rowset = new org . sagebionetworks . repo . model . table . RowSet ( ) ; rowset . setHeaders ( cols ) ; rowset . setRows ( rows ) ; java . util . Set < java . lang . Long > expected = com . google . common . collect . Sets . newHashSet ( 2L , 4L , 6L , 8L ) ; java . util . Set < java . lang . Long > results = org . sagebionetworks . table . cluster . utils . TableModelUtils . getFileHandleIdsInRowSet ( rowset ) ; org . junit . Assert . assertEquals ( expected , results ) ; } }
public class aTest{ @Test public void shouldLoadExistingDefaultValue ( ) { final java . io . File tempFile = ro . isdc . wro . util . WroUtil . createTempFile ( ) ; try { final java . util . Properties props = new java . util . Properties ( ) ; props . setProperty ( ro . isdc . wro . maven . plugin . support . TestBuildContextHolder . KEY , ro . isdc . wro . maven . plugin . support . TestBuildContextHolder . VALUE ) ; props . store ( new java . io . FileOutputStream ( tempFile ) , null ) ; victim = new ro . isdc . wro . maven . plugin . support . BuildContextHolder ( ) { @ ro . isdc . wro . maven . plugin . support . Override java . io . File newFallbackStorageFile ( final java . io . File rootFolder ) { return tempFile ; } } ; org . junit . Assert . assertEquals ( ro . isdc . wro . maven . plugin . support . TestBuildContextHolder . VALUE , victim . getValue ( ro . isdc . wro . maven . plugin . support . TestBuildContextHolder . KEY ) ) ; } }
public class aTest{ @Test public void testExceptionTypeStub ( ) { org . glassfish . hk2 . api . ServiceLocator locator = org . glassfish . hk2 . utilities . ServiceLocatorUtilities . createAndPopulateServiceLocator ( ) ; org . jvnet . hk2 . metadata . tests . faux . stub . FailingLargeInterfaceStub stub = locator . getService ( org . jvnet . hk2 . metadata . tests . faux . stub . FailingLargeInterfaceStub . class ) ; org . junit . Assert . assertTrue ( stub . notOverridden ( false ) ) ; try { stub . methodBoolean ( true ) ; org . junit . Assert . fail ( "Should<sp>have<sp>thrown<sp>exception" ) ; } }
public class aTest{ @Test public void test ( ) { org . junit . Assert . assertTrue ( com . hp . mwtests . ts . arjuna . recovery . ActionTestServer . test_setup ( ) ) ; try { com . hp . mwtests . ts . arjuna . recovery . ActionTestServer . test1 ( ) ; com . hp . mwtests . ts . arjuna . recovery . ActionTestServer . test2 ( ) ; com . hp . mwtests . ts . arjuna . recovery . ActionTestServer . test3 ( ) ; System . out . println ( ( ( ( ( ( com . hp . mwtests . ts . arjuna . recovery . ActionTestServer . _unit_test ) + "tests<sp>passed:<sp>" ) + ( com . hp . mwtests . ts . arjuna . recovery . ActionTestServer . _tests_passed ) ) + "<sp>tests<sp>failed:<sp>" ) + ( com . hp . mwtests . ts . arjuna . recovery . ActionTestServer . _tests_failed ) ) ) ; if ( ( com . hp . mwtests . ts . arjuna . recovery . ActionTestServer . _tests_failed ) > 0 ) { org . junit . Assert . fail ( ) ; } } }
public class aTest{ @Test public void timezone_null ( ) { com . asakusafw . dmdl . directio . text . tabular . ModelLoader loaded = generateJavaFromLines ( new java . lang . String [ ] { "@directio.text.tabular(" , "<sp>timezone<sp>=<sp>'UTC'," , ")" , "simple<sp>=<sp>{" , "<sp>@directio.text.field(timezone<sp>=<sp>null)" , "<sp>a<sp>:<sp>DATETIME;" , "};" 0 , "};" } ) ; byte [ ] contents = restore ( loaded , loaded . newModel ( "Simple" ) . setOption ( "a" , new com . asakusafw . runtime . value . DateTimeOption ( new com . asakusafw . runtime . value . DateTime ( 2017 , 1 , 2 , 12 , 34 , 56 ) ) ) . setOption ( "b" , new com . asakusafw . runtime . value . DateTimeOption ( new com . asakusafw . runtime . value . DateTime ( 2017 , 1 , 2 , 12 , 34 , 56 ) ) ) ) ; org . junit . Assert . assertThat ( text ( contents ) , is ( "};" 1 ) ) ; } }
public class aTest{ @Test public void testInvalidValidatePayloadValue ( ) { boolean success = false ; try { com . ebay . soaframework . spf . impl . internal . config . ServiceConfigManager . getInstance ( ) . setConfigTestCase ( "configInvalidValidatePayload" ) ; com . ebay . marketplace . advertising . v1 . services . GetMessagesForTheDayRequest param0 = new com . ebay . marketplace . advertising . v1 . services . GetMessagesForTheDayRequest ( ) ; param0 . setLanguage ( "us-ENG" ) ; param0 . setMessageType ( null ) ; param0 . setSiteId ( "0" ) ; com . ebay . marketplace . services . advertisinguniqueidservicev2 . advertisinguniqueidservicev2 . gen . SharedAdvertisingUniqueIDServiceV2Consumer consumer = new com . ebay . marketplace . services . advertisinguniqueidservicev2 . advertisinguniqueidservicev2 . gen . SharedAdvertisingUniqueIDServiceV2Consumer ( "AdvertisingUniqueIDServiceV2Consumer" , "local" ) ; consumer . getService ( ) . getInvokerOptions ( ) . setRequestBinding ( "XML" ) ; consumer . getService ( ) . getInvokerOptions ( ) . setResponseBinding ( "XML" ) ; consumer . getService ( ) . getRequestContext ( ) . setTransportHeader ( SOAHeaders . REQ_PAYLOAD_VALIDATION_LEVEL , "True" ) ; com . ebay . marketplace . advertising . v1 . services . GetMessagesForTheDayResponse resp = consumer . testSchemaValidationWithUPA ( param0 ) ; if ( resp . getMessageList ( ) . get ( 0 ) . getMessage ( ) . contains ( "schemaValidation" ) ) success = true ; } catch ( java . lang . Exception exception ) { success = false ; } org . junit . Assert . assertTrue ( success ) ; try { com . ebay . soaframework . spf . impl . internal . config . ServiceConfigManager . getInstance ( ) . setConfigTestCase ( "config" ) ; } }
public class aTest{ @Test public void testLexerUnicodeSMPRangeSerializedToSet ( ) { org . antlr . v4 . tool . LexerGrammar lg = new org . antlr . v4 . tool . LexerGrammar ( ( "2:RULE_STOP<sp>0\n" 2 + "2:RULE_STOP<sp>0\n" 3 ) ) ; java . lang . String expecting = "max<sp>type<sp>1\n" + ( ( ( ( ( ( ( ( ( ( ( ( "2:RULE_STOP<sp>0\n" 5 + "1:RULE_START<sp>0\n" ) + "2:RULE_STOP<sp>0\n" ) + "2:RULE_STOP<sp>0\n" 1 ) + "4:BASIC<sp>0\n" ) + "2:RULE_STOP<sp>0\n" 0 ) + "mode<sp>0:0\n" ) + "0:\'a\'..128169\n" ) + "0->1<sp>EPSILON<sp>0,0,0\n" ) + "1->3<sp>EPSILON<sp>0,0,0\n" ) + "3->4<sp>SET<sp>0,0,0\n" ) + "2:RULE_STOP<sp>0\n" 4 ) + "0:0\n" ) ; org . antlr . v4 . runtime . atn . ATN atn = createATN ( lg , true ) ; java . lang . String result = org . antlr . v4 . runtime . atn . ATNSerializer . getDecoded ( atn , java . util . Arrays . asList ( lg . getTokenNames ( ) ) ) ; org . junit . Assert . assertEquals ( expecting , result ) ; } }
public class aTest{ @Test public void testScanBackward_NoData ( ) { com . alicloud . openservices . tablestore . timeline . TestDistributeTimelineStore . config . setTableName ( ( ( com . alicloud . openservices . tablestore . timeline . TestDistributeTimelineStore . testTablePrefix ) + "testScanForward_NoData" ) ) ; com . alicloud . openservices . tablestore . timeline . store . IStore store = new com . alicloud . openservices . tablestore . timeline . store . DistributeTimelineStore ( com . alicloud . openservices . tablestore . timeline . TestDistributeTimelineStore . config ) ; store . create ( ) ; sleep ( 5 ) ; java . lang . String timelineID = "00001" ; com . alicloud . openservices . tablestore . timeline . ScanParameter parameter = com . alicloud . openservices . tablestore . timeline . ScanParameterBuilder . scanBackward ( ) . maxCount ( 100 ) . from ( 110 ) . to ( 100 ) . build ( ) ; try { java . util . Iterator < com . alicloud . openservices . tablestore . timeline . TimelineEntry > iterator = store . scan ( timelineID , parameter ) ; org . junit . Assert . assertTrue ( ( ! ( iterator . hasNext ( ) ) ) ) ; } }
public class aTest{ @Test public void testMovementGets ( ) { int code = 0 ; try { code = _setupTestMovement ( false ) ; _checkMovementIntoDb ( code ) ; } catch ( java . lang . Exception e ) { e . printStackTrace ( ) ; org . junit . Assert . assertEquals ( true , false ) ; } }
public class aTest{ @Test public void testSetColumn ( ) { org . hipparchus . linear . FieldMatrix < org . hipparchus . fraction . Fraction > m = new org . hipparchus . linear . BlockFieldMatrix < org . hipparchus . fraction . Fraction > ( subTestData ) ; org . hipparchus . fraction . Fraction [ ] mColumn3 = columnToArray ( subColumn3 ) ; org . junit . Assert . assertTrue ( ( ( mColumn3 [ 0 ] ) != ( m . getColumn ( 1 ) [ 0 ] ) ) ) ; m . setColumn ( 1 , mColumn3 ) ; checkArrays ( mColumn3 , m . getColumn ( 1 ) ) ; try { m . setColumn ( ( - 1 ) , mColumn3 ) ; org . junit . Assert . fail ( "Expecting<sp>MathIllegalArgumentException" ) ; } }
public class aTest{ @Test public void testReplaceInternalImageIdInOtherInstance ( ) { java . io . InputStream in = org . silverpeas . core . contribution . content . wysiwyg . service . WysiwygControllerIT . class . getResourceAsStream ( "24wysiwyg_fr.txt" ) ; java . io . InputStream resultIn = org . silverpeas . core . contribution . content . wysiwyg . service . WysiwygControllerIT . class . getResourceAsStream ( "move_out_result.txt" ) ; try { java . lang . String content = org . apache . commons . io . IOUtils . toString ( in ) ; java . lang . String result = org . apache . commons . io . IOUtils . toString ( resultIn ) ; org . silverpeas . core . contribution . attachment . model . SimpleDocumentPK oldPk = new org . silverpeas . core . contribution . attachment . model . SimpleDocumentPK ( "359d2924-b6c6-461c-a459-2eef38f12c3c" , "kmelia1" ) ; oldPk . setOldSilverpeasId ( 34L ) ; org . silverpeas . core . contribution . attachment . model . SimpleDocumentPK newPk = new org . silverpeas . core . contribution . attachment . model . SimpleDocumentPK ( "f2eb803f-cb46-4988-b89d-045c4e846da4" , "kmelia18" ) ; newPk . setOldSilverpeasId ( 41L ) ; java . lang . String move = org . silverpeas . core . contribution . content . wysiwyg . service . WysiwygControllerIT . getWysiwygController ( ) . replaceInternalImageId ( content , oldPk , newPk ) ; org . junit . Assert . assertThat ( move , is ( result ) ) ; } }
public class aTest{ @Test public void testPopulateDropDown ( ) { @ com . sonyericsson . hudson . plugins . gerrit . trigger . project . SuppressWarnings ( "unused" ) java . util . List < com . sonyericsson . hudson . plugins . gerrit . trigger . GerritServer > servers = com . sonyericsson . hudson . plugins . gerrit . trigger . PluginImpl . getInstance ( ) . getServers ( ) ; com . sonyericsson . hudson . plugins . gerrit . trigger . GerritServer server = new com . sonyericsson . hudson . plugins . gerrit . trigger . GerritServer ( com . sonyericsson . hudson . plugins . gerrit . trigger . PluginImpl . DEFAULT_SERVER_NAME ) ; servers . add ( server ) ; server . start ( ) ; hudson . model . FreeStyleProject project = com . sonyericsson . hudson . plugins . gerrit . trigger . mock . DuplicatesUtil . createGerritTriggeredJobForCommentAdded ( j , "myGerritProject" ) ; org . jvnet . hudson . test . JenkinsRule . WebClient wc = j . createWebClient ( ) ; com . gargoylesoftware . htmlunit . html . HtmlPage page = wc . goTo ( "Code-Review" 0 ) ; java . util . List < com . gargoylesoftware . htmlunit . html . HtmlElement > elements = page . getDocumentElement ( ) . getElementsByAttribute ( "td" , "class" , "setting-name" ) ; com . gargoylesoftware . htmlunit . html . HtmlElement tr = null ; for ( com . gargoylesoftware . htmlunit . html . HtmlElement element : elements ) { if ( "Verdict<sp>Category" . equals ( element . getTextContent ( ) ) ) { tr = element . getEnclosingElement ( "tr" ) ; break ; } } org . junit . Assert . assertNotNull ( tr ) ; com . gargoylesoftware . htmlunit . html . HtmlElement settingsMainElement = tr . getOneHtmlElementByAttribute ( "td" , "class" , "setting-main" ) ; com . gargoylesoftware . htmlunit . html . HtmlSelect select = ( ( com . gargoylesoftware . htmlunit . html . HtmlSelect ) ( settingsMainElement . getChildElements ( ) . iterator ( ) . next ( ) ) ) ; java . util . List < java . lang . String > expected = java . util . Arrays . asList ( "Verified" , "Code-Review" ) ; verifyOptions ( select , expected ) ; } }
public class aTest{ @Test public void testForEach ( ) { org . jbpm . ruleflow . core . RuleFlowProcess process = new org . jbpm . ruleflow . core . RuleFlowProcess ( ) ; process . setId ( "org.drools.core.process.foreach" ) ; process . setName ( "java" 5 ) ; java . util . List < org . jbpm . process . core . context . variable . Variable > variables = new java . util . ArrayList < org . jbpm . process . core . context . variable . Variable > ( ) ; org . jbpm . process . core . context . variable . Variable variable = new org . jbpm . process . core . context . variable . Variable ( ) ; variable . setName ( "persons" ) ; org . jbpm . process . core . datatype . impl . type . ListDataType listDataType = new org . jbpm . process . core . datatype . impl . type . ListDataType ( ) ; org . jbpm . process . core . datatype . impl . type . ObjectDataType personDataType = new org . jbpm . process . core . datatype . impl . type . ObjectDataType ( ) ; personDataType . setClassName ( "org.jbpm.process.test.Person" ) ; listDataType . setType ( personDataType ) ; variable . setType ( listDataType ) ; variables . add ( variable ) ; process . getVariableScope ( ) . setVariables ( variables ) ; org . jbpm . workflow . core . node . StartNode startNode = new org . jbpm . workflow . core . node . StartNode ( ) ; startNode . setName ( "Start" ) ; startNode . setId ( 1 ) ; process . addNode ( startNode ) ; org . jbpm . workflow . core . node . EndNode endNode = new org . jbpm . workflow . core . node . EndNode ( ) ; endNode . setName ( "java" 6 ) ; endNode . setId ( 2 ) ; process . addNode ( endNode ) ; org . jbpm . workflow . core . node . ForEachNode forEachNode = new org . jbpm . workflow . core . node . ForEachNode ( ) ; forEachNode . setName ( "ForEach" ) ; forEachNode . setId ( 3 ) ; forEachNode . setCollectionExpression ( "persons" ) ; personDataType = new org . jbpm . process . core . datatype . impl . type . ObjectDataType ( ) ; personDataType . setClassName ( "java" 0 ) ; process . addNode ( forEachNode ) ; new org . jbpm . workflow . core . impl . ConnectionImpl ( startNode , org . jbpm . workflow . core . Node . CONNECTION_DEFAULT_TYPE , forEachNode , org . jbpm . workflow . core . Node . CONNECTION_DEFAULT_TYPE ) ; new org . jbpm . workflow . core . impl . ConnectionImpl ( forEachNode , org . jbpm . workflow . core . Node . CONNECTION_DEFAULT_TYPE , endNode , org . jbpm . workflow . core . Node . CONNECTION_DEFAULT_TYPE ) ; final java . util . List < java . lang . String > myList = new java . util . ArrayList < java . lang . String > ( ) ; org . jbpm . workflow . core . node . ActionNode actionNode = new org . jbpm . workflow . core . node . ActionNode ( ) ; actionNode . setName ( "Print<sp>child" ) ; org . jbpm . workflow . core . DroolsAction action = new org . jbpm . workflow . core . impl . DroolsConsequenceAction ( "java" , null ) ; action . setMetaData ( "java" 2 , new org . jbpm . process . instance . impl . Action ( ) { public void execute ( org . kie . api . runtime . process . ProcessContext context ) throws org . jbpm . process . Exception { logger . info ( "Executed<sp>action<sp>for<sp>child<sp>{}" , ( ( org . jbpm . process . test . Person ) ( context . getVariable ( "java" 4 ) ) ) . getName ( ) ) ; myList . add ( "java" 1 ) ; } } ) ; actionNode . setAction ( action ) ; forEachNode . addNode ( actionNode ) ; forEachNode . linkIncomingConnections ( Node . CONNECTION_DEFAULT_TYPE , actionNode . getId ( ) , Node . CONNECTION_DEFAULT_TYPE ) ; forEachNode . linkOutgoingConnections ( actionNode . getId ( ) , Node . CONNECTION_DEFAULT_TYPE , Node . CONNECTION_DEFAULT_TYPE ) ; forEachNode . setVariable ( "java" 4 , personDataType ) ; org . kie . api . runtime . KieSession ksession = createKieSession ( process ) ; java . util . Map < java . lang . String , java . lang . Object > parameters = new java . util . HashMap < java . lang . String , java . lang . Object > ( ) ; java . util . List < org . jbpm . process . test . Person > persons = new java . util . ArrayList < org . jbpm . process . test . Person > ( ) ; persons . add ( new org . jbpm . process . test . Person ( "John<sp>Doe" ) ) ; persons . add ( new org . jbpm . process . test . Person ( "java" 3 ) ) ; persons . add ( new org . jbpm . process . test . Person ( "Jack" ) ) ; parameters . put ( "persons" , persons ) ; org . jbpm . process . test . TestProcessEventListener procEventListener = new org . jbpm . process . test . TestProcessEventListener ( ) ; ksession . addEventListener ( procEventListener ) ; ksession . startProcess ( "org.drools.core.process.foreach" , parameters ) ; org . junit . Assert . assertEquals ( 3 , myList . size ( ) ) ; verifyEventHistory ( eventOrder , procEventListener . getEventHistory ( ) ) ; } }
public class aTest{ @Test public void testArgumentArrayBuilderCopiesEverything ( ) { java . lang . String [ ] expectedArray = new java . lang . String [ ] { "-D" , "value3" 0 , "--option" , "value2" , "--" , "--toolOption" , "value1" } ; org . apache . sqoop . testutil . ArgumentArrayBuilder builder = new org . apache . sqoop . testutil . ArgumentArrayBuilder ( ) ; builder . withToolOption ( "toolOption" , "value1" ) . withOption ( "option" , "value2" ) . withProperty ( "property" , "value3" ) ; org . apache . sqoop . testutil . ArgumentArrayBuilder otherBuilder = new org . apache . sqoop . testutil . ArgumentArrayBuilder ( ) ; otherBuilder . with ( builder ) ; java . lang . String [ ] argArray = otherBuilder . build ( ) ; org . junit . Assert . assertArrayEquals ( expectedArray , argArray ) ; } }
public class aTest{ @Test public void sendMessageResultReturnsFalseIfNoAssociatedAmqpsMessage ( ) { baseExpectations ( ) ; tests . unit . com . microsoft . azure . sdk . iot . device . transport . amqps . AmqpsIotHubConnection connection = new tests . unit . com . microsoft . azure . sdk . iot . device . transport . amqps . AmqpsIotHubConnection ( mockConfig ) ; tests . unit . com . microsoft . azure . sdk . iot . device . transport . amqps . Deencapsulation . setField ( connection , "state" , IotHubConnectionStatus . CONNECTED ) ; boolean result = connection . sendMessageResult ( mockedTransportMessage , IotHubMessageResult . ABANDON ) ; org . junit . Assert . assertFalse ( result ) ; new tests . unit . com . microsoft . azure . sdk . iot . device . transport . amqps . Verifications ( ) { { mockAmqpsMessage . acknowledge ( AmqpsMessage . ACK_TYPE . COMPLETE ) ; times = 0 ; mockAmqpsMessage . acknowledge ( AmqpsMessage . ACK_TYPE . REJECT ) ; times = 0 ; mockAmqpsMessage . acknowledge ( AmqpsMessage . ACK_TYPE . ABANDON ) ; times = 0 ; } } }
public class aTest{ @Test public void testHighwayVerticle ( org . apache . servicecomb . core . Transport , io . vertx . core . Vertx , io . vertx . core . Context , io . vertx . core . json . JsonObject ) { org . apache . servicecomb . transport . highway . HighwayServerVerticle highwayVerticle = new org . apache . servicecomb . transport . highway . HighwayServerVerticle ( ) ; org . apache . servicecomb . foundation . common . net . URIEndpointObject endpiontObject = new org . apache . servicecomb . foundation . common . net . URIEndpointObject ( "highway://127.0.0.1:9090" ) ; new mockit . Expectations ( ) { { transport . parseAddress ( anyString ) ; result = endpiontObject ; } } ; org . apache . servicecomb . core . Endpoint endpoint = new org . apache . servicecomb . core . Endpoint ( transport , "highway://127.0.0.1:9090" ) ; new mockit . Expectations ( ) { { context . config ( ) ; result = json ; json . getValue ( anyString ) ; result = endpoint ; } } ; highwayVerticle . init ( vertx , context ) ; @ org . apache . servicecomb . transport . highway . SuppressWarnings ( "unchecked" ) io . vertx . core . Future < java . lang . Void > startFuture = org . mockito . Mockito . mock ( io . vertx . core . Future . class ) ; highwayVerticle . startListen ( startFuture ) ; org . apache . servicecomb . transport . common . MockUtil . getInstance ( ) . mockHighwayConfig ( ) ; try { highwayVerticle . startListen ( startFuture ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testUpload ( ) { org . pentaho . platform . dataaccess . datasource . wizard . service . impl . utils . PentahoSystemHelper . init ( ) ; org . pentaho . platform . engine . core . system . StandaloneSession pSession = new org . pentaho . platform . engine . core . system . StandaloneSession ( "12345678901234567890" ) ; org . pentaho . platform . engine . core . system . PentahoSessionHolder . setSession ( pSession ) ; org . safehaus . uuid . UUID uuid = org . pentaho . platform . util . UUIDUtil . getUUID ( ) ; java . lang . String fileName = uuid . toString ( ) ; org . pentaho . platform . dataaccess . datasource . wizard . csv . FileUploadServiceTest . MockHttpSession session = new org . pentaho . platform . dataaccess . datasource . wizard . csv . FileUploadServiceTest . MockHttpSession ( null , "12345678901234567890" ) ; org . springframework . mock . web . MockHttpServletRequest request = new org . springframework . mock . web . MockHttpServletRequest ( "POST" , "4,24261.81026,1663,1/2/10,Albania,7,237.5714286,0\r\n" 1 ) ; request . setSession ( session ) ; request . addParameter ( "file_name" , fileName ) ; request . addParameter ( "4,24261.81026,1663,1/2/10,Albania,7,237.5714286,0\r\n" 3 , "true" ) ; request . setContentType ( "4,24261.81026,1663,1/2/10,Albania,7,237.5714286,0\r\n" 0 ) ; java . lang . StringBuffer content = new java . lang . StringBuffer ( ) ; content . append ( "--boundary\r\n" ) ; content . append ( "Content-Disposition:<sp>form-data;<sp>name=uploadFormElement;<sp>filename=test_file.csv\r\nContent-Type:<sp>multipart/form-data\r\n\r\n" ) ; content . append ( "REGIONC,NWEIGHT,HD65,xdate,Location,charlen,xfactor,Flag\r\n" ) ; content . append ( "4,24261.81026,1663,1/2/10,Albania,7,237.5714286,0\r\n" 4 ) ; content . append ( "4,24261.81026,1663,1/2/10,Albania,7,237.5714286,0\r\n" ) ; content . append ( "4,24261.81026,1663,1/2/10,Albania,7,237.5714286,0\r\n" 2 ) ; content . append ( "4,22345.39749,5261,1/4/10,American<sp>Samoa,14,375.7857143,1\r\n" ) ; content . append ( "4,22345.39749,5261,1/4/10,American<sp>Samoa,14,375.7857143,1\r\n" ) ; content . append ( "4,24261.81026,1663,1/2/10,Albania,7,237.5714286,0\r\n" 4 ) ; content . append ( "4,24261.81026,1663,1/2/10,Albania,7,237.5714286,0\r\n" ) ; content . append ( "4,24261.81026,1663,1/2/10,Albania,7,237.5714286,0\r\n" 2 ) ; content . append ( "4,22345.39749,5261,1/4/10,American<sp>Samoa,14,375.7857143,1\r\n" ) ; content . append ( "--boundary--\r\n" ) ; request . setContent ( content . toString ( ) . getBytes ( ) ) ; org . pentaho . platform . dataaccess . datasource . wizard . service . impl . UploadFileDebugServlet uploadServlet = new org . pentaho . platform . dataaccess . datasource . wizard . service . impl . UploadFileDebugServlet ( ) ; org . springframework . mock . web . MockHttpServletResponse response = new org . springframework . mock . web . MockHttpServletResponse ( ) ; uploadServlet . service ( request , response ) ; response . getWriter ( ) . flush ( ) ; response . getWriter ( ) . close ( ) ; fileName = response . getContentAsString ( ) ; java . lang . String path = org . pentaho . platform . engine . core . system . PentahoSystem . getApplicationContext ( ) . getSolutionPath ( org . pentaho . platform . dataaccess . datasource . wizard . csv . FileUploadServiceTest . TMP_FILE_PATH ) ; java . lang . String filenameWithPath = ( path + ( java . io . File . separatorChar ) ) + fileName ; java . io . File file = new java . io . File ( filenameWithPath ) ; org . junit . Assert . assertTrue ( file . exists ( ) ) ; if ( file . exists ( ) ) { file . delete ( ) ; } } }
public class aTest{ @Test public void date_time_properties_should_not_have_a_VALUE_parameter ( ) { class DateTestScribe < T extends ezvcard . property . VCardProperty > extends ezvcard . io . scribe . VCardPropertyScribe < T > { private final ezvcard . VCardDataType dataType ; public DateTestScribe ( java . lang . Class < T > clazz , java . lang . String name , ezvcard . VCardDataType dataType ) { ( clazz , name ) ; this . dataType = dataType ; } @ ezvcard . io . text . Override protected ezvcard . VCardDataType _defaultDataType ( ezvcard . VCardVersion version ) { return ezvcard . VCardDataType . DATE_AND_OR_TIME ; } @ ezvcard . io . text . Override protected ezvcard . VCardDataType _dataType ( T property , ezvcard . VCardVersion version ) { return dataType ; } @ ezvcard . io . text . Override protected java . lang . String _writeText ( T property , ezvcard . io . text . WriteContext context ) { return "value" ; } @ ezvcard . io . text . Override protected T _parseText ( java . lang . String value , ezvcard . VCardDataType dataType , ezvcard . parameter . VCardParameters parameters , ezvcard . io . ParseContext context ) { return null ; } } class DateProperty extends ezvcard . property . VCardProperty { } class DateTimeProperty extends ezvcard . property . VCardProperty { } class TimeProperty extends ezvcard . property . VCardProperty { } class DateAndOrTimeProperty extends ezvcard . property . VCardProperty { } ezvcard . VCard vcard = new ezvcard . VCard ( ) ; vcard . addProperty ( new DateProperty ( ) ) ; vcard . addProperty ( new DateTimeProperty ( ) ) ; vcard . addProperty ( new TimeProperty ( ) ) ; vcard . addProperty ( new DateAndOrTimeProperty ( ) ) ; java . io . StringWriter sw = new java . io . StringWriter ( ) ; ezvcard . io . text . VCardWriter vcw = new ezvcard . io . text . VCardWriter ( sw , ezvcard . VCardVersion . V4_0 ) ; vcw . registerScribe ( new DateTestScribe < DateProperty > ( DateProperty . class , "DATE" , ezvcard . VCardDataType . DATE ) ) ; vcw . registerScribe ( new DateTestScribe < DateTimeProperty > ( DateTimeProperty . class , "DATETIME" , ezvcard . VCardDataType . DATE_TIME ) ) ; vcw . registerScribe ( new DateTestScribe < TimeProperty > ( TimeProperty . class , "DATETIME" 1 , ezvcard . VCardDataType . TIME ) ) ; vcw . registerScribe ( new DateTestScribe < DateAndOrTimeProperty > ( DateAndOrTimeProperty . class , "DATEANDORTIME" , ezvcard . VCardDataType . DATE_AND_OR_TIME ) ) ; vcw . setAddProdId ( false ) ; vcw . write ( vcard ) ; java . lang . String actual = sw . toString ( ) ; java . lang . String expected = "BEGIN:VCARD\r\n" + ( ( ( ( ( "DATETIME" 0 + "DATE:value\r\n" ) + "DATETIME:value\r\n" ) + "TIME:value\r\n" ) + "DATEANDORTIME:value\r\n" ) + "END:VCARD\r\n" ) ; org . junit . Assert . assertEquals ( actual , expected ) ; } }
public class aTest{ @Test public void testModularGB2 ( ) { java . lang . String [ ] vars = new java . lang . String [ ] { "x" , "y" , "z" } ; cc . redberry . rings . poly . multivar . MultivariatePolynomial < cc . redberry . rings . bigint . BigInteger > a = cc . redberry . rings . poly . multivar . MultivariatePolynomial . parse ( "Modular:<sp>" 0 , cc . redberry . rings . poly . multivar . Z , cc . redberry . rings . poly . multivar . MonomialOrder . GREVLEX , vars ) ; cc . redberry . rings . poly . multivar . MultivariatePolynomial < cc . redberry . rings . bigint . BigInteger > b = cc . redberry . rings . poly . multivar . MultivariatePolynomial . parse ( "x^5<sp>+<sp>2*y^3*z^2<sp>+<sp>13*y^2*z^3<sp>+<sp>5*y*z^4" , cc . redberry . rings . poly . multivar . Z , cc . redberry . rings . poly . multivar . MonomialOrder . GREVLEX , vars ) ; cc . redberry . rings . poly . multivar . MultivariatePolynomial < cc . redberry . rings . bigint . BigInteger > c = cc . redberry . rings . poly . multivar . MultivariatePolynomial . parse ( "8*x^3<sp>+<sp>12*y^3<sp>+<sp>x*z^2<sp>+<sp>3" , cc . redberry . rings . poly . multivar . Z , cc . redberry . rings . poly . multivar . MonomialOrder . GREVLEX , vars ) ; cc . redberry . rings . poly . multivar . MultivariatePolynomial < cc . redberry . rings . bigint . BigInteger > d = cc . redberry . rings . poly . multivar . MultivariatePolynomial . parse ( "7*x^2*y^4<sp>+<sp>18*x*y^3*z^2<sp>+<sp>y^3*z^3" , cc . redberry . rings . poly . multivar . Z , cc . redberry . rings . poly . multivar . MonomialOrder . GREVLEX , vars ) ; cc . redberry . rings . poly . multivar . List < cc . redberry . rings . poly . multivar . MultivariatePolynomial < cc . redberry . rings . bigint . BigInteger > > gens = cc . redberry . rings . poly . multivar . Arrays . asList ( a , b , c , d ) ; cc . redberry . rings . poly . multivar . List < cc . redberry . rings . poly . multivar . MultivariatePolynomial < cc . redberry . rings . bigint . BigInteger > > expected = cc . redberry . rings . poly . multivar . Arrays . asList ( cc . redberry . rings . poly . multivar . MultivariatePolynomial . parse ( "x" , cc . redberry . rings . poly . multivar . Z , cc . redberry . rings . poly . multivar . MonomialOrder . GREVLEX , vars ) , cc . redberry . rings . poly . multivar . MultivariatePolynomial . parse ( "z^2" , cc . redberry . rings . poly . multivar . Z , cc . redberry . rings . poly . multivar . MonomialOrder . GREVLEX , vars ) , cc . redberry . rings . poly . multivar . MultivariatePolynomial . parse ( "1<sp>+<sp>4*y^3" , cc . redberry . rings . poly . multivar . Z , cc . redberry . rings . poly . multivar . MonomialOrder . GREVLEX , vars ) ) ; for ( int i = 0 ; i < 1 ; ++ i ) { long start ; start = java . lang . System . nanoTime ( ) ; cc . redberry . rings . poly . multivar . List < cc . redberry . rings . poly . multivar . MultivariatePolynomial < cc . redberry . rings . bigint . BigInteger > > mod = ModularGB ( gens , cc . redberry . rings . poly . multivar . MonomialOrder . GREVLEX ) ; System . out . println ( ( "Modular:<sp>" + ( nanosecondsToString ( ( ( java . lang . System . nanoTime ( ) ) - start ) ) ) ) ) ; org . junit . Assert . assertEquals ( expected , mod ) ; } } }
public class aTest{ @Test public void deleteAndQueryInA ( ) { final com . google . appengine . api . datastore . Entity entity = getService ( ) . get ( com . google . appengine . api . datastore . KeyFactory . createKey ( "QT" , 2 ) ) ; org . junit . Assert . assertNotNull ( entity ) ; org . jboss . test . capedwarf . cluster . test . QueryTest . wrap ( new java . util . concurrent . Callable < java . lang . Void > ( ) { @ org . jboss . test . capedwarf . cluster . test . Override public org . jboss . test . capedwarf . cluster . test . Void call ( ) throws org . jboss . test . capedwarf . cluster . test . Exception { getService ( ) . delete ( entity . getKey ( ) ) ; return null ; } }
public class aTest{ @Test public void testMissingFile ( ) { final java . io . File temp = java . io . File . createTempFile ( "testFactory" , "java" ) ; try { temp . delete ( ) ; final com . pogofish . jadt . sink . FileSink sink = new com . pogofish . jadt . sink . FileSink ( temp . getAbsolutePath ( ) ) ; try { sink . write ( "hello" ) ; } finally { sink . close ( ) ; } final java . io . BufferedReader reader = new java . io . BufferedReader ( new java . io . InputStreamReader ( new java . io . FileInputStream ( temp ) , "UTF-8" ) ) ; final java . lang . String contents = reader . readLine ( ) ; org . junit . Assert . assertEquals ( "hello" , contents ) ; } }
public class aTest{ @Test public void checkClosedConnectionAfterFailover ( ) { try ( java . sql . Connection connection = getNewConnection ( "&retriesAllDown=6" , true ) ) { java . sql . Statement st = connection . createStatement ( ) ; int masterServerId = getServerId ( connection ) ; stopProxy ( masterServerId ) ; try { st . execute ( "SELECT<sp>1" ) ; org . junit . Assert . fail ( ) ; } catch ( java . sql . SQLException e ) { } org . junit . Assert . assertTrue ( st . isClosed ( ) ) ; restartProxy ( masterServerId ) ; try { st = connection . createStatement ( ) ; st . execute ( "SELECT<sp>1" ) ; } }
public class aTest{ @Test public void coreSimulatorServicesKilledSuccessfully ( ) { ImmutableList . Builder < Map . Entry < com . facebook . buck . util . ProcessExecutorParams , com . facebook . buck . util . FakeProcess > > fakeProcessesBuilder = com . google . common . collect . ImmutableList . builder ( ) ; fakeProcessesBuilder . add ( new java . util . AbstractMap . SimpleImmutableEntry ( com . facebook . buck . apple . simulator . AppleCoreSimulatorServiceControllerTest . LAUNCHCTL_LIST_PARAMS , new com . facebook . buck . util . FakeProcess ( 0 , ( "87823\t0\tcom.apple.CoreSimulator.CoreSimulatorService.117.15.1.lkhDXxRPp5yy\n" + ( ( "87823\t0\tcom.apple.CoreSimulator.CoreSimulatorService.117.15.1.lkhDXxRPp5yy\n" 0 + ".launchd_sim\n" ) + "74614\t0\tcom.apple.iphonesimulator.6564\n" ) ) , "" ) ) ) ; fakeProcessesBuilder . add ( new java . util . AbstractMap . SimpleImmutableEntry ( com . facebook . buck . util . ProcessExecutorParams . builder ( ) . setCommand ( com . google . common . collect . ImmutableList . of ( "launchctl" , "remove" , "com.apple.CoreSimulator.CoreSimulatorService.117.15.1.lkhDXxRPp5yy" ) ) . build ( ) , new com . facebook . buck . util . FakeProcess ( 0 ) ) ) ; fakeProcessesBuilder . add ( new java . util . AbstractMap . SimpleImmutableEntry ( com . facebook . buck . util . ProcessExecutorParams . builder ( ) . setCommand ( com . google . common . collect . ImmutableList . of ( "launchctl" , "remove" , ( "com.apple.CoreSimulator.SimDevice.CC1B0BAD-BAE6-4A53-92CF-F79850654057." + "launchd_sim" ) ) ) . build ( ) , new com . facebook . buck . util . FakeProcess ( 0 ) ) ) ; fakeProcessesBuilder . add ( new java . util . AbstractMap . SimpleImmutableEntry ( com . facebook . buck . util . ProcessExecutorParams . builder ( ) . setCommand ( com . google . common . collect . ImmutableList . of ( "launchctl" , "remove" , "com.apple.iphonesimulator.6564" ) ) . build ( ) , new com . facebook . buck . util . FakeProcess ( 0 ) ) ) ; com . facebook . buck . util . FakeProcessExecutor fakeProcessExecutor = new com . facebook . buck . util . FakeProcessExecutor ( fakeProcessesBuilder . build ( ) ) ; com . facebook . buck . apple . simulator . AppleCoreSimulatorServiceController appleCoreSimulatorServiceController = new com . facebook . buck . apple . simulator . AppleCoreSimulatorServiceController ( fakeProcessExecutor ) ; org . junit . Assert . assertThat ( appleCoreSimulatorServiceController . killSimulatorProcesses ( ) , org . hamcrest . Matchers . is ( true ) ) ; } }
public class aTest{ @Test public void testZZDeleteteAllTemplatesBadRequest ( ) { io . symcpe . wraith . actions . alerts . templated . AlertTemplate tpl = new io . symcpe . wraith . actions . alerts . templated . AlertTemplate ( ) ; io . symcpe . hendrix . api . storage . Tenant tenant = io . symcpe . hendrix . api . dao . TemplateManager . getInstance ( ) . getTenant ( em , io . symcpe . hendrix . api . dao . TestTemplateManager . TENANT_ID_1 ) ; io . symcpe . hendrix . api . storage . AlertTemplates templates = new io . symcpe . hendrix . api . storage . AlertTemplates ( ) ; short id = io . symcpe . hendrix . api . dao . TemplateManager . getInstance ( ) . createNewTemplate ( em , templates , tenant ) . getTemplateId ( ) ; tpl . setTemplateId ( id ) ; tpl . setBody ( "test" ) ; tpl . setDestination ( "test@xyz.com" ) ; tpl . setMedia ( "mail" ) ; tpl . setTemplateName ( "Test" ) ; tpl . setThrottleDuration ( 2 ) ; tpl . setThrottleLimit ( 2 ) ; id = io . symcpe . hendrix . api . dao . TemplateManager . getInstance ( ) . saveTemplate ( em , templates , templates . getTenant ( ) , tpl , am ) . getTemplateId ( ) ; org . junit . Assert . assertEquals ( id , templates . getTemplateId ( ) ) ; io . symcpe . wraith . rules . Rule rul = new io . symcpe . wraith . rules . SimpleRule ( ( ( short ) ( 0 ) ) , "simple-rule2" , true , new io . symcpe . wraith . conditions . relational . EqualsCondition ( "host" , "symcpe2" ) , new io . symcpe . wraith . actions . Action [ ] { new io . symcpe . wraith . actions . alerts . templated . TemplatedAlertAction ( ( ( short ) ( 0 ) ) , id ) } ) ; io . symcpe . hendrix . api . dao . RulesManager . getInstance ( ) . saveRule ( em , new io . symcpe . hendrix . api . storage . Rules ( ) , tenant , rul , am ) ; try { io . symcpe . hendrix . api . dao . TemplateManager . getInstance ( ) . deleteTemplates ( em , templates . getTenant ( ) , am ) ; org . junit . Assert . fail ( "Can't<sp>reach<sp>here<sp>this<sp>request<sp>should<sp>fail" ) ; } }
public class aTest{ @Test public void MethodThatWillCallAsyncMethod ( ) { java . lang . String vol = new java . lang . String ( "Hello" ) ; org . apache . cloudstack . framework . async . AsyncCallbackDispatcher < org . apache . cloudstack . framework . codestyle . AsyncSampleEventDrivenStyleCaller , java . lang . Object > caller = org . apache . cloudstack . framework . async . AsyncCallbackDispatcher . create ( this ) ; org . apache . cloudstack . framework . async . AsyncCallFuture < java . lang . String > future = _ds . createVolume ( vol ) ; try { java . lang . String result = future . get ( ) ; org . junit . Assert . assertEquals ( result , vol ) ; } }
public class aTest{ @Test public void recordCallToBaseInterfaceMethodOnCaptureSubInterfaceImplementation ( mockit . MockedParametersWithCapturingTest$ISub ) { new mockit . Expectations ( ) { { mock . doSomething ( ) ; result = 123 ; } } ; mockit . MockedParametersWithCapturingTest . ISub impl = new mockit . MockedParametersWithCapturingTest . ISub ( ) { @ mockit . Override public int doSomething ( ) { return - 1 ; } } ; int i = impl . doSomething ( ) ; org . junit . Assert . assertEquals ( 123 , i ) ; } }
public class aTest{ @Test public void testVertexTypeUniqueConstraintValid ( ) { org . junit . Assert . assertNull ( com . puresoltechnologies . ductiledb . core . graph . schema . DuctileDBSchemaManagerIT . schemaManager . getPropertyDefinition ( ElementType . VERTEX , "property" ) ) ; com . puresoltechnologies . ductiledb . core . graph . schema . PropertyDefinition < java . lang . String > definition = new com . puresoltechnologies . ductiledb . core . graph . schema . PropertyDefinition ( com . puresoltechnologies . ductiledb . core . graph . ElementType . VERTEX , "property" , java . lang . String . class , UniqueConstraint . TYPE ) ; com . puresoltechnologies . ductiledb . core . graph . schema . DuctileDBSchemaManagerIT . schemaManager . defineProperty ( definition ) ; java . util . Set < java . lang . String > types1 = new java . util . HashSet ( ) ; types1 . add ( "type1" ) ; java . util . Set < java . lang . String > types2 = new java . util . HashSet ( ) ; types2 . add ( "type2" ) ; java . util . Map < java . lang . String , java . lang . Object > properties = new java . util . HashMap ( ) ; properties . put ( "property" , "value" ) ; try { com . puresoltechnologies . ductiledb . core . graph . schema . DuctileDBSchemaManagerIT . graph . addVertex ( types1 , properties ) ; com . puresoltechnologies . ductiledb . core . graph . schema . DuctileDBSchemaManagerIT . graph . addVertex ( types2 , properties ) ; } }
public class aTest{ @Test public void testGetAndValidateStorageUnitInvalidStorageUnitStatus ( ) { org . finra . herd . model . api . xml . BusinessObjectDataKey businessObjectDataKey = new org . finra . herd . model . api . xml . BusinessObjectDataKey ( BDEF_NAMESPACE , BDEF_NAME , FORMAT_USAGE_CODE , FORMAT_FILE_TYPE_CODE , FORMAT_VERSION , PARTITION_VALUE , NO_SUBPARTITION_VALUES , DATA_VERSION ) ; org . finra . herd . model . jpa . BusinessObjectDataEntity businessObjectDataEntity = new org . finra . herd . model . jpa . BusinessObjectDataEntity ( ) ; businessObjectDataEntity . setId ( org . finra . herd . service . impl . ID ) ; org . finra . herd . model . jpa . StorageEntity storageEntity = new org . finra . herd . model . jpa . StorageEntity ( ) ; storageEntity . setName ( org . finra . herd . service . impl . STORAGE_NAME ) ; org . finra . herd . model . jpa . StorageUnitStatusEntity storageUnitStatusEntity = new org . finra . herd . model . jpa . StorageUnitStatusEntity ( ) ; storageUnitStatusEntity . setCode ( org . finra . herd . service . impl . INVALID_VALUE ) ; org . finra . herd . model . jpa . StorageUnitEntity storageUnitEntity = new org . finra . herd . model . jpa . StorageUnitEntity ( ) ; storageUnitEntity . setStorage ( storageEntity ) ; storageUnitEntity . setStatus ( storageUnitStatusEntity ) ; when ( storageUnitDao . getStorageUnitsByStoragePlatformAndBusinessObjectData ( StoragePlatformEntity . S3 , businessObjectDataEntity ) ) . thenReturn ( java . util . Arrays . asList ( storageUnitEntity ) ) ; when ( businessObjectDataHelper . businessObjectDataKeyToString ( businessObjectDataKey ) ) . thenReturn ( org . finra . herd . service . impl . BUSINESS_OBJECT_DATA_KEY_AS_STRING ) ; try { businessObjectDataInitiateDestroyHelperServiceImpl . getAndValidateStorageUnit ( businessObjectDataEntity , businessObjectDataKey ) ; junit . framework . Assert . fail ( ) ; } catch ( java . lang . IllegalArgumentException e ) { org . junit . Assert . assertEquals ( java . lang . String . format ( "Storage<sp>unit<sp>status<sp>\"%s\"<sp>is<sp>not<sp>supported<sp>by<sp>the<sp>business<sp>object<sp>data<sp>destroy<sp>feature.<sp>Storage:<sp>{%s},<sp>business<sp>object<sp>data:<sp>{%s}" , org . finra . herd . service . impl . INVALID_VALUE , org . finra . herd . service . impl . STORAGE_NAME , org . finra . herd . service . impl . BUSINESS_OBJECT_DATA_KEY_AS_STRING ) , e . getMessage ( ) ) ; } }
public class aTest{ @Test public void testSelectSsl ( ) { odbcServer . start ( Mode . ENABLED ) ; org . postgresql . Driver d = new org . postgresql . Driver ( ) ; java . util . Properties p = new java . util . Properties ( ) ; p . setProperty ( "user" , "testuser" ) ; p . setProperty ( "select<sp>*<sp>from<sp>sys.tables<sp>order<sp>by<sp>name" 3 , "testpassword" ) ; p . setProperty ( "ssl" , "true" ) ; p . setProperty ( "sslfactory" , "select<sp>*<sp>from<sp>sys.tables<sp>order<sp>by<sp>name" 1 ) ; java . sql . Connection conn = d . connect ( ( ( ( ( "jdbc:postgresql://" + ( odbcServer . addr . getHostName ( ) ) ) + "select<sp>*<sp>from<sp>sys.tables<sp>order<sp>by<sp>name" 2 ) + ( odbcServer . odbcTransport . getPort ( ) ) ) + "/parts" ) , p ) ; java . sql . Statement s = conn . createStatement ( ) ; org . junit . Assert . assertTrue ( s . execute ( "select<sp>*<sp>from<sp>sys.tables<sp>order<sp>by<sp>name" ) ) ; org . teiid . jdbc . TestMMDatabaseMetaData . compareResultSet ( "select<sp>*<sp>from<sp>sys.tables<sp>order<sp>by<sp>name" 0 , s . getResultSet ( ) ) ; p . remove ( "ssl" ) ; try { conn = d . connect ( ( ( ( ( "jdbc:postgresql://" + ( odbcServer . addr . getHostName ( ) ) ) + "select<sp>*<sp>from<sp>sys.tables<sp>order<sp>by<sp>name" 2 ) + ( odbcServer . odbcTransport . getPort ( ) ) ) + "/parts" ) , p ) ; org . junit . Assert . fail ( "should<sp>require<sp>ssl" ) ; } }
public class aTest{ @Test public void testFindApposite ( ) { edu . nyu . jet . parser . SyntacticRelationSet relations = new edu . nyu . jet . parser . SyntacticRelationSet ( ) ; java . lang . String s = "n" 2 + ( ( ( ( ( "dobj<sp>|<sp>named<sp>|<sp>27<sp>|<sp>VBD<sp>|<sp>father<sp>|<sp>3<sp>|<sp>NN<sp>\n" + "punct<sp>|<sp>father<sp>|<sp>3<sp>|<sp>NN<sp>|<sp>,<sp>|<sp>9<sp>|<sp>,<sp>\n" ) + "appos<sp>|<sp>father<sp>|<sp>3<sp>|<sp>NN<sp>|<sp>Fred_Smith<sp>|<sp>11<sp>|<sp>NNP<sp>\n" ) + "n" 3 ) + "objcomp<sp>|<sp>named<sp>|<sp>27<sp>|<sp>VBD<sp>|<sp>president<sp>|<sp>33<sp>|<sp>NN<sp>\n" ) + "punct<sp>|<sp>named<sp>|<sp>27<sp>|<sp>VBD<sp>|<sp>.<sp>|<sp>42<sp>|<sp>.<sp>\n" ) ; relations . read ( new java . io . BufferedReader ( new java . io . StringReader ( s ) ) ) ; edu . nyu . jet . parser . Document doc = new edu . nyu . jet . parser . Document ( "n" 0 ) ; java . util . Vector < edu . nyu . jet . parser . Annotation > mentions = new java . util . Vector < edu . nyu . jet . parser . Annotation > ( ) ; edu . nyu . jet . parser . Annotation h1 = new edu . nyu . jet . parser . Annotation ( "constit" , new edu . nyu . jet . parser . Span ( 3 , 9 ) , new edu . nyu . jet . parser . FeatureSet ( "cat" , "n" ) ) ; doc . addAnnotation ( h1 ) ; edu . nyu . jet . parser . Annotation m1 = new edu . nyu . jet . parser . Annotation ( "constit" , new edu . nyu . jet . parser . Span ( 0 , 9 ) , new edu . nyu . jet . parser . FeatureSet ( "cat" , "np" , "headC" , h1 ) ) ; doc . addAnnotation ( m1 ) ; mentions . add ( m1 ) ; edu . nyu . jet . parser . Annotation m2 = new edu . nyu . jet . parser . Annotation ( "constit" , new edu . nyu . jet . parser . Span ( 11 , 21 ) , new edu . nyu . jet . parser . FeatureSet ( "cat" , "np" ) ) ; doc . addAnnotation ( m2 ) ; mentions . add ( m2 ) ; edu . nyu . jet . parser . DepAnalyzer . convertRelations ( doc , relations , mentions ) ; org . junit . Assert . assertEquals ( m1 . get ( "n" 1 ) , m2 ) ; } }
public class aTest{ @Test public void testResourceNameNotSet ( ) { java . io . InputStream is ; try { is = new java . io . FileInputStream ( content ) ; ddf . catalog . resource . impl . ResourceImpl ri = new ddf . catalog . resource . impl . ResourceImpl ( is , mimeType , null ) ; org . junit . Assert . assertEquals ( null , ri . getName ( ) ) ; } }
public class aTest{ @Test public void testEncodeMongoDBDatabaseEquals ( ) { System . out . println ( ( ( com . telefonica . iot . cygnus . utils . CommonUtilsForTests . getTestTraceHead ( "[NGSICharsets.encodeMongoDBDatabase]" ) ) + "--------<sp>\'=\'<sp>(internal<sp>concatenator)<sp>is<sp>encoded<sp>as<sp>\"xffff\"<sp>(public<sp>concatenator)" ) ) ; java . lang . String in = "=" ; java . lang . String expected = "xffff" ; java . lang . String out = com . telefonica . iot . cygnus . utils . NGSICharsets . encodeMongoDBDatabase ( in ) ; try { org . junit . Assert . assertEquals ( expected , out ) ; System . out . println ( ( ( ( ( ( ( com . telefonica . iot . cygnus . utils . CommonUtilsForTests . getTestTraceHead ( "[NGSICharsets.encodeMongoDBDatabase]" ) ) + "-<sp>OK<sp>-<sp>'" ) + in ) + "'<sp>has<sp>been<sp>encoded<sp>as<sp>'" ) + expected ) + "'" ) ) ; } }
public class aTest{ @Test public void receiveInvalidLengthMessageToByteArray ( ) { final org . xnio . ByteBufferSlicePool pool = new org . xnio . ByteBufferSlicePool ( org . xnio . BufferAllocator . BYTE_BUFFER_ALLOCATOR , 10 , ( 10 * 16 ) ) ; final org . xnio . channels . FramedMessageChannel channel = new org . xnio . channels . FramedMessageChannel ( connectedChannel , pool . allocate ( ) , pool . allocate ( ) ) ; try { connectedChannel . setReadDataWithLength ( 15 , "987654321098765" ) ; connectedChannel . enableRead ( true ) ; java . nio . ByteBuffer buffer = java . nio . ByteBuffer . allocate ( 20 ) ; boolean failed = false ; try { channel . receive ( new java . nio . ByteBuffer [ ] { buffer } ) ; } catch ( java . io . IOException e ) { failed = true ; } org . junit . Assert . assertTrue ( failed ) ; } }
public class aTest{ @Test public void stateMachineTraits052Test ( ) { cruise . umple . compiler . UmpleModel model = getModelByFilename ( "T_do" 1 ) ; boolean result = false ; try { model . run ( ) ; } catch ( java . lang . Exception e ) { result = e . getMessage ( ) . contains ( "236" ) ; } finally { org . junit . Assert . assertTrue ( result ) ; cruise . umple . util . SampleFileWriter . destroy ( "T_do" 4 ) ; } }
public class aTest{ @Test public void testCreateTxtHostVmwareWithNonemptyIPAddress ( ) { com . intel . mtwilson . datatypes . TxtHostRecord hostObj = new com . intel . mtwilson . datatypes . TxtHostRecord ( ) ; hostObj . HostName = "10.1.71.146" ; hostObj . Port = 0 ; hostObj . AddOn_Connection_String = "https://10.1.71.87:443/sdk;Administrator;P@ssw0rd" ; hostObj . BIOS_Name = "EPSD" ; hostObj . BIOS_Oem = "EPSD" ; hostObj . BIOS_Version = "v60" ; hostObj . Description = "5.0.0" 0 ; hostObj . Email = "" ; hostObj . IPAddress = "1.2.3.4" ; hostObj . VMM_Name = "ESXi" ; hostObj . VMM_Version = "5.0.0-469512" ; hostObj . VMM_OSName = "VMware_ESXi" ; hostObj . VMM_OSVersion = "5.0.0" ; com . intel . mtwilson . datatypes . TxtHost hostAddObj = new com . intel . mtwilson . datatypes . TxtHost ( hostObj ) ; org . junit . Assert . assertTrue ( ( ( hostAddObj . getHostName ( ) ) != null ) ) ; } }
public class aTest{ @Test public void failsOnIllegalListProperty ( ) { org . neo4j . geoff . Subgraph geoff = new org . neo4j . geoff . Subgraph ( ) ; geoff . add ( "(fib)<sp>{\"sequence\":<sp>[1,1.0,\"two\",3,5,8,13,21,35]}" ) ; java . util . Map < java . lang . String , org . neo4j . geoff . test . PropertyContainer > out = org . neo4j . geoff . Geoff . mergeIntoNeo4j ( geoff , db , null ) ; org . neo4j . geoff . test . Transaction tx = db . beginTx ( ) ; try { org . junit . Assert . assertNotNull ( out ) ; } }
public class aTest{ @Test public void metadata_xmlPropertyTypefalse ( ) { final java . lang . String entryName = "bar/90_contents/odatacol1/00_$metadata.xml" ; final java . lang . String filename = "/00_$metadata_property_type_attr_invalid.xml" ; java . net . URL fileUrl = java . lang . ClassLoader . getSystemResource ( ( ( com . fujitsu . dc . test . unit . core . bar . BarFileValidateTest . RESOURCE_PATH ) + filename ) ) ; java . io . File file = new java . io . File ( fileUrl . getPath ( ) ) ; java . io . FileInputStream fis = null ; try { fis = new java . io . FileInputStream ( file ) ; com . fujitsu . dc . test . unit . core . bar . BarFileValidateTest . TestBarRunner testBarRunner = new com . fujitsu . dc . test . unit . core . bar . BarFileValidateTest . TestBarRunner ( ) ; boolean res = testBarRunner . registUserSchema ( entryName , fis , null ) ; org . junit . Assert . assertFalse ( res ) ; return ; } }
public class aTest{ @Test public void testNoEncoding ( ) { file = new org . pentaho . platform . api . repository2 . unified . data . simple . SimpleRepositoryFileData ( inputStreamSpy , "" , org . pentaho . platform . api . repository2 . unified . data . SimpleRepositoryFileDataTest . MIME_TYPE ) ; org . junit . Assert . assertNotNull ( file . toString ( ) ) ; verify ( inputStreamSpy ) . markSupported ( ) ; verify ( inputStreamSpy ) . mark ( Integer . MAX_VALUE ) ; try { verify ( inputStreamSpy , atLeastOnce ( ) ) . read ( any ( byte [ ] . class ) ) ; verify ( inputStreamSpy ) . reset ( ) ; } }
public class aTest{ @Test public void testBasic ( ) { com . github . os72 . protobuf . dynamic . DynamicSchemaTest . log ( "at@sis.gov.uk" 0 ) ; com . github . os72 . protobuf . dynamic . DynamicSchema . Builder schemaBuilder = com . github . os72 . protobuf . dynamic . DynamicSchema . newBuilder ( ) ; schemaBuilder . setName ( "PersonSchemaDynamic.proto" ) ; com . github . os72 . protobuf . dynamic . MessageDefinition msgDef = com . github . os72 . protobuf . dynamic . MessageDefinition . newBuilder ( "Person" ) . addField ( "at@sis.gov.uk" 1 , "int32" , "id" , 1 ) . addField ( "at@sis.gov.uk" 1 , "string" , "name" , 2 ) . addField ( "optional" , "string" , "email" , 3 ) . build ( ) ; schemaBuilder . addMessageDefinition ( msgDef ) ; com . github . os72 . protobuf . dynamic . DynamicSchema schema = schemaBuilder . build ( ) ; com . github . os72 . protobuf . dynamic . DynamicSchemaTest . log ( schema ) ; com . google . protobuf . DynamicMessage . Builder msgBuilder = schema . newMessageBuilder ( "Person" ) ; com . google . protobuf . Descriptors . Descriptor msgDesc = msgBuilder . getDescriptorForType ( ) ; com . google . protobuf . DynamicMessage msg = msgBuilder . setField ( msgDesc . findFieldByName ( "id" ) , 1 ) . setField ( msgDesc . findFieldByName ( "name" ) , "Alan<sp>Turing" ) . setField ( msgDesc . findFieldByName ( "email" ) , "at@sis.gov.uk" ) . build ( ) ; com . github . os72 . protobuf . dynamic . DynamicSchemaTest . log ( msg ) ; com . github . os72 . protobuf . dynamic . PersonSchema . Person person = PersonSchema . Person . newBuilder ( ) . setId ( 1 ) . setName ( "Alan<sp>Turing" ) . setEmail ( "at@sis.gov.uk" ) . build ( ) ; org . junit . Assert . assertEquals ( person . toString ( ) , msg . toString ( ) ) ; } }
public class aTest{ @Test public void testTWQnENoQNoEWithDEFUnix ( ) { if ( ( java . io . File . separatorChar ) != '/' ) { org . opennms . test . mock . MockUtil . println ( "--------<sp>Skipping<sp>testTWQnENoQNoEWithDEFUnix<sp>since<sp>File.separator<sp>is<sp>not<sp>/<sp>------------" ) ; org . opennms . test . mock . MockUtil . println ( "--------<sp>Be<sp>sure<sp>to<sp>run<sp>the<sp>tests<sp>on<sp>Unix<sp>too!<sp>---------" ) ; return ; } java . lang . String input = "No<sp>quote,<sp>no<sp>escapes,<sp>but<sp>DEF:test=snmp/42/test.jrb:test:AVERAGE" ; java . lang . String [ ] expected = new java . lang . String [ ] { "No" , "escapes," 0 , "no" , "escapes," , "but" , "DEF:test=snmp/42/test.jrb:test:AVERAGE" } ; java . lang . String [ ] actual = org . opennms . netmgt . rrd . jrobin . JRobinRrdStrategy . tokenizeWithQuotingAndEscapes ( input , "<sp>" , false , "" ) ; org . junit . Assert . assertEquals ( java . util . Arrays . asList ( expected ) , java . util . Arrays . asList ( actual ) ) ; } }
public class aTest{ @Test public void testDoMapConfig ( ) { System . out . println ( ( ( getTestTraceHead ( "[NGSIGroupingInterceptor.doMapConfig]" ) ) + "--------<sp>A<sp>mapped<sp>ContextElement<sp>can<sp>be<sp>obtained<sp>from<sp>the<sp>Name<sp>Mappings" ) ) ; com . telefonica . iot . cygnus . interceptors . NGSINameMappingsInterceptor nameMappingsInterceptor = new com . telefonica . iot . cygnus . interceptors . NGSINameMappingsInterceptor ( null , false ) ; nameMappingsInterceptor . loadNameMappings ( nameMappingsStrConfig ) ; com . telefonica . iot . cygnus . containers . NotifyContextRequest . ContextElement originalCE ; com . telefonica . iot . cygnus . containers . NotifyContextRequest . ContextElement expectedCE ; try { originalCE = com . telefonica . iot . cygnus . utils . NGSIUtilsForTests . createJsonContextElement ( originalCEStrConfig ) ; expectedCE = com . telefonica . iot . cygnus . utils . NGSIUtilsForTests . createJsonContextElement ( expectedCEStrConfig ) ; } catch ( java . lang . Exception e ) { System . out . println ( ( ( getTestTraceHead ( "[NGSIGroupingInterceptor.doMapConfig]" ) ) + "-<sp>FAIL<sp>-<sp>There<sp>was<sp>some<sp>problem<sp>when<sp>parsing<sp>the<sp>ContextElements" ) ) ; throw new java . lang . AssertionError ( e . getMessage ( ) ) ; } org . apache . commons . lang3 . tuple . ImmutableTriple < java . lang . String , java . lang . String , com . telefonica . iot . cygnus . containers . NotifyContextRequest . ContextElement > map = nameMappingsInterceptor . doMap ( originalServiceConfig , originalServicePathConfig , originalCE ) ; com . telefonica . iot . cygnus . containers . NotifyContextRequest . ContextElement mappedCE = map . getRight ( ) ; boolean equals = true ; if ( ( ! ( mappedCE . getType ( ) . equals ( expectedCE . getType ( ) ) ) ) || ( ! ( expectedServicePathConfig . equals ( map . getMiddle ( ) ) ) ) ) { equals = false ; } else { for ( int j = 0 ; j < ( mappedCE . getAttributes ( ) . size ( ) ) ; j ++ ) { com . telefonica . iot . cygnus . containers . NotifyContextRequest . ContextAttribute mappedCA = mappedCE . getAttributes ( ) . get ( j ) ; com . telefonica . iot . cygnus . containers . NotifyContextRequest . ContextAttribute expectedCA = expectedCE . getAttributes ( ) . get ( j ) ; if ( ( ! ( mappedCA . getName ( ) . equals ( expectedCA . getName ( ) ) ) ) || ( ! ( mappedCA . getType ( ) . equals ( expectedCA . getType ( ) ) ) ) ) { equals = false ; break ; } } } try { org . junit . Assert . assertTrue ( equals ) ; System . out . println ( ( ( getTestTraceHead ( "[NGSIGroupingInterceptor.doMapConfig]" ) ) + "-<sp>OK<sp>-<sp>The<sp>mapped<sp>NotifyContextRequest<sp>is<sp>equals<sp>to<sp>the<sp>expected<sp>one" ) ) ; } }
public class aTest{ @Test public void testTypeRefractionOnQuery2 ( ) { java . lang . String source = "<sp>don(<sp>k,<sp>D.class<sp>);<sp>\n" 5 + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "query<sp>queryA1\n" 8 + "<sp>don(<sp>k,<sp>A.class<sp>);<sp>\n" 6 ) + "<sp>don(<sp>k,<sp>A.class<sp>);<sp>\n" 7 ) + "<sp>don(<sp>k,<sp>D.class<sp>);<sp>\n" 4 ) + "<sp>don(<sp>k,<sp>A.class<sp>);<sp>\n" 9 ) + "<sp>don(<sp>k,<sp>D.class<sp>);<sp>\n" 4 ) + "query<sp>queryA1\n" 5 ) + "<sp>don(<sp>k,<sp>D.class<sp>);<sp>\n" 7 ) + "<sp>don(<sp>k,<sp>D.class<sp>);<sp>\n" 3 ) + "<sp>don(<sp>k,<sp>D.class<sp>);<sp>\n" 6 ) + "<sp>don(<sp>k,<sp>D.class<sp>);<sp>\n" 0 ) + "<sp>don(<sp>k,<sp>A.class<sp>);<sp>\n" 4 ) + "<sp>don(<sp>k,<sp>D.class<sp>);<sp>\n" 1 ) + "query<sp>queryA1\n" 3 ) + "query<sp>queryA1\n" 5 ) + "declare<sp>Kore\n" ) + "<sp>@Traitable\n" ) + "query<sp>queryA1\n" 1 ) + "query<sp>queryA1\n" 5 ) + "query<sp>queryA1\n" 6 ) + "then\n" ) + "<sp>don(<sp>k,<sp>D.class<sp>);<sp>\n" 2 ) + "<sp>don(<sp>k,<sp>C.class<sp>);<sp>\n" ) + "<sp>don(<sp>k,<sp>D.class<sp>);<sp>\n" ) + "query<sp>queryA1\n" 9 ) + "<sp>don(<sp>k,<sp>A.class<sp>);<sp>\n" 8 ) + "<sp>don(<sp>k,<sp>A.class<sp>);<sp>\n" ) + "<sp>don(<sp>k,<sp>F.class<sp>);<sp>\n" ) + "<sp>don(<sp>k,<sp>G.class<sp>);<sp>\n" ) + "query<sp>queryA1\n" 4 ) + "query<sp>queryA1\n" 1 ) + "query<sp>queryA1\n" 5 ) + "query<sp>queryA1\n" 2 ) + "<sp>don(<sp>k,<sp>A.class<sp>);<sp>\n" 5 ) + "query<sp>queryA1\n" 7 ) + "then<sp>\n" ) + "<sp>don(<sp>k,<sp>A.class<sp>);<sp>\n" 0 ) + "query<sp>queryA1\n" 1 ) + "<sp>don(<sp>k,<sp>A.class<sp>);<sp>\n" 1 ) + "query<sp>queryA1\n" ) + "query<sp>queryA1\n" 0 ) + "query<sp>queryA1\n" 1 ) + "query<sp>queryA1\n" 5 ) ; org . kie . api . runtime . KieSession ks = getSessionFromString ( source ) ; org . drools . core . factmodel . traits . TraitFactory . setMode ( mode , ks . getKieBase ( ) ) ; java . util . List list = new java . util . ArrayList ( ) ; ks . setGlobal ( "<sp>don(<sp>k,<sp>A.class<sp>);<sp>\n" 3 , list ) ; ks . fireAllRules ( ) ; org . kie . api . runtime . rule . QueryResults res ; res = ks . getQueryResults ( "<sp>don(<sp>k,<sp>A.class<sp>);<sp>\n" 2 ) ; org . junit . Assert . assertEquals ( 1 , res . size ( ) ) ; } }
public class aTest{ @Test public void testQueueBrowserNextElementWithNoMessage ( ) { try ( org . apache . qpid . jms . test . testpeer . TestAmqpPeer testPeer = new org . apache . qpid . jms . test . testpeer . TestAmqpPeer ( ) ) { javax . jms . Connection connection = testFixture . establishConnecton ( testPeer ) ; connection . start ( ) ; testPeer . expectBegin ( ) ; javax . jms . Session session = connection . createSession ( false , Session . AUTO_ACKNOWLEDGE ) ; javax . jms . Queue queue = session . createQueue ( "myQueue" ) ; testPeer . expectQueueBrowserAttach ( ) ; testPeer . expectLinkFlow ( false , false , org . hamcrest . Matchers . equalTo ( org . apache . qpid . proton . amqp . UnsignedInteger . valueOf ( JmsDefaultPrefetchPolicy . DEFAULT_QUEUE_BROWSER_PREFETCH ) ) ) ; testPeer . expectLinkFlow ( true , true , org . hamcrest . Matchers . equalTo ( org . apache . qpid . proton . amqp . UnsignedInteger . valueOf ( JmsDefaultPrefetchPolicy . DEFAULT_QUEUE_BROWSER_PREFETCH ) ) ) ; testPeer . expectLinkFlow ( false , false , org . hamcrest . Matchers . equalTo ( org . apache . qpid . proton . amqp . UnsignedInteger . valueOf ( JmsDefaultPrefetchPolicy . DEFAULT_QUEUE_BROWSER_PREFETCH ) ) ) ; testPeer . expectDetach ( true , true , true ) ; javax . jms . QueueBrowser browser = session . createBrowser ( queue ) ; java . util . Enumeration < ? > queueView = browser . getEnumeration ( ) ; org . junit . Assert . assertNotNull ( queueView ) ; try { queueView . nextElement ( ) ; org . junit . Assert . fail ( "Should<sp>have<sp>thrown<sp>an<sp>exception<sp>due<sp>to<sp>there<sp>being<sp>no<sp>more<sp>elements" ) ; } }
public class aTest{ @Test public void testAnnotationAbstract ( ) { try { org . eclipse . xtend2 . lib . StringConcatenation _builder = new org . eclipse . xtend2 . lib . StringConcatenation ( ) ; _builder . append ( "annotation<sp>Foo<sp>{" ) ; _builder . newLine ( ) ; _builder . append ( "}" ) ; _builder . newLine ( ) ; final org . eclipse . xtext . common . types . JvmAnnotationType inferred = this . _iXtendJvmAssociations . getInferredAnnotationType ( this . annotationType ( _builder . toString ( ) ) ) ; org . junit . Assert . assertTrue ( inferred . isAbstract ( ) ) ; } }
public class aTest{ @Test public void testGetProjectsByLanguage ( ) { try { java . util . List < br . ufpe . cin . groundhog . Project > projects = searchGitHub . getAllProjectsByLanguage ( "java" ) ; org . junit . Assert . assertNotNull ( projects ) ; } }
public class aTest{ @Test public void testReadLongs ( ) { org . apache . activemq . artemis . core . client . impl . LargeMessageControllerImpl buffer = createBufferWithLongs ( 3 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 ) ; for ( int i = 1 ; i <= 15 ; i ++ ) { org . junit . Assert . assertEquals ( i , buffer . readLong ( ) ) ; } }
public class aTest{ @Test public void testXPathHelperWithXmlFile ( ) { try { java . lang . String xmlString = getFileContentsAsString ( ( ( ddf . catalog . impl . XPathHelperTest . TEST_DATA_PATH ) + ( ddf . catalog . impl . XPathHelperTest . INPUT_FILE ) ) ) ; ddf . util . XPathHelper xHelper = new ddf . util . XPathHelper ( xmlString ) ; org . w3c . dom . NodeList nodeList = ( ( org . w3c . dom . NodeList ) ( xHelper . evaluate ( ddf . catalog . impl . XPathHelperTest . XPATH_EXPRESSION , XPathConstants . NODESET , new ddf . catalog . impl . MockNamespaceResolver ( ) ) ) ) ; ddf . catalog . impl . XPathHelperTest . LOGGER . debug ( "testXPathHelper_WithXmlFile()<sp>-<sp>nodeList<sp>length<sp>=<sp>{}" , nodeList . getLength ( ) ) ; org . junit . Assert . assertEquals ( 6 , nodeList . getLength ( ) ) ; } }
public class aTest{ @Test public void testReplaySubjectEmissionSubscriptionRace ( ) { io . reactivex . Scheduler s = io . reactivex . schedulers . Schedulers . io ( ) ; io . reactivex . Scheduler . Worker worker = io . reactivex . schedulers . Schedulers . io ( ) . createWorker ( ) ; try { for ( int i = 0 ; i < 50000 ; i ++ ) { if ( ( i % 1000 ) == 0 ) { System . out . println ( i ) ; } final com . jakewharton . rxrelay2 . ReplayRelay < java . lang . Object > rs = com . jakewharton . rxrelay2 . ReplayRelay . createWithSize ( 2 ) ; final java . util . concurrent . CountDownLatch finish = new java . util . concurrent . CountDownLatch ( 1 ) ; final java . util . concurrent . CountDownLatch start = new java . util . concurrent . CountDownLatch ( 1 ) ; worker . schedule ( new java . lang . Runnable ( ) { @ com . jakewharton . rxrelay2 . Override public void run ( ) { try { start . await ( ) ; } catch ( java . lang . Exception e1 ) { e1 . printStackTrace ( ) ; } rs . accept ( 1 ) ; } } ) ; final java . util . concurrent . atomic . AtomicReference < java . lang . Object > o = new java . util . concurrent . atomic . AtomicReference < java . lang . Object > ( ) ; rs . subscribeOn ( s ) . observeOn ( io . reactivex . schedulers . Schedulers . io ( ) ) . subscribe ( new io . reactivex . observers . DefaultObserver < java . lang . Object > ( ) { @ com . jakewharton . rxrelay2 . Override protected void onStart ( ) { super . onStart ( ) ; } @ com . jakewharton . rxrelay2 . Override public void onComplete ( ) { o . set ( ( - 1 ) ) ; finish . countDown ( ) ; } @ com . jakewharton . rxrelay2 . Override public void onError ( java . lang . Throwable e ) { o . set ( e ) ; finish . countDown ( ) ; } @ com . jakewharton . rxrelay2 . Override public void onNext ( java . lang . Object t ) { o . set ( t ) ; finish . countDown ( ) ; } } ) ; start . countDown ( ) ; if ( ! ( finish . await ( 5 , TimeUnit . SECONDS ) ) ) { System . out . println ( o . get ( ) ) ; System . out . println ( rs . hasObservers ( ) ) ; org . junit . Assert . fail ( ( "Timeout<sp>@<sp>" + i ) ) ; break ; } else { org . junit . Assert . assertEquals ( 1 , o . get ( ) ) ; } } } }
public class aTest{ @Test public void testGenerateKeySet ( ) { for ( de . rub . nds . tlsattacker . core . constants . CipherSuite suite : de . rub . nds . tlsattacker . core . constants . CipherSuite . getImplemented ( ) ) { for ( de . rub . nds . tlsattacker . core . constants . ProtocolVersion version : de . rub . nds . tlsattacker . core . constants . ProtocolVersion . values ( ) ) { try { if ( ( version == ( de . rub . nds . tlsattacker . core . constants . ProtocolVersion . SSL2 ) ) || ( version == ( de . rub . nds . tlsattacker . core . constants . ProtocolVersion . SSL3 ) ) ) { continue ; } if ( ( version . isTLS13 ( ) ) != ( suite . isTLS13 ( ) ) ) { continue ; } de . rub . nds . tlsattacker . core . state . TlsContext context = new de . rub . nds . tlsattacker . core . state . TlsContext ( ) ; context . setSelectedCipherSuite ( suite ) ; context . setSelectedProtocolVersion ( version ) ; org . junit . Assert . assertNotNull ( de . rub . nds . tlsattacker . core . record . cipher . cryptohelper . KeySetGenerator . generateKeySet ( context ) ) ; } }
public class aTest{ @Test public void testBuildFilePathRootServicePathNoEncoding ( ) { System . out . println ( ( ( ( getTestTraceHead ( "someId=someType" 2 ) ) + "--------<sp>When<sp>no<sp>encoding<sp>and<sp>when<sp>a<sp>root<sp>service-path<sp>is<sp>notified/defaulted<sp>the<sp>HDFS<sp>file<sp>path<sp>is<sp>" ) + "the<sp>encoding<sp>of<sp><service>/<entity>/<entity>.txt" ) ) ; java . lang . String backendImpl = null ; java . lang . String backendMaxConns = null ; java . lang . String backendMaxConnsPerRoute = null ; java . lang . String batchSize = null ; java . lang . String batchTime = null ; java . lang . String batchTTL = null ; java . lang . String csvSeparator = null ; java . lang . String dataModel = null ; java . lang . String enableEncoding = "someId=someType" 6 ; java . lang . String enableGrouping = null ; java . lang . String enableLowercase = null ; java . lang . String fileFormat = null ; java . lang . String host = null ; java . lang . String password = "mypassword" ; java . lang . String port = null ; java . lang . String username = "someId=someType" 1 ; java . lang . String hive = "someId=someType" 6 ; java . lang . String krb5 = "someId=someType" 6 ; java . lang . String token = "someId=someType" 3 ; java . lang . String serviceAsNamespace = null ; com . telefonica . iot . cygnus . sinks . NGSIHDFSSink sink = new com . telefonica . iot . cygnus . sinks . NGSIHDFSSink ( ) ; sink . configure ( createContext ( backendImpl , backendMaxConns , backendMaxConnsPerRoute , batchSize , batchTime , batchTTL , csvSeparator , dataModel , enableEncoding , enableGrouping , enableLowercase , fileFormat , host , password , port , username , hive , krb5 , token , serviceAsNamespace ) ) ; java . lang . String service = "someService" ; java . lang . String servicePath = "someId=someType" 4 ; java . lang . String entity = "someId=someType" ; try { java . lang . String builtTableName = sink . buildFilePath ( service , servicePath , entity ) ; java . lang . String expecetedTableName = "someService/someId_someType/someId_someType.txt" ; try { org . junit . Assert . assertEquals ( expecetedTableName , builtTableName ) ; System . out . println ( ( ( ( ( ( getTestTraceHead ( "someId=someType" 2 ) ) + "-<sp>OK<sp>-<sp>'" ) + builtTableName ) + "someId=someType" 5 ) + "<service>/<entity>/<entity>.txt" ) ) ; } }
public class aTest{ @Test public void testRejectZero ( ) { int [ ] numChunk = new int [ ] { 0 , 1 } ; try { new org . apache . tez . runtime . library . cartesianproduct . CartesianProductCombination ( numChunk ) ; org . junit . Assert . assertTrue ( false ) ; } }
public class aTest{ @Test public void listViaMBean ( ) { javax . management . remote . JMXConnector connector = null ; try { connector = this . getJMXConnector ( ) ; javax . management . MBeanServerConnection connection = connector . getMBeanServerConnection ( ) ; javax . management . ObjectName name = new javax . management . ObjectName ( "org.apache.karaf:type=feature,name=root" ) ; javax . management . openmbean . TabularData features = ( ( javax . management . openmbean . TabularData ) ( connection . getAttribute ( name , "Features" ) ) ) ; org . junit . Assert . assertTrue ( ( ( features . size ( ) ) > 0 ) ) ; } }
public class aTest{ @Test public void parsesThreeInboundCookiesInTwoHeaders ( ) { webServer . add ( new org . webbitserver . HttpHandler ( ) { @ org . webbitserver . handler . Override public void handleHttpRequest ( org . webbitserver . HttpRequest request , org . webbitserver . HttpResponse response , org . webbitserver . HttpControl control ) throws org . webbitserver . handler . Exception { java . lang . String body = "Your<sp>cookies:" ; java . util . List < java . net . HttpCookie > cookies = sort ( request . cookies ( ) ) ; for ( java . net . HttpCookie cookie : cookies ) { body += ( ( "<sp>" + ( cookie . getName ( ) ) ) + "=" ) + ( cookie . getValue ( ) ) ; } response . header ( "a" 0 , body . length ( ) ) . content ( body ) . end ( ) ; } } ) . start ( ) . get ( ) ; java . net . URLConnection urlConnection = httpGet ( webServer , "a" 1 ) ; urlConnection . addRequestProperty ( "Cookie" , new java . net . HttpCookie ( "a" , "b" ) . toString ( ) ) ; urlConnection . addRequestProperty ( "Cookie" , ( ( ( new java . net . HttpCookie ( "c" , "d" ) . toString ( ) ) + ";<sp>" ) + ( new java . net . HttpCookie ( "e" , "a" 2 ) . toString ( ) ) ) ) ; org . junit . Assert . assertEquals ( "a" 3 , contents ( urlConnection ) ) ; } }
public class aTest{ @Test public void removeFeatures2_RemoveFeaturesByIndexSet_Success ( ) { java . lang . Integer [ ] indicesToRemove = new edu . drexel . psal . jstylo . generics . test . Integer [ ] { 1 } ; java . util . Map < java . lang . Integer , java . lang . String > expectedNewFeatures = new java . util . HashMap ( ) ; expectedNewFeatures . put ( 0 , "Feature0" ) ; expectedNewFeatures . put ( 1 , "Author_1" 2 ) ; testDataMap . initAuthor ( "Author_1" ) ; testDataMap . initAuthor ( "Author_2" ) ; java . util . concurrent . ConcurrentHashMap < java . lang . Integer , edu . drexel . psal . jstylo . generics . FeatureData > testDataValues = new java . util . concurrent . ConcurrentHashMap ( ) ; edu . drexel . psal . jstylo . generics . FeatureData testFeatureData = new edu . drexel . psal . jstylo . generics . FeatureData ( "Feature1" , "type1" , 3 ) ; testDataValues . put ( 0 , testFeatureData ) ; testDataValues . put ( 1 , testFeatureData ) ; testDataValues . put ( 2 , testFeatureData ) ; java . util . Map < java . lang . String , java . lang . Integer > testNorms = new java . util . HashMap ( ) ; testNorms . put ( "String0" , 0 ) ; testNorms . put ( "String1" , 1 ) ; testNorms . put ( "String2" , 2 ) ; edu . drexel . psal . jstylo . generics . DocumentData testDocData = new edu . drexel . psal . jstylo . generics . DocumentData ( testNorms , testDataValues ) ; System . out . println ( testDocData . getDataValues ( ) . toString ( ) ) ; testDataMap . addDocumentData ( "Author_1" , "Author_1" 0 , testDocData ) ; testDataMap . addDocumentData ( "Author_1" , "Author_1" 1 , testDocData ) ; testDataMap . addDocumentData ( "Author_2" , "Author_2_Doc_1" , testDocData ) ; testDataMap . addDocumentData ( "Author_2" , "Author_2_Doc_2" , testDocData ) ; testDataMap . addDocumentData ( "Author_2" , "Author_1" 3 , testDocData ) ; testDataMap . removeFeatures ( indicesToRemove ) ; org . junit . Assert . assertEquals ( expectedNewFeatures , testDataMap . getFeatures ( ) ) ; } }
public class aTest{ @Test public void getReservedBlockTest ( ) { org . araqne . logstorage . file . LogFileV3o srclogfile = null ; org . araqne . logstorage . file . LogFileV3o logfile = null ; try { srclogfile = new org . araqne . logstorage . file . LogFileV3o ( org . araqne . logstorage . file . LogFileV3oTest . indexFile , org . araqne . logstorage . file . LogFileV3oTest . dataFile ) ; logfile = new org . araqne . logstorage . file . LogFileV3o ( org . araqne . logstorage . file . LogFileV3oTest . holeIndexFile , org . araqne . logstorage . file . LogFileV3oTest . holeDataFile ) ; org . araqne . logstorage . file . List < org . araqne . logstorage . file . IndexBlockV3Header > srcBlocks = org . araqne . logstorage . file . LogFileV3oTest . toList ( srclogfile . getIndexBlocks ( ) ) ; org . araqne . logstorage . file . List < org . araqne . logstorage . file . IndexBlockV3Header > testBlocks = org . araqne . logstorage . file . LogFileV3oTest . toList ( logfile . getIndexBlocks ( ) ) ; logfile . reserveBlocks ( srcBlocks . subList ( 4 , srcBlocks . size ( ) ) ) ; boolean [ ] reserved = new boolean [ ] { false , false , true , false , true } ; int i = 0 ; for ( org . araqne . logstorage . file . IndexBlockV3Header block : testBlocks ) { org . junit . Assert . assertEquals ( reserved [ ( i ++ ) ] , block . isReserved ( ) ) ; } } }
public class aTest{ @Test public void testSwitchPopupToAnotherPresenter1 ( com . gwtplatform . mvp . client . PresenterWidgetTest$PresenterWidgetA , com . gwtplatform . mvp . client . PresenterWidgetTest$PresenterWidgetB , com . gwtplatform . mvp . client . PresenterWidgetTest$PresenterWidgetPopupC ) { presenterWidgetA . internalReveal ( ) ; presenterWidgetB . internalReveal ( ) ; presenterWidgetA . addToPopupSlot ( popupContentC ) ; presenterWidgetB . addToPopupSlot ( popupContentC ) ; presenterWidgetB . internalHide ( ) ; org . junit . Assert . assertFalse ( popupContentC . isVisible ( ) ) ; } }
public class aTest{ @Test public void testTemporaryQueue ( ) { javax . jms . ConnectionFactory connectionFactory = getConnectionFactory ( "a" , "a" ) ; java . lang . String message = "blah" ; java . lang . String messageRecieved = sendAndReceiveText ( connectionFactory , "clientId" , message , ( s ) -> s . createTemporaryQueue ( ) , ( d , s ) -> s . createConsumer ( d ) ) ; org . junit . Assert . assertEquals ( message , messageRecieved ) ; connectionFactory = getConnectionFactory ( "c" , "c" ) ; try { sendAndReceiveText ( connectionFactory , "clientId" , message , ( s ) -> s . createTemporaryQueue ( ) , ( d , s ) -> s . createConsumer ( d ) ) ; org . junit . Assert . fail ( "Security<sp>exception<sp>expected,<sp>but<sp>did<sp>not<sp>occur,<sp>excepetion<sp>expected<sp>as<sp>not<sp>permissioned<sp>to<sp>create<sp>a<sp>temporary<sp>queue" ) ; } }
public class aTest{ @Test public void nullProperty ( ) { javax . jcr . Node parentNode = getNode ( org . apache . jackrabbit . oak . jcr . RepositoryTest . TEST_PATH ) ; parentNode . setProperty ( "newProperty" , "some<sp>value" ) ; parentNode . getSession ( ) . save ( ) ; javax . jcr . Session session2 = createAdminSession ( ) ; try { session2 . getProperty ( ( ( org . apache . jackrabbit . oak . jcr . RepositoryTest . TEST_PATH ) + "/newProperty" ) ) . setValue ( ( ( java . lang . String ) ( null ) ) ) ; session2 . save ( ) ; } finally { session2 . logout ( ) ; } javax . jcr . Session session3 = createAnonymousSession ( ) ; try { org . junit . Assert . assertFalse ( session3 . propertyExists ( ( ( org . apache . jackrabbit . oak . jcr . RepositoryTest . TEST_PATH ) + "/newProperty" ) ) ) ; } }
public class aTest{ @Test public void testOverMaxClauses ( ) { com . stratio . cassandra . lucene . schema . Schema schema = schema ( ) . mapper ( "value2" 2 , stringMapper ( ) ) . mapper ( "name2" , stringMapper ( ) ) . mapper ( "name3" , stringMapper ( ) ) . mapper ( "name4" , stringMapper ( ) ) . mapper ( "name5" , stringMapper ( ) ) . mapper ( "name6" , stringMapper ( ) ) . build ( ) ; com . stratio . cassandra . lucene . search . condition . BooleanCondition condition = bool ( ) . maxClauses ( 5 ) . must ( match ( "value2" 2 , "value2" 1 ) ) . must ( match ( "name2" , "value2" ) ) . must ( match ( "name3" , "value2" 0 ) ) . must ( match ( "name4" , "value4" ) ) . must ( match ( "name5" , "value5" ) ) . must ( match ( "name6" , "value6" ) ) . build ( ) ; try { condition . doQuery ( schema ) ; } catch ( com . stratio . cassandra . lucene . search . condition . org . apache . lucene e ) { org . junit . Assert . assertTrue ( true ) ; return ; } }
public class aTest{ @Test public void testOutOfBoundCols ( ) { java . sql . Statement stmt = org . apache . hive . jdbc . TestJdbcDriver2 . con . createStatement ( ) ; java . sql . ResultSet res = stmt . executeQuery ( ( "select<sp>*<sp>from<sp>" + ( org . apache . hive . jdbc . TestJdbcDriver2 . tableName ) ) ) ; org . junit . Assert . assertTrue ( res . next ( ) ) ; try { res . getInt ( 200 ) ; } }
public class aTest{ @Test public void testRCFileStorage ( ) { for ( java . lang . String line : java . lang . String . format ( ( "DEFINE<sp>b64ToTuple<sp>%s(\'%s\');\n" + ( ( ( "A<sp>=<sp>load<sp>\'%s\'<sp>as<sp>(line);\n" + "A<sp>=<sp>foreach<sp>A<sp>generate<sp>b64ToTuple(line)<sp>as<sp>t;\n" ) + "A<sp>=<sp>foreach<sp>A<sp>generate<sp>FLATTEN(t);\n" ) + "STORE<sp>A<sp>into<sp>\'%s\'<sp>using<sp>%s(\'%s\');\n" ) ) , com . twitter . elephantbird . pig . load . TestRCFileProtobufStorage . B64ToTuple . class . getName ( ) , com . twitter . data . proto . tutorial . AddressBookProtos . Person . class . getName ( ) , inputDir . toURI ( ) . toString ( ) , rcfile_in . toURI ( ) . toString ( ) , com . twitter . elephantbird . pig . store . RCFileProtobufPigStorage . class . getName ( ) , com . twitter . data . proto . tutorial . AddressBookProtos . Person . class . getName ( ) ) . split ( "\n" ) ) { pigServer . registerQuery ( ( line + "\n" ) ) ; } com . twitter . elephantbird . mapreduce . io . ProtobufWritable < com . twitter . data . proto . tutorial . AddressBookProtos . Person > personWritable = com . twitter . elephantbird . mapreduce . io . ProtobufWritable . newInstance ( com . twitter . data . proto . tutorial . AddressBookProtos . Person . class ) ; org . apache . hadoop . mapreduce . RecordWriter < org . apache . hadoop . io . Writable , org . apache . hadoop . io . Writable > protoWriter = com . twitter . elephantbird . pig . load . TestRCFileProtobufStorage . createProtoWriter ( com . twitter . data . proto . tutorial . AddressBookProtos . Person . class , new java . io . File ( rcfile_in , "persons_with_unset_fields.rc" ) ) ; for ( com . twitter . data . proto . tutorial . AddressBookProtos . Person person : records ) { personWritable . set ( person ) ; protoWriter . write ( null , personWritable ) ; } protoWriter . close ( null ) ; com . twitter . elephantbird . mapreduce . io . ProtobufWritable < com . twitter . data . proto . tutorial . AddressBookProtos . PersonWithoutEmail > pweWritable = com . twitter . elephantbird . mapreduce . io . ProtobufWritable . newInstance ( com . twitter . data . proto . tutorial . AddressBookProtos . PersonWithoutEmail . class ) ; protoWriter = com . twitter . elephantbird . pig . load . TestRCFileProtobufStorage . createProtoWriter ( com . twitter . data . proto . tutorial . AddressBookProtos . PersonWithoutEmail . class , new java . io . File ( rcfile_in , "persons_with_unknows.rc" ) ) ; for ( com . twitter . data . proto . tutorial . AddressBookProtos . Person person : records ) { pweWritable . set ( com . twitter . data . proto . tutorial . AddressBookProtos . PersonWithoutEmail . newBuilder ( ) . mergeFrom ( person . toByteArray ( ) ) . build ( ) ) ; protoWriter . write ( null , pweWritable ) ; } protoWriter . close ( null ) ; pigServer . registerQuery ( java . lang . String . format ( "A<sp>=<sp>load<sp>\'%s\'<sp>using<sp>%s(\'%s\');\n" , rcfile_in . toURI ( ) . toString ( ) , com . twitter . elephantbird . pig . load . RCFileProtobufPigLoader . class . getName ( ) , com . twitter . data . proto . tutorial . AddressBookProtos . Person . class . getName ( ) ) ) ; java . util . Iterator < org . apache . pig . data . Tuple > rows = pigServer . openIterator ( "A" ) ; for ( int i = 0 ; i < 3 ; i ++ ) { for ( com . twitter . data . proto . tutorial . AddressBookProtos . Person person : records ) { java . lang . String expected = com . twitter . elephantbird . pig . load . TestRCFileProtobufStorage . personToString ( person ) ; org . junit . Assert . assertEquals ( expected , rows . next ( ) . toString ( ) ) ; } } }
public class aTest{ @Test public void testWriteBeyondFileSize ( ) { final java . nio . channels . ReadableByteChannel channel = new org . apache . hc . core5 . http . ReadableByteChannelMock ( new java . lang . String [ ] { "a" } , java . nio . charset . StandardCharsets . US_ASCII ) ; final org . apache . hc . core5 . http . nio . SessionInputBuffer inbuf = new org . apache . hc . core5 . http . impl . nio . SessionInputBufferImpl ( 1024 , 256 , 0 , java . nio . charset . StandardCharsets . US_ASCII ) ; final org . apache . hc . core5 . http . impl . BasicHttpTransportMetrics metrics = new org . apache . hc . core5 . http . impl . BasicHttpTransportMetrics ( ) ; final org . apache . hc . core5 . http . impl . nio . IdentityDecoder decoder = new org . apache . hc . core5 . http . impl . nio . IdentityDecoder ( channel , inbuf , metrics ) ; createTempFile ( ) ; final java . io . RandomAccessFile testfile = new java . io . RandomAccessFile ( this . tmpfile , "rw" ) ; try { org . junit . Assert . assertEquals ( 0 , testfile . length ( ) ) ; final java . nio . channels . FileChannel fchannel = testfile . getChannel ( ) ; try { decoder . transfer ( fchannel , 5 , 10 ) ; org . junit . Assert . fail ( "expected<sp>IOException" ) ; } }
public class aTest{ @Test public void testFileSystemImport ( ) { java . io . File file = null ; java . io . File tempfile = null ; try { java . lang . String basename = "base" ; java . util . Properties p = new java . util . Properties ( ) ; p . setProperty ( "key" , "value" ) ; tempfile = java . io . File . createTempFile ( "messagesourcetext" , null ) ; if ( tempfile . exists ( ) ) { tempfile . delete ( ) ; tempfile . mkdir ( ) ; } file = new java . io . File ( tempfile , ( basename + ".properties" ) ) ; java . io . PrintStream stream = new java . io . PrintStream ( file ) ; p . store ( stream , ( "generated<sp>by<sp>" + ( this . getClass ( ) . getName ( ) ) ) ) ; stream . close ( ) ; org . synyx . messagesource . importer . Importer importer = new org . synyx . messagesource . importer . Importer ( tempfile , target ) ; importer . importMessages ( basename ) ; org . synyx . messagesource . Messages messages = target . getMessages ( basename ) ; java . lang . String imported = messages . getMessage ( null , "key" ) ; org . junit . Assert . assertThat ( imported , org . hamcrest . CoreMatchers . is ( "value" ) ) ; } }
public class aTest{ @Test public void masterDetailOneReport ( ) { java . io . InputStream inputStream = null ; parameters . put ( "customer" , 103 ) ; parameters . put ( "order" , 10123 ) ; if ( ( inputStream = runAndRenderSampleReport ( "samplereports/Reporting<sp>Feature<sp>Examples/Drill<sp>to<sp>Details/MasterDetailOneReport.rptdesign" , "xlsx" ) ) != null ) { try { org . junit . Assert . assertNotNull ( inputStream ) ; } }
public class aTest{ @Test public void test002 ( ) { java . io . InputStream is = null ; try { is = this . getStream ( "sepa/test-pain-parse-none.xml" ) ; org . kapott . hbci . sepa . PainVersion version = org . kapott . hbci . sepa . PainVersion . autodetect ( is ) ; org . junit . Assert . assertNull ( version ) ; } }
public class aTest{ @Test public void testNames ( ) { final org . apache . kafka . common . config . ConfigDef configDef = new org . apache . kafka . common . config . ConfigDef ( ) . define ( "a" , Type . STRING , Importance . LOW , "docs" ) . define ( "b" , Type . STRING , Importance . LOW , "docs" ) ; java . util . Set < java . lang . String > names = configDef . names ( ) ; org . junit . Assert . assertEquals ( new java . util . HashSet ( java . util . Arrays . asList ( "a" , "b" ) ) , names ) ; try { names . add ( "new" ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void testBidirectionalRef ( ) { try { org . eclipse . xtend2 . lib . StringConcatenation _builder = new org . eclipse . xtend2 . lib . StringConcatenation ( ) ; _builder . append ( "package<sp>foo<sp>class<sp>ClassA<sp>extends<sp>bar.ClassB<sp>{}" ) ; org . eclipse . xtext . xbase . lib . Pair < java . lang . String , java . lang . String > _mappedTo = org . eclipse . xtext . xbase . lib . Pair . < java . lang . String , java . lang . String > of ( "foo/ClassA.xtend" , _builder . toString ( ) ) ; org . eclipse . xtend2 . lib . StringConcatenation _builder_1 = new org . eclipse . xtend2 . lib . StringConcatenation ( ) ; _builder_1 . append ( "package<sp>bar<sp>class<sp>ClassB<sp>{<sp>public<sp>foo.ClassA<sp>myField<sp>}" ) ; org . eclipse . xtext . xbase . lib . Pair < java . lang . String , java . lang . String > _mappedTo_1 = org . eclipse . xtext . xbase . lib . Pair . < java . lang . String , java . lang . String > of ( "bar/ClassB.xtend" , _builder_1 . toString ( ) ) ; final org . eclipse . emf . ecore . resource . ResourceSet resourceSet = this . compiler . resourceSet ( _mappedTo , _mappedTo_1 ) ; final java . util . List < ? extends org . eclipse . emf . ecore . resource . Resource > resources = resourceSet . getResources ( ) ; java . util . ArrayList < org . eclipse . emf . ecore . resource . Resource > _arrayList = new java . util . ArrayList < org . eclipse . emf . ecore . resource . Resource > ( resources ) ; for ( final org . eclipse . emf . ecore . resource . Resource res : _arrayList ) { { final java . util . List < org . eclipse . xtext . validation . Issue > issues = this . validator . validate ( res , CheckMode . ALL , CancelIndicator . NullImpl ) ; org . junit . Assert . assertTrue ( issues . toString ( ) , issues . isEmpty ( ) ) ; } } } }
public class aTest{ @Test public void hasResourcePermissions_noPermissions_shouldFailAsAuthenticated ( ) { authenticateSystemResource ( ) ; final java . lang . String resourceClassName = generateResourceClass ( true , false ) ; final java . lang . String customPermissionName = generateResourceClassPermission ( resourceClassName ) ; final char [ ] password = generateUniquePassword ( ) ; final com . acciente . oacc . Resource accessorResource = generateAuthenticatableResource ( password ) ; final com . acciente . oacc . Resource accessedResource = accessControlContext . createResource ( resourceClassName , accessControlContext . getDomainNameByResource ( com . acciente . oacc . SYS_RESOURCE ) , com . acciente . oacc . PasswordCredentials . newInstance ( generateUniquePassword ( ) ) ) ; final java . util . Set < com . acciente . oacc . ResourcePermission > allResourcePermissions = accessControlContext . getEffectiveResourcePermissions ( accessorResource , accessedResource ) ; org . junit . Assert . assertThat ( allResourcePermissions . isEmpty ( ) , org . hamcrest . CoreMatchers . is ( true ) ) ; accessControlContext . authenticate ( accessorResource , com . acciente . oacc . PasswordCredentials . newInstance ( password ) ) ; if ( accessControlContext . hasResourcePermissions ( accessorResource , accessedResource , com . acciente . oacc . ResourcePermissions . getInstance ( customPermissionName ) ) ) { org . junit . Assert . fail ( "checking<sp>resource<sp>permission<sp>for<sp>domain<sp>when<sp>none<sp>has<sp>been<sp>granted<sp>should<sp>not<sp>have<sp>succeeded<sp>for<sp>authenticated<sp>resource" ) ; } }
public class aTest{ @Test public void testYear ( ) { org . apache . cayenne . exp . Expression exp = org . apache . cayenne . exp . ExpressionFactory . exp ( "year(dateColumn)<sp>=<sp>2015" ) ; try { long res = org . apache . cayenne . query . ObjectSelect . query ( org . apache . cayenne . testdo . date_time . DateTestEntity . class , exp ) . selectCount ( context ) ; org . junit . Assert . assertEquals ( 1 , res ) ; } }
public class aTest{ @Test public void toStringTest ( ) { org . xwiki . uiextension . UIExtensionManager m = mock ( org . xwiki . uiextension . UIExtensionManager . class ) ; org . xwiki . uiextension . UIExtension ex = mock ( org . xwiki . uiextension . UIExtension . class ) ; org . xwiki . uiextension . UIExtensionFilter filter = mock ( org . xwiki . uiextension . UIExtensionFilter . class , "sortByParameter" ) ; org . xwiki . uiextension . UIExtensionFilter realFilter = new org . xwiki . uiextension . internal . filter . SortByParameterFilter ( ) ; java . util . Map < java . lang . String , java . lang . String > params = new java . util . HashMap ( ) ; params . put ( "title" , "Patient<sp>information" ) ; when ( ex . getParameters ( ) ) . thenReturn ( params ) ; when ( ex . getId ( ) ) . thenReturn ( "org.phenotips.patientSheet.section.patient-info" ) ; org . phenotips . configuration . RecordSection s = new org . phenotips . configuration . spi . UIXRecordSection ( ex , m , filter ) ; java . util . List < org . xwiki . uiextension . UIExtension > fields = new java . util . LinkedList ( ) ; ex = mock ( org . xwiki . uiextension . UIExtension . class ) ; params = new java . util . HashMap ( ) ; params . put ( "title" , "enabled" 6 ) ; params . put ( "enabled" , "" ) ; params . put ( org . phenotips . configuration . spi . UIXRecordSectionTest . ORDER , "enabled" 0 ) ; when ( ex . getParameters ( ) ) . thenReturn ( params ) ; fields . add ( ex ) ; ex = mock ( org . xwiki . uiextension . UIExtension . class ) ; params = new java . util . HashMap ( ) ; params . put ( "title" , "enabled" 4 ) ; params . put ( "enabled" , "true" ) ; params . put ( org . phenotips . configuration . spi . UIXRecordSectionTest . ORDER , "1" ) ; when ( ex . getParameters ( ) ) . thenReturn ( params ) ; fields . add ( ex ) ; ex = mock ( org . xwiki . uiextension . UIExtension . class ) ; params = new java . util . HashMap ( ) ; params . put ( "title" , "enabled" 1 ) ; params . put ( "enabled" , "enabled" 5 ) ; params . put ( org . phenotips . configuration . spi . UIXRecordSectionTest . ORDER , "3" ) ; when ( ex . getParameters ( ) ) . thenReturn ( params ) ; fields . add ( ex ) ; ex = mock ( org . xwiki . uiextension . UIExtension . class ) ; params = new java . util . HashMap ( ) ; when ( ex . getParameters ( ) ) . thenReturn ( params ) ; params . put ( "title" , "enabled" 2 ) ; fields . add ( ex ) ; ex = mock ( org . xwiki . uiextension . UIExtension . class ) ; params = new java . util . HashMap ( ) ; params . put ( "title" , "enabled" 7 ) ; params . put ( org . phenotips . configuration . spi . UIXRecordSectionTest . ORDER , "4" ) ; when ( ex . getParameters ( ) ) . thenReturn ( params ) ; fields . add ( ex ) ; when ( m . get ( "org.phenotips.patientSheet.section.patient-info" ) ) . thenReturn ( fields ) ; java . util . List < org . xwiki . uiextension . UIExtension > sorted = realFilter . filter ( fields , org . phenotips . configuration . spi . UIXRecordSectionTest . ORDER ) ; when ( filter . filter ( fields , org . phenotips . configuration . spi . UIXRecordSectionTest . ORDER ) ) . thenReturn ( sorted ) ; org . junit . Assert . assertEquals ( "enabled" 3 , s . toString ( ) ) ; } }
public class aTest{ @Test public void testPartialDecodeIgnoresApplicationPropertiesByDefault ( ) { org . apache . qpid . proton . amqp . messaging . Header header = new org . apache . qpid . proton . amqp . messaging . Header ( ) ; header . setDurable ( true ) ; header . setPriority ( org . apache . qpid . proton . amqp . UnsignedByte . valueOf ( ( ( byte ) ( 6 ) ) ) ) ; io . netty . buffer . ByteBuf encodedBytes = io . netty . buffer . Unpooled . buffer ( 1024 ) ; org . apache . activemq . artemis . protocol . amqp . util . NettyWritable writable = new org . apache . activemq . artemis . protocol . amqp . util . NettyWritable ( encodedBytes ) ; org . apache . qpid . proton . codec . EncoderImpl encoder = org . apache . activemq . artemis . protocol . amqp . util . TLSEncode . getEncoder ( ) ; encoder . setByteBuffer ( writable ) ; encoder . writeObject ( header ) ; encodedBytes . writeByte ( EncodingCodes . DESCRIBED_TYPE_INDICATOR ) ; encodedBytes . writeByte ( EncodingCodes . SMALLULONG ) ; encodedBytes . writeByte ( org . apache . activemq . artemis . protocol . amqp . broker . AMQPMessageTest . APPLICATION_PROPERTIES_DESCRIPTOR . byteValue ( ) ) ; encodedBytes . writeByte ( EncodingCodes . MAP8 ) ; encodedBytes . writeByte ( 2 ) ; encodedBytes . writeByte ( 2 ) ; encodedBytes . writeByte ( 255 ) ; org . apache . qpid . proton . codec . ReadableBuffer readable = new org . apache . activemq . artemis . protocol . amqp . util . NettyReadable ( encodedBytes ) ; org . apache . activemq . artemis . protocol . amqp . broker . AMQPMessage message = null ; try { message = new org . apache . activemq . artemis . protocol . amqp . broker . AMQPMessage ( 0 , readable , null , null ) ; } catch ( java . lang . Exception decodeError ) { org . junit . Assert . fail ( ( "Should<sp>not<sp>have<sp>encountered<sp>an<sp>exception<sp>on<sp>partial<sp>decode:<sp>" + ( decodeError . getMessage ( ) ) ) ) ; } org . junit . Assert . assertTrue ( message . isDurable ( ) ) ; try { message . getStringProperty ( "test" ) ; org . junit . Assert . fail ( "Should<sp>have<sp>thrown<sp>an<sp>error<sp>when<sp>attempting<sp>to<sp>decode<sp>the<sp>ApplicationProperties<sp>which<sp>are<sp>malformed." ) ; } }
public class aTest{ @Test public void testParse ( ) { java . lang . String line = "count" 7 ; java . util . Map < java . lang . String , java . lang . Object > expected = new java . util . HashMap < java . lang . String , java . lang . Object > ( ) ; expected . put ( JuniperAttackLogPattern . SEVERITY_KEY , "Emergency" ) ; expected . put ( JuniperAttackLogPattern . ID_KEY , "count" 0 ) ; expected . put ( JuniperAttackLogPattern . RULE_KEY , "SYN<sp>flood" ) ; try { expected . put ( "src-ip" , java . net . InetAddress . getByName ( "1.1.1.1" ) ) ; expected . put ( "src-port" , 1111 ) ; expected . put ( "count" 5 , java . net . InetAddress . getByName ( "22.22.22.22" ) ) ; expected . put ( "count" 4 , 22222 ) ; expected . put ( "zone-name" , "count" 6 ) ; expected . put ( "interface-name" , "count" 1 ) ; expected . put ( "count" , 100 ) ; expected . put ( "category" , "count" 2 ) ; } catch ( java . net . UnknownHostException e ) { e . printStackTrace ( ) ; } java . util . Map < java . lang . String , java . lang . Object > actual = parser . parse ( line ) ; org . junit . Assert . assertEquals ( expected , actual ) ; } }
public class aTest{ @Test public void testAsyncErrorRethrownOnCheckpointAfterFlush ( ) { final org . apache . flink . streaming . connectors . kafka . FlinkKafkaProducerBaseTest . DummyFlinkKafkaProducer < java . lang . String > producer = new org . apache . flink . streaming . connectors . kafka . FlinkKafkaProducerBaseTest . DummyFlinkKafkaProducer ( org . apache . flink . streaming . connectors . kafka . testutils . FakeStandardProducerConfig . get ( ) , new org . apache . flink . streaming . connectors . kafka . internals . KeyedSerializationSchemaWrapper ( new org . apache . flink . api . common . serialization . SimpleStringSchema ( ) ) , null ) ; producer . setFlushOnCheckpoint ( true ) ; final org . apache . kafka . clients . producer . KafkaProducer < ? , ? > mockProducer = producer . getMockKafkaProducer ( ) ; final org . apache . flink . streaming . util . OneInputStreamOperatorTestHarness < java . lang . String , java . lang . Object > testHarness = new org . apache . flink . streaming . util . OneInputStreamOperatorTestHarness ( new org . apache . flink . streaming . api . operators . StreamSink ( producer ) ) ; testHarness . open ( ) ; testHarness . processElement ( new org . apache . flink . streaming . runtime . streamrecord . StreamRecord ( "msg-1" ) ) ; testHarness . processElement ( new org . apache . flink . streaming . runtime . streamrecord . StreamRecord ( "msg-2" ) ) ; testHarness . processElement ( new org . apache . flink . streaming . runtime . streamrecord . StreamRecord ( "msg-3" ) ) ; verify ( mockProducer , times ( 3 ) ) . send ( any ( org . apache . kafka . clients . producer . ProducerRecord . class ) , any ( org . apache . kafka . clients . producer . Callback . class ) ) ; producer . getPendingCallbacks ( ) . get ( 0 ) . onCompletion ( null , null ) ; org . apache . flink . core . testutils . CheckedThread snapshotThread = new org . apache . flink . core . testutils . CheckedThread ( ) { @ org . apache . flink . streaming . connectors . kafka . Override public void go ( ) throws org . apache . flink . streaming . connectors . kafka . Exception { testHarness . snapshot ( 123L , 123L ) ; } } ; snapshotThread . start ( ) ; producer . getPendingCallbacks ( ) . get ( 1 ) . onCompletion ( null , new java . lang . Exception ( "artificial<sp>async<sp>failure<sp>for<sp>2nd<sp>message" ) ) ; producer . getPendingCallbacks ( ) . get ( 2 ) . onCompletion ( null , null ) ; try { snapshotThread . sync ( ) ; } catch ( java . lang . Exception e ) { org . junit . Assert . assertTrue ( e . getCause ( ) . getMessage ( ) . contains ( "artificial<sp>async<sp>failure<sp>for<sp>2nd<sp>message" ) ) ; return ; } }
public class aTest{ @Test public void testPutAllMisc ( ) { com . sun . sgs . test . app . util . TestScalableHashMap . txnScheduler . runTask ( new com . sun . sgs . test . util . TestAbstractKernelRunnable ( ) { public void run ( ) throws com . sun . sgs . test . app . util . Exception { java . util . Map < java . lang . Integer , java . lang . Integer > control = new java . util . HashMap < java . lang . Integer , java . lang . Integer > ( ) ; for ( int i = 0 ; i < 32 ; i ++ ) { control . put ( i , i ) ; } java . util . Map < java . lang . Integer , java . lang . Integer > test = new com . sun . sgs . app . util . ScalableHashMap < java . lang . Integer , java . lang . Integer > ( ) ; test . putAll ( control ) ; org . junit . Assert . assertEquals ( control , test ) ; } } }
public class aTest{ @Test public void globalConfigRoundtrip ( ) { final com . cloudbees . plugins . credentials . CredentialsStore store = com . cloudbees . plugins . credentials . CredentialsProvider . lookupStores ( jenkins . getInstance ( ) ) . iterator ( ) . next ( ) ; org . jenkinsci . plugins . docker . commons . credentials . DockerServerCredentials dc = new org . jenkinsci . plugins . docker . commons . credentials . DockerServerCredentials ( SYSTEM , "credentialsId" , "volumesFromString" 7 , null , null , null ) ; store . addCredentials ( com . cloudbees . plugins . credentials . domains . Domain . global ( ) , dc ) ; com . cloudbees . plugins . credentials . common . UsernamePasswordCredentials rc = new com . cloudbees . plugins . credentials . impl . UsernamePasswordCredentialsImpl ( SYSTEM , "pullCredentialsId" , null , null , null ) ; store . addCredentials ( com . cloudbees . plugins . credentials . domains . Domain . global ( ) , rc ) ; final com . nirima . jenkins . plugins . docker . DockerTemplate template = new com . nirima . jenkins . plugins . docker . DockerTemplate ( new com . nirima . jenkins . plugins . docker . DockerTemplateBase ( "volumesFromString" 4 , "pullCredentialsId" , "dnsString" , "volumesFromString" 6 , "volumesFromString" 5 , "volumesString" , "volumesFromString" , "environmentString" , "volumesFromString" 1 , 128 , 256 , 42 , 102 , "volumesFromString" 2 , true , true , true , "macAddress" , "volumesFromString" 0 ) , new io . jenkins . docker . connector . DockerComputerAttachConnector ( "jenkins" ) , "labelString" , "remoteFs" , "volumesFromString" 3 ) ; template . setPullStrategy ( DockerImagePullStrategy . PULL_NEVER ) ; template . setMode ( Node . Mode . NORMAL ) ; template . setRemoveVolumes ( true ) ; template . setRetentionStrategy ( new com . nirima . jenkins . plugins . docker . strategy . DockerOnceRetentionStrategy ( 33 ) ) ; com . nirima . jenkins . plugins . docker . DockerCloud cloud = new com . nirima . jenkins . plugins . docker . DockerCloud ( "volumesFromString" 9 , new io . jenkins . docker . client . DockerAPI ( new org . jenkinsci . plugins . docker . commons . credentials . DockerServerEndpoint ( "volumesFromString" 8 , "credentialsId" ) ) , java . util . Collections . singletonList ( template ) ) ; jenkins . getInstance ( ) . clouds . replaceBy ( java . util . Collections . singleton ( cloud ) ) ; jenkins . configRoundtrip ( ) ; org . junit . Assert . assertEquals ( cloud , jenkins . getInstance ( ) . clouds . get ( 0 ) ) ; } }
public class aTest{ @Test public void testMap2 ( ) { fr . inria . corese . core . query . QueryProcess exec = fr . inria . corese . core . query . QueryProcess . create ( ) ; java . lang . String q = "function<sp>xt:main()<sp>{" + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "let<sp>(?m<sp>=<sp>xt:map())<sp>{" + "xt:set(?m,<sp>1,<sp>1)<sp>;" ) + "xt:set(?m,<sp>01,<sp>01)<sp>;" ) + "xt:set(?m,<sp>'test',<sp>'test')<sp>;" 3 ) + "xt:set(?m,<sp>1.0e0,<sp>1.0e0)<sp>;" ) + "xt:set(?m,<sp>'1'^^xsd:int,<sp>'1'^^xsd:int)<sp>;" ) + "xt:set(?m,<sp>1,<sp>1)<sp>;" ) + "xt:set(?m,<sp>01,<sp>01)<sp>;" ) + "xt:set(?m,<sp>true,<sp>true)<sp>;" ) + "xt:set(?m,<sp>st:test,<sp>st:test)<sp>;" ) + "xt:set(?m,<sp>'test',<sp>'test')<sp>;" ) + "xt:set(?m,<sp>'test',<sp>'test')<sp>;" 0 ) + "xt:set(?m,<sp>'test',<sp>'test')<sp>;" ) + "xt:set(?m,<sp>'test',<sp>'test')<sp>;" 2 ) + "return<sp>(?m)" ) + "xt:set(?m,<sp>'test',<sp>'test')<sp>;" 1 ) + "xt:set(?m,<sp>'test',<sp>'test')<sp>;" 1 ) ; fr . inria . corese . sparql . api . IDatatype dt = exec . eval ( q ) ; org . junit . Assert . assertEquals ( 10 , dt . size ( ) ) ; } }
public class aTest{ @Test public void testRemoveEntities_15 ( ) { try { java . io . StringReader reader = new java . io . StringReader ( "abc&#;" ) ; java . io . StringWriter writer = new java . io . StringWriter ( ) ; org . milyn . xml . XmlUtil . removeEntities ( reader , writer ) ; org . junit . Assert . assertEquals ( "abc&#;" , writer . toString ( ) ) ; } }
public class aTest{ @Test public void testNullMultiCondCaseStatement ( ) { java . lang . String query = ( "SELECT<sp>CASE<sp>WHEN<sp>entity_id<sp>=<sp>'000000000000000'<sp>THEN<sp>1<sp>WHEN<sp>entity_id<sp>=<sp>'000000000000001'<sp>THEN<sp>2<sp>END<sp>FROM<sp>" + ( tableName ) ) + "<sp>WHERE<sp>organization_id=?" ; java . lang . String url = getUrl ( ) ; java . util . Properties props = org . apache . phoenix . util . PropertiesUtil . deepCopy ( org . apache . phoenix . end2end . TEST_PROPERTIES ) ; java . sql . Connection conn = java . sql . DriverManager . getConnection ( url , props ) ; try { java . sql . PreparedStatement statement = conn . prepareStatement ( query ) ; statement . setString ( 1 , tenantId ) ; java . sql . ResultSet rs = statement . executeQuery ( ) ; java . sql . ResultSetMetaData rsm = rs . getMetaData ( ) ; org . junit . Assert . assertEquals ( ResultSetMetaData . columnNullable , rsm . isNullable ( 1 ) ) ; } }
public class aTest{ @Test public void testHighlighter ( ) { try { tokens = "{{<sp>some('function')<sp>}}" ; tokenizer = new com . dubture . twig . core . documentModel . parser . TwigTokenizer ( tokens . toCharArray ( ) ) ; textRegions = new java . util . Stack < org . eclipse . wst . sse . core . internal . provisional . text . ITextRegion > ( ) ; org . junit . Assert . assertTrue ( ( ( textRegions . size ( ) ) == 0 ) ) ; while ( ! ( tokenizer . isEOF ( ) ) ) { org . eclipse . wst . sse . core . internal . provisional . text . ITextRegion region = tokenizer . getNextToken ( ) ; textRegions . push ( region ) ; } } }
public class aTest{ @Test public void testTransientConversation ( ) { try { java . lang . System . setProperty ( OpenWebBeansConfiguration . APPLICATION_SUPPORTS_CONVERSATION , "true" ) ; startContainer ( org . apache . webbeans . test . contexts . conversation . ConversationScopedBean . class ) ; org . apache . webbeans . test . contexts . conversation . ConversationScopedBean instance = getInstance ( org . apache . webbeans . test . contexts . conversation . ConversationScopedBean . class ) ; instance . setValue ( "a" ) ; instance . begin ( ) ; ensureSerialisableContext ( ) ; restartContext ( javax . enterprise . context . RequestScoped . class ) ; instance . end ( ) ; org . junit . Assert . assertNull ( instance . getValue ( ) ) ; } }
public class aTest{ @Test public void sendPingTCPDiscover ( ) { net . tomp2p . p2p . Peer sender = null ; net . tomp2p . connection . ChannelCreator cc = null ; try { net . tomp2p . peers . PeerAddress pa = net . tomp2p . peers . PeerAddress . create ( Number160 . ZERO , java . net . Inet4Address . getByName ( net . tomp2p . rpc . TestRealPing . IP ) , net . tomp2p . rpc . TestRealPing . PORT , net . tomp2p . rpc . TestRealPing . PORT , ( ( net . tomp2p . rpc . TestRealPing . PORT ) + 1 ) ) ; sender = new net . tomp2p . p2p . PeerBuilder ( new net . tomp2p . peers . Number160 ( "0x9876" ) ) . ports ( net . tomp2p . rpc . TestRealPing . PORT ) . enableMaintenance ( false ) . start ( ) ; net . tomp2p . rpc . PingRPC handshake = new net . tomp2p . rpc . PingRPC ( sender . peerBean ( ) , sender . connectionBean ( ) ) ; net . tomp2p . futures . FutureChannelCreator fcc = sender . connectionBean ( ) . reservation ( ) . create ( 0 , 1 ) ; fcc . awaitUninterruptibly ( ) ; cc = fcc . channelCreator ( ) ; net . tomp2p . futures . FutureResponse fr = handshake . pingTCPDiscover ( pa , cc , new net . tomp2p . connection . DefaultConnectionConfiguration ( ) ) ; fr . awaitUninterruptibly ( ) ; org . junit . Assert . assertEquals ( true , fr . isSuccess ( ) ) ; java . lang . Thread . sleep ( net . tomp2p . rpc . TestRealPing . WAIT ) ; } }
public class aTest{ @Test public void testTimeStampAddUnescaped ( ) { java . lang . String sqlText = ( "SELECT<sp>TIMESTAMPADD(SQL_TSI_MONTH,<sp>1,<sp>col3),<sp>col3<sp>from<sp>" + ( com . splicemachine . derby . utils . SpliceDateFunctionsIT . tableWatcherG ) ) + "<sp>order<sp>by<sp>col3" ; try ( com . splicemachine . derby . utils . ResultSet rs = methodWatcher . executeQuery ( sqlText ) ) { java . lang . String expected = "<sp>order<sp>by<sp>col3" 0 + ( ( ( ( ( ( ( ( "----------------------------------------------\n" + "<sp>order<sp>by<sp>col3" 2 ) + "2012-02-01<sp>00:00:00.0<sp>|2012-01-01<sp>00:00:00.0<sp>|\n" ) + "2012-02-29<sp>00:00:00.0<sp>|2012-01-29<sp>00:00:00.0<sp>|\n" ) + "2012-11-01<sp>00:00:00.0<sp>|2012-10-01<sp>00:00:00.0<sp>|\n" ) + "2013-01-01<sp>00:00:00.0<sp>|2012-12-01<sp>00:00:00.0<sp>|\n" ) + "<sp>order<sp>by<sp>col3" 1 ) + "2013-01-31<sp>00:00:00.0<sp>|2012-12-31<sp>00:00:00.0<sp>|\n" ) + "2013-01-31<sp>20:00:00.0<sp>|2012-12-31<sp>20:00:00.0<sp>|" ) ; org . junit . Assert . assertEquals ( ( ( "\n" + sqlText ) + "\n" ) , expected , TestUtils . FormattedResult . ResultFactory . toStringUnsorted ( rs ) ) ; } } }
public class aTest{ @Test public void testMatchExactlyActionMatchErr ( ) { queriesString = org . apache . commons . lang . StringUtils . join ( new java . lang . String [ ] { "type=node" , "enabled=true" , "type=node" 0 } , "&" ) ; target = new org . o3project . odenos . core . component . network . flow . query . BasicFlowQuery ( queriesString ) ; target . parse ( ) ; java . util . List < org . o3project . odenos . core . component . network . flow . basic . BasicFlowMatch > matches = null ; java . util . List < java . lang . String > path = null ; java . util . Map < java . lang . String , java . util . List < org . o3project . odenos . core . component . network . flow . basic . FlowAction > > edgeAction = null ; java . util . Map < java . lang . String , java . lang . String > flowAttributes = new java . util . HashMap < java . lang . String , java . lang . String > ( ) ; org . o3project . odenos . core . component . network . flow . basic . BasicFlow flow = new org . o3project . odenos . core . component . network . flow . basic . BasicFlow ( "1" , "" , "" , true , "" , "established" , matches , path , edgeAction , flowAttributes ) ; @ org . o3project . odenos . core . component . network . flow . query . SuppressWarnings ( "serial" ) java . util . List < org . o3project . odenos . core . component . network . flow . basic . FlowAction > actions = new java . util . ArrayList < org . o3project . odenos . core . component . network . flow . basic . FlowAction > ( ) { { add ( new org . o3project . odenos . core . component . network . flow . basic . FlowActionOutput ( ) { { output = "port1" ; } } ) ; add ( new org . o3project . odenos . core . component . network . flow . basic . FlowActionOutput ( ) { { output = "port2" ; } } ) ; } } ; flow . addEdgeAction ( "type=node" 1 , actions . get ( 0 ) ) ; flow . addEdgeAction ( "node02" , actions . get ( 1 ) ) ; org . junit . Assert . assertThat ( target . matchExactly ( flow ) , org . hamcrest . CoreMatchers . is ( false ) ) ; } }
public class aTest{ @Test public void testSerialize ( ) { dbm . wipeDatabase ( ) ; java . util . Random r = new java . util . Random ( 1L ) ; java . util . Set < com . github . mistertea . zombiedb . thrift . TestThrift > tts = new java . util . HashSet < com . github . mistertea . zombiedb . thrift . TestThrift > ( ) ; for ( int test = 0 ; test < 10000 ; test ++ ) { com . github . mistertea . zombiedb . thrift . TestThrift tt = new com . github . mistertea . zombiedb . thrift . TestThrift ( null , r . nextInt ( ) , r . nextLong ( ) , r . nextBoolean ( ) , ( ( byte ) ( r . nextInt ( ) ) ) , ( ( short ) ( r . nextInt ( ) ) ) , r . nextDouble ( ) , generateString ( r , 16 ) , "abc" ) ; tts . add ( tt ) ; dbm . create ( tt ) ; dbm . commit ( ) ; } dbm . dump ( com . github . mistertea . zombiedb . thrift . TestThrift . class , new java . io . File ( "." ) ) ; dbm . wipeDatabase ( ) ; dbm . load ( com . github . mistertea . zombiedb . thrift . TestThrift . class , new java . io . File ( "." ) ) ; for ( com . github . mistertea . zombiedb . thrift . TestThrift tt : tts ) { com . github . mistertea . zombiedb . thrift . TestThrift tt2 = dbm . get ( com . github . mistertea . zombiedb . thrift . TestThrift . class , tt . id ) ; org . junit . Assert . assertEquals ( tt , tt2 ) ; } }
public class aTest{ @Test public void testCloseEmptiesPool ( ) { testWithHandler ( new com . sun . mail . imap . IMAPStoreTest . IMAPTest ( ) { @ com . sun . mail . imap . Override public void init ( java . util . Properties props ) { props . setProperty ( "mail.imap.connectionpoolsize" , "2" ) ; } @ com . sun . mail . imap . Override public void test ( javax . mail . Store store , com . sun . mail . test . TestServer server ) throws java . io . IOException , javax . mail . MessagingException { store . connect ( "test" , "test" ) ; javax . mail . Folder test = store . getFolder ( "INBOX" ) ; test . open ( Folder . READ_ONLY ) ; javax . mail . Folder test2 = store . getFolder ( "INBOX" ) ; test2 . open ( Folder . READ_ONLY ) ; test . close ( false ) ; test2 . close ( false ) ; store . close ( ) ; org . junit . Assert . assertEquals ( 2 , server . clientCount ( ) ) ; server . waitForClients ( 2 ) ; } } }
public class aTest{ @Test public void testConcurrentIterationAndMutation ( ) { final java . util . concurrent . ConcurrentMap < org . terracotta . offheapstore . util . Generator . SpecialInteger , org . terracotta . offheapstore . util . Generator . SpecialInteger > m = createMap ( generator ) ; final java . util . concurrent . atomic . AtomicBoolean stopped = new java . util . concurrent . atomic . AtomicBoolean ( false ) ; java . lang . Thread mutator = new java . lang . Thread ( ( ) -> { int counter = 0 ; java . util . Random rndm = new java . util . Random ( ) ; while ( ! ( stopped . get ( ) ) ) { if ( ( ++ counter ) == 25 ) { counter = 0 ; java . lang . Thread . yield ( ) ; } int v = rndm . nextInt ( 8192 ) ; if ( rndm . nextBoolean ( ) ) { m . remove ( org . terracotta . offheapstore . generator . generate ( v ) ) ; } else { m . put ( org . terracotta . offheapstore . generator . generate ( v ) , org . terracotta . offheapstore . generator . generate ( v ) ) ; } } } ) ; mutator . start ( ) ; boolean interrupted = java . lang . Thread . interrupted ( ) ; try { for ( int i = 0 ; i < 10 ; i ++ ) { int checked = 0 ; for ( Map . Entry < org . terracotta . offheapstore . util . Generator . SpecialInteger , org . terracotta . offheapstore . util . Generator . SpecialInteger > e : m . entrySet ( ) ) { org . junit . Assert . assertEquals ( e . getKey ( ) , e . getValue ( ) ) ; checked ++ ; } }
public class aTest{ @Test public void valuesSetFromProperties ( ) { try { java . lang . System . setProperty ( MetastoreConf . ConfVars . STR_TEST_ENTRY . getVarname ( ) , "from-properties" ) ; conf = org . apache . hadoop . hive . metastore . conf . MetastoreConf . newMetastoreConf ( ) ; org . junit . Assert . assertEquals ( "from-properties" , org . apache . hadoop . hive . metastore . conf . MetastoreConf . getVar ( conf , ConfVars . STR_TEST_ENTRY ) ) ; } }
public class aTest{ @Test public void testProjectionGeneric ( ) { org . apache . avro . generic . GenericRecord savedRecord = new org . apache . avro . generic . GenericData . Record ( org . apache . crunch . test . Person . SCHEMA$ ) ; savedRecord . put ( "name" , "John<sp>Doe" ) ; savedRecord . put ( "age" , 42 ) ; savedRecord . put ( "siblingnames" , com . google . common . collect . Lists . newArrayList ( "Jimmy" , "Jane" ) ) ; populateGenericFile ( com . google . common . collect . Lists . newArrayList ( savedRecord ) , Person . SCHEMA . ) ; org . apache . crunch . io . parquet . AvroParquetFileSource < org . apache . avro . generic . GenericRecord > src = org . apache . crunch . io . parquet . AvroParquetFileSource . builder ( Person . SCHEMA . ) . includeField ( "age" ) . build ( new org . apache . hadoop . fs . Path ( avroFile . getAbsolutePath ( ) ) ) ; org . apache . crunch . Pipeline pipeline = new org . apache . crunch . impl . mr . MRPipeline ( org . apache . crunch . io . parquet . AvroParquetFileSourceTargetIT . class , tmpDir . getDefaultConfiguration ( ) ) ; org . apache . crunch . PCollection < org . apache . avro . generic . GenericRecord > genericCollection = pipeline . read ( src ) ; java . io . File outputFile = tmpDir . getFile ( "output" ) ; org . apache . crunch . Target avroFile = org . apache . crunch . io . To . avroFile ( outputFile . getAbsolutePath ( ) ) ; genericCollection . write ( avroFile ) ; pipeline . done ( ) ; org . apache . crunch . Pipeline pipeline2 = new org . apache . crunch . impl . mr . MRPipeline ( org . apache . crunch . io . parquet . AvroParquetFileSourceTargetIT . class , tmpDir . getDefaultConfiguration ( ) ) ; org . apache . crunch . PCollection < org . apache . avro . generic . GenericData . Record > ageOnly = pipeline2 . read ( new org . apache . crunch . io . avro . AvroFileSource < org . apache . avro . generic . GenericData . Record > ( new org . apache . hadoop . fs . Path ( outputFile . getAbsolutePath ( ) ) , org . apache . crunch . types . avro . Avros . generics ( src . getProjectedSchema ( ) ) ) ) ; org . apache . avro . generic . GenericData . Record person = com . google . common . collect . Iterables . getOnlyElement ( ageOnly . materialize ( ) ) ; org . junit . Assert . assertEquals ( person . get ( 0 ) , 42 ) ; try { person . get ( 1 ) ; org . junit . Assert . fail ( "Trying<sp>to<sp>get<sp>field<sp>outside<sp>of<sp>projection<sp>should<sp>fail" ) ; } }
public class aTest{ @Test public void testTableWord ( ) { setClipboarHTML ( ( "<sp>mso-border-top-alt:solid<sp>windowtext<sp>.5pt;mso-border-left-alt:solid<sp>windowtext<sp>.5pt;\n" 4 + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "<sp>mso-border-top-alt:solid<sp>windowtext<sp>.5pt;mso-border-left-alt:solid<sp>windowtext<sp>.5pt;\n" 4 + "<sp>mso-border-top-alt:solid<sp>windowtext<sp>.5pt;mso-border-left-alt:solid<sp>windowtext<sp>.5pt;\n" 4 ) + "<table<sp>class=MsoTableGrid<sp>border=1<sp>cellspacing=0<sp>cellpadding=0\n" ) + "b" 1 ) + "<sp>mso-yfti-tbllook:480;mso-padding-alt:0cm<sp>5.4pt<sp>0cm<sp>5.4pt;mso-border-insideh:\n" ) + "<sp>mso-border-top-alt:solid<sp>windowtext<sp>.5pt;mso-border-left-alt:solid<sp>windowtext<sp>.5pt;\n" 2 ) + "<sp>mso-border-top-alt:solid<sp>windowtext<sp>.5pt;mso-border-left-alt:solid<sp>windowtext<sp>.5pt;\n" 0 ) + "b" 6 ) + "<sp>mso-border-top-alt:solid<sp>windowtext<sp>.5pt;mso-border-left-alt:solid<sp>windowtext<sp>.5pt;\n" 3 ) + "b" 9 ) + "b" 7 ) + "b" 6 ) + "<sp>mso-border-top-alt:solid<sp>windowtext<sp>.5pt;mso-border-left-alt:solid<sp>windowtext<sp>.5pt;\n" 1 ) + "<sp>mso-border-top-alt:solid<sp>windowtext<sp>.5pt;mso-border-left-alt:solid<sp>windowtext<sp>.5pt;\n" 6 ) + "<sp>mso-border-top-alt:solid<sp>windowtext<sp>.5pt;mso-border-left-alt:solid<sp>windowtext<sp>.5pt;\n" 5 ) + "b" 7 ) + "b" 8 ) + "<sp><tr<sp>style=\'mso-yfti-irow:1;mso-yfti-lastrow:yes\'>\n" ) + "b" 6 ) + "<sp>border-top:none;mso-border-top-alt:solid<sp>windowtext<sp>.5pt;mso-border-alt:solid<sp>windowtext<sp>.5pt;\n" ) + "<sp>padding:0cm<sp>5.4pt<sp>0cm<sp>5.4pt\'>\n" ) + "b" 0 ) + "b" 7 ) + "<sp><td<sp>width=295<sp>valign=top<sp>style=\'width:221.4pt;border-top:none;border-left:\n" ) + "b" 4 ) + "<sp>mso-border-top-alt:solid<sp>windowtext<sp>.5pt;mso-border-left-alt:solid<sp>windowtext<sp>.5pt;\n" ) + "<sp>mso-border-top-alt:solid<sp>windowtext<sp>.5pt;mso-border-left-alt:solid<sp>windowtext<sp>.5pt;\n" 3 ) + "b" 2 ) + "b" 7 ) + "b" 8 ) + "</table>\n" ) + "<sp>mso-border-top-alt:solid<sp>windowtext<sp>.5pt;mso-border-left-alt:solid<sp>windowtext<sp>.5pt;\n" 4 ) + "<sp>mso-border-top-alt:solid<sp>windowtext<sp>.5pt;mso-border-left-alt:solid<sp>windowtext<sp>.5pt;\n" 4 ) ) ) ; final java . util . List < com . rcpcompany . uibindings . utils . IClipboardConverterManager . IResult > res = IClipboardConverterManager . Factory . getManager ( ) . getClipboardConversions ( ) ; org . junit . Assert . assertEquals ( 1 , res . size ( ) ) ; testOneResult ( res . get ( 0 ) , 1 , new java . lang . String [ ] [ ] { new java . lang . String [ ] { "b" 5 , "b" 3 } , new java . lang . String [ ] { "2" , "b" } } ) ; } }
public class aTest{ @Test public void testWaitTillReadyAppFailed ( ) { final org . apache . tez . client . TestTezClient . TezClientForTest client = configureAndCreateTezClient ( ) ; client . start ( ) ; java . lang . String msg = "Application<sp>Test<sp>Failed" ; when ( client . mockYarnClient . getApplicationReport ( client . mockAppId ) . getYarnApplicationState ( ) ) . thenReturn ( YarnApplicationState . NEW ) . thenReturn ( YarnApplicationState . FAILED ) ; when ( client . mockYarnClient . getApplicationReport ( client . mockAppId ) . getDiagnostics ( ) ) . thenReturn ( msg ) ; try { client . waitTillReady ( ) ; org . junit . Assert . fail ( ) ; } catch ( org . apache . tez . dag . api . SessionNotRunning e ) { org . junit . Assert . assertTrue ( e . getMessage ( ) . contains ( msg ) ) ; } }
public class aTest{ @Test public void testSetBlack ( ) { java . lang . String expr = "foo.alias<sp>=<sp>$0" ; org . apache . commons . jexl3 . JexlScript script = org . apache . commons . jexl3 . introspection . JEXL . createScript ( expr , "foo" , "$0" ) ; org . apache . commons . jexl3 . introspection . SandboxTest . Foo foo = new org . apache . commons . jexl3 . introspection . SandboxTest . Foo ( "42" ) ; java . lang . Object result ; result = script . execute ( null , foo , "43" ) ; org . junit . Assert . assertEquals ( "43" , result ) ; org . apache . commons . jexl3 . introspection . JexlSandbox sandbox = new org . apache . commons . jexl3 . introspection . JexlSandbox ( ) ; sandbox . black ( org . apache . commons . jexl3 . introspection . SandboxTest . Foo . class . getName ( ) ) . write ( "alias" ) ; org . apache . commons . jexl3 . JexlEngine sjexl = new org . apache . commons . jexl3 . JexlBuilder ( ) . sandbox ( sandbox ) . strict ( true ) . create ( ) ; script = sjexl . createScript ( expr , "foo" , "$0" ) ; try { result = script . execute ( null , foo , "43" ) ; org . junit . Assert . fail ( "alias<sp>should<sp>not<sp>be<sp>accessible" ) ; } }
public class aTest{ @Test public void loadPrivateKeyFromPCKS1PEMString ( ) { java . lang . String pemedPrivKey = "-----BEGIN<sp>RSA<sp>PRIVATE<sp>KEY-----\n" + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "j5bh7vjZiW0Pa/5RWqsWOXl/mp9fIZmTfcTmHPFasLpFpxvkw4mSJm3s8rstAKfF\n" 1 + "twUEfW4EzXPtfbDOr3kgMV3I/8sKXnk3aVuuaUwAgHdBRJG0LTxnPzZr10kSQRIk\n" ) + "j5bh7vjZiW0Pa/5RWqsWOXl/mp9fIZmTfcTmHPFasLpFpxvkw4mSJm3s8rstAKfF\n" 4 ) + "j5bh7vjZiW0Pa/5RWqsWOXl/mp9fIZmTfcTmHPFasLpFpxvkw4mSJm3s8rstAKfF\n" ) + "ZKv/my3n54BJr4EDCwtPmCX7kN1YriEDhnCEro8KCjCd0rDSQlv5ih9cSBvq8Xqe\n" ) + "j5bh7vjZiW0Pa/5RWqsWOXl/mp9fIZmTfcTmHPFasLpFpxvkw4mSJm3s8rstAKfF\n" 9 ) + "j5bh7vjZiW0Pa/5RWqsWOXl/mp9fIZmTfcTmHPFasLpFpxvkw4mSJm3s8rstAKfF\n" 0 ) + "HZQJI2YC1lWuo9GqGvqz3yPcXAJ2GVRSx7w7P2OzSBzr0IfPXniWCf+fgqFdVKzg\n" ) + "j5bh7vjZiW0Pa/5RWqsWOXl/mp9fIZmTfcTmHPFasLpFpxvkw4mSJm3s8rstAKfF\n" 3 ) + "j5bh7vjZiW0Pa/5RWqsWOXl/mp9fIZmTfcTmHPFasLpFpxvkw4mSJm3s8rstAKfF\n" 2 ) + "j5bh7vjZiW0Pa/5RWqsWOXl/mp9fIZmTfcTmHPFasLpFpxvkw4mSJm3s8rstAKfF\n" 5 ) + "j5bh7vjZiW0Pa/5RWqsWOXl/mp9fIZmTfcTmHPFasLpFpxvkw4mSJm3s8rstAKfF\n" 8 ) + "njx6MGnvaUbM/lajLd7qn7X7neGWGbDQcil+qCN3NP59MAfBYbpGc8ecfLc8OPmq\n" ) + "NlEo4IPwwuRQVRvZXytKn++Pnpndf74r1BoqsypM4aMilbw6bq10sv8CgYEA8Wl0\n" ) + "NlEo4IPwwuRQVRvZXytKn++Pnpndf74r1BoqsypM4aMilbw6bq10sv8CgYEA8Wl0\n" 2 ) + "j5bh7vjZiW0Pa/5RWqsWOXl/mp9fIZmTfcTmHPFasLpFpxvkw4mSJm3s8rstAKfF\n" 7 ) + "NlEo4IPwwuRQVRvZXytKn++Pnpndf74r1BoqsypM4aMilbw6bq10sv8CgYEA8Wl0\n" 0 ) + "NlEo4IPwwuRQVRvZXytKn++Pnpndf74r1BoqsypM4aMilbw6bq10sv8CgYEA8Wl0\n" 7 ) + "NlEo4IPwwuRQVRvZXytKn++Pnpndf74r1BoqsypM4aMilbw6bq10sv8CgYEA8Wl0\n" 5 ) + "JFl8dDOgWJoQIZNo2iSuOwKBgCftL/3Vcnez5VQIWzobOA+d+hPGtl9qEegMAEBd\n" ) + "j5bh7vjZiW0Pa/5RWqsWOXl/mp9fIZmTfcTmHPFasLpFpxvkw4mSJm3s8rstAKfF\n" 6 ) + "NlEo4IPwwuRQVRvZXytKn++Pnpndf74r1BoqsypM4aMilbw6bq10sv8CgYEA8Wl0\n" 1 ) + "NlEo4IPwwuRQVRvZXytKn++Pnpndf74r1BoqsypM4aMilbw6bq10sv8CgYEA8Wl0\n" 4 ) + "NlEo4IPwwuRQVRvZXytKn++Pnpndf74r1BoqsypM4aMilbw6bq10sv8CgYEA8Wl0\n" 6 ) + "YTN3JGBvqy4Z/i1/FoBBiLNA/oqCgYBFFViDDwkGsk/kvhp/7tBe\n" ) + "-----END<sp>RSA<sp>PRIVATE<sp>KEY-----\n" ) ; if ( buildType . equals ( "NlEo4IPwwuRQVRvZXytKn++Pnpndf74r1BoqsypM4aMilbw6bq10sv8CgYEA8Wl0\n" 3 ) ) { exception . expect ( com . emc . storageos . security . helpers . Exception . class ) ; } byte [ ] privKeyBytes = com . emc . storageos . security . helpers . SecurityUtil . loadPrivateKeyFromPEMString ( pemedPrivKey ) ; org . junit . Assert . assertNotNull ( privKeyBytes ) ; } }
public class aTest{ @Test public void testSubEpochs ( ) { System . out . println ( "getSubEpochs" ) ; jsat . classifiers . bayesian . AODE instance = new jsat . classifiers . bayesian . AODE ( ) ; instance . setM ( 0.1 ) ; org . junit . Assert . assertEquals ( 0.1 , instance . getM ( ) , 0.0 ) ; for ( int i = - 3 ; i < 0 ; i ++ ) try { instance . setM ( i ) ; org . junit . Assert . fail ( "Invalid<sp>value<sp>should<sp>have<sp>thrown<sp>an<sp>error" ) ; } }
public class aTest{ @Test public void testNestedAccumulateWithPrefixAnd ( ) { final java . lang . String drl = "<sp>)\n" 2 + ( ( ( ( ( ( ( ( ( ( ( "<sp>String($l:<sp>length)\n" + "<sp>)\n" 1 ) + "<sp>(and\n" ) + "<sp>)\n" 3 ) + "<sp>)\n" 0 ) + "<sp>Long()<sp>\n" ) + "<sp>;$counter:<sp>count(1);$counter<sp><=<sp>4)\n" ) + "<sp>)\n" ) + "<sp>;$mainCounter:<sp>count(1);$mainCounter<sp><=<sp>2\n" ) + "<sp>)\n" ) + "then\n<sp>" ) + "end" ) ; final org . kie . api . KieBase kieBase = new org . kie . internal . utils . KieHelper ( ) . addContent ( drl , ResourceType . DRL ) . build ( ) ; final org . kie . api . runtime . KieSession kieSession = kieBase . newKieSession ( ) ; try { kieSession . insert ( "test" ) ; kieSession . insert ( 4 ) ; org . junit . Assert . assertEquals ( 1 , kieSession . fireAllRules ( ) ) ; } }
public class aTest{ @Test public void testChunkMerge ( ) { one . nio . mem . Malloc malloc = newInstance ( 38000 ) ; long [ ] addresses = new long [ 4 * 128 ] ; for ( int i = 0 ; i < ( addresses . length ) ; i ++ ) { addresses [ i ] = malloc . malloc ( 64 ) ; } malloc . verify ( ) ; org . junit . Assert . assertTrue ( ( ( malloc . getFreeMemory ( ) ) < 192 ) ) ; for ( int i = 0 ; i < ( addresses . length ) ; i += 4 ) { malloc . free ( addresses [ i ] ) ; malloc . free ( addresses [ ( i + 1 ) ] ) ; malloc . free ( addresses [ ( i + 2 ) ] ) ; } }
public class aTest{ @Test public void testIncorrectPropertyType ( ) { org . junit . Assert . assertNull ( com . puresoltechnologies . ductiledb . core . graph . schema . DuctileDBSchemaManagerIT . schemaManager . getPropertyDefinition ( ElementType . VERTEX , "property" ) ) ; com . puresoltechnologies . ductiledb . core . graph . schema . PropertyDefinition < java . lang . String > definition = new com . puresoltechnologies . ductiledb . core . graph . schema . PropertyDefinition ( com . puresoltechnologies . ductiledb . core . graph . ElementType . VERTEX , "property" , java . lang . String . class , UniqueConstraint . NONE ) ; com . puresoltechnologies . ductiledb . core . graph . schema . DuctileDBSchemaManagerIT . schemaManager . defineProperty ( definition ) ; java . util . Set < java . lang . String > types = new java . util . HashSet ( ) ; types . add ( "type" ) ; java . util . Map < java . lang . String , java . lang . Object > properties = new java . util . HashMap ( ) ; properties . put ( "property" , 1L ) ; try { com . puresoltechnologies . ductiledb . core . graph . schema . DuctileDBSchemaManagerIT . graph . addVertex ( types , properties ) ; } }
public class aTest{ @Test public void testNotSetLoop ( ) { org . antlr . tool . Grammar g = new org . antlr . tool . Grammar ( ( "lexer<sp>grammar<sp>P;\n" + "lexer<sp>grammar<sp>P;\n" 5 ) ) ; java . lang . String expecting = ".s0->.s1\n" + ( ( ( ( ( ( ( ( ( "lexer<sp>grammar<sp>P;\n" 3 + ".s2->.s3\n" ) + ".s2->.s9\n" ) + ".s3->.s4\n" ) + ".s4-{\'\\u0000\'..\'2\',<sp>\'4\'..\'\\uFFFF\'}->.s5\n" ) + ".s5->.s3\n" ) + "lexer<sp>grammar<sp>P;\n" 4 ) + "lexer<sp>grammar<sp>P;\n" 2 ) + "lexer<sp>grammar<sp>P;\n" 1 ) + ":s7-<EOT>->.s8\n" ) ; checkRule ( g , "A" , expecting ) ; java . lang . String expectingGrammarStr = "1:7:<sp>lexer<sp>grammar<sp>P;\n" + ( "lexer<sp>grammar<sp>P;\n" 6 + "lexer<sp>grammar<sp>P;\n" 0 ) ; org . junit . Assert . assertEquals ( expectingGrammarStr , g . toString ( ) ) ; } }
public class aTest{ @Test public void testBarrier2Explicit ( ) { final com . aparapi . runtime . BarrierSupportTest . Barrrier2Kernel kernel = new com . aparapi . runtime . BarrierSupportTest . Barrrier2Kernel ( com . aparapi . runtime . BarrierSupportTest . SIZE ) ; try { final com . aparapi . Range range = com . aparapi . runtime . BarrierSupportTest . openCLDevice . createRange ( com . aparapi . runtime . BarrierSupportTest . SIZE , com . aparapi . runtime . BarrierSupportTest . SIZE ) ; targetArray = initInputArray ( ) ; kernel . setExplicit ( true ) ; kernel . setArray ( targetArray ) ; kernel . put ( targetArray ) ; kernel . execute ( range ) ; kernel . get ( targetArray ) ; org . junit . Assert . assertTrue ( validate ( ) ) ; } }
public class aTest{ @Test public void testHasPermissionWithDifferentCompanyAdmin ( ) { long resourceId = 12345 ; _resourceLocalService . addResources ( _group . getCompanyId ( ) , _group . getGroupId ( ) , 0 , com . liferay . portal . security . permission . test . PermissionCheckerTest . _MODEL_RESOURCE_NAME , resourceId , false , false , false ) ; long companyId = com . liferay . portal . kernel . security . auth . CompanyThreadLocal . getCompanyId ( ) ; try { _company = com . liferay . portal . kernel . test . util . CompanyTestUtil . addCompany ( ) ; com . liferay . portal . kernel . security . auth . CompanyThreadLocal . setCompanyId ( _company . getCompanyId ( ) ) ; _user = com . liferay . portal . kernel . test . util . UserTestUtil . addCompanyAdminUser ( _company ) ; com . liferay . portal . kernel . security . permission . PermissionChecker permissionChecker = _permissionCheckerFactory . create ( _user ) ; boolean companyAdmin = permissionChecker . isCompanyAdmin ( _company . getCompanyId ( ) ) ; org . junit . Assert . assertTrue ( companyAdmin ) ; permissionChecker . hasPermission ( 0 , com . liferay . portal . security . permission . test . PermissionCheckerTest . _MODEL_RESOURCE_NAME , resourceId , ActionKeys . VIEW ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void groupby_size ( ) { final java . lang . String rulebase = "rules/reloaded/groupby_size.prova" ; java . util . concurrent . atomic . AtomicLong count = new java . util . concurrent . atomic . AtomicLong ( ) ; java . util . Map < java . lang . String , java . lang . Object > globals = new java . util . HashMap < java . lang . String , java . lang . Object > ( ) ; globals . put ( "$Count" , count ) ; prova = new ws . prova . api2 . ProvaCommunicatorImpl ( test . ws . prova . test2 . ProvaMetadataTest . kAgent , test . ws . prova . test2 . ProvaMetadataTest . kPort , rulebase , ws . prova . api2 . ProvaCommunicatorImpl . SYNC , globals ) ; for ( int i = 0 ; i < 100 ; i ++ ) { ws . prova . kernel2 . ProvaList terms = ws . prova . reference2 . ProvaListImpl . create ( new ws . prova . kernel2 . ProvaObject [ ] { ws . prova . reference2 . ProvaConstantImpl . create ( "test" ) , ws . prova . reference2 . ProvaConstantImpl . create ( "async" ) , ws . prova . reference2 . ProvaConstantImpl . create ( 0 ) , ws . prova . reference2 . ProvaConstantImpl . create ( "inform" ) , ws . prova . reference2 . ProvaListImpl . create ( new ws . prova . kernel2 . ProvaObject [ ] { ws . prova . reference2 . ProvaConstantImpl . create ( "a" ) , ws . prova . reference2 . ProvaConstantImpl . create ( i ) } ) } ) ; prova . addMsg ( terms ) ; } try { synchronized ( this ) { wait ( 1000 ) ; org . junit . Assert . assertEquals ( 100 , count . get ( ) ) ; } } }
public class aTest{ @Test public void shouldConsolidationInSequence ( ) { io . netty . buffer . ByteBuf currentBuffer = wrappedBuffer ( wrappedBuffer ( "a" . getBytes ( CharsetUtil . US_ASCII ) ) , wrappedBuffer ( "=" . getBytes ( CharsetUtil . US_ASCII ) ) ) ; currentBuffer = wrappedBuffer ( currentBuffer , wrappedBuffer ( "c" 0. getBytes ( CharsetUtil . US_ASCII ) ) , wrappedBuffer ( "&" . getBytes ( CharsetUtil . US_ASCII ) ) ) ; currentBuffer = wrappedBuffer ( currentBuffer , wrappedBuffer ( "b" . getBytes ( CharsetUtil . US_ASCII ) ) , wrappedBuffer ( "=" . getBytes ( CharsetUtil . US_ASCII ) ) ) ; currentBuffer = wrappedBuffer ( currentBuffer , wrappedBuffer ( "c" 1. getBytes ( CharsetUtil . US_ASCII ) ) , wrappedBuffer ( "&" . getBytes ( CharsetUtil . US_ASCII ) ) ) ; currentBuffer = wrappedBuffer ( currentBuffer , wrappedBuffer ( "c" . getBytes ( CharsetUtil . US_ASCII ) ) , wrappedBuffer ( "=" . getBytes ( CharsetUtil . US_ASCII ) ) ) ; currentBuffer = wrappedBuffer ( currentBuffer , wrappedBuffer ( "3" . getBytes ( CharsetUtil . US_ASCII ) ) , wrappedBuffer ( "&" . getBytes ( CharsetUtil . US_ASCII ) ) ) ; currentBuffer = wrappedBuffer ( currentBuffer , wrappedBuffer ( "d" . getBytes ( CharsetUtil . US_ASCII ) ) , wrappedBuffer ( "=" . getBytes ( CharsetUtil . US_ASCII ) ) ) ; currentBuffer = wrappedBuffer ( currentBuffer , wrappedBuffer ( "4" . getBytes ( CharsetUtil . US_ASCII ) ) , wrappedBuffer ( "&" . getBytes ( CharsetUtil . US_ASCII ) ) ) ; currentBuffer = wrappedBuffer ( currentBuffer , wrappedBuffer ( "e" . getBytes ( CharsetUtil . US_ASCII ) ) , wrappedBuffer ( "=" . getBytes ( CharsetUtil . US_ASCII ) ) ) ; currentBuffer = wrappedBuffer ( currentBuffer , wrappedBuffer ( "5" . getBytes ( CharsetUtil . US_ASCII ) ) , wrappedBuffer ( "&" . getBytes ( CharsetUtil . US_ASCII ) ) ) ; io . netty . buffer . ByteBuf copy = currentBuffer . copy ( ) ; java . lang . String s = copy . toString ( CharsetUtil . US_ASCII ) ; org . junit . Assert . assertEquals ( "c" 2 , s ) ; currentBuffer . release ( ) ; copy . release ( ) ; } }
public class aTest{ @Test public void testDeleteUnreadableFile ( ) { final java . io . IOException ioException = new java . io . IOException ( "Unable<sp>to<sp>read" ) ; final java . nio . file . Path unreadableFilePath = java . nio . file . Paths . get ( "UnreadableFile" ) ; createFile ( unreadableFilePath ) ; try ( com . liferay . portal . kernel . test . SwappableSecurityManager swappableSecurityManager = new com . liferay . portal . kernel . test . SwappableSecurityManager ( ) { @ com . liferay . portal . fabric . netty . fileserver . Override public void checkRead ( java . lang . String file ) { if ( file . equals ( unreadableFilePath . toString ( ) ) ) { com . liferay . petra . reflect . ReflectionUtil . throwException ( ioException ) ; } } } ) { swappableSecurityManager . install ( ) ; com . liferay . portal . fabric . netty . fileserver . FileHelperUtil . delete ( true , unreadableFilePath ) ; com . liferay . portal . fabric . netty . fileserver . FileHelperUtil . delete ( unreadableFilePath ) ; org . junit . Assert . fail ( ) ; } catch ( java . lang . Exception e ) { org . junit . Assert . assertSame ( ioException , e ) ; } }
public class aTest{ @Test public void testDelta_FailsOver2GiB ( ) { try ( org . eclipse . jgit . lib . ObjectInserter . Formatter fmt = new org . eclipse . jgit . lib . ObjectInserter . Formatter ( ) ) { byte [ ] base = new byte [ ] { 'a' } ; org . eclipse . jgit . lib . ObjectId idA = fmt . idFor ( Constants . OBJ_BLOB , base ) ; org . eclipse . jgit . lib . ObjectId idB = fmt . idFor ( Constants . OBJ_BLOB , new byte [ ] { 'b' } ) ; org . eclipse . jgit . transport . PackedObjectInfo a = new org . eclipse . jgit . transport . PackedObjectInfo ( idA ) ; org . eclipse . jgit . transport . PackedObjectInfo b = new org . eclipse . jgit . transport . PackedObjectInfo ( idB ) ; org . eclipse . jgit . util . TemporaryBuffer . Heap pack = new org . eclipse . jgit . util . TemporaryBuffer . Heap ( ( 64 * 1024 ) ) ; org . eclipse . jgit . internal . storage . file . PackFileTest . packHeader ( pack , 2 ) ; a . setOffset ( pack . length ( ) ) ; org . eclipse . jgit . internal . storage . file . PackFileTest . objectHeader ( pack , Constants . OBJ_BLOB , base . length ) ; org . eclipse . jgit . internal . storage . file . PackFileTest . deflate ( pack , base ) ; java . io . ByteArrayOutputStream tmp = new java . io . ByteArrayOutputStream ( ) ; org . eclipse . jgit . internal . storage . pack . DeltaEncoder de = new org . eclipse . jgit . internal . storage . pack . DeltaEncoder ( tmp , base . length , ( 3L << 30 ) ) ; de . copy ( 0 , 1 ) ; byte [ ] delta = tmp . toByteArray ( ) ; b . setOffset ( pack . length ( ) ) ; org . eclipse . jgit . internal . storage . file . PackFileTest . objectHeader ( pack , Constants . OBJ_REF_DELTA , delta . length ) ; idA . copyRawTo ( pack ) ; org . eclipse . jgit . internal . storage . file . PackFileTest . deflate ( pack , delta ) ; byte [ ] footer = org . eclipse . jgit . internal . storage . file . PackFileTest . digest ( pack ) ; java . io . File dir = new java . io . File ( repo . getObjectDatabase ( ) . getDirectory ( ) , "pack" ) ; java . io . File packName = new java . io . File ( dir , ( ( idA . name ( ) ) + ".pack" ) ) ; java . io . File idxName = new java . io . File ( dir , ( ( idA . name ( ) ) + ".idx" ) ) ; try ( java . io . FileOutputStream f = new java . io . FileOutputStream ( packName ) ) { f . write ( pack . toByteArray ( ) ) ; } try ( java . io . FileOutputStream f = new java . io . FileOutputStream ( idxName ) ) { java . util . List < org . eclipse . jgit . transport . PackedObjectInfo > list = new java . util . ArrayList ( ) ; list . add ( a ) ; list . add ( b ) ; java . util . Collections . sort ( list ) ; new org . eclipse . jgit . internal . storage . file . PackIndexWriterV1 ( f ) . write ( list , footer ) ; } org . eclipse . jgit . internal . storage . file . PackFile packFile = new org . eclipse . jgit . internal . storage . file . PackFile ( packName , PackExt . INDEX . getBit ( ) ) ; try { packFile . get ( wc , b ) ; org . junit . Assert . fail ( "expected<sp>LargeObjectException.ExceedsByteArrayLimit" ) ; } catch ( org . eclipse . jgit . errors . LargeObjectException bad ) { org . junit . Assert . assertNull ( bad . getObjectId ( ) ) ; } }
public class aTest{ @Test public void standbySpanStreamDataSendWorkerTest1 ( ) { java . util . concurrent . CountDownLatch testCountDownLatch = new java . util . concurrent . CountDownLatch ( 2 ) ; long blockTime = 1000 ; com . navercorp . pinpoint . profiler . sender . StandbySpanStreamDataSendWorker sendWorker = new com . navercorp . pinpoint . profiler . sender . StandbySpanStreamDataSendWorker ( new com . navercorp . pinpoint . profiler . sender . StandbySpanStreamDataSendWorkerTest . TestFlushHandler ( testCountDownLatch ) , new com . navercorp . pinpoint . profiler . sender . StandbySpanStreamDataStorage ( 10 , blockTime ) ) ; sendWorker . start ( ) ; try { com . navercorp . pinpoint . profiler . sender . SpanStreamSendData spanStreamSendData = createSpanStreamSendData ( "a" . getBytes ( ) ) ; sendWorker . addStandbySpanStreamData ( spanStreamSendData ) ; spanStreamSendData = createSpanStreamSendData ( "b" . getBytes ( ) ) ; sendWorker . addStandbySpanStreamData ( spanStreamSendData ) ; boolean onEvent = testCountDownLatch . await ( ( blockTime * 2 ) , TimeUnit . MILLISECONDS ) ; org . junit . Assert . assertTrue ( onEvent ) ; } }
public class aTest{ @Test public void shouldParseQueryWithUnqualifiedPathInSelectAndUnqualifiedLocalNameInCriteriaOfJcrSql2Query ( ) { java . lang . String sql = "select<sp>[jcr:primaryType],<sp>[jcr:path]<sp>FROM<sp>[nt:base]<sp>WHERE<sp>[mode:localName]<sp>LIKE<sp>'%3%'" ; javax . jcr . query . Query query = session . getWorkspace ( ) . getQueryManager ( ) . createQuery ( sql , Query . JCR_SQL2 ) ; javax . jcr . query . QueryResult result = query . execute ( ) ; validateQuery ( ) . rowCount ( 4 ) . hasColumns ( "jcr:primaryType" , "jcr:path" ) . onEachRow ( new org . modeshape . jcr . ValidateQuery . Predicate ( ) { @ org . modeshape . jcr . Override public void validate ( int rowNumber , javax . jcr . query . Row row ) throws javax . jcr . RepositoryException { org . junit . Assert . assertNotNull ( row . getValue ( "jcr:primaryType" ) ) ; } } }
public class aTest{ @Test public void testClearAndToString ( ) { org . apache . activemq . artemis . utils . collections . TypedProperties props = new org . apache . activemq . artemis . utils . collections . TypedProperties ( ) ; java . util . concurrent . ExecutorService executorService = java . util . concurrent . Executors . newFixedThreadPool ( 1000 ) ; java . util . concurrent . atomic . AtomicBoolean hasError = new java . util . concurrent . atomic . AtomicBoolean ( ) ; java . util . concurrent . CountDownLatch countDownLatch = new java . util . concurrent . CountDownLatch ( 1 ) ; for ( int i = 0 ; i < 10000 ; i ++ ) { int g = i ; executorService . submit ( ( ) -> { try { countDownLatch . await ( ) ; for ( int h = 0 ; h < 100 ; h ++ ) { props . putSimpleStringProperty ( org . apache . activemq . artemis . api . core . SimpleString . toSimpleString ( ( "S" + h ) ) , org . apache . activemq . artemis . api . core . SimpleString . toSimpleString ( "hello" ) ) ; } props . clear ( ) ; } catch ( t ) { hasError . set ( true ) ; org . apache . activemq . artemis . utils . t . printStackTrace ( ) ; } catch ( e ) { } } ) ; } for ( int i = 0 ; i < 10 ; i ++ ) { executorService . submit ( ( ) -> { try { countDownLatch . await ( ) ; for ( int k = 0 ; k < 1000 ; k ++ ) { props . toString ( ) ; } } catch ( t ) { hasError . set ( true ) ; org . apache . activemq . artemis . utils . t . printStackTrace ( ) ; } catch ( e ) { } } ) ; } countDownLatch . countDown ( ) ; java . lang . Thread . sleep ( 1000 ) ; executorService . shutdown ( ) ; executorService . awaitTermination ( 10 , TimeUnit . SECONDS ) ; executorService . shutdown ( ) ; org . junit . Assert . assertFalse ( hasError . get ( ) ) ; } }
public class aTest{ @Test public void testValidateStringIsNotEmpty ( ) { generalSettingsPresenter . validateStringIsNotEmpty ( "NotEmptyString" , "Message" ) . then ( ( e ) -> { org . junit . Assert . assertEquals ( e , true ) ; return promises . resolve ( ) ; } }
public class aTest{ @Test public void testStatisticalMetrics ( ) { java . lang . String source = this . getClass ( ) . getResource ( "/com/cloudera/csd/tools/impala/valid.json" ) . getPath ( ) ; adapter . init ( source , null ) ; java . util . List < com . cloudera . csd . descriptors . MetricDescriptor > metrics = getMetricsForEntity ( adapter , "test_entity1_metrics" ) ; org . junit . Assert . assertEquals ( com . cloudera . csd . tools . impala . ImpalaMetricTypes . StatisticalMetricType . values ( ) . length , metrics . size ( ) ) ; for ( com . cloudera . csd . descriptors . MetricDescriptor metric : metrics ) { if ( ! ( metric . getName ( ) . contains ( "metric2" ) ) ) { continue ; } }
public class aTest{ @Test public void testHttpHeaderAuthenticationFilterUserAuthorizationInvalidConfigurationValue ( ) { userDaoTestHelper . createUserEntity ( org . finra . herd . app . security . USER_ID , true ) ; namespaceDaoTestHelper . createNamespaceEntity ( org . finra . herd . app . security . NAMESPACE ) ; namespaceDaoTestHelper . createNamespaceEntity ( org . finra . herd . app . security . NAMESPACE_2 ) ; java . util . Set < org . finra . herd . model . api . xml . NamespaceAuthorization > expectedNamespaceAuthorizations = new java . util . HashSet ( ) ; expectedNamespaceAuthorizations . add ( new org . finra . herd . model . api . xml . NamespaceAuthorization ( NAMESPACE , SUPPORTED_NAMESPACE_PERMISSIONS ) ) ; expectedNamespaceAuthorizations . add ( new org . finra . herd . model . api . xml . NamespaceAuthorization ( NAMESPACE_2 , SUPPORTED_NAMESPACE_PERMISSIONS ) ) ; setupTestFunctions ( "testRole" ) ; java . util . Map < java . lang . String , java . lang . Object > overrideMap = getDefaultSecurityEnvironmentVariables ( ) ; overrideMap . put ( ConfigurationValue . USER_NAMESPACE_AUTHORIZATION_ENABLED . getKey ( ) , "NOT_A_BOOLEAN" ) ; modifyPropertySourceInEnvironment ( overrideMap ) ; try { org . springframework . mock . web . MockHttpServletRequest request = getRequestWithHeaders ( org . finra . herd . app . security . USER_ID , "testFirstName" , "testLastName" , "testEmail" , "testRole" , "Wed,<sp>11<sp>Mar<sp>2015<sp>10:24:09" ) ; invalidateApplicationUser ( request ) ; httpHeaderAuthenticationFilter . init ( new org . springframework . mock . web . MockFilterConfig ( ) ) ; httpHeaderAuthenticationFilter . doFilter ( request , new org . springframework . mock . web . MockHttpServletResponse ( ) , new org . springframework . mock . web . MockFilterChain ( ) ) ; org . junit . Assert . assertNull ( org . springframework . security . core . context . SecurityContextHolder . getContext ( ) . getAuthentication ( ) ) ; } }
public class aTest{ @Test public void testEmitterWithOrderedQueue ( ) { java . lang . Object lock = new java . lang . Object ( ) ; java . util . List < org . apache . flink . streaming . runtime . streamrecord . StreamElement > list = new java . util . ArrayList ( ) ; org . apache . flink . streaming . api . operators . Output < org . apache . flink . streaming . runtime . streamrecord . StreamRecord < java . lang . Integer > > output = new org . apache . flink . streaming . util . CollectorOutput ( list ) ; java . util . List < org . apache . flink . streaming . runtime . streamrecord . StreamElement > expected = java . util . Arrays . asList ( new org . apache . flink . streaming . runtime . streamrecord . StreamRecord ( 1 , 0L ) , new org . apache . flink . streaming . runtime . streamrecord . StreamRecord ( 2 , 0L ) , new org . apache . flink . streaming . runtime . streamrecord . StreamRecord ( 3 , 1L ) , new org . apache . flink . streaming . runtime . streamrecord . StreamRecord ( 4 , 1L ) , new org . apache . flink . streaming . api . watermark . Watermark ( 3L ) , new org . apache . flink . streaming . runtime . streamrecord . StreamRecord ( 5 , 4L ) , new org . apache . flink . streaming . runtime . streamrecord . StreamRecord ( 6 , 4L ) ) ; org . apache . flink . streaming . api . operators . async . OperatorActions operatorActions = mock ( org . apache . flink . streaming . api . operators . async . OperatorActions . class ) ; final int capacity = 5 ; org . apache . flink . streaming . api . operators . async . queue . StreamElementQueue queue = new org . apache . flink . streaming . api . operators . async . queue . OrderedStreamElementQueue ( capacity , org . apache . flink . streaming . api . operators . async . EmitterTest . executor , operatorActions ) ; final org . apache . flink . streaming . api . operators . async . Emitter < java . lang . Integer > emitter = new org . apache . flink . streaming . api . operators . async . Emitter ( lock , output , queue , operatorActions ) ; final java . lang . Thread emitterThread = new java . lang . Thread ( emitter ) ; emitterThread . start ( ) ; try { org . apache . flink . streaming . api . operators . async . queue . StreamRecordQueueEntry < java . lang . Integer > record1 = new org . apache . flink . streaming . api . operators . async . queue . StreamRecordQueueEntry ( new org . apache . flink . streaming . runtime . streamrecord . StreamRecord ( 1 , 0L ) ) ; org . apache . flink . streaming . api . operators . async . queue . StreamRecordQueueEntry < java . lang . Integer > record2 = new org . apache . flink . streaming . api . operators . async . queue . StreamRecordQueueEntry ( new org . apache . flink . streaming . runtime . streamrecord . StreamRecord ( 2 , 1L ) ) ; org . apache . flink . streaming . api . operators . async . queue . WatermarkQueueEntry watermark1 = new org . apache . flink . streaming . api . operators . async . queue . WatermarkQueueEntry ( new org . apache . flink . streaming . api . watermark . Watermark ( 3L ) ) ; org . apache . flink . streaming . api . operators . async . queue . StreamRecordQueueEntry < java . lang . Integer > record3 = new org . apache . flink . streaming . api . operators . async . queue . StreamRecordQueueEntry ( new org . apache . flink . streaming . runtime . streamrecord . StreamRecord ( 3 , 4L ) ) ; queue . put ( record1 ) ; queue . put ( record2 ) ; queue . put ( watermark1 ) ; queue . put ( record3 ) ; record2 . complete ( java . util . Arrays . asList ( 3 , 4 ) ) ; record1 . complete ( java . util . Arrays . asList ( 1 , 2 ) ) ; record3 . complete ( java . util . Arrays . asList ( 5 , 6 ) ) ; synchronized ( lock ) { while ( ! ( queue . isEmpty ( ) ) ) { lock . wait ( ) ; } } org . junit . Assert . assertEquals ( expected , list ) ; } }
public class aTest{ @Test public void testDeleteOfTempTopicOnClosedConnection ( ) { connection = new org . apache . qpid . jms . JmsConnection ( connectionInfo , provider ) ; connection . start ( ) ; javax . jms . Session session = connection . createSession ( false , Session . AUTO_ACKNOWLEDGE ) ; javax . jms . TemporaryTopic tempTopic = session . createTemporaryTopic ( ) ; org . junit . Assert . assertNotNull ( tempTopic ) ; connection . close ( ) ; try { tempTopic . delete ( ) ; org . junit . Assert . fail ( "Should<sp>have<sp>thrown<sp>an<sp>IllegalStateException" ) ; } }
public class aTest{ @Test public void testDisableEnable ( ) { hbaseAdmin . disableReplicationPeer ( org . apache . hadoop . hbase . replication . TestReplicationSmallTests . PEER_ID ) ; byte [ ] rowkey = org . apache . hadoop . hbase . util . Bytes . toBytes ( "disable<sp>enable" ) ; org . apache . hadoop . hbase . client . Put put = new org . apache . hadoop . hbase . client . Put ( rowkey ) ; put . addColumn ( famName , row , row ) ; htable1 . put ( put ) ; org . apache . hadoop . hbase . client . Get get = new org . apache . hadoop . hbase . client . Get ( rowkey ) ; for ( int i = 0 ; i < ( NB_RETRIES ) ; i ++ ) { org . apache . hadoop . hbase . client . Result res = htable2 . get ( get ) ; if ( ( res . size ( ) ) >= 1 ) { org . junit . Assert . fail ( "Replication<sp>wasn't<sp>disabled" ) ; } else { org . apache . hadoop . hbase . replication . TestReplicationSmallTests . LOG . info ( "Row<sp>not<sp>replicated,<sp>let's<sp>wait<sp>a<sp>bit<sp>more..." ) ; java . lang . Thread . sleep ( org . apache . hadoop . hbase . replication . SLEEP_TIME ) ; } } hbaseAdmin . enableReplicationPeer ( org . apache . hadoop . hbase . replication . TestReplicationSmallTests . PEER_ID ) ; for ( int i = 0 ; i < ( NB_RETRIES ) ; i ++ ) { org . apache . hadoop . hbase . client . Result res = htable2 . get ( get ) ; if ( res . isEmpty ( ) ) { org . apache . hadoop . hbase . replication . TestReplicationSmallTests . LOG . info ( "Row<sp>not<sp>available" ) ; java . lang . Thread . sleep ( org . apache . hadoop . hbase . replication . SLEEP_TIME ) ; } else { org . junit . Assert . assertArrayEquals ( row , res . value ( ) ) ; return ; } } }
public class aTest{ @Test public void testSort ( ) { org . apache . ivy . plugins . latest . ArtifactInfo [ ] revs = toMockAI ( new java . lang . String [ ] { "0.2.0.a" , "0.2.0.b" , "0.2.0.final" , "1.0" , "1.0.0.gamma" , "0.2.0.final" 0 , "1.0.0.rc2" , "1.0.1" , "2" , "2.0.0.b006" , "0.2.0.final" 1 , "2.0.0.xyz" } ) ; java . util . List < org . apache . ivy . plugins . latest . ArtifactInfo > shuffled = new java . util . ArrayList ( java . util . Arrays . asList ( revs ) ) ; org . apache . ivy . plugins . latest . ArtifactInfo [ ] shuffledRevs = shuffled . toArray ( new org . apache . ivy . plugins . latest . ArtifactInfo [ revs . length ] ) ; org . apache . ivy . osgi . core . OsgiLatestStrategy latestRevisionStrategy = new org . apache . ivy . osgi . core . OsgiLatestStrategy ( ) ; java . util . List < org . apache . ivy . plugins . latest . ArtifactInfo > sorted = latestRevisionStrategy . sort ( shuffledRevs ) ; org . junit . Assert . assertEquals ( java . util . Arrays . asList ( revs ) , sorted ) ; } }
public class aTest{ @Test public void noElementName ( ) { java . lang . String xml = "<></>" ; com . tickaroo . tikxml . XmlReader reader = com . tickaroo . tikxml . TestUtils . readerFrom ( xml ) ; try { org . junit . Assert . assertTrue ( reader . hasElement ( ) ) ; reader . beginElement ( ) ; exception . expect ( java . io . IOException . class ) ; exception . expectMessage ( "Expected<sp>xml<sp>element<sp>name<sp>(literal<sp>expected)<sp>at<sp>path<sp>/" ) ; reader . nextElementName ( ) ; } }
public class aTest{ @Test public void testRawMemoryMapped ( ) { for ( int t = 0 ; t < 5 ; t ++ ) { @ org . jetbrains . annotations . NotNull java . io . File tempFile = java . io . File . createTempFile ( "chronicle" , "q" ) ; try { long startTime = java . lang . System . nanoTime ( ) ; @ org . jetbrains . annotations . NotNull net . openhft . chronicle . bytes . MappedFile mappedFile = net . openhft . chronicle . bytes . MappedFile . mappedFile ( tempFile , ( ( net . openhft . chronicle . bytes . MappedMemoryTest . BLOCK_SIZE ) / 2 ) , net . openhft . chronicle . core . OS . pageSize ( ) ) ; @ org . jetbrains . annotations . Nullable net . openhft . chronicle . bytes . MappedBytesStore bytesStore = mappedFile . acquireByteStore ( 1 ) ; long address = bytesStore . address ; for ( long i = 0 ; i < ( ( net . openhft . chronicle . bytes . MappedMemoryTest . BLOCK_SIZE ) / 2 ) ; i += 8L ) { net . openhft . chronicle . core . OS . memory ( ) . writeLong ( ( address + i ) , i ) ; } for ( long i = 0 ; i < ( ( net . openhft . chronicle . bytes . MappedMemoryTest . BLOCK_SIZE ) / 2 ) ; i += 8L ) { net . openhft . chronicle . core . OS . memory ( ) . writeLong ( ( address + i ) , i ) ; } bytesStore . release ( ) ; mappedFile . release ( ) ; org . junit . Assert . assertEquals ( mappedFile . referenceCounts ( ) , 0 , mappedFile . refCount ( ) ) ; net . openhft . chronicle . bytes . MappedMemoryTest . LOG . info ( ( ( ( "With<sp>RawMemory,\t\t<sp>time=<sp>" + ( ( ( 80 * ( ( java . lang . System . nanoTime ( ) ) - startTime ) ) / ( net . openhft . chronicle . bytes . MappedMemoryTest . BLOCK_SIZE ) ) / 10.0 ) ) + "<sp>ns,<sp>number<sp>of<sp>longs<sp>written=" ) + ( ( net . openhft . chronicle . bytes . MappedMemoryTest . BLOCK_SIZE ) / 8 ) ) ) ; } }
public class aTest{ @Test public void testValidAudienceJWT ( ) { try { handler . setPublicKey ( publicKey ) ; java . util . Properties props = getProperties ( ) ; props . put ( JWTRedirectAuthenticationHandler . EXPECTED_JWT_AUDIENCES , "bar" ) ; handler . init ( props ) ; com . nimbusds . jwt . SignedJWT jwt = getJWT ( "bob" , new java . util . Date ( ( ( new java . util . Date ( ) . getTime ( ) ) + 5000 ) ) , privateKey ) ; javax . servlet . http . Cookie cookie = new javax . servlet . http . Cookie ( "hadoop-jwt" , jwt . serialize ( ) ) ; javax . servlet . http . HttpServletRequest request = org . mockito . Mockito . mock ( javax . servlet . http . HttpServletRequest . class ) ; org . mockito . Mockito . when ( request . getCookies ( ) ) . thenReturn ( new javax . servlet . http . Cookie [ ] { cookie } ) ; org . mockito . Mockito . when ( request . getRequestURL ( ) ) . thenReturn ( new java . lang . StringBuffer ( org . apache . hadoop . security . authentication . server . TestJWTRedirectAuthentictionHandler . SERVICE_URL ) ) ; javax . servlet . http . HttpServletResponse response = org . mockito . Mockito . mock ( javax . servlet . http . HttpServletResponse . class ) ; org . mockito . Mockito . when ( response . encodeRedirectURL ( org . apache . hadoop . security . authentication . server . TestJWTRedirectAuthentictionHandler . SERVICE_URL ) ) . thenReturn ( org . apache . hadoop . security . authentication . server . TestJWTRedirectAuthentictionHandler . SERVICE_URL ) ; org . apache . hadoop . security . authentication . server . AuthenticationToken token = handler . alternateAuthenticate ( request , response ) ; org . junit . Assert . assertEquals ( "bob" , token . getUserName ( ) ) ; } }
public class aTest{ @Test public void testIncorrectPropertyType2 ( ) { org . junit . Assert . assertNull ( com . puresoltechnologies . ductiledb . core . graph . schema . DuctileDBSchemaManagerIT . schemaManager . getPropertyDefinition ( ElementType . VERTEX , "property" ) ) ; com . puresoltechnologies . ductiledb . core . graph . schema . PropertyDefinition < java . lang . String > definition = new com . puresoltechnologies . ductiledb . core . graph . schema . PropertyDefinition ( com . puresoltechnologies . ductiledb . core . graph . ElementType . VERTEX , "property" , java . lang . String . class , UniqueConstraint . NONE ) ; com . puresoltechnologies . ductiledb . core . graph . schema . DuctileDBSchemaManagerIT . schemaManager . defineProperty ( definition ) ; java . util . Set < java . lang . String > types = new java . util . HashSet ( ) ; types . add ( "type" ) ; try { com . puresoltechnologies . ductiledb . core . graph . DuctileDBVertex vertex = com . puresoltechnologies . ductiledb . core . graph . schema . DuctileDBSchemaManagerIT . graph . addVertex ( types , new java . util . HashMap ( ) ) ; vertex . setProperty ( "property" , 1L ) ; } }
public class aTest{ @Test public void getMetricDefSpecificAgainstSvcDontCareNeg ( ) { org . ebayopensource . turmeric . runtime . common . monitoring . MetricsRegistry registry = org . ebayopensource . turmeric . runtime . common . monitoring . MetricsRegistry . getServerInstance ( ) ; try { registerTestMetricDefs ( ) ; org . ebayopensource . turmeric . runtime . common . monitoring . MetricId specific = new org . ebayopensource . turmeric . runtime . common . monitoring . MetricId ( org . ebayopensource . turmeric . runtime . tests . monitoring . MetricsRegistryTest . METRIC_NAME_CT , org . ebayopensource . turmeric . runtime . tests . monitoring . MetricsRegistryTest . SERVICE_NAME_USER , org . ebayopensource . turmeric . runtime . tests . monitoring . MetricsRegistryTest . OP_NAME_EDIT ) ; org . ebayopensource . turmeric . runtime . common . monitoring . MetricDef def = registry . findMetricDef ( specific ) ; org . junit . Assert . assertEquals ( null , def ) ; } }
public class aTest{ @Test public void testMinimalPoints ( ) { int minimalPoints = 2 ; org . hawkular . datamining . forecast . ModelData rModel = org . hawkular . datamining . forecast . ModelReader . read ( "trendStatUpwardLowVar" ) ; org . hawkular . datamining . forecast . models . DoubleExponentialSmoothing . DoubleExOptimizer optimizer = org . hawkular . datamining . forecast . models . DoubleExponentialSmoothing . optimizer ( ) ; try { org . hawkular . datamining . forecast . models . TimeSeriesModel model = optimizer . minimizedMSE ( rModel . getData ( ) . subList ( 0 , minimalPoints ) ) ; org . junit . Assert . assertTrue ( ( model != null ) ) ; } }
public class aTest{ @Test public void testZeroValuesAreNotSaved ( ) { long timeStamp1 = java . lang . System . currentTimeMillis ( ) ; org . ebayopensource . turmeric . runtime . common . monitoring . MetricId metricId1 = new org . ebayopensource . turmeric . runtime . common . monitoring . MetricId ( "test_count" , "service1" , "operation1" ) ; org . ebayopensource . turmeric . runtime . common . monitoring . value . MetricValue metricValue1 = new org . ebayopensource . turmeric . runtime . common . monitoring . value . LongSumMetricValue ( metricId1 ) ; org . ebayopensource . turmeric . monitoring . storage . MetricValueAggregatorTestImpl aggregator1 = new org . ebayopensource . turmeric . monitoring . storage . MetricValueAggregatorTestImpl ( metricValue1 , org . ebayopensource . turmeric . runtime . common . monitoring . MetricCategory . Timing , org . ebayopensource . turmeric . runtime . common . monitoring . MonitoringLevel . NORMAL ) ; org . ebayopensource . turmeric . runtime . common . monitoring . MetricId metricId2 = new org . ebayopensource . turmeric . runtime . common . monitoring . MetricId ( "service1" 3 , "service2" , "operation2" ) ; org . ebayopensource . turmeric . runtime . common . monitoring . value . MetricValue metricValue2 = new org . ebayopensource . turmeric . runtime . common . monitoring . value . AverageMetricValue ( metricId2 ) ; org . ebayopensource . turmeric . monitoring . storage . MetricValueAggregatorTestImpl aggregator2 = new org . ebayopensource . turmeric . monitoring . storage . MetricValueAggregatorTestImpl ( metricValue2 , org . ebayopensource . turmeric . runtime . common . monitoring . MetricCategory . Timing , org . ebayopensource . turmeric . runtime . common . monitoring . MonitoringLevel . NORMAL ) ; org . ebayopensource . turmeric . runtime . common . monitoring . MetricClassifier metricClassifier1 = new org . ebayopensource . turmeric . runtime . common . monitoring . MetricClassifier ( "consumer1" , "sourceDC1" , "service1" 2 ) ; long count1 = 1L ; aggregator1 . update ( metricClassifier1 , count1 ) ; long time1 = 2L ; aggregator2 . update ( metricClassifier1 , time1 ) ; java . util . List < org . ebayopensource . turmeric . runtime . common . monitoring . value . MetricValueAggregator > aggregators = deepCopyAggregators ( aggregator1 , aggregator2 ) ; metricsStorageProvider . saveMetricSnapshot ( timeStamp1 , aggregators ) ; aggregators = deepCopyAggregators ( aggregator1 , aggregator2 ) ; long timeStamp2 = java . lang . System . currentTimeMillis ( ) ; metricsStorageProvider . saveMetricSnapshot ( timeStamp2 , aggregators ) ; org . ebayopensource . turmeric . utils . jpa . EntityManagerContext . open ( factory ) ; try { java . lang . StringBuilder jpql = new java . lang . StringBuilder ( ) ; jpql . append ( "from<sp>" ) . append ( org . ebayopensource . turmeric . runtime . common . monitoring . value . MetricValue . class . getName ( ) ) . append ( "<sp>as<sp>mv" ) ; jpql . append ( "service1" 1 ) ; javax . persistence . EntityManager entityManager = org . ebayopensource . turmeric . utils . jpa . EntityManagerContext . get ( ) ; javax . persistence . Query query = entityManager . createQuery ( jpql . toString ( ) ) ; query . setParameter ( "service1" 0 , timeStamp2 ) ; @ org . ebayopensource . turmeric . monitoring . storage . SuppressWarnings ( "unchecked" ) java . util . List < org . ebayopensource . turmeric . monitoring . model . MetricValue > metricValues = query . getResultList ( ) ; org . junit . Assert . assertTrue ( metricValues . isEmpty ( ) ) ; } }
public class aTest{ @Test public void test21deleteTag ( ) { org . apache . ranger . plugin . model . RangerTag oldTag = new org . apache . ranger . plugin . model . RangerTag ( ) ; oldTag . setId ( org . apache . ranger . rest . TestTagREST . id ) ; try { org . mockito . Mockito . when ( validator . preDeleteTag ( org . apache . ranger . rest . TestTagREST . id ) ) . thenReturn ( oldTag ) ; } catch ( java . lang . Exception e ) { } try { org . mockito . Mockito . doNothing ( ) . when ( tagStore ) . deleteTag ( org . apache . ranger . rest . TestTagREST . id ) ; } catch ( java . lang . Exception e ) { } tagREST . deleteTag ( org . apache . ranger . rest . TestTagREST . id ) ; org . junit . Assert . assertNotNull ( oldTag . getId ( ) ) ; try { org . mockito . Mockito . verify ( validator ) . preDeleteTag ( org . apache . ranger . rest . TestTagREST . id ) ; } }
public class aTest{ @Test public void testGetInOneWorkerWithConditionThatDoesNotMatch ( ) { System . out . println ( "===><sp>Test<sp>started<sp><===" ) ; org . ourgrid . system . units . BrokerUnit brokerUnit = new org . ourgrid . system . units . BrokerUnit ( org . ourgrid . system . units . BrokerUnit . BROKER_PROPERTIES_FILENAME , 1 ) ; unitManager . addUnit ( brokerUnit ) ; brokerUnit . setMaxFails ( 1 ) ; brokerUnit . setNumberOfReplicaExecutors ( 1 ) ; System . out . println ( "===><sp>WorkerUnit<sp>created<sp><===" 4 ) ; org . ourgrid . system . units . WorkerUnit workerUnit = unitManager . buildNewUnit ( org . ourgrid . system . units . WorkerUnit . class ) ; workerUnit . setPlaypenRootPath ( org . ourgrid . system . TEMP_TEST_DIR ) ; workerUnit . setStorageRootPath ( org . ourgrid . system . TEMP_TEST_DIR ) ; workerUnit . addProperty ( "===><sp>WorkerUnit<sp>created<sp><===" 3 , "linux" ) ; System . out . println ( "===><sp>WorkerUnit<sp>created<sp><===" ) ; org . ourgrid . system . units . PeerUnit peerUnit = unitManager . buildNewUnit ( org . ourgrid . system . units . PeerUnit . class ) ; System . out . println ( "===><sp>PeerUnit<sp>created<sp><===" ) ; brokerUnit . initKeys ( ) ; peerUnit . initKeys ( ) ; workerUnit . initKeys ( ) ; System . out . println ( "===><sp>Remote<sp>entities<sp>running<sp>(1<sp>worker,<sp>1<sp>peer,<sp>1<sp>broker)<sp><===" ) ; conditionExpecter . waitUntilConditionIsMet ( new org . ourgrid . system . condition . PeerHasTheWorkerInStateCondition ( peerUnit , workerUnit , org . ourgrid . common . interfaces . to . LocalWorkerState . IDLE ) ) ; System . out . println ( "===><sp>Set<sp>workers<sp><===" ) ; conditionExpecter . waitUntilConditionIsMet ( new org . ourgrid . system . condition . BrokerHasAPeerInTheState ( brokerUnit , peerUnit , PeerTestState . UP ) ) ; System . out . println ( "===><sp>Set<sp>peer<sp><===" ) ; java . io . File putSourceTempFile = org . ourgrid . common . util . TempFileManager . createTempFileWithBogusData ( getClass ( ) . getSimpleName ( ) , "" , tempFileDir , ( 4096 * 10 ) ) ; java . lang . String [ ] putSources = new java . lang . String [ ] { putSourceTempFile . getPath ( ) } ; java . lang . String [ ] playpenDests = new java . lang . String [ ] { putSourceTempFile . getName ( ) } ; java . lang . String [ ] getDests = new java . lang . String [ ] { ( putSourceTempFile . getPath ( ) ) + "_get" } ; java . lang . String [ ] putConditions = new java . lang . String [ ] { "" } ; java . lang . String [ ] getConditions = new java . lang . String [ ] { "===><sp>WorkerUnit<sp>created<sp><===" 2 } ; int jobID = brokerUnit . addJob ( org . ourgrid . system . units . UnitUtil . buildASmallSleepJobWithPuts ( 1 , putConditions , putSources , playpenDests , getConditions , getDests ) ) ; System . out . println ( "===><sp>job<sp>added<sp><===" ) ; conditionExpecter . waitUntilConditionIsMet ( new org . ourgrid . system . condition . BrokerJobFinishedCondition ( brokerUnit , jobID ) ) ; System . out . println ( "===><sp>WorkerUnit<sp>created<sp><===" 0 ) ; java . io . File getFile = new java . io . File ( getDests [ 0 ] ) ; org . junit . Assert . assertFalse ( getFile . exists ( ) ) ; System . out . println ( "===><sp>WorkerUnit<sp>created<sp><===" 1 ) ; } }
public class aTest{ @Test public void testCreateLogicalRouterApiExceptionRollbackRouterAndSwitchPort ( ) { resource . configure ( "NiciraNvpResource" , parameters ) ; final com . cloud . network . nicira . LogicalRouter lrc = mock ( com . cloud . network . nicira . LogicalRouter . class ) ; final com . cloud . network . nicira . LogicalRouterPort lrp = mock ( com . cloud . network . nicira . LogicalRouterPort . class ) ; final com . cloud . network . nicira . LogicalSwitchPort lsp = mock ( com . cloud . network . nicira . LogicalSwitchPort . class ) ; when ( lrc . getUuid ( ) ) . thenReturn ( "ccccc" ) ; when ( lrp . getUuid ( ) ) . thenReturn ( "ddddd" ) . thenReturn ( "eeeee" ) ; when ( lsp . getUuid ( ) ) . thenReturn ( "fffff" ) ; when ( nvpApi . createLogicalRouter ( ( ( com . cloud . network . nicira . LogicalRouter ) ( any ( ) ) ) ) ) . thenReturn ( lrc ) ; when ( nvpApi . createLogicalRouterPort ( eq ( "ccccc" ) , ( ( com . cloud . network . nicira . LogicalRouterPort ) ( any ( ) ) ) ) ) . thenReturn ( lrp ) ; when ( nvpApi . createLogicalSwitchPort ( eq ( "lrouter" 0 ) , ( ( com . cloud . network . nicira . LogicalSwitchPort ) ( any ( ) ) ) ) ) . thenReturn ( lsp ) ; when ( nvpApi . createLogicalRouterNatRule ( ( ( java . lang . String ) ( any ( ) ) ) , ( ( com . cloud . network . nicira . NatRule ) ( any ( ) ) ) ) ) . thenThrow ( new com . cloud . network . nicira . NiciraNvpApiException ( ) ) ; final com . cloud . agent . api . CreateLogicalRouterCommand clrc = new com . cloud . agent . api . CreateLogicalRouterCommand ( "aaaaa" , 50 , "lrouter" 0 , "lrouter" , "publiccidr" , "nexthop" , "lrouter" 1 , "owner" ) ; final com . cloud . agent . api . CreateLogicalRouterAnswer clra = ( ( com . cloud . agent . api . CreateLogicalRouterAnswer ) ( resource . executeRequest ( clrc ) ) ) ; org . junit . Assert . assertFalse ( clra . getResult ( ) ) ; verify ( nvpApi , atLeast ( 1 ) ) . deleteLogicalRouter ( eq ( "ccccc" ) ) ; verify ( nvpApi , atLeast ( 1 ) ) . deleteLogicalSwitchPort ( eq ( "lrouter" 0 ) , eq ( "fffff" ) ) ; } }
public class aTest{ @Test public void testBrokerExchangeSendAuthorized ( ) { java . lang . String name = "myTopic" ; org . apache . activemq . command . ActiveMQDestination dest = new org . apache . activemq . command . ActiveMQTopic ( name ) ; org . apache . activemq . command . ActiveMQTextMessage message = new org . apache . activemq . command . ActiveMQTextMessage ( ) ; message . setDestination ( dest ) ; message . setText ( "Hello,<sp>world!" ) ; org . apache . shiro . subject . Subject subject = new org . apache . activemq . shiro . authz . AuthorizationFilterTest . PermsSubject ( ) { @ org . apache . activemq . shiro . authz . Override public boolean isPermitted ( org . apache . shiro . authz . Permission toCheck ) { org . apache . shiro . authz . Permission assigned = createPerm ( "topic:myTopic:write" ) ; org . junit . Assert . assertEquals ( assigned . toString ( ) , toCheck . toString ( ) ) ; return assigned . implies ( toCheck ) ; } } }
public class aTest{ @Test public void testSkip ( ) { fontReader . skip ( 100 ) ; org . junit . Assert . assertEquals ( 100 , fontReader . readTTFByte ( ) ) ; try { fontReader . skip ( 156 ) ; org . junit . Assert . fail ( "FileFontReaderTest<sp>Failed<sp>testSkip" ) ; } }
public class aTest{ @Test public void test_instrumentation_expection_fallthrough ( ) { org . gridkit . vicluster . ViNode node = node ( "test_instrumentation_expection_fallthrough" ) ; org . gridkit . nanocloud . interceptor . Intercept . callSite ( ) . onTypes ( org . gridkit . vicluster . isolate . InstrumentationFeatureTest . class ) . onMethod ( "explode" ) . doInvoke ( new org . gridkit . vicluster . isolate . InstrumentationFeatureTest . LongReturnValueShifter ( ( - 111111 ) ) ) . apply ( node ) ; node . exec ( new java . util . concurrent . Callable < java . lang . Void > ( ) { @ org . gridkit . vicluster . isolate . Override public org . gridkit . vicluster . isolate . Void call ( ) throws org . gridkit . vicluster . isolate . Exception { try { org . gridkit . vicluster . isolate . InstrumentationFeatureTest . explode ( "test" ) ; org . junit . Assert . fail ( "Exception<sp>expected" ) ; } catch ( java . lang . IllegalStateException e ) { org . junit . Assert . assertEquals ( "test" , e . getMessage ( ) ) ; } }
public class aTest{ @Test public void testIsUnique4 ( ) { org . dresdenocl . modelinstancetype . test . tests . TestModelInstanceCollection . msg = org . dresdenocl . modelinstancetype . test . msg . ModelInstanceTypeTestSuiteMessages . TestModelInstanceCollection_IsUniqueIsWrong ; org . dresdenocl . modelinstancetype . test . tests . TestModelInstanceCollection . msg = org . eclipse . osgi . util . NLS . bind ( org . dresdenocl . modelinstancetype . test . tests . TestModelInstanceCollection . msg , org . dresdenocl . modelinstancetype . test . tests . TestModelInstanceCollection . type_set , "" ) ; for ( org . dresdenocl . modelinstancetype . types . IModelInstanceCollection < ? > aCollection : org . dresdenocl . modelinstancetype . test . tests . TestModelInstanceCollection . instances_set ) { org . junit . Assert . assertTrue ( org . dresdenocl . modelinstancetype . test . tests . TestModelInstanceCollection . msg , aCollection . isUnique ( ) ) ; } }
public class aTest{ @Test public void testJavaSourceFileAndDrlDeploy ( ) { final java . lang . String java = ">\n" 3 + "public<sp>class<sp>JavaSourceMessage<sp>{<sp>}\n" ; final java . lang . String drl = ">\n" 3 + ( ( ( ( ">\n" 2 + "kbase1/drl1.drl" 2 ) + "kbase1/drl1.drl" 0 ) + "then\n" ) + "end\n" ) ; final java . lang . String kmodule = ">\n" 5 http : final org . kie . api . KieServices ks = KieServices . Factory . get ( ) ; final org . kie . api . builder . ReleaseId releaseId1 = ks . newReleaseId ( "org.kie" , ">\n" 7 , "1.0.0" ) ; final org . kie . api . io . Resource javaResource = org . kie . internal . io . ResourceFactory . newByteArrayResource ( java . getBytes ( ) ) . setResourceType ( ResourceType . JAVA ) . setSourcePath ( "org/drools/compiler/JavaSourceMessage.java" ) ; final org . kie . api . io . Resource drlResource = org . kie . internal . io . ResourceFactory . newByteArrayResource ( drl . getBytes ( ) ) . setResourceType ( ResourceType . DRL ) . setSourcePath ( "kbase1/drl1.drl" ) ; final org . kie . api . builder . KieModule km = createAndDeployJar ( ks , kmodule , releaseId1 , javaResource , drlResource ) ; final org . kie . api . runtime . KieContainer kieContainer = ks . newKieContainer ( km . getReleaseId ( ) ) ; try { final java . lang . Class < ? > messageClass = kieContainer . getClassLoader ( ) . loadClass ( ">\n" 9 ) ; org . junit . Assert . assertNotNull ( messageClass ) ; } }
public class aTest{ @Test public void testXPlus0MatrixMatrix ( ) { org . ujmp . core . Matrix m1 = createMatrixWithAnnotation ( 5 , 7 ) ; org . ujmp . core . Matrix m2 = createMatrixWithAnnotation ( 5 , 7 ) ; m1 . randn ( Ret . ORIG ) ; org . ujmp . core . Matrix m3 = m1 . plus ( m2 ) ; org . junit . Assert . assertEquals ( getLabel ( ) , m1 , m3 ) ; if ( m1 instanceof org . ujmp . core . interfaces . Erasable ) { ( ( org . ujmp . core . interfaces . Erasable ) ( m1 ) ) . erase ( ) ; } }
public class aTest{ @Test public void testRemoveEngine ( ) { final org . eclipse . thym . core . engine . HybridMobileEngine [ ] engines = new org . eclipse . thym . core . engine . HybridMobileEngine [ 2 ] ; engines [ 0 ] = new org . eclipse . thym . core . engine . HybridMobileEngine ( "android" , "6.0.0" , null ) ; engines [ 1 ] = new org . eclipse . thym . core . engine . HybridMobileEngine ( "ios" , "4.4.0" , null ) ; manager . updateEngines ( engines ) ; org . eclipse . thym . core . jobs . JobUtils . waitForIdle ( ) ; org . eclipse . core . resources . IWorkspaceRunnable runnable = new org . eclipse . core . resources . IWorkspaceRunnable ( ) { @ org . eclipse . thym . core . test . Override public void run ( org . eclipse . core . runtime . IProgressMonitor monitor ) throws org . eclipse . core . runtime . CoreException { manager . removeEngine ( engines [ 0 ] , monitor , true ) ; org . eclipse . thym . core . config . Widget w = org . eclipse . thym . core . config . WidgetModel . getModel ( testproject . hybridProject ( ) ) . getWidgetForRead ( ) ; org . junit . Assert . assertEquals ( 1 , w . getEngines ( ) . size ( ) ) ; checkEnginesPersistedCorrectly ( new org . eclipse . thym . core . engine . HybridMobileEngine [ ] { engines [ 1 ] } ) ; } } }
public class aTest{ @Test public void testIntegration ( ) { try { i5 . las2peer . api . p2p . ServiceNameVersion serviceNameVersion = i5 . las2peer . api . p2p . ServiceNameVersion . fromString ( "i5.las2peer.testServices.testPackage2.UsingService@1.0" ) ; i5 . las2peer . p2p . LocalNode serviceNode = new i5 . las2peer . p2p . LocalNodeManager ( ) . newNode ( "export/jars/" ) ; serviceNode . launch ( ) ; i5 . las2peer . security . ServiceAgentImpl serviceAgent = serviceNode . startService ( serviceNameVersion , "a<sp>pass" ) ; i5 . las2peer . security . ServiceAgentImpl localServiceAgent = serviceNode . getNodeServiceCache ( ) . getServiceAgentInstance ( serviceNameVersion , true , true , null ) . getServiceAgent ( ) ; org . junit . Assert . assertSame ( serviceAgent , localServiceAgent ) ; serviceNode . stopService ( serviceAgent ) ; try { serviceNode . getNodeServiceCache ( ) . getServiceAgentInstance ( serviceNameVersion , true , true , null ) ; org . junit . Assert . fail ( "AgentNotRegisteredException<sp>exptected!" ) ; } }
public class aTest{ @Test public void testSetRow ( ) { org . apache . commons . math . linear . RealMatrix m = new org . apache . commons . math . linear . BlockRealMatrix ( subTestData ) ; org . junit . Assert . assertTrue ( ( ( subRow3 [ 0 ] [ 0 ] ) != ( m . getRow ( 0 ) [ 0 ] ) ) ) ; m . setRow ( 0 , subRow3 [ 0 ] ) ; checkArrays ( subRow3 [ 0 ] , m . getRow ( 0 ) ) ; try { m . setRow ( ( - 1 ) , subRow3 [ 0 ] ) ; org . junit . Assert . fail ( "Expecting<sp>OutOfRangeException" ) ; } }
public class aTest{ @Test public void testClearInParameters ( ) { stmt . prepare ( ( ( org . eclipse . birt . report . data . oda . jdbc . StatementTest . SELECT_SQL ) + "<sp>where<sp>col6<sp>=<sp>?" ) ) ; stmt . setTimestamp ( 1 , java . sql . Timestamp . valueOf ( "2000-01-01<sp>12:00:00.0000" ) ) ; org . eclipse . birt . report . data . oda . jdbc . ResultSet rs = ( ( org . eclipse . birt . report . data . oda . jdbc . ResultSet ) ( stmt . executeQuery ( ) ) ) ; org . junit . Assert . assertTrue ( rs . next ( ) ) ; stmt . clearInParameters ( ) ; try { rs = ( ( org . eclipse . birt . report . data . oda . jdbc . ResultSet ) ( stmt . executeQuery ( ) ) ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void testGetSourceClusterAction ( ) { java . util . Map < java . lang . String , java . lang . String > props = new java . util . HashMap ( ) ; org . apache . ambari . server . controller . ivory . Feed feed = new org . apache . ambari . server . controller . ivory . Feed ( "Feed1" , "d" , "s" , "sch" , "source" , "st" , "end" , "l" , "sa" , "a" 0 , "st" , "end" , "l" , "a" , props ) ; org . junit . Assert . assertEquals ( "sa" , feed . getSourceClusterAction ( ) ) ; } }
public class aTest{ @Test public void testFacetQueryPerformanceWithMins ( ) { System . out . println ( "testFacetQueryPerformanceWithMins" ) ; int facetCount = 200 ; org . apache . lucene . index . IndexReader reader = createIndex ( _docCount , facetCount , false ) ; org . apache . lucene . search . Query [ ] facets = new org . apache . lucene . search . Query [ facetCount ] ; for ( int i = 0 ; i < facetCount ; i ++ ) { facets [ i ] = new org . apache . lucene . search . TermQuery ( new org . apache . lucene . index . Term ( ( "facet" + i ) , "value" ) ) ; } long min = 1000 ; long [ ] minimumsBeforeReturning = new long [ facets . length ] ; for ( int i = 0 ; i < ( minimumsBeforeReturning . length ) ; i ++ ) { minimumsBeforeReturning [ i ] = min ; } java . util . concurrent . ExecutorService executor = null ; try { for ( int t = 0 ; t < 5 ; t ++ ) { executor = getThreadPool ( 10 ) ; org . apache . lucene . search . IndexSearcher indexSearcher = new org . apache . lucene . search . IndexSearcher ( reader , executor ) ; org . apache . blur . lucene . search . FacetExecutor facetExecutor = new org . apache . blur . lucene . search . FacetExecutor ( facets . length , minimumsBeforeReturning ) ; org . apache . blur . lucene . search . FacetQuery facetQuery = new org . apache . blur . lucene . search . FacetQuery ( new org . apache . lucene . search . TermQuery ( new org . apache . lucene . index . Term ( "f1" , "value" ) ) , facets , facetExecutor ) ; long t1 = java . lang . System . nanoTime ( ) ; indexSearcher . search ( facetQuery , 10 ) ; facetExecutor . processFacets ( executor ) ; executor . shutdown ( ) ; executor . awaitTermination ( 10 , TimeUnit . SECONDS ) ; long t2 = java . lang . System . nanoTime ( ) ; System . out . println ( ( ( t2 - t1 ) / 1000000.0 ) ) ; for ( int i = 0 ; i < ( facetExecutor . length ( ) ) ; i ++ ) { org . junit . Assert . assertTrue ( ( ( facetExecutor . get ( i ) ) >= min ) ) ; } } } }
public class aTest{ @Test public void recursiveDelegateMethodWithInvocationParameterNotUsedForProceeding ( mockit . ReentrantDelegateTest$RealClass ) { new mockit . Expectations ( ) { { rc . nonRecursiveMethod ( anyInt ) ; result = new mockit . Delegate ( ) { @ mockit . Mock int delegate ( mockit . Invocation inv , int i ) { if ( i > 1 ) return i ; mockit . ReentrantDelegateTest . RealClass it = inv . getInvokedInstance ( ) ; return it . nonRecursiveMethod ( ( i + 1 ) ) ; } } ; } } ; int result = rc . nonRecursiveMethod ( 1 ) ; org . junit . Assert . assertEquals ( 2 , result ) ; } }
public class aTest{ @Test public void primitiveDeserializeString ( ) { com . google . firebase . database . MapperTest . StringBean bean = com . google . firebase . database . MapperTest . deserialize ( "{'value':<sp>'foo'}" , com . google . firebase . database . MapperTest . StringBean . class ) ; org . junit . Assert . assertEquals ( "foo" , bean . value ) ; try { com . google . firebase . database . MapperTest . deserialize ( "{'value':<sp>1.1}" , com . google . firebase . database . MapperTest . StringBean . class ) ; org . junit . Assert . fail ( "Should<sp>throw" ) ; } }
public class aTest{ @Test public void testFullRetrieval ( ) { final java . lang . String [ ] [ ] results = new java . lang . String [ ] [ ] { new java . lang . String [ ] { "[StringCol]=FooString" , "[NullableStringCol]=FooNullableFooString" , "[NullableIntCol]=6" , "[IntCol]=5" , "[NullableStringCol]=FooNullableFooString" 1 } , new java . lang . String [ ] { "[StringCol]=Foo2String" , "[NullableStringCol]=null" , "[NullableIntCol]=null" , "[IntCol]=4" , "[NullableStringCol]=FooNullableFooString" 1 } } ; final java . util . List < java . lang . String > colNames = getColumnNames ( "[NullableStringCol]=FooNullableFooString" 0 ) ; final java . sql . ResultSet rset = stmt . executeQuery ( "select<sp>*<sp>from<sp>fooTable" ) ; int i = 0 ; while ( rset . next ( ) ) { final java . util . List < java . lang . String > lst = java . util . Arrays . asList ( results [ i ] ) ; for ( final java . lang . String colName : colNames ) { lst . contains ( java . lang . String . format ( "[%s]=%s" , colName , rset . getString ( colName ) ) ) ; } i ++ ; } org . junit . Assert . assertEquals ( 2 , i ) ; rset . close ( ) ; } }
public class aTest{ @Test public void testSynchronizedPoolKeyedObjectPool ( ) { try { org . apache . commons . pool2 . PoolUtils . synchronizedPool ( ( ( org . apache . commons . pool2 . KeyedObjectPool < java . lang . Object , java . lang . Object > ) ( null ) ) ) ; org . junit . Assert . fail ( "PoolUtils.synchronizedPool(KeyedObjectPool)<sp>must<sp>not<sp>allow<sp>a<sp>null<sp>pool." ) ; } catch ( final java . lang . IllegalArgumentException iae ) { } final java . util . List < java . lang . String > calledMethods = new java . util . ArrayList ( ) ; try ( @ org . apache . commons . pool2 . SuppressWarnings ( "unchecked" ) final org . apache . commons . pool2 . KeyedObjectPool < java . lang . Object , java . lang . Object > kop = org . apache . commons . pool2 . TestPoolUtils . createProxy ( org . apache . commons . pool2 . KeyedObjectPool . class , calledMethods ) ) { final org . apache . commons . pool2 . KeyedObjectPool < java . lang . Object , java . lang . Object > skop = org . apache . commons . pool2 . PoolUtils . synchronizedPool ( kop ) ; final java . util . List < java . lang . String > expectedMethods = org . apache . commons . pool2 . TestPoolUtils . invokeEveryMethod ( skop ) ; org . junit . Assert . assertEquals ( expectedMethods , calledMethods ) ; } }
public class aTest{ @Test public void testServletRoute ( ) { org . apache . camel . CamelContext camelctx = new org . apache . camel . impl . DefaultCamelContext ( ) ; camelctx . addRoutes ( new org . apache . camel . builder . RouteBuilder ( ) { @ org . wildfly . camel . test . servlet . Override public void configure ( ) throws org . wildfly . camel . test . servlet . Exception { from ( "servlet://hello?matchOnUriPrefix=true" ) . process ( new org . apache . camel . Processor ( ) { @ org . wildfly . camel . test . servlet . Override public void process ( org . apache . camel . Exchange exchange ) throws org . wildfly . camel . test . servlet . Exception { exchange . getOut ( ) . setBody ( "Hello<sp>Kermit" ) ; } } ) ; } } ) ; camelctx . start ( ) ; try { org . wildfly . camel . test . common . http . HttpRequest . HttpResponse result = org . wildfly . camel . test . common . http . HttpRequest . get ( "http://localhost:8080/camel/services/hello" ) . getResponse ( ) ; org . junit . Assert . assertEquals ( "Hello<sp>Kermit" , result . getBody ( ) ) ; } }
public class aTest{ @Test public void testLongWithUnexpectedArgument1 ( ) { final java . lang . String [ ] args = new java . lang . String [ ] { "--foo=bar" } ; final org . apache . commons . cli . Options options = new org . apache . commons . cli . Options ( ) ; options . addOption ( org . apache . commons . cli . OptionBuilder . withLongOpt ( "foo" ) . create ( 'f' ) ) ; try { parser . parse ( options , args ) ; } catch ( final org . apache . commons . cli . UnrecognizedOptionException e ) { org . junit . Assert . assertEquals ( "--foo=bar" , e . getOption ( ) ) ; return ; } }
public class aTest{ @Test public void singleFieldTest ( ) { if ( org . apache . hyracks . storage . am . bloomfilter . LOGGER . isInfoEnabled ( ) ) { org . apache . hyracks . storage . am . bloomfilter . LOGGER . info ( "TESTING<sp>BLOOM<sp>FILTER" ) ; } org . apache . hyracks . storage . common . buffercache . IBufferCache bufferCache = harness . getBufferCache ( ) ; int numElements = 100 ; int [ ] keyFields = new int [ ] { 0 } ; org . apache . hyracks . storage . am . bloomfilter . impls . BloomFilter bf = new org . apache . hyracks . storage . am . bloomfilter . impls . BloomFilter ( bufferCache , harness . getFileReference ( ) , keyFields ) ; double acceptanleFalsePositiveRate = 0.1 ; int maxBucketsPerElement = org . apache . hyracks . storage . am . bloomfilter . impls . BloomCalculations . maxBucketsPerElement ( numElements ) ; org . apache . hyracks . storage . am . bloomfilter . impls . BloomFilterSpecification bloomFilterSpec = org . apache . hyracks . storage . am . bloomfilter . impls . BloomCalculations . computeBloomSpec ( maxBucketsPerElement , acceptanleFalsePositiveRate ) ; bf . create ( ) ; bf . activate ( ) ; org . apache . hyracks . storage . common . IIndexBulkLoader builder = bf . createBuilder ( numElements , bloomFilterSpec . getNumHashes ( ) , bloomFilterSpec . getNumBucketsPerElements ( ) ) ; int fieldCount = 2 ; org . apache . hyracks . dataflow . common . comm . io . ArrayTupleBuilder tupleBuilder = new org . apache . hyracks . dataflow . common . comm . io . ArrayTupleBuilder ( fieldCount ) ; org . apache . hyracks . dataflow . common . comm . io . ArrayTupleReference tuple = new org . apache . hyracks . dataflow . common . comm . io . ArrayTupleReference ( ) ; int maxKey = 1000 ; java . util . TreeSet < java . lang . Integer > uniqueKeys = new java . util . TreeSet ( ) ; java . util . ArrayList < java . lang . Integer > keys = new java . util . ArrayList ( ) ; while ( ( uniqueKeys . size ( ) ) < numElements ) { int key = ( rnd . nextInt ( ) ) % maxKey ; uniqueKeys . add ( key ) ; } for ( java . lang . Integer i : uniqueKeys ) { keys . add ( i ) ; } for ( int i = 0 ; i < ( keys . size ( ) ) ; ++ i ) { org . apache . hyracks . dataflow . common . utils . TupleUtils . createIntegerTuple ( tupleBuilder , tuple , keys . get ( i ) , i ) ; builder . add ( tuple ) ; } builder . end ( ) ; bf . pinAllPages ( ) ; long [ ] hashes = org . apache . hyracks . storage . am . bloomfilter . impls . BloomFilter . createHashArray ( ) ; for ( int i = 0 ; i < ( keys . size ( ) ) ; ++ i ) { org . apache . hyracks . dataflow . common . utils . TupleUtils . createIntegerTuple ( tupleBuilder , tuple , keys . get ( i ) , i ) ; org . junit . Assert . assertTrue ( bf . contains ( tuple , hashes ) ) ; } }
public class aTest{ @Test public void testStripCrecentialsFromOriginUrl ( java . lang . String , java . lang . String ) { pl . project13 . maven . git . GitDataProvider gitDataProvider = mock ( pl . project13 . maven . git . GitDataProvider . class ) ; when ( gitDataProvider . stripCredentialsFromOriginUrl ( org . mockito . ArgumentMatchers . any ( ) ) ) . thenCallRealMethod ( ) ; java . lang . String result = gitDataProvider . stripCredentialsFromOriginUrl ( input ) ; org . junit . Assert . assertEquals ( expected , result ) ; } }
public class aTest{ @Test public void testLexerTwoRules ( ) { org . antlr . v4 . tool . LexerGrammar lg = new org . antlr . v4 . tool . LexerGrammar ( ( "mode<sp>0:0\n" 6 + ( "A<sp>:<sp>\'a\'<sp>;\n" + "mode<sp>0:0\n" 1 ) ) ) ; java . lang . String expecting = "max<sp>type<sp>2\n" + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "mode<sp>0:0\n" 7 + "1:RULE_START<sp>0\n" ) + "2:RULE_STOP<sp>0\n" ) + "3:RULE_START<sp>1\n" ) + "mode<sp>0:0\n" 0 ) + "1:RULE_START<sp>0\n" 0 ) + "1:RULE_START<sp>0\n" 3 ) + "mode<sp>0:0\n" 4 ) + "8:BASIC<sp>1\n" ) + "rule<sp>0:1<sp>1\n" ) + "mode<sp>0:0\n" 9 ) + "mode<sp>0:0\n" ) + "mode<sp>0:0\n" 8 ) + "mode<sp>0:0\n" 2 ) + "1:RULE_START<sp>0\n" 4 ) + "1:RULE_START<sp>0\n" 1 ) + "5->6<sp>ATOM<sp>97,0,0\n" ) + "6->2<sp>EPSILON<sp>0,0,0\n" ) + "mode<sp>0:0\n" 5 ) + "1:RULE_START<sp>0\n" 2 ) + "mode<sp>0:0\n" 3 ) ; org . antlr . v4 . runtime . atn . ATN atn = createATN ( lg , true ) ; java . lang . String result = org . antlr . v4 . runtime . atn . ATNSerializer . getDecoded ( atn , java . util . Arrays . asList ( lg . getRuleNames ( ) ) , java . util . Arrays . asList ( lg . getTokenNames ( ) ) ) ; org . junit . Assert . assertEquals ( expecting , result ) ; } }
public class aTest{ @Test public void testTicketCallback ( ) { wechat . js ( ) . getTicket ( accessToken , TicketType . CARD , new me . hao0 . wechat . core . Callback < me . hao0 . wechat . model . js . Ticket > ( ) { @ me . hao0 . wechat . Override public void onSuccess ( me . hao0 . wechat . model . js . Ticket ticket ) { org . junit . Assert . assertNotNull ( ticket ) ; System . out . println ( ticket ) ; } }
public class aTest{ @Test public void testGetClusterTable ( ) { org . apache . blur . manager . clusterstatus . ZookeeperClusterStatusTest . LOG . warn ( "testGetCluster" ) ; createTable ( org . apache . blur . manager . clusterstatus . ZookeeperClusterStatusTest . TEST ) ; org . junit . Assert . assertEquals ( org . apache . blur . manager . clusterstatus . ZookeeperClusterStatusTest . DEFAULT , clusterStatus2 . getCluster ( false , org . apache . blur . manager . clusterstatus . ZookeeperClusterStatusTest . TEST ) ) ; new org . apache . blur . manager . clusterstatus . ZookeeperClusterStatusTest . WaitForAnswerToBeCorrect ( 20L ) { @ org . apache . blur . manager . clusterstatus . Override public java . lang . Object run ( ) { return clusterStatus2 . getCluster ( true , org . apache . blur . manager . clusterstatus . ZookeeperClusterStatusTest . TEST ) ; } } }
public class aTest{ @Test public void testCollisionWithSingleMerge ( ) { try { i5 . las2peer . p2p . PastryNodeImpl node1 = nodes . get ( 0 ) ; i5 . las2peer . security . UserAgentImpl smith = i5 . las2peer . testing . MockAgentFactory . getAdam ( ) ; smith . unlock ( "adamspass" ) ; i5 . las2peer . persistency . EnvelopeVersion envelope1 = node1 . createUnencryptedEnvelope ( "test" , smith . getPublicKey ( ) , "Successfully<sp>stored<sp>artifact<sp>" 2 ) ; i5 . las2peer . persistency . EnvelopeVersion envelope2 = node1 . createUnencryptedEnvelope ( "test" , smith . getPublicKey ( ) , "Hello<sp>World<sp>2!" ) ; node1 . storeEnvelopeAsync ( envelope1 , smith , new i5 . las2peer . persistency . StorageStoreResultHandler ( ) { @ i5 . las2peer . persistency . Override public void onResult ( java . io . Serializable serializable , int successfulOperations ) { System . out . println ( ( ( "Successfully<sp>stored<sp>artifact<sp>" + successfulOperations ) + "<sp>times" ) ) ; node1 . storeEnvelopeAsync ( envelope2 , smith , new i5 . las2peer . persistency . StorageStoreResultHandler ( ) { @ i5 . las2peer . persistency . Override public void onResult ( java . io . Serializable serializable , int successfulOperations ) { if ( successfulOperations > 0 ) { System . out . println ( ( ( "Successfully<sp>stored<sp>artifact<sp>" + successfulOperations ) + "<sp>times" ) ) ; System . out . println ( "Successfully<sp>stored<sp>artifact<sp>" 5 ) ; node1 . fetchEnvelopeAsync ( "test" , new i5 . las2peer . persistency . StorageEnvelopeHandler ( ) { @ i5 . las2peer . persistency . Override public void onEnvelopeReceived ( i5 . las2peer . persistency . EnvelopeVersion envelope ) { try { org . junit . Assert . assertEquals ( 2 , envelope . getVersion ( ) ) ; java . lang . String content = ( ( java . lang . String ) ( envelope . getContent ( ) ) ) ; System . out . println ( ( ( "Successfully<sp>stored<sp>artifact<sp>" 4 + content ) + "'" ) ) ; asyncTestState = true ; } }
public class aTest{ @Test public void testVersionSafety ( ) { try { i5 . las2peer . p2p . PastryNodeImpl node1 = nodes . get ( 0 ) ; i5 . las2peer . security . UserAgentImpl smith = i5 . las2peer . testing . MockAgentFactory . getAdam ( ) ; smith . unlock ( "adamspass" ) ; i5 . las2peer . persistency . EnvelopeVersion env = node1 . createUnencryptedEnvelope ( "test" , smith . getPublicKey ( ) , "This<sp>is<sp>las2peer!" ) ; node1 . storeEnvelope ( env , smith ) ; i5 . las2peer . p2p . PastryNodeImpl node2 = nodes . remove ( 1 ) ; node2 . shutDown ( ) ; i5 . las2peer . persistency . EnvelopeVersion updated = node1 . createUnencryptedEnvelope ( env , "This<sp>is<sp>las2peer<sp>again!" ) ; long start = java . lang . System . currentTimeMillis ( ) ; node1 . storeEnvelope ( updated , smith ) ; long stop = java . lang . System . currentTimeMillis ( ) ; System . out . println ( ( stop - start ) ) ; node2 = i5 . las2peer . testing . TestSuite . addNode ( node1 . getPort ( ) , STORAGE_MODE . FILESYSTEM , 1L ) ; nodes . set ( 1 , node2 ) ; i5 . las2peer . persistency . EnvelopeVersion fetched = node2 . fetchEnvelope ( "test" ) ; org . junit . Assert . assertEquals ( updated . getContent ( ) , fetched . getContent ( ) ) ; } }
public class aTest{ @Test public void testGenerateLocalizedXLIFF ( ) { createTestData ( ) ; com . box . l10n . mojito . service . tm . TMTextUnit tmTextUnit1 = tmService . addTMTextUnit ( tmId , assetId , "Application<sp>Name" 0 , "Application<sp>Name" , "This<sp>text<sp>is<sp>shown<sp>in<sp>the<sp>start<sp>screen<sp>of<sp>the<sp>application.<sp>Keep<sp>it<sp>short." ) ; com . box . l10n . mojito . service . tm . TMTextUnit tmTextUnit2 = tmService . addTMTextUnit ( tmId , assetId , "home" , "Home" , "This<sp>is<sp>the<sp>text<sp>displayed<sp>in<sp>the<sp>link<sp>that<sp>takes<sp>the<sp>user<sp>to<sp>the<sp>home<sp>page." ) ; com . box . l10n . mojito . service . tm . TMTextUnit tmTextUnit3 = tmService . addTMTextUnit ( tmId , assetId , "fail_integrity_check" , "I<sp>fail<sp>integrity<sp>check" , null ) ; com . box . l10n . mojito . service . tm . RepositoryLocale repositoryLocale = repositoryLocaleRepository . findByRepositoryAndLocale_Bcp47Tag ( repository , "fr-FR" ) ; com . box . l10n . mojito . service . tm . Locale locale = repositoryLocale . getLocale ( ) ; tmService . addCurrentTMTextUnitVariant ( tmTextUnit1 . getId ( ) , locale . getId ( ) , "Application<sp>Name" 1 ) ; com . box . l10n . mojito . service . tm . TMTextUnitVariant variant1 = tmService . addCurrentTMTextUnitVariant ( tmTextUnit1 . getId ( ) , locale . getId ( ) , "Nom<sp>de<sp>l'application" ) ; tmService . addTMTextUnitCurrentVariant ( tmTextUnit3 . getId ( ) , locale . getId ( ) , "!?!?!?!?!" , null , TMTextUnitVariant . Status . REVIEW_NEEDED , false ) ; java . lang . String sourceXLIFF = getSourceXLIFFContent ( com . google . common . collect . Lists . newArrayList ( tmTextUnit1 , tmTextUnit2 , tmTextUnit3 ) ) ; java . lang . String localizedAsset = tmService . generateLocalized ( asset , sourceXLIFF , repositoryLocale , null , null , null , Status . ALL , InheritanceMode . USE_PARENT ) ; java . lang . String expectedLocalizedXLIFF = getExpectedLocalizedXLIFFContent ( locale . getBcp47Tag ( ) , tmTextUnit1 , tmTextUnit2 , tmTextUnit3 , variant1 ) ; org . junit . Assert . assertEquals ( removeLeadingAndTrailingSpacesOnEveryLine ( expectedLocalizedXLIFF ) , removeLeadingAndTrailingSpacesOnEveryLine ( localizedAsset ) ) ; } }
public class aTest{ @Test public void testCreation ( ) { org . ccnx . ccn . impl . support . Log . info ( Log . FAC_TEST , "Starting<sp>testCreation" ) ; java . lang . Integer faceID = new java . lang . Integer ( ( - 142 ) ) ; org . ccnx . ccn . profiles . ccnd . FaceManager mgr = null ; try { mgr = new org . ccnx . ccn . profiles . ccnd . FaceManager ( putHandle ) ; faceID = mgr . createFace ( NetworkProtocol . UDP , "10.1.1.1" , new java . lang . Integer ( org . ccnx . ccn . impl . CCNNetworkManager . DEFAULT_AGENT_PORT ) ) ; System . out . println ( ( "Created<sp>face:<sp>" + faceID ) ) ; } catch ( org . ccnx . ccn . profiles . ccnd . CCNDaemonException e ) { System . out . println ( ( ( ( "Exception<sp>" + ( e . getClass ( ) . getName ( ) ) ) + ",<sp>message:<sp>" ) + ( e . getMessage ( ) ) ) ) ; System . out . println ( "Failed<sp>to<sp>create<sp>face." ) ; e . printStackTrace ( ) ; org . junit . Assert . fail ( "Failed<sp>to<sp>create<sp>face." ) ; } org . junit . Assert . assertNotNull ( mgr ) ; try { mgr . deleteFace ( faceID ) ; } }
public class aTest{ @Test public void testRemoveEntities_3 ( ) { try { java . io . StringReader reader = new java . io . StringReader ( "abcdefg" ) ; java . io . StringWriter writer = new java . io . StringWriter ( ) ; org . milyn . xml . XmlUtil . removeEntities ( reader , writer ) ; org . junit . Assert . assertEquals ( "abcdefg" , writer . toString ( ) ) ; } }
public class aTest{ @Test public void testNotDuplicatedDisplayedMnemonic ( ) { es . gob . afirma . ui . principal . EnsobradoAccessibilityTest . logger . info ( "testNotDuplicatedDisplayedMnemonic" ) ; try { final es . gob . afirma . ui . principal . Ensobrado ensobradoPanel = new es . gob . afirma . ui . principal . Ensobrado ( ) ; final java . util . List < java . lang . Integer > keyCodes = new java . util . ArrayList ( ) ; java . util . Set < java . lang . Integer > keyCodesSet = null ; getKeyCodeList ( ensobradoPanel , keyCodes ) ; keyCodesSet = new java . util . HashSet ( keyCodes ) ; org . junit . Assert . assertTrue ( ( ( keyCodesSet . size ( ) ) == ( keyCodes . size ( ) ) ) ) ; } }
public class aTest{ @Test public void testS3OutputModule ( ) { com . amazonaws . services . s3 . model . InitiateMultipartUploadResult result = new com . amazonaws . services . s3 . model . InitiateMultipartUploadResult ( ) ; result . setUploadId ( uploadId ) ; com . amazonaws . services . s3 . model . PutObjectResult objResult = new com . amazonaws . services . s3 . model . PutObjectResult ( ) ; objResult . setETag ( "SuccessFullyUploaded" ) ; com . amazonaws . services . s3 . model . UploadPartResult partResult = new com . amazonaws . services . s3 . model . UploadPartResult ( ) ; partResult . setPartNumber ( 1 ) ; partResult . setETag ( "SuccessFullyPartUploaded" ) ; org . mockito . MockitoAnnotations . initMocks ( this ) ; when ( org . apache . apex . malhar . lib . fs . s3 . S3OutputModuleMockTest . client . initiateMultipartUpload ( any ( com . amazonaws . services . s3 . model . InitiateMultipartUploadRequest . class ) ) ) . thenReturn ( result ) ; when ( org . apache . apex . malhar . lib . fs . s3 . S3OutputModuleMockTest . client . putObject ( any ( com . amazonaws . services . s3 . model . PutObjectRequest . class ) ) ) . thenReturn ( objResult ) ; when ( org . apache . apex . malhar . lib . fs . s3 . S3OutputModuleMockTest . client . uploadPart ( any ( com . amazonaws . services . s3 . model . UploadPartRequest . class ) ) ) . thenReturn ( partResult ) ; when ( org . apache . apex . malhar . lib . fs . s3 . S3OutputModuleMockTest . client . completeMultipartUpload ( any ( com . amazonaws . services . s3 . model . CompleteMultipartUploadRequest . class ) ) ) . thenReturn ( completeMultiPart ( ) ) ; org . apache . apex . malhar . lib . fs . s3 . S3OutputModuleMockTest . Application app = new org . apache . apex . malhar . lib . fs . s3 . S3OutputModuleMockTest . Application ( ) ; org . apache . hadoop . conf . Configuration conf = new org . apache . hadoop . conf . Configuration ( ) ; conf . set ( "dt.operator.HDFSInputModule.prop.files" , inputDir ) ; conf . set ( "dt.operator.HDFSInputModule.prop.blockSize" , "output<sp>file<sp>exist" 5 ) ; conf . set ( "dt.operator.HDFSInputModule.prop.blocksThreshold" , "output<sp>file<sp>exist" 3 ) ; conf . set ( "output<sp>file<sp>exist" 6 , "output<sp>file<sp>exist" 7 ) ; conf . set ( "output<sp>file<sp>exist" 1 , "accessKey" ) ; conf . set ( "dt.operator.S3OutputModule.prop.secretAccessKey" , "secretKey" ) ; conf . set ( "dt.operator.S3OutputModule.prop.bucketName" , "output<sp>file<sp>exist" 4 ) ; conf . set ( "output<sp>file<sp>exist" 2 , outputDir ) ; org . apache . hadoop . fs . Path outDir = new org . apache . hadoop . fs . Path ( ( "output<sp>file<sp>exist" 0 + ( new java . io . File ( outputDir ) . getAbsolutePath ( ) ) ) ) ; final org . apache . hadoop . fs . Path outputFilePath = new org . apache . hadoop . fs . Path ( ( ( ( outDir . toString ( ) ) + ( java . io . File . separator ) ) + ( org . apache . apex . malhar . lib . fs . s3 . S3OutputModuleMockTest . FILE ) ) ) ; final org . apache . hadoop . fs . FileSystem fs = org . apache . hadoop . fs . FileSystem . newInstance ( outDir . toUri ( ) , new org . apache . hadoop . conf . Configuration ( ) ) ; com . datatorrent . api . LocalMode lma = com . datatorrent . api . LocalMode . newInstance ( ) ; lma . prepareDAG ( app , conf ) ; com . datatorrent . api . LocalMode . Controller lc = lma . getController ( ) ; lc . setHeartbeatMonitoringEnabled ( true ) ; ( ( com . datatorrent . stram . StramLocalCluster ) ( lc ) ) . setExitCondition ( new java . util . concurrent . Callable < java . lang . Boolean > ( ) { @ org . apache . apex . malhar . lib . fs . s3 . Override public org . apache . apex . malhar . lib . fs . s3 . Boolean call ( ) throws org . apache . apex . malhar . lib . fs . s3 . Exception { return fs . exists ( outputFilePath ) ; } } ) ; lc . run ( 10000 ) ; org . junit . Assert . assertTrue ( "output<sp>file<sp>exist" , fs . exists ( outputFilePath ) ) ; } }
public class aTest{ @Test public void testDoUpdateForClusterCreate_SingleHostProperty__exportedValue_UsingMinusSymbolInHostGroupName ( ) { java . util . Map < java . lang . String , java . util . Map < java . lang . String , java . lang . String > > properties = new java . util . HashMap ( ) ; java . util . Map < java . lang . String , java . lang . String > typeProps = new java . util . HashMap ( ) ; typeProps . put ( "yarn.resourcemanager.hostname" , "%HOSTGROUP::os-amb-r6-secha-1427972156-hbaseha-3-6%" ) ; properties . put ( "yarn-site" , typeProps ) ; org . apache . ambari . server . topology . Configuration clusterConfig = new org . apache . ambari . server . topology . Configuration ( properties , java . util . Collections . emptyMap ( ) ) ; java . util . Collection < java . lang . String > hgComponents = new java . util . HashSet ( ) ; hgComponents . add ( "NAMENODE" ) ; hgComponents . add ( "SECONDARY_NAMENODE" ) ; hgComponents . add ( "%HOSTGROUP::os-amb-r6-secha-1427972156-hbaseha-3-6%" 0 ) ; org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup group1 = new org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup ( "%HOSTGROUP::os-amb-r6-secha-1427972156-hbaseha-3-6%" 1 , hgComponents , java . util . Collections . singleton ( "testhost" ) ) ; java . util . Collection < java . lang . String > hgComponents2 = new java . util . HashSet ( ) ; hgComponents2 . add ( "DATANODE" ) ; hgComponents2 . add ( "HDFS_CLIENT" ) ; org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup group2 = new org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup ( "group2" , hgComponents2 , java . util . Collections . singleton ( "testhost2" ) ) ; java . util . Collection < org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup > hostGroups = new java . util . HashSet ( ) ; hostGroups . add ( group1 ) ; hostGroups . add ( group2 ) ; org . apache . ambari . server . topology . ClusterTopology topology = createClusterTopology ( bp , clusterConfig , hostGroups ) ; org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessor updater = new org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessor ( topology ) ; updater . doUpdateForClusterCreate ( ) ; java . lang . String updatedVal = topology . getConfiguration ( ) . getFullProperties ( ) . get ( "yarn-site" ) . get ( "yarn.resourcemanager.hostname" ) ; org . junit . Assert . assertEquals ( "testhost" , updatedVal ) ; } }
public class aTest{ @Test public void testDelete ( ) { java . lang . StringBuilder requestBuilder = new java . lang . StringBuilder ( ) ; requestBuilder . append ( ( ( "GET<sp>" + ( HttpServerTest . URI ) ) + "<sp>HTTP/1.1" ) ) . append ( java . lang . System . getProperty ( "line.separator" ) ) . append ( "Cookie:<sp>theme=light;<sp>sessionToken=abc123" ) ; java . io . ByteArrayInputStream inputStream = new java . io . ByteArrayInputStream ( requestBuilder . toString ( ) . getBytes ( ) ) ; java . io . ByteArrayOutputStream outputStream = new java . io . ByteArrayOutputStream ( ) ; org . nanohttpd . protocols . http . HTTPSession session = this . testServer . createSession ( this . tempFileManager , inputStream , outputStream ) ; session . execute ( ) ; org . nanohttpd . protocols . http . content . CookieHandler cookieHandler = session . getCookies ( ) ; org . nanohttpd . protocols . http . response . Response response = org . nanohttpd . protocols . http . response . Response . newFixedLengthResponse ( "" ) ; cookieHandler . delete ( "name" ) ; cookieHandler . unloadQueue ( response ) ; java . lang . String setCookieHeader = response . getCookieHeaders ( ) . get ( 0 ) ; java . text . SimpleDateFormat dateFormat = new java . text . SimpleDateFormat ( "EEE,<sp>dd<sp>MMM<sp>yyyy<sp>HH:mm:ss<sp>z" , java . util . Locale . US ) ; dateFormat . setTimeZone ( java . util . TimeZone . getTimeZone ( "GMT" ) ) ; java . lang . String dateString = setCookieHeader . split ( ";" ) [ 1 ] . split ( "=" ) [ 1 ] . trim ( ) ; java . util . Date date = dateFormat . parse ( dateString ) ; org . junit . Assert . assertTrue ( "GET<sp>" 0 , ( ( date . compareTo ( new java . util . Date ( ) ) ) < 0 ) ) ; } }
public class aTest{ @Test public void testServiceCallSuccess ( ) { queue . clear ( ) ; java . util . List < com . example . customerservice . Customer > customers = customerService . getCustomersByName ( "test" ) ; org . junit . Assert . assertEquals ( 2 , customers . size ( ) ) ; java . util . List < org . talend . esb . sam . common . event . Event > eventsList = new java . util . ArrayList < org . talend . esb . sam . common . event . Event > ( ) ; while ( ! ( queue . isEmpty ( ) ) ) { eventsList . add ( queue . remove ( ) ) ; } }
public class aTest{ @Test public void testDeleteInstanceRequest ( ) { try { com . fit2cloud . aliyun . ecs . model . request . DeleteInstanceRequest r = new com . fit2cloud . aliyun . ecs . model . request . DeleteInstanceRequest ( "i-25zh2k2lv" ) ; com . fit2cloud . aliyun . Response response = client . deleteInstance ( r ) ; System . out . println ( ( "testDeleteInstanceString<sp>::<sp>" + response ) ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testChanceReplace ( ) { java . lang . String [ ] strings = new java . lang . String [ 10000 ] ; for ( int i = 0 ; i < 10000 ; i ++ ) { strings [ i ] = regexodus . BasicTest . exampleASCII ( ) ; } regexodus . Replacer r1 = new regexodus . Pattern ( "\\w" ) . replacer ( new regexodus . ChanceSubstitution ( "!" , 0.25 , 0 ) ) ; regexodus . Replacer r2 = new regexodus . Pattern ( "\\w" , "i" ) . replacer ( new regexodus . ChanceSubstitution ( "!" , 0.25 , 0 ) ) ; java . lang . StringBuilder sb = new java . lang . StringBuilder ( 5000 ) ; java . lang . StringBuilder sb2 = new java . lang . StringBuilder ( 5000 ) ; boolean found ; for ( int i = 0 ; i < 10000 ; i ++ ) { r1 . replace ( strings [ i ] , sb ) ; r2 . replace ( strings [ i ] , sb2 ) ; org . junit . Assert . assertEquals ( sb . toString ( ) , sb2 . toString ( ) ) ; } }
public class aTest{ @Test public void testToString ( ) { toStringTest ( "/foo" 2 ) ; toStringTest ( "/foo" ) ; toStringTest ( "/foo/bar" ) ; toStringTest ( "foo" ) ; toStringTest ( "foo/bar" ) ; toStringTest ( "/foo/bar#boo" ) ; toStringTest ( "foo/bar#boo" ) ; boolean emptyException = false ; try { toStringTest ( "" ) ; } catch ( java . lang . IllegalArgumentException e ) { emptyException = true ; } org . junit . Assert . assertTrue ( emptyException ) ; if ( Path . WINDOWS ) { toStringTest ( "/foo" 4 ) ; toStringTest ( "/foo" 0 ) ; toStringTest ( "c:foo" ) ; toStringTest ( "c:foo/bar" ) ; toStringTest ( "c:foo/bar" ) ; toStringTest ( "/foo" 1 ) ; toStringTest ( "/foo" 3 ) ; toStringTest ( "C:foo/bar#boo" ) ; } } }
public class aTest{ @Test public void testVisibilityLabelsForUserWithNoAuths ( ) { java . lang . String user = "admin" ; java . lang . String [ ] auths = new java . lang . String [ ] { org . apache . hadoop . hbase . security . visibility . TestVisibilityLabelsWithACL . SECRET } ; try ( org . apache . hadoop . hbase . client . Connection conn = org . apache . hadoop . hbase . client . ConnectionFactory . createConnection ( org . apache . hadoop . hbase . security . visibility . TestVisibilityLabelsWithACL . conf ) ) { org . apache . hadoop . hbase . security . visibility . VisibilityClient . clearAuths ( conn , auths , user ) ; org . apache . hadoop . hbase . security . visibility . VisibilityClient . setAuths ( conn , auths , "user1" ) ; } org . apache . hadoop . hbase . TableName tableName = org . apache . hadoop . hbase . TableName . valueOf ( TEST_NAME . getMethodName ( ) ) ; final org . apache . hadoop . hbase . client . Table table = org . apache . hadoop . hbase . security . visibility . TestVisibilityLabelsWithACL . createTableAndWriteDataWithLabels ( tableName , org . apache . hadoop . hbase . security . visibility . TestVisibilityLabelsWithACL . SECRET ) ; org . apache . hadoop . hbase . security . access . SecureTestUtil . grantOnTable ( org . apache . hadoop . hbase . security . visibility . TestVisibilityLabelsWithACL . TEST_UTIL , org . apache . hadoop . hbase . security . visibility . TestVisibilityLabelsWithACL . NORMAL_USER1 . getShortName ( ) , tableName , null , null , Permission . Action . READ ) ; org . apache . hadoop . hbase . security . access . SecureTestUtil . grantOnTable ( org . apache . hadoop . hbase . security . visibility . TestVisibilityLabelsWithACL . TEST_UTIL , org . apache . hadoop . hbase . security . visibility . TestVisibilityLabelsWithACL . NORMAL_USER2 . getShortName ( ) , tableName , null , null , Permission . Action . READ ) ; java . security . PrivilegedExceptionAction < java . lang . Void > getAction = new java . security . PrivilegedExceptionAction < java . lang . Void > ( ) { @ org . apache . hadoop . hbase . security . visibility . Override public org . apache . hadoop . hbase . security . visibility . Void run ( ) throws org . apache . hadoop . hbase . security . visibility . Exception { org . apache . hadoop . hbase . client . Get g = new org . apache . hadoop . hbase . client . Get ( org . apache . hadoop . hbase . security . visibility . TestVisibilityLabelsWithACL . row1 ) ; g . setAuthorizations ( new org . apache . hadoop . hbase . security . visibility . Authorizations ( org . apache . hadoop . hbase . security . visibility . TestVisibilityLabelsWithACL . SECRET , org . apache . hadoop . hbase . security . visibility . TestVisibilityLabelsWithACL . CONFIDENTIAL ) ) ; try ( org . apache . hadoop . hbase . client . Connection connection = org . apache . hadoop . hbase . client . ConnectionFactory . createConnection ( org . apache . hadoop . hbase . security . visibility . TestVisibilityLabelsWithACL . conf ) ; org . apache . hadoop . hbase . client . Table t = connection . getTable ( table . getName ( ) ) ) { org . apache . hadoop . hbase . client . Result result = t . get ( g ) ; org . junit . Assert . assertTrue ( result . isEmpty ( ) ) ; } }
public class aTest{ @Test public void testSessionRegisteredCorrectly ( ) { final quickfix . SessionSettings settings = quickfix . SessionSettingsTest . setUpSession ( null ) ; settings . setString ( Session . SETTING_USE_DATA_DICTIONARY , "N" ) ; quickfix . JdbcTestSupport . setHypersonicSettings ( settings ) ; final quickfix . SessionID sessionID = new quickfix . SessionID ( "FIX.4.2" , "SENDER-sessionRegister" , "TARGET-sessionRegister" ) ; settings . setString ( sessionID , "ConnectionType" , "acceptor" ) ; final quickfix . DefaultSessionFactory factory = new quickfix . DefaultSessionFactory ( new quickfix . UnitTestApplication ( ) , new quickfix . MemoryStoreFactory ( ) , new quickfix . JdbcLogFactory ( settings ) ) ; try { try ( quickfix . Session session = factory . create ( sessionID , settings ) ) { org . junit . Assert . assertNotNull ( session ) ; } } }
public class aTest{ @Test public void testParseExceptionMissingToken ( ) { try { org . openscience . cdk . iupac . parser . NomParser . generate ( "ethol" , org . openscience . cdk . silent . SilentChemObjectBuilder . getInstance ( ) ) ; org . junit . Assert . fail ( "Molecule<sp>was<sp>successfully<sp>generated<sp>but<sp>should<sp>have<sp>thrown<sp>a<sp>ParseException" ) ; } catch ( org . openscience . cdk . iupac . parser . ParseException pe ) { org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testBadStart ( ) { java . lang . String appName = "unknown_app" ; try { int result = org . apache . hadoop . yarn . service . client . TestApiServiceClient . badAsc . actionStart ( appName ) ; org . junit . Assert . assertEquals ( org . apache . hadoop . yarn . service . client . EXIT_EXCEPTION_THROWN , result ) ; } }
public class aTest{ @Test public void testSynchronizeWithExceptionInLock ( ) { java . util . concurrent . locks . Lock [ ] locks = new java . util . concurrent . locks . Lock [ 10 ] ; for ( int i = 0 ; i < ( locks . length ) ; i ++ ) locks [ i ] = mock ( java . util . concurrent . locks . Lock . class ) ; java . lang . RuntimeException ex = new java . lang . RuntimeException ( ) ; doThrow ( ex ) . when ( locks [ 5 ] ) . lock ( ) ; try { org . apache . ignite . ml . inference . storage . model . DefaultModelStorage . synchronize ( ( ) -> { } , locks ) ; org . junit . Assert . fail ( ) ; } catch ( java . lang . RuntimeException e ) { org . junit . Assert . assertEquals ( ex , e ) ; } }
public class aTest{ @Test public void testPasteAtEndOf ( ) { search = null ; search = new org . odftoolkit . odfdom . incubator . search . TextNavigation ( "delete" , doc ) ; org . odftoolkit . odfdom . incubator . search . TextSelection sel = null ; org . odftoolkit . odfdom . incubator . search . TextNavigation search1 = new org . odftoolkit . odfdom . incubator . search . TextNavigation ( "change" , doc ) ; if ( search1 . hasNext ( ) ) { sel = ( ( org . odftoolkit . odfdom . incubator . search . TextSelection ) ( search1 . getCurrentItem ( ) ) ) ; } int i = 0 ; while ( search . hasNext ( ) ) { org . odftoolkit . odfdom . incubator . search . TextSelection item = ( ( org . odftoolkit . odfdom . incubator . search . TextSelection ) ( search . getCurrentItem ( ) ) ) ; i ++ ; try { sel . pasteAtEndOf ( item ) ; } catch ( org . odftoolkit . odfdom . incubator . search . InvalidNavigationException e ) { org . junit . Assert . fail ( e . getMessage ( ) ) ; } } int j = 0 ; search = new org . odftoolkit . odfdom . incubator . search . TextNavigation ( "deletechange" , doc ) ; while ( search . hasNext ( ) ) { j ++ ; } org . junit . Assert . assertTrue ( ( i == j ) ) ; try { doc . save ( org . odftoolkit . odfdom . utils . ResourceUtilities . newTestOutputFile ( org . odftoolkit . odfdom . incubator . search . TextSelectionTest . SAVE_FILE_COPYTO1 ) ) ; } }
public class aTest{ @Test public void testGetCodeBlockClassWithIndentedCommentsForFieldAndMethod ( ) { setName ( cls , "thing" 4 ) ; setComment ( cls , "thing" 5 ) ; com . thoughtworks . qdox . model . JavaMethod mth = com . thoughtworks . qdox . model . JavaClassTest . mock ( com . thoughtworks . qdox . model . JavaMethod . class ) ; when ( mth . getName ( ) ) . thenReturn ( "thing" 7 ) ; com . thoughtworks . qdox . model . JavaClass stringType = newJavaClass ( "String" ) ; when ( mth . getReturnType ( ) ) . thenReturn ( stringType ) ; when ( mth . getComment ( ) ) . thenReturn ( "Hello<sp>Method" ) ; setMethods ( cls , java . util . Collections . singletonList ( mth ) ) ; com . thoughtworks . qdox . model . JavaField fld = com . thoughtworks . qdox . model . JavaClassTest . mock ( com . thoughtworks . qdox . model . JavaField . class ) ; when ( fld . getType ( ) ) . thenReturn ( stringType ) ; when ( fld . getName ( ) ) . thenReturn ( "thing" ) ; when ( fld . getComment ( ) ) . thenReturn ( "Hello<sp>Field" ) ; when ( fld . getDeclaringClass ( ) ) . thenReturn ( cls ) ; setFields ( cls , java . util . Collections . singletonList ( fld ) ) ; java . lang . String expected = "" + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "/**\n" + "<sp>*<sp>Hello<sp>World\n" ) + "<sp>*/\n" ) + "class<sp>MyClass<sp>{\n" ) + "thing" 9 ) + "thing" 1 ) + "thing" 0 ) + "thing" 3 ) + "thing" 8 ) + "thing" 9 ) + "thing" 1 ) + "thing" 6 ) + "thing" 3 ) + "\tString<sp>thingy();\n" ) + "thing" 9 ) + "thing" 2 ) ; org . junit . Assert . assertEquals ( expected , cls . getCodeBlock ( ) ) ; } }
public class aTest{ @Test public void testOverload ( ) { try { kontraktor . BasicTest . Overload ov = AsActor ( kontraktor . BasicTest . Overload . class ) ; org . junit . Assert . assertTrue ( false ) ; } }
public class aTest{ @Test public void routeChat_echoResponse ( ) { final java . util . List < io . grpc . examples . routeguide . RouteNote > notesDelivered = new java . util . ArrayList ( ) ; io . grpc . examples . routeguide . RouteGuideGrpc . RouteGuideImplBase routeChatImpl = new io . grpc . examples . routeguide . RouteGuideGrpc . RouteGuideImplBase ( ) { @ io . grpc . examples . routeguide . Override public io . grpc . stub . StreamObserver < io . grpc . examples . routeguide . RouteNote > routeChat ( final io . grpc . stub . StreamObserver < io . grpc . examples . routeguide . RouteNote > responseObserver ) { io . grpc . stub . StreamObserver < io . grpc . examples . routeguide . RouteNote > requestObserver = new io . grpc . stub . StreamObserver < io . grpc . examples . routeguide . RouteNote > ( ) { @ io . grpc . examples . routeguide . Override public void onNext ( io . grpc . examples . routeguide . RouteNote value ) { notesDelivered . add ( value ) ; responseObserver . onNext ( value ) ; } @ io . grpc . examples . routeguide . Override public void onError ( java . lang . Throwable t ) { responseObserver . onError ( t ) ; } @ io . grpc . examples . routeguide . Override public void onCompleted ( ) { responseObserver . onCompleted ( ) ; } } ; return requestObserver ; } } ; serviceRegistry . addService ( routeChatImpl ) ; client . routeChat ( ) . await ( 1 , TimeUnit . SECONDS ) ; java . lang . String [ ] messages = new java . lang . String [ ] { "First<sp>message" , "Second<sp>message" , "Third<sp>message" , "Fourth<sp>message" } ; for ( int i = 0 ; i < 4 ; i ++ ) { verify ( testHelper ) . onMessage ( notesDelivered . get ( i ) ) ; org . junit . Assert . assertEquals ( messages [ i ] , notesDelivered . get ( i ) . getMessage ( ) ) ; } }
public class aTest{ @Test public void testLocalIndexCreationWithDefaultFamilyOption ( ) { java . sql . Connection conn1 = java . sql . DriverManager . getConnection ( getUrl ( ) ) ; try { java . sql . Statement statement = conn1 . createStatement ( ) ; java . lang . String tableName = generateUniqueName ( ) ; java . lang . String indexName = generateUniqueName ( ) ; statement . execute ( ( ( ( "create<sp>table<sp>" + tableName ) + "<sp>(id<sp>integer<sp>not<sp>null,fn<sp>varchar," ) + "\"ln\"<sp>varchar<sp>constraint<sp>pk<sp>primary<sp>key(id))<sp>DEFAULT_COLUMN_FAMILY=\'F\'" ) ) ; statement . execute ( ( ( "upsert<sp>into<sp>" + tableName ) + "<sp>values(1,'fn','ln')" ) ) ; statement . execute ( ( ( ( ( "SELECT<sp>COUNT(*)<sp>FROM<sp>" 0 + indexName ) + "<sp>on<sp>" ) + tableName ) + "<sp>(fn)" ) ) ; statement . execute ( ( ( "upsert<sp>into<sp>" + tableName ) + "<sp>values(2,'fn1','ln1')" ) ) ; java . sql . ResultSet rs = statement . executeQuery ( ( "SELECT<sp>COUNT(*)<sp>FROM<sp>" + indexName ) ) ; org . junit . Assert . assertTrue ( rs . next ( ) ) ; } }
public class aTest{ @Test public void FileMOVEIf_Match ( ) { final java . lang . String depth = "infinity" ; final java . lang . String destFileName = "destFile.txt" ; final java . lang . String destination = com . fujitsu . dc . test . unit . core . UrlUtils . box ( com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . CELL_NAME , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . BOX_NAME , destFileName ) ; try { com . fujitsu . dc . test . utils . DavResourceUtils . createWebDavFile ( com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . TOKEN , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . CELL_NAME , ( ( ( com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . BOX_NAME ) + "/" ) + ( com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . FILE_NAME ) ) , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . FILE_BODY , MediaType . TEXT_PLAIN , HttpStatus . SC_CREATED ) ; java . lang . String url = com . fujitsu . dc . test . unit . core . UrlUtils . box ( com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . CELL_NAME , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . BOX_NAME , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . FILE_NAME ) ; com . fujitsu . dc . test . jersey . DcRequest req = com . fujitsu . dc . test . jersey . DcRequest . move ( url ) ; req . header ( HttpHeaders . AUTHORIZATION , AbstractCase . BEARER_MASTER_TOKEN ) ; req . header ( HttpHeaders . DESTINATION , destination ) ; req . header ( HttpHeaders . DEPTH , depth ) ; req . header ( HttpHeaders . OVERWRITE , "T" ) ; req . header ( HttpHeaders . IF_MATCH , "*" ) ; com . fujitsu . dc . test . jersey . DcResponse response = com . fujitsu . dc . test . jersey . AbstractCase . request ( req ) ; org . junit . Assert . assertEquals ( HttpStatus . SC_CREATED , response . getStatusCode ( ) ) ; com . fujitsu . dc . test . utils . DavResourceUtils . getWebDav ( com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . CELL_NAME , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . TOKEN , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . BOX_NAME , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . FILE_NAME , HttpStatus . SC_NOT_FOUND ) ; com . fujitsu . dc . test . utils . DavResourceUtils . getWebDav ( com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . CELL_NAME , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . TOKEN , com . fujitsu . dc . test . jersey . box . dav . file . MoveFileHeaderValidateTest . BOX_NAME , destFileName , HttpStatus . SC_OK ) ; } }
public class aTest{ @Test public void synchronizationWithPeerNodeListAndDuplicates ( ) { com . vmware . xenon . common . test . VerificationHost h = null ; org . junit . rules . TemporaryFolder tmpFolder = new org . junit . rules . TemporaryFolder ( ) ; tmpFolder . create ( ) ; try { setUp ( this . nodeCount ) ; this . host . setNodeGroupQuorum ( 1 ) ; java . util . Map < java . lang . String , com . vmware . xenon . services . common . ExampleService . ExampleServiceState > exampleStatesPerSelfLink = new java . util . HashMap ( ) ; int dupServiceCount = this . serviceCount ; java . util . concurrent . atomic . AtomicInteger counter = new java . util . concurrent . atomic . AtomicInteger ( ) ; java . util . Map < java . net . URI , com . vmware . xenon . services . common . ExampleService . ExampleServiceState > dupStates = new java . util . HashMap ( ) ; for ( com . vmware . xenon . common . test . VerificationHost v : this . host . getInProcessHostMap ( ) . values ( ) ) { counter . set ( 0 ) ; java . net . URI factoryUri = com . vmware . xenon . common . UriUtils . buildFactoryUri ( v , com . vmware . xenon . services . common . ExampleService . class ) ; dupStates = this . host . doFactoryChildServiceStart ( null , dupServiceCount , com . vmware . xenon . services . common . ExampleService . ExampleServiceState . class , ( o ) -> { com . vmware . xenon . services . common . ExampleService . ExampleServiceState s = new com . vmware . xenon . services . common . ExampleService . ExampleServiceState ( ) ; s . documentSelfLink = "duplicateExampleInstance-" + ( counter . incrementAndGet ( ) ) ; s . name = s . documentSelfLink ; o . setBody ( s ) ; } , factoryUri ) ; } for ( com . vmware . xenon . services . common . ExampleService . ExampleServiceState s : dupStates . values ( ) ) { exampleStatesPerSelfLink . put ( s . documentSelfLink , s ) ; } this . serviceCount = exampleStatesPerSelfLink . size ( ) ; java . util . Collection < java . net . URI > peerNodeGroupUris = new java . util . ArrayList ( ) ; java . lang . StringBuilder peerNodes = new java . lang . StringBuilder ( ) ; for ( com . vmware . xenon . common . test . VerificationHost peer : this . host . getInProcessHostMap ( ) . values ( ) ) { peerNodeGroupUris . add ( com . vmware . xenon . common . UriUtils . buildUri ( peer , ServiceUriPaths . DEFAULT_NODE_GROUP ) ) ; peerNodes . append ( peer . getUri ( ) . toString ( ) ) . append ( "," ) ; } for ( java . net . URI nodeGroup : this . host . getNodeGroupMap ( ) . values ( ) ) { this . host . subscribeForNodeGroupConvergence ( nodeGroup , ( ( this . nodeCount ) + 1 ) , ( o , e ) -> { if ( e != null ) { this . host . log ( "Error<sp>in<sp>notification:<sp>%s" , com . vmware . xenon . common . Utils . toString ( e ) ) ; } } ) ; } h = new com . vmware . xenon . common . test . VerificationHost ( ) ; int quorum = ( this . host . getPeerCount ( ) ) + 1 ; java . lang . String mainHostId = "main-" + ( VerificationHost . hostNumber . incrementAndGet ( ) ) ; java . lang . String [ ] args = new java . lang . String [ ] { "--port=0" , "--id=" + mainHostId , "--bindAddress=127.0.0.1" , "--sandbox=" + ( tmpFolder . getRoot ( ) . getAbsolutePath ( ) ) , "--peerNodes=" + ( peerNodes . toString ( ) ) } ; h . initialize ( args ) ; h . setPeerSynchronizationEnabled ( this . isPeerSynchronizationEnabled ) ; h . setMaintenanceIntervalMicros ( TimeUnit . MILLISECONDS . toMicros ( VerificationHost . FAST_MAINT_INTERVAL_MILLIS ) ) ; h . start ( ) ; java . net . URI mainHostNodeGroupUri = com . vmware . xenon . common . UriUtils . buildUri ( h , ServiceUriPaths . DEFAULT_NODE_GROUP ) ; int totalCount = ( this . nodeCount ) + 1 ; peerNodeGroupUris . add ( mainHostNodeGroupUri ) ; this . host . waitForNodeGroupIsAvailableConvergence ( ) ; this . host . waitForNodeGroupConvergence ( peerNodeGroupUris , totalCount , totalCount , true ) ; this . host . setNodeGroupQuorum ( quorum , mainHostNodeGroupUri ) ; this . host . setNodeGroupQuorum ( quorum ) ; this . host . scheduleSynchronizationIfAutoSyncDisabled ( this . replicationNodeSelector ) ; int peerNodeCount = h . getInitialPeerHosts ( ) . size ( ) ; org . junit . Assert . assertTrue ( ( totalCount >= ( peerNodeCount + 1 ) ) ) ; verifyReplicatedInConflictPost ( dupStates ) ; this . host . addPeerNode ( h ) ; this . host . waitForReplicatedFactoryChildServiceConvergence ( getFactoriesPerNodeGroup ( this . replicationTargetFactoryLink ) , exampleStatesPerSelfLink , this . exampleStateConvergenceChecker , this . serviceCount , 0 , this . replicationFactor ) ; doStateUpdateReplicationTest ( Action . PATCH , this . serviceCount , this . updateCount , 0 , this . exampleStateUpdateBodySetter , this . exampleStateConvergenceChecker , exampleStatesPerSelfLink ) ; java . net . URI exampleFactoryUri = this . host . getPeerServiceUri ( ExampleService . FACTORY_LINK ) ; waitForReplicatedFactoryServiceAvailable ( com . vmware . xenon . common . UriUtils . buildUri ( exampleFactoryUri , ExampleService . FACTORY_LINK ) , ServiceUriPaths . DEFAULT_NODE_SELECTOR ) ; } }
public class aTest{ @Test public void testbuilder ( ) { final java . lang . String host = "localhost" ; final int port = 9000 ; java . lang . Exception exception = null ; try { com . flipkart . ranger . serviceprovider . ServiceProvider < com . flipkart . ranger . finder . unsharded . UnshardedClusterInfo > serviceProvider = com . flipkart . ranger . ServiceProviderBuilders . unshardedServiceProviderBuilder ( ) . withConnectionString ( testingCluster . getConnectString ( ) ) . withNamespace ( "test" ) . withServiceName ( "test-service" ) . withSerializer ( new com . flipkart . ranger . model . Serializer < com . flipkart . ranger . finder . unsharded . UnshardedClusterInfo > ( ) { @ com . flipkart . ranger . serviceprovider . Override public byte [ ] serialize ( com . flipkart . ranger . model . ServiceNode < com . flipkart . ranger . finder . unsharded . UnshardedClusterInfo > data ) { try { return objectMapper . writeValueAsBytes ( data ) ; } catch ( com . fasterxml . jackson . core . JsonProcessingException e ) { e . printStackTrace ( ) ; } return null ; } } ) . withHostname ( host ) . withPort ( port ) . buildServiceDiscovery ( ) ; } catch ( java . lang . Exception e ) { exception = e ; } org . junit . Assert . assertTrue ( ( exception instanceof java . lang . IllegalArgumentException ) ) ; com . flipkart . ranger . serviceprovider . ServiceProvider < com . flipkart . ranger . finder . unsharded . UnshardedClusterInfo > serviceProvider = com . flipkart . ranger . ServiceProviderBuilders . unshardedServiceProviderBuilder ( ) . withConnectionString ( testingCluster . getConnectString ( ) ) . withNamespace ( "test" ) . withServiceName ( "test-service" ) . withSerializer ( new com . flipkart . ranger . model . Serializer < com . flipkart . ranger . finder . unsharded . UnshardedClusterInfo > ( ) { @ com . flipkart . ranger . serviceprovider . Override public byte [ ] serialize ( com . flipkart . ranger . model . ServiceNode < com . flipkart . ranger . finder . unsharded . UnshardedClusterInfo > data ) { try { return objectMapper . writeValueAsBytes ( data ) ; } }
public class aTest{ @Test public void structuredData ( ) { org . apache . logging . log4j . ThreadContext . put ( "loginId" , "loginId" 2 ) ; org . apache . logging . log4j . ThreadContext . put ( "ipAddress" , "loginId" 1 ) ; org . apache . logging . log4j . ThreadContext . put ( "locale" , Locale . US . getDisplayName ( ) ) ; final org . apache . logging . log4j . message . StructuredDataMessage msg = new org . apache . logging . log4j . message . StructuredDataMessage ( "loginId" 5 , "loginId" 3 , "Transfer" ) ; msg . put ( "loginId" 0 , "123456" ) ; msg . put ( "FromAccount" , "123457" ) ; msg . put ( "Amount" , "200.00" ) ; logger . info ( org . apache . logging . log4j . MarkerManager . getMarker ( "EVENT" ) , msg ) ; org . apache . logging . log4j . ThreadContext . clearMap ( ) ; final java . util . List < org . apache . logging . log4j . core . LogEvent > events = app . getEvents ( ) ; org . junit . Assert . assertEquals ( ( "loginId" 4 + ( events . size ( ) ) ) , 1 , events . size ( ) ) ; } }
public class aTest{ @Test public void testReset ( ) { org . numenta . nupic . network . sensor . Sensor < java . io . File > sensor = org . numenta . nupic . network . sensor . Sensor . create ( FileSensor :: create , org . numenta . nupic . network . sensor . SensorParams . create ( Keys :: path , "" , org . numenta . nupic . datagen . ResourceLocator . path ( "rec-center-hourly-4reset.csv" ) ) ) ; org . numenta . nupic . Parameters p = org . numenta . nupic . network . NetworkTestHarness . getParameters ( ) . copy ( ) ; p = p . union ( org . numenta . nupic . network . NetworkTestHarness . getHotGymTestEncoderParams ( ) ) ; p . set ( KEY . RANDOM , new org . numenta . nupic . util . MersenneTwister ( 42 ) ) ; p . set ( KEY . AUTO_CLASSIFY , Boolean . TRUE ) ; p . set ( KEY . INFERRED_FIELDS , org . numenta . nupic . network . NetworkTestHarness . getInferredFieldsMap ( "consumption" , org . numenta . nupic . algorithms . CLAClassifier . class ) ) ; org . numenta . nupic . network . sensor . HTMSensor < java . io . File > htmSensor = ( ( org . numenta . nupic . network . sensor . HTMSensor < java . io . File > ) ( sensor ) ) ; org . numenta . nupic . network . Network n = org . numenta . nupic . network . Network . create ( "test<sp>network" , p ) ; final org . numenta . nupic . network . Layer < int [ ] > l = new org . numenta . nupic . network . Layer ( n ) ; l . add ( htmSensor ) ; l . subscribe ( new rx . Observer < org . numenta . nupic . network . Inference > ( ) { @ org . numenta . nupic . network . Override public void onCompleted ( ) { } @ org . numenta . nupic . network . Override public void onError ( java . lang . Throwable e ) { e . printStackTrace ( ) ; } @ org . numenta . nupic . network . Override public void onNext ( org . numenta . nupic . network . Inference output ) { if ( l . getSensor ( ) . getMetaInfo ( ) . isReset ( ) ) { ( trueCount ) ++ ; } } } ) ; l . start ( ) ; try { l . getLayerThread ( ) . join ( ) ; org . junit . Assert . assertEquals ( 3 , trueCount ) ; } }
public class aTest{ @Test public void write_Quality ( ) { if ( ! ( isJp2KakDriverAvailable ) ) return ; it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . LOGGER . info ( "Testing<sp>JP2<sp>Write<sp>operation<sp>with<sp>Quality<sp>option<sp>setting" ) ; final java . io . File inputFile = it . geosolutions . resources . TestData . file ( this , it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . testFileName ) ; org . junit . Assert . assertTrue ( inputFile . exists ( ) ) ; final float firstQualityParam = 99.2F ; final float secondQualityParam = 1.0F ; final java . lang . String fileName1 = new java . lang . StringBuffer ( "Quality-" ) . append ( java . lang . Float . toString ( firstQualityParam ) ) . append ( "f-.jp2" ) . toString ( ) ; final java . lang . String fileName2 = new java . lang . StringBuffer ( "Quality-" ) . append ( java . lang . Float . toString ( secondQualityParam ) ) . append ( "f-.jp2" ) . toString ( ) ; final java . io . File outputFile1 = it . geosolutions . resources . TestData . temp ( this , fileName1 , it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . deleteTempFilesOnExit ) ; final java . io . File outputFile2 = it . geosolutions . resources . TestData . temp ( this , fileName2 , it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . deleteTempFilesOnExit ) ; final javax . media . jai . ParameterBlockJAI pbjImageRead = new javax . media . jai . ParameterBlockJAI ( "Input" 0 ) ; pbjImageRead . setParameter ( "Input" , inputFile ) ; if ( it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . ENABLE_SUBSAMPLING ) { javax . imageio . ImageReadParam readParam = new javax . imageio . ImageReadParam ( ) ; readParam . setSourceSubsampling ( 4 , 4 , 0 , 0 ) ; pbjImageRead . setParameter ( "readParam" , readParam ) ; } }
public class aTest{ @Test public void getDatesRespresentation ( ) { java . text . DateFormat df = new java . text . SimpleDateFormat ( "GMT+0" 6 ) ; df . setTimeZone ( java . util . TimeZone . getTimeZone ( "GMT+0" ) ) ; org . geotoolkit . temporal . util . PeriodUtilities instance = new org . geotoolkit . temporal . util . PeriodUtilities ( df ) ; java . util . SortedSet < java . util . Date > dates ; dates = new java . util . TreeSet ( ) ; dates . add ( df . parse ( "2009-10-31T00:00:00Z" 0 ) ) ; dates . add ( df . parse ( "GMT+0" 2 ) ) ; dates . add ( df . parse ( "GMT+0" 7 ) ) ; dates . add ( df . parse ( "2004-02-04T00:00:00Z" ) ) ; dates . add ( df . parse ( "GMT+0" 9 ) ) ; dates . add ( df . parse ( "2009-10-31T00:00:00Z" 9 ) ) ; dates . add ( df . parse ( "2005-11-23T00:00:00Z" 1 ) ) ; dates . add ( df . parse ( "2009-10-31T00:00:00Z" 1 ) ) ; dates . add ( df . parse ( "2009-10-31T00:00:00Z" 2 ) ) ; dates . add ( df . parse ( "2004-03-17T00:00:00Z" ) ) ; dates . add ( df . parse ( "2005-03-02T00:00:00Z" ) ) ; dates . add ( df . parse ( "GMT+0" 8 ) ) ; dates . add ( df . parse ( "2005-11-23T00:00:00Z" 5 ) ) ; dates . add ( df . parse ( "GMT+0" 1 ) ) ; dates . add ( df . parse ( "2005-11-23T00:00:00Z" ) ) ; dates . add ( df . parse ( "GMT+0" 0 ) ) ; dates . add ( df . parse ( "2009-10-31T00:00:00Z" ) ) ; dates . add ( df . parse ( "2010-10-31T00:00:00Z" ) ) ; dates . add ( df . parse ( "2009-10-31T00:00:00Z" 7 ) ) ; expResult = "2005-11-23T00:00:00Z" 4 ; result = instance . getDatesRespresentation ( dates ) ; org . junit . Assert . assertEquals ( expResult , result ) ; } }
public class aTest{ @Test public void testExtractWeekDayNameDate ( ) { java . lang . String sqlText = ( "select<sp>d,<sp>EXTRACT(WEEKDAYNAME<sp>FROM<sp>d)<sp>as<sp>\"WEEKDAYNAME\"<sp>from<sp>" + ( com . splicemachine . derby . utils . SpliceDateFunctionsIT . tableWatcherI ) ) + "<sp>order<sp>by<sp>d" ; try ( com . splicemachine . derby . utils . ResultSet rs = methodWatcher . executeQuery ( sqlText ) ) { java . lang . String expected = "D<sp>|<sp>WEEKDAYNAME<sp>|\n" + ( ( ( ( ( ( "--------------------------\n" + "2009-01-02<sp>|<sp>Friday<sp>|\n" ) + "2009-07-02<sp>|<sp>Thursday<sp>|\n" ) + "2009-09-02<sp>|<sp>Wednesday<sp>|\n" ) + "2012-12-31<sp>|<sp>Monday<sp>|\n" ) + "2012-12-31<sp>|<sp>Monday<sp>|\n" ) + "<sp>from<sp>" 0 ) ; org . junit . Assert . assertEquals ( ( ( "\n" + sqlText ) + "\n" ) , expected , TestUtils . FormattedResult . ResultFactory . toStringUnsorted ( rs ) ) ; } } }
public class aTest{ @Test public void check_PersonTestSQLs ( ) { org . nutz . dao . SqlManager sqls = new org . nutz . dao . impl . FileSqlManager ( "org/nutz/dao/test/sqls/sqls.sqls" ) ; java . lang . String [ ] keys = new java . lang . String [ ] { ".abc.drop" , ".abc.create" , ".abc.drop" 0 , ".abc.update" , "abc.fetch" , "abc.query" , ".student.drop" , ".student.create" , ".student2.drop" , ".student2.create" } ; for ( int i = 0 ; i < ( keys . length ) ; i ++ ) { org . junit . Assert . assertEquals ( keys [ i ] , sqls . keys ( ) [ i ] ) ; } } }
public class aTest{ @Test public void testUniformRandomPayloadGenerator ( ) { org . apache . kafka . trogdor . workload . PayloadIterator iter = new org . apache . kafka . trogdor . workload . PayloadIterator ( new org . apache . kafka . trogdor . workload . UniformRandomPayloadGenerator ( 1234 , 456 , 0 ) ) ; byte [ ] prev = iter . next ( ) ; for ( int uniques = 0 ; uniques < 1000 ; ) { byte [ ] cur = iter . next ( ) ; org . junit . Assert . assertEquals ( prev . length , cur . length ) ; if ( ! ( java . util . Arrays . equals ( prev , cur ) ) ) { uniques ++ ; } } }
public class aTest{ @Test public void test_SimpleLevel ( ) { org . apache . avro . generic . GenericRecord inputRecord = new org . apache . avro . generic . GenericRecordBuilder ( inputSimpleSchema ) . set ( "a" , "a" ) . set ( "b" , "b" ) . set ( "c" , "c" ) . build ( ) ; java . util . List < java . lang . String > keyList = java . util . Arrays . asList ( "c" , "a" , "d" ) ; java . lang . String transformedIndexedRecord = ( "{'key':<sp>{'c':<sp>'c',<sp>'a':<sp>'a',<sp>'d':<sp>null},<sp>" + "a" 1 ) . replaceAll ( "\\\'" , "\"" ) ; org . apache . avro . generic . IndexedRecord outputRecord = org . talend . components . adapter . beam . kv . KeyValueUtils . transformToKV ( inputRecord , org . talend . components . adapter . beam . kv . SchemaGeneratorUtils . extractKeyValues ( inputRecord . getSchema ( ) , keyList ) ) ; org . junit . Assert . assertEquals ( transformedIndexedRecord , outputRecord . toString ( ) ) ; org . apache . avro . Schema kvSchema = org . talend . components . adapter . beam . kv . SchemaGeneratorUtils . mergeKeyValues ( outputRecord . getSchema ( ) ) ; java . lang . String mergedRecord = "{\'c\':<sp>\'c\',<sp>\'a\':<sp>\'a\',<sp>\'d\':<sp>null,<sp>\'b\':<sp>\'b\'}" . replaceAll ( "\\\'" , "\"" ) ; org . junit . Assert . assertEquals ( mergedRecord , org . talend . components . adapter . beam . kv . KeyValueUtils . transformFromKV ( outputRecord , kvSchema ) . toString ( ) ) ; } }
public class aTest{ @Test public void testWhereCompound1 ( ) { org . apache . ibatis . session . SqlSession sqlSession = tk . mybatis . mapper . mapper . MybatisHelper . getSqlSession ( ) ; try { tk . mybatis . mapper . mapper . CountryMapper mapper = sqlSession . getMapper ( tk . mybatis . mapper . mapper . CountryMapper . class ) ; tk . mybatis . mapper . entity . Example example = tk . mybatis . mapper . entity . Example . builder ( tk . mybatis . mapper . model . Country . class ) . where ( tk . mybatis . mapper . util . Sqls . custom ( ) . andBetween ( "id" , 35 , 50 ) . orLessThan ( "id" , 40 ) . orIsNull ( "countryname" ) ) . build ( ) ; java . util . List < tk . mybatis . mapper . model . Country > countries = mapper . selectByExample ( example ) ; org . junit . Assert . assertEquals ( 50 , countries . size ( ) ) ; } }
public class aTest{ @Test public void testPositiveParsePath ( ) { java . lang . String [ ] urls = new java . lang . String [ ] { "hdfs://hostname.test.com:8020/path1/path2/path3" , "hdfs://hostname.test.com:8020//path1/path2/path3" , "hdfs://hostname.test.com:8020/path1//path2/path3" , "Unexpected<sp>path<sp>in<sp>" 0 , "Unexpected<sp>path<sp>in<sp>" 0 , "hdfs://hostname.test.com:8020/path1/path2//path3//" , "hdfs://hostname.test.com:8020///path1/path2/path3" , "hdfs://hostname.test.com:8020/path1///path2/path3" , "hdfs://hostname.test.com:8020/path1/path2///path3" , "hdfs://hostname.test.com:8020/path1/path2/path3///" , "Unexpected<sp>path<sp>in<sp>" 2 , "hdfs:///path1/path2/path3" } ; java . lang . String path = "Unexpected<sp>path<sp>in<sp>" 1 ; for ( java . lang . String url : urls ) { java . lang . String result = org . apache . sentry . hdfs . PathsUpdate . parsePath ( url ) ; org . junit . Assert . assertEquals ( ( "Unexpected<sp>path<sp>in<sp>" + url ) , path , result ) ; } } }
public class aTest{ @Test public void testCustomQuarterlySchedule ( ) { final java . time . ZonedDateTime firstDayOf2016 = java . time . ZonedDateTime . of ( 2016 , 1 , 1 , 0 , 0 , 0 , 0 , com . cronutils . model . time . UTC ) ; final java . time . ZonedDateTime startWithLastDayOf2015 = java . time . ZonedDateTime . of ( 2015 , 12 , 31 , 0 , 0 , 0 , 0 , com . cronutils . model . time . UTC ) ; final java . lang . String customQuarterlyStartingWithDay47OfYear = "0<sp>0<sp>0<sp>?<sp>*<sp>?<sp>*<sp>47/91" ; final com . cronutils . model . time . ExecutionTime executionTime = com . cronutils . model . time . ExecutionTime . forCron ( parser . parse ( customQuarterlyStartingWithDay47OfYear ) ) ; final java . time . ZonedDateTime [ ] expectedExecutionTimes = new java . time . ZonedDateTime [ 4 ] ; for ( int j = 0 ; j < 4 ; j ++ ) { expectedExecutionTimes [ j ] = firstDayOf2016 . withDayOfYear ( ( 47 + ( j * 91 ) ) ) ; } java . time . ZonedDateTime start = startWithLastDayOf2015 ; for ( final java . time . ZonedDateTime expectedExecutionTime : expectedExecutionTimes ) { final java . util . Optional < java . time . ZonedDateTime > nextExecution = executionTime . nextExecution ( start ) ; if ( nextExecution . isPresent ( ) ) { final java . time . ZonedDateTime actual = nextExecution . get ( ) ; org . junit . Assert . assertEquals ( expectedExecutionTime , actual ) ; start = expectedExecutionTime . plusSeconds ( 1 ) ; } }
public class aTest{ @Test public void testSorteerLegePL ( ) { try { final nl . moderniseringgba . migratie . conversie . model . brp . BrpPersoonslijst pl = nl . moderniseringgba . migratie . conversie . viewer . BrpStapelSorter . sorteerPersoonslijst ( null , foutMelder ) ; org . junit . Assert . assertNull ( pl ) ; } }
public class aTest{ @Test public void testRedeliveryOnSingleConsumer ( ) { javax . jms . ConnectionFactory connectionFactory = new org . apache . activemq . ActiveMQConnectionFactory ( brokerUrl ) ; javax . jms . Connection connection = connectionFactory . createConnection ( ) ; connection . start ( ) ; populateDestinationWithInterleavedProducer ( nbMessages , destinationName , connection ) ; { java . util . concurrent . atomic . AtomicInteger received = new java . util . concurrent . atomic . AtomicInteger ( ) ; java . util . Map < java . lang . String , java . lang . Boolean > rolledback = new java . util . concurrent . ConcurrentHashMap < java . lang . String , java . lang . Boolean > ( ) ; javax . jms . Session session = connection . createSession ( true , Session . AUTO_ACKNOWLEDGE ) ; javax . jms . Destination destination = session . createQueue ( destinationName ) ; javax . jms . MessageConsumer consumer = session . createConsumer ( destination ) ; while ( ( received . get ( ) ) < ( nbMessages ) ) { javax . jms . TextMessage msg = ( ( javax . jms . TextMessage ) ( consumer . receive ( 6000000 ) ) ) ; if ( msg != null ) { if ( ( msg != null ) && ( ( rolledback . put ( msg . getText ( ) , Boolean . TRUE ) ) != null ) ) { org . apache . activemq . JmsRollbackRedeliveryTest . LOG . info ( ( ( ( ( ( "Received<sp>message<sp>" + ( msg . getText ( ) ) ) + "<sp>(" ) + ( received . getAndIncrement ( ) ) ) + ")" ) + ( msg . getJMSMessageID ( ) ) ) ) ; org . junit . Assert . assertTrue ( msg . getJMSRedelivered ( ) ) ; session . commit ( ) ; } }
public class aTest{ @Test public void test4 ( ) { when ( contextRepository . peek ( any ( ) ) ) . thenReturn ( new com . creactiviti . piper . core . context . MapContext ( ) ) ; com . creactiviti . piper . core . task . SwitchTaskDispatcher dispatcher = new com . creactiviti . piper . core . task . SwitchTaskDispatcher ( taskDispatcher , taskRepo , messenger , contextRepository ) ; com . creactiviti . piper . core . task . SimpleTaskExecution task = com . creactiviti . piper . core . task . SimpleTaskExecution . create ( ) ; task . set ( "k1" 0 , java . util . Arrays . asList ( com . google . common . collect . ImmutableMap . of ( "key" , "k1" , "tasks" , java . util . Arrays . asList ( com . google . common . collect . ImmutableMap . of ( "type" , "print" ) ) ) , com . google . common . collect . ImmutableMap . of ( "key" , "k2" , "tasks" , java . util . Arrays . asList ( com . google . common . collect . ImmutableMap . of ( "type" , "sleep" ) ) ) ) ) ; task . set ( "default" , java . util . Collections . singletonMap ( "value" , "k1" 1 ) ) ; task . set ( "expression" , "k1" 2 ) ; dispatcher . dispatch ( task ) ; org . mockito . ArgumentCaptor < java . lang . String > arg1 = org . mockito . ArgumentCaptor . forClass ( java . lang . String . class ) ; org . mockito . ArgumentCaptor < com . creactiviti . piper . core . task . TaskExecution > arg2 = org . mockito . ArgumentCaptor . forClass ( com . creactiviti . piper . core . task . TaskExecution . class ) ; verify ( messenger , times ( 1 ) ) . send ( arg1 . capture ( ) , arg2 . capture ( ) ) ; org . junit . Assert . assertEquals ( "k1" 1 , arg2 . getValue ( ) . getOutput ( ) ) ; } }
public class aTest{ @Test public void testBrainregions ( ) { java . lang . String [ ] brs = new java . lang . String [ ] { "bams2004" , "bams2013" , "dong" , "hof" , "paxinos" , "swanson" , "neuronames" } ; for ( java . lang . String br : brs ) { org . apache . uima . jcas . JCas jCas = getTokenizedTestCas ( "with<sp>claustrum<sp>and" ) ; runPipeline ( jCas , ch . epfl . bbp . uima . LexicaHelper . getConceptMapper ( ( "brainregions/" + br ) ) , createEngineDescription ( ch . epfl . bbp . uima . ae . DeduplicatorAnnotator . class , ch . epfl . bbp . uima . BlueUima . PARAM_ANNOTATION_CLASSES , new java . lang . String [ ] { "ch.epfl.bbp.uima.types.BrainRegionDictTerm" } ) ) ; java . util . Collection < ch . epfl . bbp . uima . types . BrainRegionDictTerm > b = select ( jCas , ch . epfl . bbp . uima . types . BrainRegionDictTerm . class ) ; ch . epfl . bbp . uima . LexicaHelperTest . LOG . debug ( ch . epfl . bbp . uima . typesystem . To . string ( b ) ) ; org . junit . Assert . assertEquals ( ( "with<sp>claustrum<sp>and" 0 + br ) , 1 , b . size ( ) ) ; } } }
public class aTest{ @Test public void testGetAllModels ( ) { try { java . util . List < qa . qcri . aidr . dbmanager . dto . ModelDTO > modelDTOs = qa . qcri . aidr . dbmanager . ejb . remote . facade . imp . TestModelResourceFacadeImp . modelResourceFacadeImp . getAllModels ( ) ; org . junit . Assert . assertNotNull ( modelDTOs ) ; } }
public class aTest{ @Test public void normalize_slashes ( ) { java . lang . String [ ] cases = new java . lang . String [ ] { "file:/foo/bar" , "file:/foo/bar/" , "file:///foo/bar/" , "file:///foo/bar" } ; java . lang . String previous = null ; for ( java . lang . String uri : cases ) { java . lang . String normalized = org . springframework . ide . vscode . commons . util . UriUtil . normalize ( uri ) ; if ( previous != null ) { org . junit . Assert . assertEquals ( previous , normalized ) ; } }
public class aTest{ @Test public void validationOfDoublePropertyDifferentTagHierachyD_Namespace ( ) { java . lang . String room = "<?xml<sp>version='1.0'<sp>encoding='UTF-8'?>" + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "Rooms" 3 http : org . apache . olingo . odata2 . api . edm . EdmEntitySet entitySet = org . apache . olingo . odata2 . testutil . mock . MockFacade . getMockEdm ( ) . getDefaultEntityContainer ( ) . getEntitySet ( "Rooms" ) ; java . io . InputStream reqContent = createContentAsStream ( room ) ; org . apache . olingo . odata2 . client . api . ep . EntityStream stream = new org . apache . olingo . odata2 . client . api . ep . EntityStream ( ) ; stream . setContent ( reqContent ) ; stream . setReadProperties ( org . apache . olingo . odata2 . client . api . ep . DeserializerProperties . init ( ) . build ( ) ) ; org . apache . olingo . odata2 . client . core . ep . deserializer . XmlEntityDeserializer xec = new org . apache . olingo . odata2 . client . core . ep . deserializer . XmlEntityDeserializer ( ) ; org . apache . olingo . odata2 . api . ep . entry . ODataEntry result = xec . readEntry ( entitySet , stream ) ; org . junit . Assert . assertNotNull ( result ) ; } }
public class aTest{ @Test public void testDeleteMseaMa ( ) { org . onosproject . yang . gen . v1 . mseacfm . rev20160229 . mseacfm . mefcfm . maintenancedomain . MaintenanceAssociation ma1300 = new org . onosproject . yang . gen . v1 . mseacfm . rev20160229 . mseacfm . mefcfm . maintenancedomain . DefaultMaintenanceAssociation ( ) ; org . onosproject . yang . gen . v1 . mseacfm . rev20160229 . mseacfm . mefcfm . maintenancedomain . maintenanceassociation . manameandtypecombo . NamePrimaryVid pvid1300Name = new org . onosproject . yang . gen . v1 . mseacfm . rev20160229 . mseacfm . mefcfm . maintenancedomain . maintenanceassociation . manameandtypecombo . DefaultNamePrimaryVid ( ) ; pvid1300Name . namePrimaryVid ( org . onosproject . yang . gen . v1 . mseacfm . rev20160229 . mseacfm . mefcfm . maintenancedomain . maintenanceassociation . manameandtypecombo . nameprimaryvid . NamePrimaryVidUnion . fromString ( "1300" ) ) ; ma1300 . id ( ( ( short ) ( 1300 ) ) ) ; ma1300 . maNameAndTypeCombo ( pvid1300Name ) ; org . onosproject . yang . gen . v1 . mseacfm . rev20160229 . mseacfm . mefcfm . MaintenanceDomain md = new org . onosproject . yang . gen . v1 . mseacfm . rev20160229 . mseacfm . mefcfm . DefaultMaintenanceDomain ( ) ; org . onosproject . yang . gen . v1 . mseacfm . rev20160229 . mseacfm . mefcfm . maintenancedomain . mdnameandtypecombo . NameCharacterString mdName = new org . onosproject . yang . gen . v1 . mseacfm . rev20160229 . mseacfm . mefcfm . maintenancedomain . mdnameandtypecombo . DefaultNameCharacterString ( ) ; mdName . name ( new org . onosproject . yang . gen . v1 . mseatypes . rev20160229 . mseatypes . Identifier45 ( "md-13" ) ) ; md . mdNameAndTypeCombo ( mdName ) ; md . id ( ( ( short ) ( 13 ) ) ) ; md . addToMaintenanceAssociation ( ma1300 ) ; org . onosproject . yang . gen . v1 . mseacfm . rev20160229 . mseacfm . MefCfm mefCfm = new org . onosproject . yang . gen . v1 . mseacfm . rev20160229 . mseacfm . DefaultMefCfm ( ) ; mefCfm . addToMaintenanceDomain ( md ) ; org . onosproject . yang . gen . v1 . mseacfm . rev20160229 . MseaCfmOpParam mseaCfm = new org . onosproject . yang . gen . v1 . mseacfm . rev20160229 . MseaCfmOpParam ( ) ; mseaCfm . mefCfm ( mefCfm ) ; try { boolean deleted = mseaCfmService . deleteMseaMa ( mseaCfm , session , DatastoreId . RUNNING ) ; org . junit . Assert . assertTrue ( deleted ) ; } }
public class aTest{ @Test public void sendEventSendsMessageCorrectlyToIotHub ( ) { baseExpectations ( ) ; openExpectations ( ) ; final byte [ ] msgBody = new byte [ ] { 97 , 98 , 99 } ; new tests . unit . com . microsoft . azure . sdk . iot . device . transport . mqtt . NonStrictExpectations ( ) { { mockedMessage . getBytes ( ) ; result = msgBody ; mockDeviceMessaging . send ( mockedMessage ) ; } } ; tests . unit . com . microsoft . azure . sdk . iot . device . transport . mqtt . MqttIotHubConnection connection = new tests . unit . com . microsoft . azure . sdk . iot . device . transport . mqtt . MqttIotHubConnection ( mockConfig ) ; tests . unit . com . microsoft . azure . sdk . iot . device . transport . mqtt . Deencapsulation . setField ( connection , "listener" , mockedIotHubListener ) ; connection . open ( mockedQueue , mockedScheduledExecutorService ) ; tests . unit . com . microsoft . azure . sdk . iot . device . transport . mqtt . IotHubStatusCode result = connection . sendMessage ( mockedMessage ) ; org . junit . Assert . assertEquals ( IotHubStatusCode . OK_EMPTY , result ) ; new tests . unit . com . microsoft . azure . sdk . iot . device . transport . mqtt . Verifications ( ) { { mockDeviceMessaging . send ( mockedMessage ) ; times = 1 ; } } }
public class aTest{ @Test public void nullNonNullableProperty ( ) { final org . apache . olingo . fit . proxy . staticservice . odatawcfservice . types . StoredPI storedPI = container . getStoredPIs ( ) . getByKey ( 1000 ) ; storedPI . setPIName ( null ) ; try { container . flush ( ) ; org . junit . Assert . fail ( ) ; } catch ( org . apache . olingo . ext . proxy . api . ODataFlushException e ) { org . junit . Assert . assertNotNull ( e ) ; } }
public class aTest{ @Test public void testGetReferenceUpdateSuccess ( ) { com . sun . sgs . test . impl . service . data . TestDataServiceImpl . txnScheduler . runTask ( new com . sun . sgs . test . impl . service . data . TestDataServiceImpl . InitialTestRunnable ( ) { public void run ( ) throws java . lang . Exception { super . run ( ) ; com . sun . sgs . test . util . DummyManagedObject dummy2 = new com . sun . sgs . test . util . DummyManagedObject ( ) ; dummy2 . setValue ( "A" ) ; dummy . setNext ( dummy2 ) ; } } , com . sun . sgs . test . impl . service . data . TestDataServiceImpl . taskOwner ) ; com . sun . sgs . test . impl . service . data . TestDataServiceImpl . txnScheduler . runTask ( new com . sun . sgs . test . util . TestAbstractKernelRunnable ( ) { public void run ( ) { dummy = ( ( com . sun . sgs . test . util . DummyManagedObject ) ( com . sun . sgs . test . impl . service . data . TestDataServiceImpl . service . getBinding ( "dummy" ) ) ) ; com . sun . sgs . test . util . DummyManagedObject dummy2 = dummy . getNextForUpdate ( ) ; dummy2 . value = "B" ; } } , com . sun . sgs . test . impl . service . data . TestDataServiceImpl . taskOwner ) ; com . sun . sgs . test . impl . service . data . TestDataServiceImpl . txnScheduler . runTask ( new com . sun . sgs . test . util . TestAbstractKernelRunnable ( ) { public void run ( ) { dummy = ( ( com . sun . sgs . test . util . DummyManagedObject ) ( com . sun . sgs . test . impl . service . data . TestDataServiceImpl . service . getBinding ( "dummy" ) ) ) ; com . sun . sgs . test . util . DummyManagedObject dummy2 = dummy . getNext ( ) ; org . junit . Assert . assertEquals ( "B" , dummy2 . value ) ; } } }
public class aTest{ @Test public void ( ) { java . lang . String targetFileName = java . lang . String . format ( "adsWriteFailure_%s_%d.log" , "000" , 1234567890123L ) ; org . junit . Assert . assertFalse ( checkAcceptResult ( targetFileName ) ) ; } }
public class aTest{ @Test public void testDeserializeFromMappedFile ( ) { java . nio . file . Path file = org . roaringbitmap . TestSerializationViaByteBuffer . dir . resolve ( java . util . UUID . randomUUID ( ) . toString ( ) ) ; java . nio . file . Files . createFile ( file ) ; try ( org . roaringbitmap . RandomAccessFile raf = new org . roaringbitmap . RandomAccessFile ( file . toFile ( ) , "rw" ) ) { java . nio . ByteBuffer buffer = raf . getChannel ( ) . map ( org . roaringbitmap . READ_WRITE , 0 , serialised . length ) ; buffer . put ( serialised ) ; buffer . flip ( ) ; buffer . order ( order ) ; org . roaringbitmap . RoaringBitmap deserialised = new org . roaringbitmap . RoaringBitmap ( ) ; deserialised . deserialize ( buffer ) ; org . junit . Assert . assertEquals ( input , deserialised ) ; } }
public class aTest{ @Test public void testAdminRefreshQueuesWithLocalConfigurationProvider ( ) { rm = new org . apache . hadoop . yarn . server . resourcemanager . MockRM ( configuration ) ; rm . init ( configuration ) ; rm . start ( ) ; org . apache . hadoop . yarn . server . resourcemanager . scheduler . capacity . CapacityScheduler cs = ( ( org . apache . hadoop . yarn . server . resourcemanager . scheduler . capacity . CapacityScheduler ) ( rm . getRMContext ( ) . getScheduler ( ) ) ) ; int maxAppsBefore = cs . getConfiguration ( ) . getMaximumSystemApplications ( ) ; try { rm . adminService . refreshQueues ( org . apache . hadoop . yarn . server . api . protocolrecords . RefreshQueuesRequest . newInstance ( ) ) ; org . junit . Assert . assertEquals ( maxAppsBefore , cs . getConfiguration ( ) . getMaximumSystemApplications ( ) ) ; } }
public class aTest{ @Test public void testProcessRequestWithMessageFilterThrowException ( ) { final java . lang . String topic = "topic1" ; final int maxSize = 1024 ; final com . taobao . metamorphosis . cluster . Partition partition = new com . taobao . metamorphosis . cluster . Partition ( "0-0" ) ; final long offset = 12 ; final com . taobao . metamorphosis . cluster . Broker broker = new com . taobao . metamorphosis . cluster . Broker ( 0 , "meta://localhost:0" ) ; final int msgId = 1111 ; final byte [ ] data = com . taobao . metamorphosis . utils . MessageUtils . makeMessageBuffer ( msgId , new com . taobao . metamorphosis . network . PutCommand ( topic , partition . getPartition ( ) , "hello" . getBytes ( ) , null , 0 , 0 ) ) . array ( ) ; final com . taobao . metamorphosis . client . consumer . FetchRequest request = new com . taobao . metamorphosis . client . consumer . FetchRequest ( broker , 0 , new com . taobao . metamorphosis . client . consumer . TopicPartitionRegInfo ( topic , partition , offset ) , maxSize ) ; final com . taobao . metamorphosis . client . consumer . SimpleFetchManager . FetchRequestRunner runner = this . fetchManager . new com . taobao . metamorphosis . client . consumer . SimpleFetchManager . FetchRequestRunner ( ) ; org . easymock . EasyMock . expect ( this . consumer . fetch ( request , ( - 1 ) , null ) ) . andReturn ( new com . taobao . metamorphosis . consumer . MessageIterator ( topic , data ) ) ; final java . util . concurrent . atomic . AtomicReference < com . taobao . metamorphosis . Message > msg = new java . util . concurrent . atomic . AtomicReference < com . taobao . metamorphosis . Message > ( ) ; org . easymock . EasyMock . expect ( this . consumer . getMessageListener ( topic ) ) . andReturn ( new com . taobao . metamorphosis . client . consumer . MessageListener ( ) { @ com . taobao . metamorphosis . client . consumer . Override public void recieveMessages ( final com . taobao . metamorphosis . Message message ) { org . junit . Assert . fail ( ) ; } @ com . taobao . metamorphosis . client . consumer . Override public java . util . concurrent . Executor getExecutor ( ) { return null ; } } ) ; org . easymock . EasyMock . expect ( this . consumer . getMessageFilter ( topic ) ) . andReturn ( new com . taobao . metamorphosis . consumer . ConsumerMessageFilter ( ) { @ com . taobao . metamorphosis . client . consumer . Override public boolean accept ( java . lang . String group , com . taobao . metamorphosis . Message message ) { throw new java . lang . RuntimeException ( ) ; } } ) ; org . easymock . EasyMock . expect ( this . consumer . getConsumerConfig ( ) ) . andReturn ( new com . taobao . metamorphosis . client . consumer . ConsumerConfig ( "test" ) ) ; org . easymock . EasyMock . replay ( this . consumer ) ; runner . processRequest ( request ) ; org . easymock . EasyMock . verify ( this . consumer ) ; org . junit . Assert . assertNull ( msg . get ( ) ) ; } }
public class aTest{ @Test public void testNonAsciiReturnValues ( ) { java . lang . String nonAsciiTableName = "nonAsciiTable" ; java . lang . String nonAsciiString = "'<sp>into<sp>table<sp>" 1 ; org . apache . hadoop . fs . Path nonAsciiFilePath = new org . apache . hadoop . fs . Path ( org . apache . hive . jdbc . TestJdbcDriver2 . dataFileDir , "non_ascii_tbl.txt" ) ; java . sql . Statement stmt = org . apache . hive . jdbc . TestJdbcDriver2 . con . createStatement ( ) ; stmt . execute ( "set<sp>hive.support.concurrency<sp>=<sp>false" ) ; stmt . execute ( ( ( ( "create<sp>table<sp>" + nonAsciiTableName ) + "<sp>(key<sp>int,<sp>value<sp>string)<sp>" ) + "row<sp>format<sp>delimited<sp>fields<sp>terminated<sp>by<sp>'|'" ) ) ; stmt . execute ( ( ( ( "load<sp>data<sp>local<sp>inpath<sp>'" + ( nonAsciiFilePath . toString ( ) ) ) + "'<sp>into<sp>table<sp>" ) + nonAsciiTableName ) ) ; java . sql . ResultSet rs = stmt . executeQuery ( ( ( "select<sp>value<sp>from<sp>" + nonAsciiTableName ) + "<sp>limit<sp>1" ) ) ; while ( rs . next ( ) ) { java . lang . String resultValue = rs . getString ( 1 ) ; org . junit . Assert . assertTrue ( resultValue . equalsIgnoreCase ( nonAsciiString ) ) ; } }
public class aTest{ @Test public void testRemove ( ) { com . sun . sgs . test . app . util . TestScalableDeque . txnScheduler . runTask ( new com . sun . sgs . test . util . TestAbstractKernelRunnable ( ) { public void run ( ) throws com . sun . sgs . test . app . util . Exception { com . sun . sgs . app . util . ScalableDeque < java . lang . Integer > d = new com . sun . sgs . app . util . ScalableDeque < java . lang . Integer > ( ) ; d . add ( 2 ) ; org . junit . Assert . assertEquals ( 2 , ( ( int ) ( d . remove ( ) ) ) ) ; } } }
public class aTest{ @Test public void testGetLinkedProjects ( ) { final java . util . concurrent . ExecutorService service = java . util . concurrent . Executors . newSingleThreadExecutor ( ) ; org . eclipse . sw360 . components . db . Project project = handler . getProjectById ( "R1B" 5 , user ) ; final java . util . concurrent . Future < org . eclipse . sw360 . components . db . List < org . eclipse . sw360 . components . db . ProjectLink > > completionFuture = service . submit ( ( ) -> handler . getLinkedProjects ( project , true , user ) ) ; service . shutdown ( ) ; service . awaitTermination ( 10 , TimeUnit . SECONDS ) ; final org . eclipse . sw360 . components . db . List < org . eclipse . sw360 . components . db . ProjectLink > linkedProjects = completionFuture . get ( ) ; org . eclipse . sw360 . datahandler . thrift . components . ReleaseLink releaseLinkR1A = new org . eclipse . sw360 . datahandler . thrift . components . ReleaseLink ( "R1A" , "vendor" , "component1" , "R1B" 2 , "R1B" 7 , false ) . setReleaseRelationship ( ReleaseRelationship . REFERRED ) . setMainlineState ( MainlineState . MAINLINE ) . setNodeId ( "R1A" ) . setComponentType ( ComponentType . OSS ) ; org . eclipse . sw360 . datahandler . thrift . components . ReleaseLink releaseLinkR1B = new org . eclipse . sw360 . datahandler . thrift . components . ReleaseLink ( "R1B" , "vendor" , "component1" , "releaseB" , "R1B" 1 , false ) . setReleaseRelationship ( ReleaseRelationship . REFERRED ) . setMainlineState ( MainlineState . MAINLINE ) . setNodeId ( "R1B" ) . setComponentType ( ComponentType . OSS ) ; org . eclipse . sw360 . datahandler . thrift . components . ReleaseLink releaseLinkR2A = new org . eclipse . sw360 . datahandler . thrift . components . ReleaseLink ( "R1B" 4 , "vendor" , "component2" , "R1B" 2 , "R1B" 9 , false ) . setReleaseRelationship ( ReleaseRelationship . REFERRED ) . setMainlineState ( MainlineState . MAINLINE ) . setNodeId ( "R1B" 4 ) . setComponentType ( ComponentType . COTS ) ; org . eclipse . sw360 . datahandler . thrift . components . ReleaseLink releaseLinkR2B = new org . eclipse . sw360 . datahandler . thrift . components . ReleaseLink ( "R1B" 6 , "vendor" , "component2" , "releaseB" , "vendor<sp>component2<sp>releaseB" , false ) . setReleaseRelationship ( ReleaseRelationship . REFERRED ) . setMainlineState ( MainlineState . MAINLINE ) . setNodeId ( "R1B" 6 ) . setComponentType ( ComponentType . COTS ) ; org . eclipse . sw360 . components . db . ProjectLink link3 = new org . eclipse . sw360 . components . db . ProjectLink ( "P3" , "R1B" 3 ) . setRelation ( ProjectRelationship . REFERRED ) . setNodeId ( "P3" ) . setParentNodeId ( "component1" 0 ) . setProjectType ( ProjectType . CUSTOMER ) . setState ( ProjectState . ACTIVE ) . setTreeLevel ( 2 ) . setLinkedReleases ( org . eclipse . sw360 . components . db . Arrays . asList ( releaseLinkR2A , releaseLinkR2B ) ) . setSubprojects ( org . eclipse . sw360 . components . db . Collections . emptyList ( ) ) ; org . eclipse . sw360 . components . db . ProjectLink link4 = new org . eclipse . sw360 . components . db . ProjectLink ( "P4" , "project4" ) . setRelation ( ProjectRelationship . CONTAINED ) . setNodeId ( "P4" ) . setParentNodeId ( "component1" 0 ) . setProjectType ( ProjectType . CUSTOMER ) . setState ( ProjectState . ACTIVE ) . setTreeLevel ( 2 ) . setSubprojects ( org . eclipse . sw360 . components . db . Collections . emptyList ( ) ) ; org . eclipse . sw360 . components . db . ProjectLink link2 = new org . eclipse . sw360 . components . db . ProjectLink ( "component1" 0 , "R1B" 8 ) . setRelation ( ProjectRelationship . CONTAINED ) . setNodeId ( "component1" 0 ) . setParentNodeId ( "R1B" 5 ) . setProjectType ( ProjectType . CUSTOMER ) . setState ( ProjectState . ACTIVE ) . setTreeLevel ( 1 ) . setLinkedReleases ( org . eclipse . sw360 . components . db . Arrays . asList ( releaseLinkR1A , releaseLinkR1B ) ) . setSubprojects ( org . eclipse . sw360 . components . db . Arrays . asList ( link3 , link4 ) ) ; org . eclipse . sw360 . components . db . ProjectLink link1 = new org . eclipse . sw360 . components . db . ProjectLink ( "R1B" 5 , "R1B" 0 ) . setRelation ( ProjectRelationship . UNKNOWN ) . setNodeId ( "R1B" 5 ) . setProjectType ( ProjectType . CUSTOMER ) . setState ( ProjectState . ACTIVE ) . setTreeLevel ( 0 ) . setSubprojects ( org . eclipse . sw360 . components . db . Arrays . asList ( link2 ) ) ; stripRandomPartsOfNodeIds ( linkedProjects ) ; org . junit . Assert . assertThat ( linkedProjects , org . hamcrest . Matchers . contains ( link1 ) ) ; } }
public class aTest{ @Test public void testModifyWithOr ( ) { java . lang . String drl = ( ( ( ( ( ( ( ( ( ( ( ( ( ( "import<sp>" + ( java . util . List . class . getCanonicalName ( ) ) ) + "\n" ) + "import<sp>" ) + ( java . util . concurrent . atomic . AtomicBoolean . class . getCanonicalName ( ) ) ) + "\n" ) + "\n" ) + "<sp>$l<sp>:<sp>List()\n" 0 ) + "<sp>$l<sp>:<sp>List()\n" ) + "<sp>(<sp>String()<sp>from<sp>$l\n" ) + "<sp>or\n" ) + "<sp>String()<sp>from<sp>$l<sp>)\n" ) + "<sp>$b<sp>:<sp>AtomicBoolean(<sp>get()<sp>)\n" ) + "then" ) + "<sp>modify($b)<sp>{<sp>set(false)<sp>}\n" ) + "<sp>$l<sp>:<sp>List()\n" 1 ; org . kie . api . runtime . KieSession kieSession = new org . kie . internal . utils . KieHelper ( ) . addContent ( drl , ResourceType . DRL ) . build ( ) . newKieSession ( ) ; kieSession . insert ( java . util . Arrays . asList ( "test" ) ) ; kieSession . insert ( new java . util . concurrent . atomic . AtomicBoolean ( true ) ) ; org . junit . Assert . assertEquals ( 1 , kieSession . fireAllRules ( ) ) ; } }
public class aTest{ @Test public void testCorrelation ( ) { int port = getPort ( ) ; org . eclipse . jetty . client . HttpClient httpClient = new org . eclipse . jetty . client . HttpClient ( ) ; httpClient . start ( ) ; for ( int i = 0 ; i < 200 ; i ++ ) { java . lang . String correlId = "RESTY" + ( org . apache . commons . lang . RandomStringUtils . randomNumeric ( 10 ) ) ; javax . jms . TextMessage message = session . createTextMessage ( correlId ) ; message . setStringProperty ( "correlationId" , correlId ) ; message . setJMSCorrelationID ( correlId ) ; org . apache . activemq . web . RestTest . LOG . info ( ( "Sending:<sp>" + correlId ) ) ; producer . send ( message ) ; final java . lang . StringBuffer buf = new java . lang . StringBuffer ( ) ; final java . util . concurrent . CountDownLatch latch = asyncRequest ( httpClient , ( ( "http://localhost:" + port ) + "/message/test?readTimeout=1000&type=queue&clientId=test" ) , buf ) ; latch . await ( ) ; org . apache . activemq . web . RestTest . LOG . info ( ( "Received:<sp>" + ( buf . toString ( ) ) ) ) ; org . junit . Assert . assertEquals ( correlId , buf . toString ( ) ) ; } }
public class aTest{ @Test public void testCardinality ( ) { java . lang . String grammar = "grammar<sp>T;\n" + ( ( ( ( ( "options<sp>{output=AST;}\n" + "tokens<sp>{BLOCK;}\n" ) + "a<sp>:<sp>ID<sp>ID<sp>INT<sp>INT<sp>INT<sp>-><sp>(ID<sp>INT)+;\n" ) + "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" ) + "TParser" 2 ) + "WS<sp>:<sp>(\'<sp>\'|\'\\n\')<sp>{$channel=HIDDEN;}<sp>;\n" ) ; execParser ( "TParser" 1 , grammar , "TParser" , "TLexer" , "a" , "a<sp>b<sp>3<sp>4<sp>5" , debug ) ; java . lang . String expecting = "TParser" 0 ; java . lang . String found = getFirstLineOfException ( ) ; org . junit . Assert . assertEquals ( expecting , found ) ; } }
public class aTest{ @Test public void testMkdirs ( ) { org . apache . hadoop . conf . Configuration conf = new org . apache . hadoop . conf . Configuration ( ) ; org . apache . hadoop . fs . LocalFileSystem fs = org . apache . hadoop . fs . FileSystem . getLocal ( conf ) ; org . apache . hadoop . fs . Path test_dir = new org . apache . hadoop . fs . Path ( org . apache . hadoop . fs . TestLocalFileSystem . TEST_ROOT_DIR , "test_dir" ) ; org . apache . hadoop . fs . Path test_file = new org . apache . hadoop . fs . Path ( org . apache . hadoop . fs . TestLocalFileSystem . TEST_ROOT_DIR , "file1" ) ; org . junit . Assert . assertTrue ( fs . mkdirs ( test_dir ) ) ; writeFile ( fs , test_file , 1 ) ; org . apache . hadoop . fs . Path bad_dir = new org . apache . hadoop . fs . Path ( test_file , "another_dir" ) ; try { fs . mkdirs ( bad_dir ) ; org . junit . Assert . fail ( "Failed<sp>to<sp>detect<sp>existing<sp>file<sp>in<sp>path" ) ; } }
public class aTest{ @Test public void mockingAnAnnotatedPublicInterface ( mockit . MockedAnnotationsTest$AnInterface ) { mockit . Annotation [ ] mockClassAnnotations = mock . getClass ( ) . getAnnotations ( ) ; org . junit . Assert . assertEquals ( 0 , mockClassAnnotations . length ) ; } }
public class aTest{ @Test public void Regression_When_superMapper_plugin_in_use_Should_not_break_regular_object_inection ( ) { class Injectable { } class TestSystem extends net . mostlyoriginal . api . plugin . extendedcomponentmapper . BaseSystem { private net . mostlyoriginal . api . plugin . extendedcomponentmapper . M < net . mostlyoriginal . api . plugin . extendedcomponentmapper . TestMarker > mTest ; @ com . artemis . annotations . Wire Injectable injectable ; @ net . mostlyoriginal . api . plugin . extendedcomponentmapper . Override protected void processSystem ( ) { org . junit . Assert . assertNotNull ( injectable ) ; } } }
public class aTest{ @Test public void testLogin ( ) { try { indexPage . clickLogin ( ) ; loginPage . login ( "alice" , "password" ) ; org . junit . Assert . assertEquals ( profilePage . getUsername ( ) , "alice" ) ; profilePage . clickLogout ( ) ; } }
public class aTest{ @Test public void testSwitchPresenterWidgetToAnotherPresenter1 ( com . gwtplatform . mvp . client . PresenterWidgetTest$PresenterWidgetA , com . gwtplatform . mvp . client . PresenterWidgetTest$PresenterWidgetB , com . gwtplatform . mvp . client . PresenterWidgetTest$PresenterWidgetC ) { com . gwtplatform . mvp . client . presenter . slots . Slot < com . gwtplatform . mvp . client . PresenterWidget < ? > > slotCinA = new com . gwtplatform . mvp . client . presenter . slots . Slot ( ) ; com . gwtplatform . mvp . client . presenter . slots . Slot < com . gwtplatform . mvp . client . PresenterWidget < ? > > slotCinB = new com . gwtplatform . mvp . client . presenter . slots . Slot ( ) ; presenterWidgetA . internalReveal ( ) ; presenterWidgetB . internalReveal ( ) ; presenterWidgetA . setInSlot ( slotCinA , contentC ) ; presenterWidgetB . setInSlot ( slotCinB , contentC ) ; presenterWidgetB . internalHide ( ) ; org . junit . Assert . assertFalse ( contentC . isVisible ( ) ) ; } }
public class aTest{ @Test public void testGetJavaScriptOption ( ) { org . odlabs . wiquery . ui . datepicker . ArrayOfMonthNames arrays = new org . odlabs . wiquery . ui . datepicker . ArrayOfMonthNames ( "Janvier" , "Fevrier" , "Mars" , "Avril" , "Avril" 1 , "Juin" , "Juillet" , "Aout" , "Avril" 0 , "Avril" 2 , "Novembre" , "Decembre" ) ; java . lang . String expectedJavascript = "Avril" 3 + ( ",'Mai','Juin','Juillet','Aout','Septembre','Octobre'" + "Avril" 4 ) ; java . lang . String generatedJavascript = arrays . getJavascriptOption ( ) . toString ( ) ; org . odlabs . wiquery . ui . datepicker . ArrayOfMonthNamesTestCase . log . info ( expectedJavascript ) ; org . odlabs . wiquery . ui . datepicker . ArrayOfMonthNamesTestCase . log . info ( generatedJavascript ) ; org . junit . Assert . assertEquals ( generatedJavascript , expectedJavascript ) ; } }
public class aTest{ @Test public void testDisableInactivePeer ( ) { admin . enablePeer ( "2" ) ; utility2 . shutdownMiniHBaseCluster ( ) ; byte [ ] rowkey = org . apache . hadoop . hbase . util . Bytes . toBytes ( "disable<sp>inactive<sp>peer" ) ; org . apache . hadoop . hbase . client . Put put = new org . apache . hadoop . hbase . client . Put ( rowkey ) ; put . add ( famName , row , row ) ; htable1 . put ( put ) ; java . lang . Thread . sleep ( ( ( SLEEP_TIME ) * ( NB_RETRIES ) ) ) ; admin . disablePeer ( "2" ) ; utility2 . startMiniHBaseCluster ( 1 , 2 ) ; org . apache . hadoop . hbase . client . Get get = new org . apache . hadoop . hbase . client . Get ( rowkey ) ; for ( int i = 0 ; i < ( NB_RETRIES ) ; i ++ ) { org . apache . hadoop . hbase . client . Result res = htable2 . get ( get ) ; if ( ( res . size ( ) ) >= 1 ) { org . junit . Assert . fail ( "Replication<sp>wasn't<sp>disabled" ) ; } else { org . apache . hadoop . hbase . replication . TestReplicationDisableInactivePeer . LOG . info ( "Row<sp>not<sp>replicated,<sp>let's<sp>wait<sp>a<sp>bit<sp>more..." ) ; java . lang . Thread . sleep ( org . apache . hadoop . hbase . replication . SLEEP_TIME ) ; } } admin . enablePeer ( "2" ) ; java . lang . Thread . sleep ( ( ( SLEEP_TIME ) * ( NB_RETRIES ) ) ) ; for ( int i = 0 ; i < ( NB_RETRIES ) ; i ++ ) { org . apache . hadoop . hbase . client . Result res = htable2 . get ( get ) ; if ( ( res . size ( ) ) == 0 ) { org . apache . hadoop . hbase . replication . TestReplicationDisableInactivePeer . LOG . info ( "Row<sp>not<sp>available" ) ; java . lang . Thread . sleep ( ( ( SLEEP_TIME ) * ( NB_RETRIES ) ) ) ; } else { org . junit . Assert . assertArrayEquals ( res . value ( ) , row ) ; return ; } } }
public class aTest{ @Test public void writeMoreFiles ( ) { try { for ( int i = 1 ; i <= ( NUM_FILES ) ; i ++ ) { final java . io . OutputStream outStream = getFileStore ( ) . write ( ( "myFile" + i ) ) ; org . junit . Assert . assertNotNull ( outStream ) ; outStream . write ( ( ( CONTENT ) + i ) . getBytes ( ) ) ; outStream . close ( ) ; try { java . lang . Thread . sleep ( 10 ) ; } }
public class aTest{ @Test public void testSymmetricKeysWithStore ( ) { org . ccnx . ccn . impl . support . Log . info ( Log . FAC_TEST , "Starting<sp>testSymmetricKeysWithStore" ) ; javax . crypto . SecretKey sk = org . ccnx . ccn . impl . security . keys . SymmetricKeyTest . kg . generateKey ( ) ; org . ccnx . ccn . KeyManager km = putHandle . keyManager ( ) ; try { km . saveVerificationKey ( sk , null , null , null , null ) ; org . ccnx . ccn . protocol . PublisherPublicKeyDigest publisher = new org . ccnx . ccn . protocol . PublisherPublicKeyDigest ( sk ) ; org . ccnx . ccn . impl . security . keys . BasicKeyManager km2 = new org . ccnx . ccn . impl . security . keys . BasicKeyManager ( ) ; km2 . initialize ( ) ; org . ccnx . ccn . CCNHandle handle2 = km2 . handle ( ) ; org . ccnx . ccn . protocol . ContentName name = org . ccnx . ccn . impl . security . keys . SymmetricKeyTest . testHelper . getTestChildName ( "testSymmetricKeysWithStore" , "testString" ) ; org . ccnx . ccn . io . content . CCNStringObject testString1 = new org . ccnx . ccn . io . content . CCNStringObject ( name , "A<sp>test!" , org . ccnx . ccn . impl . CCNFlowControl . SaveType . RAW , publisher , null , putHandle ) ; org . ccnx . ccn . impl . security . keys . SymmetricKeyTest . flosser . handleNamespace ( name ) ; testString1 . save ( ) ; org . ccnx . ccn . io . content . CCNStringObject testString2 = new org . ccnx . ccn . io . content . CCNStringObject ( name , publisher , handle2 ) ; testString2 . waitForData ( SystemConfiguration . EXTRA_LONG_TIMEOUT ) ; org . junit . Assert . assertEquals ( testString2 . string ( ) , "A<sp>test!" ) ; testString1 . close ( ) ; testString2 . close ( ) ; org . ccnx . ccn . impl . security . keys . SymmetricKeyTest . flosser . stopMonitoringNamespaces ( ) ; } }
public class aTest{ @Test public void findProblems ( ) { org . osgi . framework . ServiceReference < com . liferay . blade . api . Migration > sr = _context . getServiceReference ( com . liferay . blade . api . Migration . class ) ; com . liferay . blade . api . Migration m = _context . getService ( sr ) ; java . util . List < com . liferay . blade . api . Problem > problems = m . findProblems ( new java . io . File ( "jsptests/liferay-portlet-icon-back/" ) , new com . liferay . blade . util . NullProgressMonitor ( ) ) ; org . junit . Assert . assertEquals ( "" , 1 , problems . size ( ) ) ; boolean found = false ; for ( com . liferay . blade . api . Problem problem : problems ) { if ( problem . file . getName ( ) . endsWith ( "LiferayPortletIconBack.jsp" ) ) { if ( ( problem . lineNumber ) == 1 ) { found = true ; } } } }
public class aTest{ @Test public void testAddIncludes ( ) { org . walkmod . commands . AddTransformationCommand command = new org . walkmod . commands . AddTransformationCommand ( "imports-cleaner" , "mychain" , false , null , null , null , null , false ) ; java . io . File file = new java . io . File ( "src/test/resources/yaml/addIncludes.yml" ) ; if ( file . exists ( ) ) { file . delete ( ) ; } file . createNewFile ( ) ; org . apache . commons . io . FileUtils . write ( file , "" ) ; org . walkmod . conf . providers . YAMLConfigurationProvider prov = new org . walkmod . conf . providers . YAMLConfigurationProvider ( file . getPath ( ) ) ; try { prov . createConfig ( ) ; org . walkmod . conf . entities . TransformationConfig transfCfg = command . buildTransformationCfg ( ) ; prov . addTransformationConfig ( "mychain" , null , transfCfg , false , null , null ) ; prov . setWriter ( "mychain" , "eclipse-writer" , null , false , null ) ; prov . addIncludesToChain ( "mychain" , java . util . Arrays . asList ( "foo" ) , false , true , false ) ; java . lang . String output = org . apache . commons . io . FileUtils . readFileToString ( file ) ; System . out . println ( output ) ; org . junit . Assert . assertTrue ( output . contains ( "foo" ) ) ; } }
public class aTest{ @Test public void lookup_abuse_contact ( ) { final net . ripe . db . whois . common . rpsl . RpslObject ABUSE_CONTACT_ROLE = net . ripe . db . whois . common . rpsl . RpslObject . parse ( ( "" + ( ( ( "role:<sp>Abuse<sp>Contact\n" + "abuse-mailbox:<sp>abuse@test.net\n" 6 ) + "abuse-mailbox:<sp>abuse@test.net\n" ) + "abuse-mailbox:<sp>abuse@test.net\n" 7 ) ) ) ; final net . ripe . db . whois . common . rpsl . RpslObject ABUSE_CONTACT_ORGANISATION = net . ripe . db . whois . common . rpsl . RpslObject . parse ( ( "" + ( ( ( ( ( ( ( ( "organisation:<sp>ORG-RN1-TEST\n" + "193.0.0.1" 0 ) + "org-type:<sp>OTHER\n" ) + "abuse-mailbox:<sp>abuse@test.net\n" 3 ) + "abuse-mailbox:<sp>abuse@test.net\n" 0 ) + "abuse-mailbox:<sp>abuse@test.net\n" 5 ) + "193.0.0.1" 1 ) + "mnt-by:<sp>OWNER-MNT\n" ) + "abuse-mailbox:<sp>abuse@test.net\n" 7 ) ) ) ; final net . ripe . db . whois . common . rpsl . RpslObject ABUSE_CONTACT_INETNUM = net . ripe . db . whois . common . rpsl . RpslObject . parse ( ( "" + ( ( ( ( ( ( ( ( ( "abuse-mailbox:<sp>abuse@test.net\n" 2 + "193.0.0.1" 2 ) + "descr:<sp>some<sp>description\n" ) + "org:<sp>ORG-RN1-TEST\n" ) + "country:<sp>NL\n" ) + "abuse-mailbox:<sp>abuse@test.net\n" 4 ) + "abuse-mailbox:<sp>abuse@test.net\n" 9 ) + "abuse-mailbox:<sp>abuse@test.net\n" 1 ) + "mnt-by:<sp>OWNER-MNT\n" ) + "abuse-mailbox:<sp>abuse@test.net\n" 7 ) ) ) ; databaseHelper . addObjects ( ABUSE_CONTACT_ROLE , ABUSE_CONTACT_ORGANISATION , ABUSE_CONTACT_INETNUM ) ; resetIpTrees ( ) ; final net . ripe . db . whois . api . rest . domain . AbuseContact abuseContact = restClient . request ( ) . lookupAbuseContact ( "193.0.0.1" ) ; org . junit . Assert . assertThat ( abuseContact . getEmail ( ) , org . hamcrest . Matchers . is ( "abuse-mailbox:<sp>abuse@test.net\n" 8 ) ) ; } }
public class aTest{ @Test public void testSlashes ( ) { java . lang . String grammar = "[@3,7:8=\'/\\\',<4>,1:7]\n" 0 + ( ( ( ( "[@3,7:8=\'/\\\',<4>,1:7]\n" 2 + "Slash<sp>:<sp>\'/\';\n" ) + "Vee<sp>:<sp>\'\\\\/\';\n" ) + "[@3,7:8=\'/\\\',<4>,1:7]\n" 3 ) + "WS<sp>:<sp>[<sp>\\t]<sp>-><sp>skip;" ) ; java . lang . String found = execLexer ( "L.g4" , grammar , "[@3,7:8=\'/\\\',<4>,1:7]\n" 1 , "\\<sp>/<sp>\\/<sp>/\\" ) ; java . lang . String expecting = "[@0,0:0=\'\\\',<1>,1:0]\n" + ( ( ( "[@1,2:2=\'/\',<2>,1:2]\n" + "[@2,4:5=\'\\/\',<3>,1:4]\n" ) + "[@3,7:8=\'/\\\',<4>,1:7]\n" ) + "[@4,9:8=\'<EOF>\',<-1>,1:9]\n" ) ; org . junit . Assert . assertEquals ( expecting , found ) ; } }
public class aTest{ @Test public void test ( java . lang . String ) { edu . psu . swe . scim . spec . protocol . filter . FilterTest . LOG . info ( ( "Running<sp>Filter<sp>Parser<sp>test<sp>on<sp>input:<sp>" + filterText ) ) ; edu . psu . swe . scim . spec . protocol . search . Filter filter = new edu . psu . swe . scim . spec . protocol . search . Filter ( filterText ) ; edu . psu . swe . scim . spec . protocol . filter . FilterExpression expression = filter . getExpression ( ) ; edu . psu . swe . scim . spec . protocol . filter . FilterTest . LOG . info ( ( "Parsed<sp>String:<sp>" + ( expression . toFilter ( ) ) ) ) ; org . junit . Assert . assertNotNull ( expression ) ; } }
public class aTest{ @Test public void testBug7084 ( ) { java . nio . file . Path flatRepo = java . nio . file . Files . createTempDirectory ( "flat-repo" ) ; try { org . eclipse . ceylon . cmr . api . RepositoryManager repositoryManager = org . eclipse . ceylon . cmr . ceylon . CeylonUtils . repoManager ( ) . systemRepo ( "../dist/dist/repo" ) . userRepos ( java . util . Arrays . asList ( ( "flat:" + ( flatRepo . toAbsolutePath ( ) . toString ( ) ) ) ) ) . buildManager ( ) ; java . io . File artifact = repositoryManager . getArtifact ( null , "ceylon.collection" , "1.3.2" ) ; org . junit . Assert . assertNotNull ( artifact ) ; java . nio . file . Files . copy ( artifact . toPath ( ) , flatRepo . resolve ( "ceylon.collection-1.3.2.jar" ) ) ; org . eclipse . ceylon . cmr . ceylon . loader . ModuleLoader moduleLoader = new org . eclipse . ceylon . module . loader . FlatpathModuleLoader ( repositoryManager , null , java . util . Collections . < java . lang . String , java . lang . String > emptyMap ( ) , true ) ; try { moduleLoader . loadModule ( "ceylon.collection" , "1.3.2" ) ; } }
public class aTest{ @Test public void getMessageFromReceiverLinkSuperFailed ( ) { java . lang . String deviceId = "deviceId" ; java . lang . String linkName = "receiver" ; byte [ ] bytes = new byte [ 1 ] ; new mockit . NonStrictExpectations ( ) { { mockReceiver . current ( ) ; result = mockDelivery ; mockDelivery . isReadable ( ) ; result = false ; mockDelivery . isPartial ( ) ; result = false ; mockDeviceClientConfig . getDeviceId ( ) ; result = "deviceId" ; } } ; tests . unit . com . microsoft . azure . sdk . iot . device . transport . amqps . AmqpsDeviceMethods amqpsDeviceMethods = mockit . Deencapsulation . newInstance ( tests . unit . com . microsoft . azure . sdk . iot . device . transport . amqps . AmqpsDeviceMethods . class , mockDeviceClientConfig ) ; mockit . Deencapsulation . invoke ( amqpsDeviceMethods , "openLinks" , mockSession ) ; mockit . Deencapsulation . setField ( amqpsDeviceMethods , "receiverLink" , mockReceiver ) ; mockit . Deencapsulation . setField ( amqpsDeviceMethods , "senderLink" , mockSender ) ; mockit . Deencapsulation . setField ( amqpsDeviceMethods , "receiverLinkTag" , linkName ) ; tests . unit . com . microsoft . azure . sdk . iot . device . transport . amqps . AmqpsMessage amqpsMessage = mockit . Deencapsulation . invoke ( amqpsDeviceMethods , "getMessageFromReceiverLink" , linkName ) ; org . junit . Assert . assertNull ( amqpsMessage ) ; new mockit . Verifications ( ) { { mockReceiver . current ( ) ; times = 1 ; mockDelivery . isReadable ( ) ; times = 1 ; mockDelivery . isPartial ( ) ; times = 0 ; } } }
public class aTest{ @Test public void testEmptyConfig ( ) { org . apache . hadoop . conf . Configuration conf = new org . apache . hadoop . conf . Configuration ( ) ; conf . set ( "TestRange" , "" ) ; java . net . ServerSocket socket = new java . net . ServerSocket ( ) ; java . net . InetSocketAddress address = new java . net . InetSocketAddress ( "0.0.0.0" , 0 ) ; try { org . apache . hadoop . ipc . Server . bind ( socket , address , 10 , conf , "TestRange" ) ; org . junit . Assert . assertTrue ( socket . isBound ( ) ) ; } }
public class aTest{ @Test public void testCustomComponentMonitorCanBeSpecifiedWhenParentAndCAFAreSpecified ( ) { com . picocontainer . classname . DefaultClassLoadingPicoContainer parent = new com . picocontainer . classname . DefaultClassLoadingPicoContainer ( ) ; java . io . Reader script = new java . io . StringReader ( ( "monitor<sp>=<sp>WriterComponentMonitor.new(writer)<sp>\n" 0 + ( ( ( ( ( ( ( ( ( "StringWriter<sp>=<sp>java.io.StringWriter\n" + "WriterComponentMonitor<sp>=<sp>com.picocontainer.monitors.WriterComponentMonitor\n" ) + "Caching<sp>=<sp>com.picocontainer.behaviors.Caching\n" ) + "ConstructorInjection<sp>=<sp>com.picocontainer.injectors.ConstructorInjection\n" ) + "writer<sp>=<sp>StringWriter.new\n" ) + "monitor<sp>=<sp>WriterComponentMonitor.new(writer)<sp>\n" ) + "container(:parent<sp>=><sp>$parent,<sp>:component_adapter_factory<sp>=><sp>Caching.new().wrap(ConstructorInjection.new),<sp>:component_monitor<sp>=><sp>monitor)<sp>{\n" ) + "<sp>component(A)\n" ) + "<sp>component(:key<sp>=><sp>StringWriter,<sp>:instance<sp>=><sp>writer)\n" ) + "}" ) ) ) ; com . picocontainer . PicoContainer pico = buildContainer ( script , parent , com . picocontainer . script . jruby . JRubyContainerBuilderTestCase . ASSEMBLY_SCOPE ) ; java . io . StringWriter writer = pico . getComponent ( java . io . StringWriter . class ) ; org . junit . Assert . assertTrue ( ( ( writer . toString ( ) . length ( ) ) > 0 ) ) ; } }
public class aTest{ @Test public void testSuperclass ( ) { java . lang . String drl = "package<sp>org.drools.compiler.integrationtests\n" + ( ( ( ( ( "import<sp>org.drools.compiler.*;\n" + "rule<sp>R1\n" ) + "<sp>when\n" ) + "<sp>Person(<sp>address#LongAddress.country<sp>str[startsWith]<sp>\"United\"<sp>)\n" ) + "wrong<sp>number<sp>of<sp>rules<sp>fired" 1 ) + "wrong<sp>number<sp>of<sp>rules<sp>fired" 0 ) ; org . kie . api . KieBase kbase = loadKnowledgeBaseFromString ( drl ) ; org . kie . api . runtime . KieSession ksession = kbase . newKieSession ( ) ; try { org . drools . compiler . Person mark1 = new org . drools . compiler . Person ( "mark" ) ; mark1 . setAddress ( new org . drools . compiler . Address ( ) ) ; ksession . insert ( mark1 ) ; org . drools . compiler . Person mark2 = new org . drools . compiler . Person ( "mark" ) ; mark2 . setAddress ( new org . drools . compiler . LongAddress ( "United<sp>Kingdom" ) ) ; ksession . insert ( mark2 ) ; org . drools . compiler . Person mark3 = new org . drools . compiler . Person ( "mark" ) ; mark3 . setAddress ( new org . drools . compiler . LongAddress ( "Czech<sp>Republic" ) ) ; ksession . insert ( mark3 ) ; org . junit . Assert . assertEquals ( "wrong<sp>number<sp>of<sp>rules<sp>fired" , 1 , ksession . fireAllRules ( ) ) ; } }
public class aTest{ @Test public void testGetReadersBeforeFailure ( ) { final com . questdb . store . factory . configuration . JournalMetadata < ? > m = new com . questdb . store . factory . configuration . JournalStructure ( "x" ) . $date ( "ts" ) . $ ( ) . build ( ) ; ( ( com . questdb . store . factory . WriterFactory ) ( getFactory ( ) ) ) . writer ( m ) . close ( ) ; try ( final com . questdb . store . factory . CachingReaderFactory rf = new com . questdb . store . factory . CachingReaderFactory ( factoryContainer . getConfiguration ( ) , 1000 , 2 ) ) { com . questdb . std . ObjList < com . questdb . store . Journal > readers = new com . questdb . std . ObjList ( ) ; try { do { readers . add ( rf . reader ( m ) ) ; } while ( true ) ; } catch ( com . questdb . ex . FactoryFullException e ) { org . junit . Assert . assertEquals ( rf . getMaxEntries ( ) , readers . size ( ) ) ; } }
public class aTest{ @Test public void testSearchTokenParamNoValue ( ) { ca . uhn . fhir . jpa . provider . r4 . Patient patient = new ca . uhn . fhir . jpa . provider . r4 . Patient ( ) ; patient . addIdentifier ( ) . setSystem ( "urn:system" ) . setValue ( "testSearchTokenParam001" ) ; patient . addName ( ) . setFamily ( "Tester" ) . addGiven ( "testSearchTokenParam1" ) ; patient . addCommunication ( ) . getLanguage ( ) . setText ( "testSearchTokenParamComText" ) . addCoding ( ) . setCode ( "testSearchTokenParamDisplay" 0 ) . setSystem ( "testSearchTokenParamSystem" ) . setDisplay ( "testSearchTokenParamDisplay" ) ; myPatientDao . create ( patient , mySrd ) ; patient = new ca . uhn . fhir . jpa . provider . r4 . Patient ( ) ; patient . addIdentifier ( ) . setSystem ( "urn:system" ) . setValue ( "testSearchTokenParam002" ) ; patient . addName ( ) . setFamily ( "Tester" ) . addGiven ( "testSearchTokenParam2" ) ; myPatientDao . create ( patient , mySrd ) ; patient = new ca . uhn . fhir . jpa . provider . r4 . Patient ( ) ; patient . addIdentifier ( ) . setSystem ( "urn:system2" ) . setValue ( "testSearchTokenParam002" ) ; patient . addName ( ) . setFamily ( "Tester" ) . addGiven ( "testSearchTokenParam2" ) ; myPatientDao . create ( patient , mySrd ) ; ca . uhn . fhir . jpa . provider . r4 . Bundle response = ourClient . search ( ) . forResource ( ca . uhn . fhir . jpa . provider . r4 . Patient . class ) . where ( Patient . IDENTIFIER . hasSystemWithAnyCode ( "urn:system" ) ) . returnBundle ( ca . uhn . fhir . jpa . provider . r4 . Bundle . class ) . execute ( ) ; org . junit . Assert . assertEquals ( 2 , response . getEntry ( ) . size ( ) ) ; } }
public class aTest{ @Test public void dumpActual_file ( ) { java . io . File file = new java . io . File ( folder . getRoot ( ) , "testing" ) ; com . asakusafw . testdriver . MockDataModelSink sink = new com . asakusafw . testdriver . MockDataModelSink ( ) ; com . asakusafw . testdriver . MockFlowDriverOutput < org . apache . hadoop . io . Text > mock = com . asakusafw . testdriver . MockFlowDriverOutput . text ( getClass ( ) , new com . asakusafw . testdriver . MockTestDataToolProvider ( ) { @ com . asakusafw . testdriver . Override public com . asakusafw . testdriver . core . DataModelSinkFactory getDataModelSinkFactory ( java . net . URI uri ) { org . junit . Assert . assertThat ( new java . io . File ( uri ) . getName ( ) , is ( file . getName ( ) ) ) ; return new com . asakusafw . testdriver . core . DataModelSinkFactory ( ) { @ com . asakusafw . testdriver . Override public < T > com . asakusafw . testdriver . core . DataModelSink createSink ( com . asakusafw . testdriver . core . DataModelDefinition < T > definition , com . asakusafw . testdriver . core . TestContext context ) { return sink ; } } }
public class aTest{ @Test public void testServerResourcesPage ( ) { final java . lang . String jndiName = "jdbcResource" + ( generateRandomString ( ) ) ; final java . lang . String description = "devtest<sp>test<sp>for<sp>server->resources<sp>page-<sp>" + jndiName ; final java . lang . String tableID = "server" 1 ; gotoDasPage ( ) ; org . glassfish . admingui . devtests . JdbcTest jdbcTest = new org . glassfish . admingui . devtests . JdbcTest ( ) ; jdbcTest . createJDBCResource ( jndiName , description , "server" , "server" ) ; gotoDasPage ( ) ; clickAndWait ( "propertyForm:serverInstTabs:resources" ) ; java . lang . String prefix = getTableRowByValue ( tableID , jndiName , "col1" ) ; org . junit . Assert . assertTrue ( isTextPresent ( prefix , jndiName , tableID ) ) ; int jdbcCount = getTableRowCountByValue ( tableID , "server" 0 , "col3:type" ) ; int customCount = getTableRowCountByValue ( tableID , "Custom<sp>Resources" , "col3:type" ) ; org . openqa . selenium . support . ui . Select select = new org . openqa . selenium . support . ui . Select ( driver . findElement ( org . openqa . selenium . By . id ( "propertyForm:resourcesTable:topActionsGroup1:filter_list" ) ) ) ; select . selectByVisibleText ( "Custom<sp>Resources" ) ; waitForTableRowCount ( tableID , customCount ) ; select = new org . openqa . selenium . support . ui . Select ( driver . findElement ( org . openqa . selenium . By . id ( "propertyForm:resourcesTable:topActionsGroup1:filter_list" ) ) ) ; select . selectByVisibleText ( "server" 0 ) ; waitForTableRowCount ( tableID , jdbcCount ) ; java . lang . String clickId = ( getTableRowByValue ( tableID , jndiName , "col1" ) ) + "col1:link" ; clickByIdAction ( clickId ) ; waitForButtonEnabled ( "propertyForm:propertyContentPage:topButtons:saveButton" ) ; clickByIdAction ( "propertyForm:propertyContentPage:topButtons:saveButton" ) ; jdbcTest . deleteJDBCResource ( jndiName , "server" , "server" ) ; } }
public class aTest{ @Test public void testProjections2 ( ) { org . apache . avro . Schema project = org . spf4j . avro . schema . Schemas . project ( DemoRecordInfo . SCHEMA . , "demoRecord.id" , "bubu" ) ; org . junit . Assert . assertNull ( project ) ; } }
public class aTest{ @Test public void testAppend ( ) { System . out . println ( "Testing<sp>HDFSBackendImplREST.append" ) ; try { backend . setHttpClient ( mockHttpClientAppend ) ; backend . append ( dirPath , data ) ; } catch ( java . lang . Exception e ) { org . junit . Assert . fail ( e . getMessage ( ) ) ; } finally { org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testExistingSetter ( ) { try { org . eclipse . xtend2 . lib . StringConcatenation _builder = new org . eclipse . xtend2 . lib . StringConcatenation ( ) ; _builder . append ( "class<sp>Foo<sp>{" ) ; _builder . newLine ( ) ; _builder . append ( "\t" ) ; _builder . append ( "@Property<sp>int<sp>foo" ) ; _builder . newLine ( ) ; _builder . append ( "\t" ) ; _builder . append ( "def<sp>setFoo(int<sp>foo)<sp>{" ) ; _builder . newLine ( ) ; _builder . append ( "\t\t" ) ; _builder . append ( "_foo<sp>=<sp>5" ) ; _builder . newLine ( ) ; _builder . append ( "\t" ) ; _builder . append ( "}" ) ; _builder . newLine ( ) ; _builder . append ( "}" ) ; _builder . newLine ( ) ; final org . eclipse . xtext . util . IAcceptor < org . eclipse . xtext . xbase . testing . CompilationTestHelper . Result > _function = ( org . eclipse . xtext . xbase . testing . CompilationTestHelper . Result it ) -> { try { final java . lang . Object instance = it . getCompiledClass ( ) . newInstance ( ) ; final java . lang . reflect . Method setFoo = it . getCompiledClass ( ) . getDeclaredMethod ( "setFoo" , . class ) ; setFoo . invoke ( instance , java . lang . Integer . valueOf ( 1 ) ) ; final java . lang . reflect . Method getFoo = it . getCompiledClass ( ) . getDeclaredMethod ( "getFoo" ) ; org . junit . Assert . assertEquals ( java . lang . Integer . valueOf ( 5 ) , getFoo . invoke ( instance ) ) ; } }
public class aTest{ @Test public void byId_twoCols_qdsl ( ) { long start = java . lang . System . currentTimeMillis ( ) ; for ( int i = 0 ; i < ( com . querydsl . jpa . QueryPerformanceTest . iterations ) ; i ++ ) { com . querydsl . jpa . domain . QCat cat = com . querydsl . jpa . domain . QCat . cat ; com . querydsl . core . Tuple row = query ( ) . from ( cat ) . where ( cat . id . eq ( ( i + 100 ) ) ) . select ( cat . id , cat . name ) . fetchOne ( ) ; org . junit . Assert . assertNotNull ( row ) ; } }
public class aTest{ @Test public void testSmallPlaintextWithRawKey ( ) { byte [ ] keyValue = com . google . crypto . tink . subtle . Random . randBytes ( com . google . crypto . tink . mac . MacIntegrationTest . HMAC_KEY_SIZE ) ; com . google . crypto . tink . proto . Keyset . Key primary = com . google . crypto . tink . TestUtil . createKey ( com . google . crypto . tink . TestUtil . createHmacKeyData ( keyValue , 16 ) , 42 , KeyStatusType . ENABLED , OutputPrefixType . RAW ) ; com . google . crypto . tink . KeysetHandle keysetHandle = com . google . crypto . tink . TestUtil . createKeysetHandle ( com . google . crypto . tink . TestUtil . createKeyset ( primary ) ) ; com . google . crypto . tink . Mac mac = keysetHandle . getPrimitive ( com . google . crypto . tink . Mac . class ) ; byte [ ] plaintext = "blah" . getBytes ( "UTF-8" ) ; byte [ ] tag = mac . computeMac ( plaintext ) ; org . junit . Assert . assertEquals ( 16 , tag . length ) ; try { mac . verifyMac ( tag , plaintext ) ; } }
public class aTest{ @Test public void should_skip_result_with_different_capitalization_when_case_sensitively_search_is_enabled_for_user ( ) { setCaseSensitiveSearchForUserAndCheckAssertionForGivenSearchString ( "failure" , new jenkins . widgets . HistoryPageFilterCaseSensitiveSearchTest . SearchResultAssertFunction ( ) { @ jenkins . widgets . Override public void doAssertion ( jenkins . widgets . HistoryPageFilter < hudson . model . ModelObject > historyPageFilter ) { org . junit . Assert . assertEquals ( 0 , historyPageFilter . runs . size ( ) ) ; } } }
public class aTest{ @Test public void testConfigFilesSize ( ) { int fileSize = 8 * ( org . xtreemfs . common . benchmark . BenchmarkUtils . KiB_IN_BYTES ) ; configBuilder . setFilesize ( fileSize ) ; org . xtreemfs . common . libxtreemfs . Volume volume = performBenchmark ( ( 1L * ( org . xtreemfs . common . benchmark . BenchmarkUtils . MiB_IN_BYTES ) ) , configBuilder , BenchmarkType . FILES_WRITE ) ; int numberOfFiles = ( org . xtreemfs . common . benchmark . BenchmarkUtils . MiB_IN_BYTES ) / ( 8 * ( org . xtreemfs . common . benchmark . BenchmarkUtils . KiB_IN_BYTES ) ) ; for ( int i = 0 ; i < numberOfFiles ; i ++ ) { long fileSizeActual = volume . getAttr ( org . xtreemfs . common . benchmark . ControllerIntegrationTest . userCredentials , ( "benchmarks/randomBenchmark/benchFile" + i ) ) . getSize ( ) ; org . junit . Assert . assertEquals ( fileSize , fileSizeActual ) ; } }
public class aTest{ @Test public void shouldTestEcaNtmTrafficReceiver ( ) { eu . smartenit . sbox . main . EconomicAnalyzerNetworkTrafficReceiverTest . logger . info ( "--Testing<sp>economic<sp>analyzer<sp>and<sp>traffic<sp>receiver<sp>initialize.<sp>--" ) ; eu . smartenit . sbox . ntm . NetworkTrafficManager ntm = new eu . smartenit . sbox . ntm . NetworkTrafficManager ( ) ; ntm . initialize ( NetworkTrafficManagerDTMMode . TRAFFIC_RECEIVER ) ; org . junit . Assert . assertTrue ( true ) ; eu . smartenit . sbox . eca . EconomicAnalyzer eca = new eu . smartenit . sbox . eca . EconomicAnalyzer ( ntm . getDtmTrafficManager ( ) ) ; for ( int i = 0 ; i < 25 ; i ++ ) { eu . smartenit . sbox . db . dto . XVector xVector = new eu . smartenit . sbox . db . dto . XVector ( ) ; xVector . setSourceAsNumber ( 100 ) ; xVector . addVectorValueForLink ( new eu . smartenit . sbox . db . dto . SimpleLinkID ( "1" , "ISP-A" ) , 500L ) ; xVector . addVectorValueForLink ( new eu . smartenit . sbox . db . dto . SimpleLinkID ( "2" , "ISP-A" ) , 800L ) ; ntm . getDtmTrafficManager ( ) . updateXVector ( xVector ) ; java . util . List < eu . smartenit . sbox . db . dto . ZVector > zVectorList = new java . util . ArrayList < eu . smartenit . sbox . db . dto . ZVector > ( ) ; eu . smartenit . sbox . db . dto . ZVector zVector = new eu . smartenit . sbox . db . dto . ZVector ( ) ; zVector . setSourceAsNumber ( 100 ) ; zVector . addVectorValueForLink ( new eu . smartenit . sbox . db . dto . SimpleLinkID ( "1" , "ISP-A" ) , 100L ) ; zVector . addVectorValueForLink ( new eu . smartenit . sbox . db . dto . SimpleLinkID ( "2" , "ISP-A" ) , 100L ) ; zVectorList . add ( zVector ) ; eca . updateXZVectors ( xVector , zVectorList ) ; } }
public class aTest{ @Test public void test_BrowserFunction_callback_with_boolean ( ) { org . junit . Assume . assumeFalse ( webkit1SkipMsg ( ) , isWebkit1 ) ; java . util . concurrent . atomic . AtomicBoolean javaCallbackExecuted = new java . util . concurrent . atomic . AtomicBoolean ( false ) ; class JavascriptCallback extends org . eclipse . swt . browser . BrowserFunction { JavascriptCallback ( org . eclipse . swt . browser . Browser browser , java . lang . String name ) { ( browser , name ) ; } @ org . eclipse . swt . tests . junit . Override public java . lang . Object function ( java . lang . Object [ ] arguments ) { java . lang . Boolean returnBool = ( ( java . lang . Boolean ) ( arguments [ 0 ] ) ) ; javaCallbackExecuted . set ( returnBool ) ; return null ; } } java . lang . String htmlWithScript = "<html><head>\n" + ( ( ( ( ( ( ( ( "<script<sp>language=\"JavaScript\">\n" + "function<sp>callCustomFunction()<sp>{\n" ) + "<sp>document.body.style.backgroundColor<sp>=<sp>\'red\'\n" ) + "\t\tjsCallbackToJava(true)\n" ) + "<sp>document.body.style.backgroundColor<sp>=<sp>\'red\'\n" 0 ) + "</script>\n" ) + "</head>\n" ) + "<body><sp>I\'m<sp>going<sp>to<sp>make<sp>a<sp>callback<sp>to<sp>java<sp></body>\n" ) + "</html>\n" ) ; browser . setText ( htmlWithScript ) ; new JavascriptCallback ( browser , "<sp>document.body.style.backgroundColor<sp>=<sp>\'red\'\n" 1 ) ; browser . addProgressListener ( callCustomFunctionUponLoad ) ; shell . open ( ) ; boolean passed = waitForPassCondition ( javaCallbackExecuted :: get ) ; java . lang . String message = "<sp>document.body.style.backgroundColor<sp>=<sp>\'red\'\n" 2 ; org . junit . Assert . assertTrue ( message , passed ) ; } }
public class aTest{ @Test public void testConcurrentFinishAndReleaseMemory ( ) { final java . util . concurrent . CountDownLatch doneLatch = new java . util . concurrent . CountDownLatch ( 1 ) ; final java . util . concurrent . CountDownLatch blockLatch = new java . util . concurrent . CountDownLatch ( 1 ) ; org . apache . flink . runtime . io . disk . iomanager . AsynchronousBufferFileWriter spillWriter = mock ( org . apache . flink . runtime . io . disk . iomanager . AsynchronousBufferFileWriter . class ) ; doAnswer ( new org . mockito . stubbing . Answer < java . lang . Void > ( ) { @ org . apache . flink . runtime . io . network . partition . Override public org . apache . flink . runtime . io . network . partition . Void answer ( org . mockito . invocation . InvocationOnMock invocation ) throws java . lang . Throwable { blockLatch . countDown ( ) ; doneLatch . await ( ) ; return null ; } } ) . when ( spillWriter ) . close ( ) ; org . apache . flink . runtime . io . disk . iomanager . IOManager ioManager = mock ( org . apache . flink . runtime . io . disk . iomanager . IOManager . class ) ; when ( ioManager . createBufferFileWriter ( nullable ( FileIOChannel . ID . class ) ) ) . thenReturn ( spillWriter ) ; final org . apache . flink . runtime . io . network . partition . SpillableSubpartition partition = new org . apache . flink . runtime . io . network . partition . SpillableSubpartition ( 0 , mock ( org . apache . flink . runtime . io . network . partition . ResultPartition . class ) , ioManager ) ; org . junit . Assert . assertEquals ( 0 , partition . releaseMemory ( ) ) ; java . util . concurrent . ExecutorService executor = java . util . concurrent . Executors . newSingleThreadExecutor ( ) ; java . util . concurrent . Future < java . lang . Void > blockingFinish = executor . submit ( new java . util . concurrent . Callable < java . lang . Void > ( ) { @ org . apache . flink . runtime . io . network . partition . Override public org . apache . flink . runtime . io . network . partition . Void call ( ) throws org . apache . flink . runtime . io . network . partition . Exception { partition . finish ( ) ; return null ; } } }
public class aTest{ @Test public void isExpired ( ) { final net . ripe . db . whois . update . keycert . PgpPublicKeyWrapper subject = net . ripe . db . whois . update . keycert . PgpPublicKeyWrapper . parse ( net . ripe . db . whois . common . rpsl . RpslObject . parse ( ( "certif:<sp>=XcVO\n" 6 + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "certif:<sp>Comment:<sp>GPGTools<sp>-<sp>http://gpgtools.org\n" 1 + "owner:<sp>Expired<sp><expired@ripe.net>\n" ) + "fingerpr:<sp>610A<sp>2457<sp>2BA3<sp>A575<sp>5F85<sp>4DD8<sp>5E62<sp>6C72<sp>C88C<sp>A438\n" ) + "certif:<sp>-----BEGIN<sp>PGP<sp>PUBLIC<sp>KEY<sp>BLOCK-----\n" ) + "certif:<sp>=XcVO\n" 7 ) + "certif:<sp>Comment:<sp>GPGTools<sp>-<sp>http://gpgtools.org\n" ) + "certif:<sp>=XcVO\n" 2 ) + "certif:<sp>=XcVO\n" 1 ) + "certif:<sp>=XcVO\n" 5 ) + "certif:<sp>yoRJqGii/1Z47FuudeJp1axQs1JER3OJ64IHuLblFIT7oS+YWBLopc1JABEBAAG0\n" ) + "certif:<sp>=XcVO\n" 3 ) + "certif:<sp>=XcVO\n" 8 ) + "certif:<sp>Comment:<sp>GPGTools<sp>-<sp>http://gpgtools.org\n" 3 ) + "certif:<sp>Comment:<sp>GPGTools<sp>-<sp>http://gpgtools.org\n" 4 ) + "certif:<sp>=XcVO\n" 9 ) + "certif:<sp>vHVECSOB0q32CN/wSrvVzL6hP8RuO0gwwVQH1V8KCYiY6kDEk33Qb4f1bTo+Wbi6\n" ) + "certif:<sp>9yFvn1OvLh3/idb3U1qSq2+Y6Snl/kvgoVJQuS9x1NePtCYL2kheTAGiswg6CxTF\n" ) + "certif:<sp>=XcVO\n" 4 ) + "certif:<sp>gAAKCRBeYmxyyIykON13BACeqmXZNe9H/SK2AMiFLIx2Zfyw/P0cKabn3Iaan7iF\n" ) + "certif:<sp>Comment:<sp>GPGTools<sp>-<sp>http://gpgtools.org\n" 0 ) + "certif:<sp>=XcVO\n" 0 ) + "certif:<sp>xw==\n" ) + "certif:<sp>=XcVO\n" ) + "certif:<sp>Comment:<sp>GPGTools<sp>-<sp>http://gpgtools.org\n" 2 ) + "certif:<sp>Comment:<sp>GPGTools<sp>-<sp>http://gpgtools.org\n" 5 ) + "certif:<sp>Comment:<sp>GPGTools<sp>-<sp>http://gpgtools.org\n" 6 ) ) ) ) ; org . junit . Assert . assertThat ( subject . isExpired ( dateTimeProvider ) , org . hamcrest . Matchers . is ( true ) ) ; } }
public class aTest{ @Test public void v10SerializeDeserialize ( ) { com . rusticisoftware . tincan . v10x . StatementsQuery query = new com . rusticisoftware . tincan . v10x . StatementsQuery ( ) ; query . setActivityID ( new java . net . URI ( "http://example.com/activity" ) ) ; query . setAgent ( getAgent ( "agent" 2 , "mbox" , "mailto:joeuser@example.com" ) ) ; query . setAscending ( true ) ; query . setFormat ( QueryResultFormat . EXACT ) ; query . setLimit ( 10 ) ; query . setRegistration ( java . util . UUID . randomUUID ( ) ) ; query . setRelatedActivities ( true ) ; query . setRelatedAgents ( true ) ; query . setSince ( new org . joda . time . DateTime ( ) ) ; query . setUntil ( new org . joda . time . DateTime ( ) ) ; query . setVerbID ( "agent" 3 ) ; java . util . List < java . lang . String > expected = new java . util . ArrayList < java . lang . String > ( java . util . Arrays . asList ( new java . lang . String [ ] { "agent" , "agent" 0 , "agent" 1 , "registration" , "related_activities" , "related_agents" , "agent" 4 , "until" , "limit" , "format" , "agent" 5 } ) ) ; java . util . Map < java . lang . String , java . lang . String > paramMap = query . toParameterMap ( ) ; for ( java . lang . String key : expected ) { org . junit . Assert . assertTrue ( paramMap . containsKey ( key ) ) ; } } }
public class aTest{ @Test public void testCloseScannerWhileSuspending ( ) { try ( org . apache . hadoop . hbase . client . ResultScanner scanner = org . apache . hadoop . hbase . client . TestAsyncTableScannerCloseWhileSuspending . TABLE . getScanner ( new org . apache . hadoop . hbase . client . Scan ( ) . setMaxResultSize ( 1 ) ) ) { org . apache . hadoop . hbase . client . TestAsyncTableScannerCloseWhileSuspending . TEST_UTIL . waitFor ( 10000 , 100 , new org . apache . hadoop . hbase . Waiter . ExplainingPredicate < java . lang . Exception > ( ) { @ org . apache . hadoop . hbase . client . Override public boolean evaluate ( ) throws org . apache . hadoop . hbase . client . Exception { return ( ( org . apache . hadoop . hbase . client . AsyncTableResultScanner ) ( scanner ) ) . isSuspended ( ) ; } @ org . apache . hadoop . hbase . client . Override public java . lang . String explainFailure ( ) throws org . apache . hadoop . hbase . client . Exception { return "The<sp>given<sp>scanner<sp>has<sp>been<sp>suspended<sp>in<sp>time" ; } } ) ; org . junit . Assert . assertEquals ( 1 , getScannersCount ( ) ) ; } }
public class aTest{ @Test public void testLookupPerfAPI ( ) { org . apache . directory . ldap . client . api . LdapConnection connection = org . apache . directory . server . integ . ServerIntegrationUtils . getAdminConnection ( getLdapServer ( ) ) ; org . apache . directory . api . ldap . model . entry . Entry entry = connection . lookup ( "uid=admin,ou=system" ) ; org . junit . Assert . assertNotNull ( entry ) ; long t0 = java . lang . System . currentTimeMillis ( ) ; for ( int i = 0 ; i < 50 ; i ++ ) { for ( int j = 0 ; j < 10000 ; j ++ ) { entry = connection . lookup ( "uid=admin,ou=system" ) ; } }
public class aTest{ @Test public void testEncodeCKANUnderscore ( ) { System . out . println ( ( ( com . telefonica . iot . cygnus . utils . CommonUtilsForTests . getTestTraceHead ( "[NGSICharsets.encodeCKAN]" ) ) + "--------<sp>Underscore<sp>is<sp>not<sp>encoded" ) ) ; java . lang . String in = "_" ; java . lang . String expected = "_" ; java . lang . String out = com . telefonica . iot . cygnus . utils . NGSICharsets . encodeCKAN ( in ) ; try { org . junit . Assert . assertEquals ( expected , out ) ; System . out . println ( ( ( ( ( com . telefonica . iot . cygnus . utils . CommonUtilsForTests . getTestTraceHead ( "[NGSICharsets.encodeCKAN]" ) ) + "-<sp>OK<sp>-<sp>'" ) + in ) + "'<sp>has<sp>not<sp>been<sp>encoded" ) ) ; } }
public class aTest{ @Test public void testQuarterTimestamp ( ) { java . lang . String sqlText = ( "select<sp>ts,<sp>QUARTER(ts)<sp>as<sp>\"QUARTER\"<sp>from<sp>" + ( com . splicemachine . derby . utils . SpliceDateFunctionsIT . tableWatcherI ) ) + "2009-07-02<sp>11:22:33.04<sp>|<sp>3<sp>|\n" 0 ; try ( com . splicemachine . derby . utils . ResultSet rs = methodWatcher . executeQuery ( sqlText ) ) { java . lang . String expected = "2009-07-02<sp>11:22:33.04<sp>|<sp>3<sp>|\n" 1 + ( ( ( ( ( ( "----------------------------------\n" + "2009-01-02<sp>11:22:33.04<sp>|<sp>1<sp>|\n" ) + "2009-07-02<sp>11:22:33.04<sp>|<sp>3<sp>|\n" ) + "2009-09-02<sp>11:22:33.04<sp>|<sp>3<sp>|\n" ) + "2012-12-31<sp>00:00:00.03<sp>|<sp>4<sp>|\n" ) + "<sp>2012-12-31<sp>20:38:40.0<sp>|<sp>4<sp>|\n" ) + "2013-12-31<sp>05:22:33.04<sp>|<sp>4<sp>|" ) ; org . junit . Assert . assertEquals ( ( ( "\n" + sqlText ) + "\n" ) , expected , TestUtils . FormattedResult . ResultFactory . toStringUnsorted ( rs ) ) ; } } }
public class aTest{ @Test public void searchUsersWhoCanStartProcessWithActorInitiatorAndFilterManagedBy ( ) { final org . bonitasoft . engine . identity . model . SUser sUser1 = createEnabledSUser ( "firstname1" , "lastname1" , "firstname2" 4 ) ; final org . bonitasoft . engine . identity . model . SUser sUser2 = createEnabledSUser ( "firstname2" , "firstname2" 1 , "firstname2" 3 , sUser1 . getId ( ) ) ; final org . bonitasoft . engine . identity . model . SUser sUser3 = createEnabledSUser ( "firstname3" , "lastname3" , "pwd3" , sUser2 . getId ( ) ) ; final org . bonitasoft . engine . identity . model . SUser sUser4 = createEnabledSUser ( "firstname4" , "lastname4" , "pwd4" ) ; final org . bonitasoft . engine . core . process . definition . model . SProcessDefinition sProcessDefinition = createSProcessDefinitionWithSActor ( "firstname2" 0 , "1.0" , "firstname2" 2 , true , java . util . Arrays . asList ( sUser2 , sUser3 , sUser4 ) ) ; getTransactionService ( ) . begin ( ) ; final org . bonitasoft . engine . persistence . FilterOption filterManagedBy = new org . bonitasoft . engine . persistence . FilterOption ( org . bonitasoft . engine . identity . model . SUser . class , org . bonitasoft . engine . identity . UserSearchDescriptor . MANAGER_USER_ID , sUser1 . getId ( ) ) ; final org . bonitasoft . engine . persistence . OrderByOption orderByFirstName = new org . bonitasoft . engine . persistence . OrderByOption ( org . bonitasoft . engine . identity . model . SUser . class , org . bonitasoft . engine . identity . UserSearchDescriptor . FIRST_NAME , org . bonitasoft . engine . persistence . OrderByType . ASC ) ; final org . bonitasoft . engine . persistence . QueryOptions searchOptions = new org . bonitasoft . engine . persistence . QueryOptions ( 0 , 5 , java . util . Arrays . asList ( orderByFirstName ) , java . util . Arrays . asList ( filterManagedBy ) , null ) ; final java . util . List < org . bonitasoft . engine . identity . model . SUser > result = processDefinitionService . searchUsersWhoCanStartProcessDeploymentInfo ( sProcessDefinition . getId ( ) , searchOptions ) ; getTransactionService ( ) . complete ( ) ; org . junit . Assert . assertEquals ( sUser2 , result . get ( 0 ) ) ; deleteSProcessDefinition ( sProcessDefinition ) ; deleteSUsers ( sUser1 , sUser2 ) ; } }
public class aTest{ @Test public void testWithTinkeyEciesAesCtrHmacAead ( ) { if ( com . google . crypto . tink . TestUtil . isAndroid ( ) ) { System . out . println ( "testWithTinkeyEciesAesGcmHkdf<sp>doesn't<sp>work<sp>on<sp>Android,<sp>skipping" ) ; return ; } com . google . crypto . tink . HybridDecrypt hybridDecrypt = com . google . crypto . tink . CleartextKeysetHandle . read ( com . google . crypto . tink . BinaryKeysetReader . withFile ( new java . io . File ( "testdata/ecies_private_keyset.bin" ) ) ) . getPrimitive ( com . google . crypto . tink . HybridDecrypt . class ) ; com . google . crypto . tink . HybridEncrypt hybridEncrypt = com . google . crypto . tink . CleartextKeysetHandle . read ( com . google . crypto . tink . BinaryKeysetReader . withFile ( new java . io . File ( "testdata/ecies_public_keyset.bin" ) ) ) . getPrimitive ( com . google . crypto . tink . HybridEncrypt . class ) ; byte [ ] plaintext = com . google . crypto . tink . subtle . Random . randBytes ( 20 ) ; byte [ ] contextInfo = com . google . crypto . tink . subtle . Random . randBytes ( 20 ) ; byte [ ] ciphertext = hybridEncrypt . encrypt ( plaintext , contextInfo ) ; + ( plaintext . length ) ) + 16 ) , ciphertext . length ) ; org . junit . Assert . assertArrayEquals ( plaintext , hybridDecrypt . decrypt ( ciphertext , contextInfo ) ) ; } }
public class aTest{ @Test public void itShouldNotFailWithCommand ( ) { waitAndClean ( ) ; session . activateOnCurrentThread ( ) ; try ( com . orientechnologies . orient . core . sql . executor . OResultSet res = session . command ( "insert<sp>into<sp>V<sp>set<sp>name<sp>=<sp>'foo'" ) ) { org . junit . Assert . assertEquals ( 1 , res . stream ( ) . count ( ) ) ; } }
public class aTest{ @Test public void simpleTest ( ) { com . bedatadriven . rebar . sql . client . SqlDatabase db = com . bedatadriven . rebar . sql . server . TestUtil . openUniqueDb ( ) ; final com . bedatadriven . rebar . sql . client . util . SqlKeyValueTable table = new com . bedatadriven . rebar . sql . client . util . SqlKeyValueTable ( db , "sync_regions" , "id" , "lastUpdate" ) ; db . execute ( table . createTableIfNotExists ( ) , com . bedatadriven . rebar . async . NullCallback . forVoid ( ) ) ; table . put ( "foo" , "bar" ) ; table . get ( "foo" , new com . google . gwt . user . client . rpc . AsyncCallback < java . lang . String > ( ) { @ com . bedatadriven . rebar . sql . server . Override public void onSuccess ( java . lang . String result ) { org . junit . Assert . assertThat ( result , org . hamcrest . CoreMatchers . equalTo ( "bar" ) ) ; } }
public class aTest{ @Test public void readData2 ( ) { org . mockito . Mockito . when ( filereader . fileExist ( org . mockito . Mockito . anyString ( ) ) ) . thenReturn ( true ) ; org . mockito . Mockito . when ( filereader . readAllLine ( org . mockito . Mockito . anyString ( ) ) ) . thenReturn ( new java . lang . String [ ] { + delta ( sum ) "1.413913535177E9<sp>CONNECTED" , "1.413913591806E9<sp>CONNECTED" , "1.413913610613E9<sp>OFF" , "1.413913620683E9<sp>DISCONNECTED" , "1.413913620883E9<sp>CONNECTED" , "1.413913622533E9<sp>CONNECTED" } ) ; bluetoothReader . setFileReader ( filereader ) ; java . util . List < com . att . aro . core . peripheral . pojo . BluetoothInfo > bluetoothInfo = bluetoothReader . readData ( "/" , 0 , 0 ) ; org . junit . Assert . assertTrue ( ( ( bluetoothInfo . size ( ) ) > 0 ) ) ; } }
public class aTest{ @Test public void testHttpProxy ( java . lang . String ) { int port = eu . trentorise . opendata . jackan . test . ckan . Tests . findFreePort ( ) ; org . littleshoot . proxy . HttpProxyServer server = org . littleshoot . proxy . impl . DefaultHttpProxyServer . bootstrap ( ) . withPort ( port ) . start ( ) ; try { eu . trentorise . opendata . jackan . CkanClient client = eu . trentorise . opendata . jackan . CkanClient . builder ( ) . setCatalogUrl ( eu . trentorise . opendata . jackan . test . ckan . ReadCkanIT . DATAHUB_IO ) . setProxy ( ( ( localhost + ":" ) + port ) ) . build ( ) ; java . util . List < java . lang . String > dsl = client . getDatasetList ( 10 , 0 ) ; org . junit . Assert . assertTrue ( ( ( dsl . size ( ) ) > 0 ) ) ; } }
public class aTest{ @Test public void testGetStorageUnitEntityByBusinessObjectDataAndStorageNameStorageUnitNoExists ( ) { org . finra . herd . model . jpa . BusinessObjectDataEntity businessObjectDataEntity = new org . finra . herd . model . jpa . BusinessObjectDataEntity ( ) ; org . finra . herd . model . jpa . StorageEntity storageEntity = new org . finra . herd . model . jpa . StorageEntity ( ) ; when ( storageDao . getStorageByName ( org . finra . herd . service . helper . STORAGE_NAME ) ) . thenReturn ( storageEntity ) ; when ( storageUnitDao . getStorageUnitByBusinessObjectDataAndStorage ( businessObjectDataEntity , storageEntity ) ) . thenReturn ( null ) ; when ( businessObjectDataHelper . businessObjectDataEntityAltKeyToString ( businessObjectDataEntity ) ) . thenReturn ( org . finra . herd . service . AbstractServiceTest . BUSINESS_OBJECT_DATA_KEY_AS_STRING ) ; try { storageUnitDaoHelper . getStorageUnitEntity ( org . finra . herd . service . helper . STORAGE_NAME , businessObjectDataEntity ) ; org . junit . Assert . fail ( ) ; } catch ( org . finra . herd . model . ObjectNotFoundException e ) { org . junit . Assert . assertEquals ( java . lang . String . format ( "Could<sp>not<sp>find<sp>storage<sp>unit<sp>in<sp>\"%s\"<sp>storage<sp>for<sp>the<sp>business<sp>object<sp>data<sp>{%s}." , org . finra . herd . service . helper . STORAGE_NAME , org . finra . herd . service . AbstractServiceTest . BUSINESS_OBJECT_DATA_KEY_AS_STRING ) , e . getMessage ( ) ) ; } }
public class aTest{ @Test public void testGenerateInsertAction ( ) { java . lang . String createTable = "CREATE<sp>TABLE<sp>User<sp>{Required<sp>Int64<sp>user_id;<sp>Required<sp>String<sp>name;<sp>}<sp>" + "primary<sp>key(user_id),<sp>entity<sp>group<sp>root,<sp>entity<sp>group<sp>key(user_id);" ; java . lang . String insert = "Insert<sp>into<sp>User(user_id,name)<sp>values(1,'binlijin');" ; try { com . alibaba . wasp . meta . TableSchemaCacheReader reader = com . alibaba . wasp . meta . TableSchemaCacheReader . getInstance ( com . alibaba . wasp . plan . action . TestInsertAction . conf ) ; com . alibaba . wasp . plan . action . TestInsertAction . context . setTsr ( reader ) ; com . alibaba . wasp . plan . parser . druid . DruidDQLParser dqlParser = new com . alibaba . wasp . plan . parser . druid . DruidDQLParser ( com . alibaba . wasp . plan . action . TestInsertAction . conf , null ) ; com . alibaba . wasp . plan . parser . druid . DruidDDLParser ddlParser = new com . alibaba . wasp . plan . parser . druid . DruidDDLParser ( com . alibaba . wasp . plan . action . TestInsertAction . conf ) ; com . alibaba . wasp . plan . parser . druid . DruidDMLParser dmlParser = new com . alibaba . wasp . plan . parser . druid . DruidDMLParser ( com . alibaba . wasp . plan . action . TestInsertAction . conf , null ) ; com . alibaba . wasp . plan . parser . WaspParser druidParser = new com . alibaba . wasp . plan . parser . WaspParser ( ddlParser , dqlParser , dmlParser ) ; com . alibaba . wasp . plan . action . TestInsertAction . context . setSql ( createTable ) ; druidParser . generatePlan ( com . alibaba . wasp . plan . action . TestInsertAction . context ) ; com . alibaba . wasp . plan . Plan plan = com . alibaba . wasp . plan . action . TestInsertAction . context . getPlan ( ) ; if ( plan instanceof com . alibaba . wasp . plan . CreateTablePlan ) { com . alibaba . wasp . plan . CreateTablePlan createPlan = ( ( com . alibaba . wasp . plan . CreateTablePlan ) ( plan ) ) ; com . alibaba . wasp . meta . FTable table = createPlan . getTable ( ) ; com . alibaba . wasp . meta . TableSchemaCacheReader . getService ( com . alibaba . wasp . plan . action . TestInsertAction . conf ) . createTable ( table ) ; } com . alibaba . wasp . plan . action . TestInsertAction . context . setSql ( insert ) ; druidParser . generatePlan ( com . alibaba . wasp . plan . action . TestInsertAction . context ) ; plan = com . alibaba . wasp . plan . action . TestInsertAction . context . getPlan ( ) ; if ( plan instanceof com . alibaba . wasp . plan . InsertPlan ) { com . alibaba . wasp . plan . InsertPlan insertPlan = ( ( com . alibaba . wasp . plan . InsertPlan ) ( plan ) ) ; java . util . List < com . alibaba . wasp . plan . action . InsertAction > actions = insertPlan . getActions ( ) ; org . junit . Assert . assertEquals ( actions . size ( ) , 1 ) ; } } }
public class aTest{ @Test public void testDatatypeValue6 ( ) { java . lang . String init = "'01'^^xsd:integer,<sp>'1'^^xsd:integer" 2 + ( ( ( ( ( ( ( ( "graph<sp>us:g1<sp>{<sp><Jack><sp>" + "rdf:value<sp>" ) + "<sp>1,<sp>1.0,<sp>'1'^^xsd:long,<sp>1e0,<sp>'1'^^xsd:double,<sp>'1'^^xsd:float,<sp>" ) + "'01'^^xsd:integer,<sp>'1'^^xsd:integer" 4 ) + "'1'^^xsd:boolean,<sp>'0'^^xsd:boolean<sp>.<sp>}" ) + "graph<sp>us:g2<sp>{<sp><Jack><sp>rdf:value<sp>false,<sp>'01'^^xsd:double,<sp>'1.0'^^xsd:float," ) + "'01'^^xsd:integer,<sp>'1'^^xsd:integer" ) + ",<sp>'1'^^xsd:short,<sp>'1'^^xsd:byte<sp>,<sp>'1'^^xsd:int,<sp>'01'^^xsd:byte,<sp>'1'^^xsd:byte}" ) + "'01'^^xsd:integer,<sp>'1'^^xsd:integer" 1 ) ; java . lang . String q = "'01'^^xsd:integer,<sp>'1'^^xsd:integer" 3 + ( ( ( ( "graph<sp>?g<sp>{" + "?x<sp>?p<sp>?v" ) + "'01'^^xsd:integer,<sp>'1'^^xsd:integer" 1 ) + "'01'^^xsd:integer,<sp>'1'^^xsd:integer" 0 ) + "order<sp>by<sp>?x<sp>?v<sp>" ) ; fr . inria . corese . core . Graph g = createGraph ( ) ; fr . inria . corese . core . query . QueryProcess exec = fr . inria . corese . core . query . QueryProcess . create ( g ) ; exec . query ( init ) ; fr . inria . corese . kgram . core . Mappings map = exec . query ( q ) ; org . junit . Assert . assertEquals ( 20 , map . size ( ) ) ; } }
public class aTest{ @Test public void testExceptions ( ) { org . springframework . security . core . context . SecurityContextHolder . getContext ( ) . setAuthentication ( null ) ; javax . security . auth . Subject subject = new javax . security . auth . Subject ( ) ; org . pentaho . platform . osgi . SpringSecurityLoginModuleTest . TestCallbackHandler testCallbackHandler = new org . pentaho . platform . osgi . SpringSecurityLoginModuleTest . TestCallbackHandler ( "joe" ) ; org . pentaho . platform . osgi . SpringSecurityLoginModule loginModule = new org . pentaho . platform . osgi . SpringSecurityLoginModule ( ) ; org . springframework . security . authentication . AuthenticationManager authenticationManager = mock ( org . springframework . security . authentication . AuthenticationManager . class ) ; org . pentaho . platform . api . engine . IUserRoleListService userRoleListService = mock ( org . pentaho . platform . api . engine . IUserRoleListService . class ) ; org . pentaho . platform . api . engine . IAuthorizationPolicy authorizationPolicy = mock ( org . pentaho . platform . api . engine . IAuthorizationPolicy . class ) ; org . springframework . security . core . Authentication authentication = mock ( org . springframework . security . core . Authentication . class ) ; java . util . Collection authorities = java . util . Arrays . asList ( new org . springframework . security . core . GrantedAuthority [ ] { new org . springframework . security . core . authority . SimpleGrantedAuthority ( "Authenticated" ) , new org . springframework . security . core . authority . SimpleGrantedAuthority ( "Administrator" ) } ) ; org . springframework . security . core . Authentication authentication2 = mock ( org . springframework . security . core . Authentication . class ) ; java . util . Collection authorities2 = java . util . Arrays . asList ( new org . springframework . security . core . GrantedAuthority [ ] { new org . springframework . security . core . authority . SimpleGrantedAuthority ( "Authenticated" ) , new org . springframework . security . core . authority . SimpleGrantedAuthority ( "Should<sp>have<sp>thrown<sp>UnsupportedCallbackException" 1 ) } ) ; org . pentaho . platform . engine . core . system . PentahoSystem . registerObject ( userRoleListService , org . pentaho . platform . api . engine . IUserRoleListService . class ) ; when ( authorizationPolicy . isAllowed ( AdministerSecurityAction . NAME ) ) . thenReturn ( true ) . thenReturn ( true ) . thenReturn ( false ) ; when ( authentication . getAuthorities ( ) ) . thenReturn ( authorities ) ; when ( authentication . getName ( ) ) . thenReturn ( "joe" ) ; when ( authentication . isAuthenticated ( ) ) . thenReturn ( true ) ; when ( authentication2 . getAuthorities ( ) ) . thenReturn ( authorities2 ) ; when ( authentication2 . getName ( ) ) . thenReturn ( "pat" ) ; when ( authentication2 . isAuthenticated ( ) ) . thenReturn ( true ) ; when ( authenticationManager . authenticate ( argThat ( new org . pentaho . platform . osgi . SpringSecurityLoginModuleTest . AuthenticationManagerMatcher ( "joe" ) ) ) ) . thenReturn ( authentication ) ; when ( authenticationManager . authenticate ( argThat ( new org . pentaho . platform . osgi . SpringSecurityLoginModuleTest . AuthenticationManagerMatcher ( "pat" ) ) ) ) . thenReturn ( authentication ) ; when ( authenticationManager . authenticate ( argThat ( new org . pentaho . platform . osgi . SpringSecurityLoginModuleTest . AuthenticationManagerMatcher ( "suzy" ) ) ) ) . thenThrow ( new org . springframework . security . core . userdetails . UsernameNotFoundException ( "Error" ) ) ; when ( userRoleListService . getRolesForUser ( null , "joe" ) ) . thenReturn ( java . util . Arrays . < java . lang . String > asList ( "Authenticated" , "Administrator" ) ) ; when ( userRoleListService . getRolesForUser ( null , "pat" ) ) . thenReturn ( java . util . Arrays . < java . lang . String > asList ( "Authenticated" , "Should<sp>have<sp>thrown<sp>UnsupportedCallbackException" 1 ) ) ; loginModule . setAuthenticationManager ( authenticationManager ) ; loginModule . setAuthorizationPolicy ( authorizationPolicy ) ; loginModule . initialize ( subject , testCallbackHandler , java . util . Collections . emptyMap ( ) , java . util . Collections . emptyMap ( ) ) ; loginModule . login ( ) ; loginModule . commit ( ) ; verify ( authenticationManager ) . authenticate ( argThat ( new org . pentaho . platform . osgi . SpringSecurityLoginModuleTest . AuthenticationManagerMatcher ( "joe" ) ) ) ; org . junit . Assert . assertEquals ( 4 , subject . getPrincipals ( ) . size ( ) ) ; subject . getPrincipals ( ) . toArray ( ) [ 3 ] . equals ( "karaf_admin" ) ; testCallbackHandler = new org . pentaho . platform . osgi . SpringSecurityLoginModuleTest . TestCallbackHandler ( "ioe" ) ; loginModule . initialize ( subject , testCallbackHandler , java . util . Collections . emptyMap ( ) , java . util . Collections . emptyMap ( ) ) ; try { loginModule . login ( ) ; org . junit . Assert . fail ( "Should<sp>have<sp>thrown<sp>UnsupportedCallbackException" 0 ) ; } }
public class aTest{ @Test public void testItemFailureRethrownOnInvoke ( ) { final org . apache . flink . streaming . connectors . elasticsearch . ElasticsearchSinkBaseTest . DummyElasticsearchSink < java . lang . String > sink = new org . apache . flink . streaming . connectors . elasticsearch . ElasticsearchSinkBaseTest . DummyElasticsearchSink ( new java . util . HashMap < java . lang . String , java . lang . String > ( ) , new org . apache . flink . streaming . connectors . elasticsearch . ElasticsearchSinkBaseTest . SimpleSinkFunction < java . lang . String > ( ) , new org . apache . flink . streaming . connectors . elasticsearch . util . NoOpFailureHandler ( ) ) ; final org . apache . flink . streaming . util . OneInputStreamOperatorTestHarness < java . lang . String , java . lang . Object > testHarness = new org . apache . flink . streaming . util . OneInputStreamOperatorTestHarness ( new org . apache . flink . streaming . api . operators . StreamSink ( sink ) ) ; testHarness . open ( ) ; sink . setMockItemFailuresListForNextBulkItemResponses ( java . util . Collections . singletonList ( new java . lang . Exception ( "artificial<sp>failure<sp>for<sp>record" ) ) ) ; testHarness . processElement ( new org . apache . flink . streaming . runtime . streamrecord . StreamRecord ( "msg" ) ) ; verify ( sink . getMockBulkProcessor ( ) , times ( 1 ) ) . add ( any ( org . elasticsearch . action . index . IndexRequest . class ) ) ; sink . manualBulkRequestWithAllPendingRequests ( ) ; try { testHarness . processElement ( new org . apache . flink . streaming . runtime . streamrecord . StreamRecord ( "next<sp>msg" ) ) ; } catch ( java . lang . Exception e ) { org . junit . Assert . assertTrue ( e . getCause ( ) . getMessage ( ) . contains ( "artificial<sp>failure<sp>for<sp>record" ) ) ; return ; } }
public class aTest{ @Test public void test_BrowserFunction_callback_afterPageReload ( ) { org . junit . Assume . assumeFalse ( webkit1SkipMsg ( ) , isWebkit1 ) ; java . util . concurrent . atomic . AtomicBoolean javaCallbackExecuted = new java . util . concurrent . atomic . AtomicBoolean ( false ) ; java . util . concurrent . atomic . AtomicInteger callCount = new java . util . concurrent . atomic . AtomicInteger ( 0 ) ; class JavascriptCallback extends org . eclipse . swt . chromium . BrowserFunction { JavascriptCallback ( org . eclipse . swt . chromium . Browser browser , java . lang . String name ) { ( browser , name ) ; } @ org . eclipse . swt . tests . junit . Override public java . lang . Object function ( java . lang . Object [ ] arguments ) { if ( ( callCount . get ( ) ) == 0 ) { callCount . set ( 1 ) ; browser . setText ( "2nd<sp>page<sp>load" ) ; } else { javaCallbackExecuted . set ( true ) ; } return null ; } } browser . setText ( "1st<sp>(initial)<sp>page<sp>load" ) ; new JavascriptCallback ( browser , "jsCallbackToJava" ) ; browser . execute ( "jsCallbackToJava()" ) ; browser . addProgressListener ( org . eclipse . swt . browser . ProgressListener . completedAdapter ( ( e ) -> browser . execute ( "jsCallbackToJava()" ) ) ) ; shell . open ( ) ; boolean passed = waitForPassCondition ( javaCallbackExecuted :: get ) ; java . lang . String message = "A<sp>javascript<sp>callback<sp>should<sp>work<sp>after<sp>a<sp>page<sp>has<sp>been<sp>reloaded.<sp>But<sp>something<sp>went<sp>wrong" ; org . junit . Assert . assertTrue ( message , passed ) ; } }
public class aTest{ @Test public void testConfNameNodeRPCAddr ( ) { try { java . util . Collection < java . net . URI > namenodes = org . apache . hadoop . hdfs . DFSUtil . getInternalNsRpcUris ( smartContext . getConf ( ) ) ; java . util . List < java . net . URI > uriList = new java . util . ArrayList ( namenodes ) ; org . smartdata . conf . SmartConf conf = new org . smartdata . conf . SmartConf ( ) ; java . lang . String dbFile = org . smartdata . metastore . TestDBUtil . getUniqueEmptySqliteDBFile ( ) ; java . lang . String dbUrl = ( org . smartdata . metastore . utils . MetaStoreUtils . SQLITE_URL_PREFIX ) + dbFile ; conf . set ( SmartConfKeys . SMART_METASTORE_DB_URL_KEY , dbUrl ) ; org . smartdata . server . SmartServer ssm = null ; try { ssm = org . smartdata . server . SmartServer . launchWith ( conf ) ; java . lang . Thread . sleep ( 2000 ) ; } catch ( java . lang . Exception e ) { org . junit . Assert . fail ( "Should<sp>work<sp>without<sp>specifying<sp>NN" ) ; } finally { if ( ssm != null ) { ssm . shutdown ( ) ; } } conf . set ( SmartConfKeys . SMART_DFS_NAMENODE_RPCSERVER_KEY , uriList . get ( 0 ) . toString ( ) ) ; java . lang . String [ ] args = new java . lang . String [ ] { "-D" , ( ( org . smartdata . conf . SmartConfKeys . SMART_DFS_NAMENODE_RPCSERVER_KEY ) + "=" ) + ( uriList . get ( 0 ) . toString ( ) ) } ; org . smartdata . server . SmartServer regServer = org . smartdata . server . SmartServer . launchWith ( args , conf ) ; org . junit . Assert . assertNotNull ( regServer ) ; java . lang . Thread . sleep ( 1000 ) ; regServer . shutdown ( ) ; args = new java . lang . String [ ] { "-h" } ; org . smartdata . server . SmartServer . launchWith ( args , conf ) ; } }
public class aTest{ @Test public void testEvaluateTwoIndexTwoVarOrder4 ( ) { final java . lang . String indexSparqlString = "" + ( ( ( ( "SELECT<sp>?e<sp>?c<sp>?l<sp>" + "{" ) + "<sp>?e<sp>a<sp>?c<sp>.<sp>" ) + "<sp>?e<sp><http://www.w3.org/2000/01/rdf-schema#label><sp>?l<sp>" ) + "}" ) ; final java . lang . String indexSparqlString2 = "" + ( ( ( ( "SELECT<sp>?e<sp>?o<sp>?l<sp>" + "{" ) + "<sp>?e<sp><uri:talksTo><sp>?o<sp>.<sp>" ) + "<sp>?e<sp><uri:talksTo><sp>?o<sp>.<sp>" 0 ) + "}" ) ; final java . lang . String queryString = "" + ( ( ( ( ( ( "SELECT<sp>?e<sp>?c<sp>?l<sp>?o<sp>" + "{" ) + "<sp>?e<sp>a<sp>?c<sp>.<sp>" ) + "<sp>?e<sp><http://www.w3.org/2000/01/rdf-schema#label><sp>?l<sp>.<sp>" ) + "<sp>?e<sp><uri:talksTo><sp>?o<sp>.<sp>" ) + "<sp>?e<sp><uri:talksTo><sp>?o<sp>.<sp>" 0 ) + "}" ) ; final org . eclipse . rdf4j . query . parser . sparql . SPARQLParser sp = new org . eclipse . rdf4j . query . parser . sparql . SPARQLParser ( ) ; final org . eclipse . rdf4j . query . parser . ParsedQuery index1 = sp . parseQuery ( indexSparqlString , null ) ; final org . eclipse . rdf4j . query . parser . ParsedQuery index2 = sp . parseQuery ( indexSparqlString2 , null ) ; final java . util . List < org . apache . rya . indexing . external . tupleSet . ExternalTupleSet > index = com . google . common . collect . Lists . newArrayList ( ) ; final org . apache . rya . indexing . external . tupleSet . SimpleExternalTupleSet ais1 = new org . apache . rya . indexing . external . tupleSet . SimpleExternalTupleSet ( ( ( org . eclipse . rdf4j . query . algebra . Projection ) ( index1 . getTupleExpr ( ) ) ) ) ; final org . apache . rya . indexing . external . tupleSet . SimpleExternalTupleSet ais2 = new org . apache . rya . indexing . external . tupleSet . SimpleExternalTupleSet ( ( ( org . eclipse . rdf4j . query . algebra . Projection ) ( index2 . getTupleExpr ( ) ) ) ) ; index . add ( ais1 ) ; index . add ( ais2 ) ; final org . eclipse . rdf4j . query . parser . ParsedQuery pq = sp . parseQuery ( queryString , null ) ; final org . eclipse . rdf4j . query . algebra . TupleExpr tup = pq . getTupleExpr ( ) . clone ( ) ; provider . setIndices ( index ) ; final org . apache . rya . indexing . pcj . matching . PCJOptimizer pcj = new org . apache . rya . indexing . pcj . matching . PCJOptimizer ( index , false , provider ) ; pcj . optimize ( tup , null , null ) ; final org . apache . rya . indexing . IndexPlanValidator . IndexPlanValidator ipv = new org . apache . rya . indexing . IndexPlanValidator . IndexPlanValidator ( false ) ; org . junit . Assert . assertEquals ( false , ipv . isValid ( tup ) ) ; } }
public class aTest{ @Test public void testSearchWithToLongString ( ) { int total = 5 ; for ( int i = 0 ; i < total ; i ++ ) { com . liferay . data . engine . service . test . DEDataEngineTestUtil . insertDEDataRecordCollection ( _adminUser , _group , ( "y" + i ) , ( "y" + i ) , _deDataDefinitionService , _deDataRecordCollectionService ) ; } java . lang . String longStringText = com . liferay . petra . string . StringBundler . concat ( "Lorem<sp>ipsum<sp>dolor<sp>sit<sp>amet,<sp>consectetur<sp>adipiscing<sp>elit,<sp>sed<sp>do<sp>" , "eiusmod<sp>tempor<sp>incididunt<sp>ut<sp>labore<sp>et<sp>dolore<sp>magna<sp>aliqua.<sp>Ut<sp>" , "enim<sp>ad<sp>minim<sp>veniam,<sp>quis<sp>nostrud<sp>exercitation<sp>ullamco<sp>laboris<sp>" , "nisi<sp>ut<sp>aliquip<sp>ex<sp>ea<sp>commodo<sp>consequat.<sp>Duis<sp>aute<sp>irure<sp>dolor<sp>" , "in<sp>reprehenderit<sp>in<sp>voluptate<sp>velit<sp>esse<sp>cillum<sp>dolore<sp>eu<sp>fugiat<sp>" , "nulla<sp>pariatur.<sp>Excepteur<sp>sint<sp>occaecat<sp>cupidatat<sp>non<sp>proident,<sp>" , "sunt<sp>in<sp>culpa<sp>qui<sp>officia<sp>deserunt<sp>mollit<sp>anim<sp>id<sp>est<sp>laborum." ) ; com . liferay . data . engine . service . test . DEDataEngineTestUtil . insertDEDataRecordCollection ( _adminUser , _group , longStringText , "Name" , _deDataDefinitionService , _deDataRecordCollectionService ) ; java . util . List < com . liferay . data . engine . model . DEDataRecordCollection > deDataRecordCollections = searchDEDataRecordCollection ( _group , longStringText ) ; com . liferay . portal . search . test . util . IdempotentRetryAssert . retryAssert ( 3 , TimeUnit . SECONDS , ( ) -> { org . junit . Assert . assertEquals ( deDataRecordCollections . toString ( ) , 6 , deDataRecordCollections . size ( ) ) ; return null ; } }
public class aTest{ @Test public void testSimplePattern ( ) { org . kie . workbench . common . widgets . client . datamodel . AsyncPackageDataModelOracle oracle = mock ( org . kie . workbench . common . widgets . client . datamodel . AsyncPackageDataModelOracle . class ) ; org . drools . workbench . models . datamodel . rule . FactPattern pattern = new org . drools . workbench . models . datamodel . rule . FactPattern ( ) ; pattern . setBoundName ( "pp" ) ; pattern . setFactType ( "House" ) ; model . addLhsItem ( pattern ) ; org . drools . workbench . models . datamodel . rule . FactPattern pattern2 = new org . drools . workbench . models . datamodel . rule . FactPattern ( ) ; org . drools . workbench . models . datamodel . rule . SingleFieldConstraint constraint = new org . drools . workbench . models . datamodel . rule . SingleFieldConstraint ( ) ; constraint . setFactType ( "House" ) ; constraint . setFieldName ( "this" ) ; constraint . setFieldType ( "org.mortgages.House" ) ; pattern2 . addConstraint ( constraint ) ; model . addLhsItem ( pattern ) ; when ( oracle . getFieldClassName ( "House" , "this" ) ) . thenReturn ( "org.mortgages.House" ) ; org . drools . workbench . screens . guided . rule . client . editor . util . ConstraintValueEditorHelper helper = new org . drools . workbench . screens . guided . rule . client . editor . util . ConstraintValueEditorHelper ( model , oracle , "House" , "this" , constraint , "House" , new org . kie . soup . project . datamodel . oracle . DropDownData ( ) ) ; helper . isApplicableBindingsInScope ( "pp" , new org . uberfire . client . callbacks . Callback < java . lang . Boolean > ( ) { @ org . drools . workbench . screens . guided . rule . client . editor . Override public void callback ( java . lang . Boolean result ) { org . junit . Assert . assertTrue ( result ) ; } } }
public class aTest{ @Test public void testSolutionRightSide ( ) { org . apache . commons . math . analysis . UnivariateRealFunction f = new org . apache . commons . math . analysis . SinFunction ( ) ; org . apache . commons . math . analysis . solvers . UnivariateRealSolver solver = getSolver ( ) ; double left = - 1.5 ; double right = 0.05 ; for ( int i = 0 ; i < 10 ; i ++ ) { double solution = getSolution ( solver , 100 , f , left , right , AllowedSolution . RIGHT_SIDE ) ; if ( ! ( java . lang . Double . isNaN ( solution ) ) ) { org . junit . Assert . assertTrue ( ( solution >= 0.0 ) ) ; } }
public class aTest{ @Test public void testNestedFullDepJoin ( ) { org . teiid . query . optimizer . capabilities . BasicSourceCapabilities pm1Caps = org . teiid . query . optimizer . TestOptimizer . getTypicalCapabilities ( ) ; pm1Caps . setCapabilitySupport ( Capability . QUERY_FROM_JOIN_INNER , false ) ; pm1Caps . setCapabilitySupport ( Capability . QUERY_FROM_JOIN_OUTER , false ) ; org . teiid . query . optimizer . capabilities . BasicSourceCapabilities caps = org . teiid . query . optimizer . TestOptimizer . getTypicalCapabilities ( ) ; caps . setCapabilitySupport ( Capability . FULL_DEPENDENT_JOIN , true ) ; org . teiid . query . metadata . TransformationMetadata metadata = org . teiid . query . optimizer . TestOptimizer . example1 ( ) ; org . teiid . query . unittest . RealMetadataFactory . setCardinality ( "pm1.g1" , 5 , metadata ) ; org . teiid . query . unittest . RealMetadataFactory . setCardinality ( "b" 1 , 5 , metadata ) ; org . teiid . query . unittest . RealMetadataFactory . setCardinality ( "b" 0 , 1000 , metadata ) ; org . teiid . query . unittest . RealMetadataFactory . setCardinality ( "b" 3 , 10000 , metadata ) ; org . teiid . query . optimizer . capabilities . FakeCapabilitiesFinder capFinder = new org . teiid . query . optimizer . capabilities . FakeCapabilitiesFinder ( ) ; capFinder . addCapabilities ( "pm1" , pm1Caps ) ; capFinder . addCapabilities ( "pm2" , caps ) ; org . teiid . query . util . CommandContext cc = new org . teiid . query . util . CommandContext ( ) ; cc . setBufferManager ( org . teiid . common . buffer . BufferManagerFactory . getStandaloneBufferManager ( ) ) ; org . teiid . query . processor . ProcessorPlan plan = org . teiid . query . optimizer . TestOptimizer . getPlan ( org . teiid . query . optimizer . TestOptimizer . helpGetCommand ( "b" 2 , metadata ) , metadata , capFinder , null , true , cc ) ; org . teiid . query . optimizer . TestOptimizer . checkAtomicQueries ( new java . lang . String [ ] { "WITH<sp>TEIID_TEMP__1<sp>(col1,<sp>col2,<sp>col3,<sp>col4)<sp>AS<sp>(<dependent<sp>values>)<sp>SELECT<sp>g_0.col2<sp>AS<sp>c_0,<sp>g_0.col3<sp>AS<sp>c_1,<sp>g_0.col4<sp>AS<sp>c_2<sp>FROM<sp>TEIID_TEMP__1<sp>AS<sp>g_0<sp>LEFT<sp>OUTER<sp>JOIN<sp>pm2.g2<sp>AS<sp>g_1<sp>ON<sp>g_0.col1<sp>=<sp>g_1.e4<sp>ORDER<sp>BY<sp>c_0" } , plan ) ; org . teiid . query . optimizer . TestOptimizer . checkNodeTypes ( plan , TestOptimizer . FULL_PUSHDOWN ) ; java . util . List < ? > [ ] expected = new java . util . List < ? > [ ] { java . util . Arrays . asList ( new java . lang . Object [ ] { "a" , 2 , 1.0 } ) } ; org . teiid . query . processor . HardcodedDataManager dataManager = new org . teiid . query . processor . HardcodedDataManager ( org . teiid . query . unittest . RealMetadataFactory . example1Cached ( ) ) ; dataManager . addData ( "SELECT<sp>g_0.e2<sp>AS<sp>c_0<sp>FROM<sp>g2<sp>AS<sp>g_0<sp>ORDER<sp>BY<sp>c_0" , new java . util . List < ? > [ ] { java . util . Arrays . asList ( 1 ) , java . util . Arrays . asList ( 2 ) } ) ; dataManager . addData ( "SELECT<sp>g_0.e1<sp>AS<sp>c_0,<sp>g_0.e2<sp>AS<sp>c_1<sp>FROM<sp>g1<sp>AS<sp>g_0<sp>ORDER<sp>BY<sp>c_0" , new java . util . List < ? > [ ] { java . util . Arrays . asList ( "a" , 1 ) , java . util . Arrays . asList ( "b" , 2 ) } ) ; dataManager . addData ( "SELECT<sp>g_0.e1<sp>AS<sp>c_0,<sp>g_0.e2<sp>AS<sp>c_1,<sp>g_0.e4<sp>AS<sp>c_2<sp>FROM<sp>g3<sp>AS<sp>g_0<sp>WHERE<sp>g_0.e1<sp>IN<sp>('a',<sp>'b')<sp>AND<sp>g_0.e2<sp>IN<sp>(1,<sp>2)<sp>ORDER<sp>BY<sp>c_0" , new java . util . List < ? > [ ] { java . util . Arrays . asList ( "a" , 2 , 1.0 ) } ) ; dataManager . addData ( "WITH<sp>TEIID_TEMP__1<sp>(e4,<sp>e1,<sp>e2,<sp>e2)<sp>AS<sp>(?)<sp>SELECT<sp>g_0.col2<sp>AS<sp>c_0,<sp>g_0.col3<sp>AS<sp>c_1,<sp>g_0.col4<sp>AS<sp>c_2<sp>FROM<sp>TEIID_TEMP__1<sp>AS<sp>g_0<sp>LEFT<sp>OUTER<sp>JOIN<sp>g2<sp>AS<sp>g_1<sp>ON<sp>g_0.col1<sp>=<sp>g_1.e4<sp>ORDER<sp>BY<sp>c_0" , new java . util . List < ? > [ ] { java . util . Arrays . asList ( "a" , 2 , 1.0 ) } ) ; org . teiid . query . processor . TestProcessor . TestProcessor . helpProcess ( plan , dataManager , expected ) ; org . teiid . language . Select select = ( ( org . teiid . language . Select ) ( dataManager . getPushdownCommands ( ) . get ( 3 ) ) ) ; java . util . List < ? extends java . util . List < ? > > vals = select . getWith ( ) . getItems ( ) . get ( 0 ) . getDependentValues ( ) ; org . junit . Assert . assertEquals ( 1 , vals . size ( ) ) ; } }
public class aTest{ @Test public void collectConcurrentResults ( ) { java . util . concurrent . ExecutorService ex = java . util . concurrent . Executors . newFixedThreadPool ( 10 ) ; try { java . util . List < io . trane . future . Promise < java . lang . Integer > > promises = java . util . stream . Stream . generate ( ( ) -> io . trane . future . Promise . < java . lang . Integer > apply ( ) ) . limit ( 20000 ) . collect ( toList ( ) ) ; java . util . concurrent . atomic . AtomicBoolean start = new java . util . concurrent . atomic . AtomicBoolean ( ) ; io . trane . future . Future < java . util . List < java . lang . Integer > > future = io . trane . future . Future . collect ( promises ) ; for ( io . trane . future . Promise < java . lang . Integer > p : promises ) { ex . submit ( ( ) -> { while ( true ) { if ( start . get ( ) ) break ; } p . setValue ( p . hashCode ( ) ) ; } ) ; } start . set ( true ) ; java . util . List < java . lang . Integer > expected = promises . stream ( ) . map ( ( p ) -> p . hashCode ( ) ) . collect ( toList ( ) ) ; java . util . List < java . lang . Integer > result = future . get ( java . time . Duration . ofSeconds ( 1 ) ) ; org . junit . Assert . assertArrayEquals ( expected . toArray ( ) , result . toArray ( ) ) ; } }
public class aTest{ @Test public void testDeleteThrow ( ) { com . j256 . ormlite . dao . Dao < com . j256 . ormlite . dao . Foo , java . lang . Integer > dao = createDao ( com . j256 . ormlite . dao . Foo . class , true ) ; com . j256 . ormlite . dao . Foo foo = new com . j256 . ormlite . dao . Foo ( ) ; org . junit . Assert . assertEquals ( 1 , dao . create ( foo ) ) ; com . j256 . ormlite . support . DatabaseConnection conn = connectionSource . getReadWriteConnection ( com . j256 . ormlite . dao . FOO_TABLE_NAME ) ; try { conn . close ( ) ; dao . delete ( foo ) ; } }
public class aTest{ @Test public void checkResourceIsCloseable ( ) { org . apache . commons . pool2 . impl . GenericObjectPoolConfig config = new org . apache . commons . pool2 . impl . GenericObjectPoolConfig ( ) ; config . setMaxTotal ( 1 ) ; config . setBlockWhenExhausted ( false ) ; java . util . List < redis . clients . jedis . JedisShardInfo > shards = new java . util . ArrayList < redis . clients . jedis . JedisShardInfo > ( ) ; shards . add ( new redis . clients . jedis . JedisShardInfo ( new java . net . URI ( "redis://:foobared@localhost:6380" ) ) ) ; shards . add ( new redis . clients . jedis . JedisShardInfo ( new java . net . URI ( "redis://:foobared@localhost:6379" ) ) ) ; redis . clients . jedis . ShardedJedisPool pool = new redis . clients . jedis . ShardedJedisPool ( config , shards ) ; redis . clients . jedis . ShardedJedis jedis = pool . getResource ( ) ; try { jedis . set ( "hello" , "jedis" ) ; } finally { jedis . close ( ) ; } redis . clients . jedis . ShardedJedis jedis2 = pool . getResource ( ) ; try { org . junit . Assert . assertEquals ( jedis , jedis2 ) ; } }
public class aTest{ @Test public void testSetColumn ( ) { org . apache . commons . math . linear . RealMatrix m = new org . apache . commons . math . linear . BlockRealMatrix ( subTestData ) ; double [ ] mColumn3 = columnToArray ( subColumn3 ) ; org . junit . Assert . assertTrue ( ( ( mColumn3 [ 0 ] ) != ( m . getColumn ( 1 ) [ 0 ] ) ) ) ; m . setColumn ( 1 , mColumn3 ) ; checkArrays ( mColumn3 , m . getColumn ( 1 ) ) ; try { m . setColumn ( ( - 1 ) , mColumn3 ) ; org . junit . Assert . fail ( "Expecting<sp>OutOfRangeException" ) ; } }
public class aTest{ @Test public void testSearchNonascii ( ) { int total = 5 ; for ( int i = 0 ; i < total ; i ++ ) { com . liferay . data . engine . service . test . DEDataEngineTestUtil . insertDEDataDefinition ( _adminUser , _group , ( "Description" + i ) , ( "Name" + i ) , _deDataDefinitionService ) ; } com . liferay . data . engine . service . test . DEDataEngineTestUtil . insertDEDataDefinition ( _adminUser , _group , "nonascii" , "Name" , _deDataDefinitionService ) ; java . util . List < com . liferay . data . engine . model . DEDataDefinition > deDataDefinitions = searchDEDataDefinitions ( _group , "nonascii" ) ; com . liferay . portal . search . test . util . IdempotentRetryAssert . retryAssert ( 3 , TimeUnit . SECONDS , ( ) -> { org . junit . Assert . assertEquals ( deDataDefinitions . toString ( ) , 1 , deDataDefinitions . size ( ) ) ; return null ; } }
public class aTest{ @Test public void testWALCoprocessorLoaded ( ) { org . apache . hadoop . hbase . regionserver . wal . HLog log = new org . apache . hadoop . hbase . regionserver . wal . HLog ( org . apache . hadoop . hbase . regionserver . wal . TestHLog . fs , org . apache . hadoop . hbase . regionserver . wal . TestHLog . dir , org . apache . hadoop . hbase . regionserver . wal . TestHLog . oldLogDir , org . apache . hadoop . hbase . regionserver . wal . TestHLog . conf ) ; try { org . apache . hadoop . hbase . regionserver . wal . WALCoprocessorHost host = log . getCoprocessorHost ( ) ; org . apache . hadoop . hbase . Coprocessor c = host . findCoprocessor ( org . apache . hadoop . hbase . coprocessor . SampleRegionWALObserver . class . getName ( ) ) ; org . junit . Assert . assertNotNull ( c ) ; } }
public class aTest{ @Test public void test_evaluate_evaluation_failed_exception ( ) { org . junit . Assume . assumeFalse ( webkit1SkipMsg ( ) , isWebkit1 ) ; final java . util . concurrent . atomic . AtomicInteger exception = new java . util . concurrent . atomic . AtomicInteger ( ( - 1 ) ) ; browser . addProgressListener ( org . eclipse . swt . browser . ProgressListener . completedAdapter ( ( event ) -> { try { browser . evaluate ( "return<sp>runSomeUndefinedFunctionInJavaScriptWhichCausesUndefinedError()" ) ; } catch ( e ) { exception . set ( e . code ) ; } } ) ) ; browser . setText ( "<html><body>HelloWorld</body></html>" ) ; shell . open ( ) ; java . util . concurrent . atomic . AtomicReference < java . lang . String > additionalErrorInfo = new java . util . concurrent . atomic . AtomicReference ( "" ) ; boolean passed = waitForPassCondition ( ( ) -> { if ( ( exception . get ( ) ) != ( - 1 ) ) { if ( ( exception . get ( ) ) == SWT . ERROR_FAILED_EVALUATE ) { return true ; } else { additionalErrorInfo . set ( ( "Invalid<sp>exception<sp>thrown:<sp>" + ( exception . get ( ) ) ) ) ; } } return false ; } ) ; java . lang . String message = ( "" . equals ( additionalErrorInfo . get ( ) ) ) ? "Javascript<sp>did<sp>not<sp>throw<sp>an<sp>error.<sp>Test<sp>timed<sp>out" : "Javascript<sp>threw<sp>an<sp>error,<sp>but<sp>not<sp>the<sp>right<sp>one." + ( additionalErrorInfo . get ( ) ) ; org . junit . Assert . assertTrue ( message , passed ) ; } }
public class aTest{ @Test public void givenHttpClient_executeAsyncGetRequestWithAsyncHandler ( ) { org . asynchttpclient . Request unboundGetRequest = org . asynchttpclient . Dsl . get ( "http://www.baeldung.com" ) . build ( ) ; com . baeldung . asynchttpclient . AsyncHttpClientLiveTest . HTTP_CLIENT . executeRequest ( unboundGetRequest , new org . asynchttpclient . AsyncHandler < java . lang . Integer > ( ) { int responseStatusCode = - 1 ; @ com . baeldung . asynchttpclient . Override public com . baeldung . asynchttpclient . State onStatusReceived ( org . asynchttpclient . HttpResponseStatus responseStatus ) { responseStatusCode = responseStatus . getStatusCode ( ) ; return State . CONTINUE ; } @ com . baeldung . asynchttpclient . Override public com . baeldung . asynchttpclient . State onHeadersReceived ( io . netty . handler . codec . http . HttpHeaders headers ) { return State . CONTINUE ; } @ com . baeldung . asynchttpclient . Override public com . baeldung . asynchttpclient . State onBodyPartReceived ( org . asynchttpclient . HttpResponseBodyPart bodyPart ) { return State . CONTINUE ; } @ com . baeldung . asynchttpclient . Override public void onThrowable ( java . lang . Throwable t ) { } @ com . baeldung . asynchttpclient . Override public com . baeldung . asynchttpclient . Integer onCompleted ( ) { org . junit . Assert . assertEquals ( 200 , responseStatusCode ) ; return responseStatusCode ; } } }
public class aTest{ @Test public void openBundleURLNonFile ( ) { final java . net . URL url = getClass ( ) . getResource ( "/workflowrun.bundle.zip" ) ; org . junit . Assert . assertNotNull ( url ) ; java . net . URLStreamHandler handler = new java . net . URLStreamHandler ( ) { @ org . apache . taverna . robundle . Override protected java . net . URLConnection openConnection ( java . net . URL u ) throws java . io . IOException { return url . openConnection ( ) ; } } }
public class aTest{ @Test public void twoPatternsOneFilter_test ( ) { final org . eclipse . rdf4j . model . ValueFactory vf = org . eclipse . rdf4j . model . impl . SimpleValueFactory . getInstance ( ) ; final org . eclipse . rdf4j . model . Value geo = vf . createLiteral ( "Point(0<sp>0)" , GeoConstants . XMLSCHEMA_OGC_WKT ) ; final org . eclipse . rdf4j . model . Value temp = vf . createLiteral ( new org . apache . rya . indexing . TemporalInstantRfc3339 ( 2015 , 12 , 30 , 12 , 0 , 0 ) . toString ( ) ) ; final org . eclipse . rdf4j . model . IRI tempPred = vf . createIRI ( org . apache . rya . indexing . geotemporal . GeoTemporalProviderTest . URI_PROPERTY_AT_TIME ) ; final java . lang . String query = ( ( ( ( ( ( ( ( ( "PREFIX<sp>geo:<sp><http://www.opengis.net/ont/geosparql#>" + ( ( ( "PREFIX<sp>geos:<sp><http://www.opengis.net/def/function/geosparql/>" + "PREFIX<sp>time:<sp><tag:rya-rdf.org,2015:temporal#>" ) + "SELECT<sp>*<sp>WHERE<sp>{<sp>" ) + "?subj<sp><" ) ) + tempPred ) + "?subj<sp><" 0 ) + "?subj<sp><" ) + ( org . apache . rya . indexing . GeoConstants . GEO_AS_WKT ) ) + "><sp>?loc<sp>.<sp>" ) + "<sp>FILTER(geos:sfContains(?loc,<sp>" ) + geo ) + "))<sp>.<sp>" ) + "}" ; final org . apache . rya . indexing . external . matching . QuerySegment < org . apache . rya . indexing . geotemporal . model . EventQueryNode > node = org . apache . rya . indexing . geotemporal . GeoTemporalTestUtils . getQueryNode ( query ) ; final java . util . List < org . apache . rya . indexing . geotemporal . model . EventQueryNode > nodes = provider . getExternalSets ( node ) ; org . junit . Assert . assertEquals ( 0 , nodes . size ( ) ) ; } }
public class aTest{ @Test public void testLoadPropertiesFromNestedClassPathSystemProperty ( ) { try { java . util . Properties properties = new java . util . Properties ( ) ; properties . put ( PropertyLoader . PropertyKey . DATA_SOURCE_UNIQUE_NAME . getKey ( ) , "jdbc/DS" ) ; java . io . File propertiesFile = com . vladmihalcea . flexypool . util . PropertiesTestUtils . setProperties ( properties ) ; java . lang . String resourceFolder = "nested" ; java . io . File newFileFolder = new java . io . File ( ( ( ( propertiesFile . getParentFile ( ) . getAbsolutePath ( ) ) + "/" ) + resourceFolder ) ) ; newFileFolder . mkdirs ( ) ; java . lang . String resourceFile = "fp.properties" ; java . io . File newFile = new java . io . File ( newFileFolder , resourceFile ) ; propertiesFile . renameTo ( newFile ) ; java . lang . String resourcePath = ( resourceFolder + "/" ) + resourceFile ; try { java . lang . System . setProperty ( PropertyLoader . PROPERTIES_FILE_PATH , resourcePath ) ; com . vladmihalcea . flexypool . config . PropertyLoader propertyLoader = new com . vladmihalcea . flexypool . config . PropertyLoader ( ) ; org . junit . Assert . assertEquals ( "jdbc/DS" , propertyLoader . getUniqueName ( ) ) ; } }
public class aTest{ @Test public void testGetHistoryWithNoSuchRevision ( ) { setUpTestRepository ( ) ; java . io . File root = new java . io . File ( repository . getSourceRoot ( ) , "mercurial" ) ; org . opengrok . indexer . history . MercurialRepository mr = ( ( org . opengrok . indexer . history . MercurialRepository ) ( org . opengrok . indexer . history . RepositoryFactory . getRepository ( root ) ) ) ; java . lang . String [ ] revisionParts = org . opengrok . indexer . history . MercurialRepositoryTest . REVISIONS [ 1 ] . split ( ":" ) ; org . junit . Assert . assertEquals ( 2 , revisionParts . length ) ; int number = java . lang . Integer . parseInt ( revisionParts [ 0 ] ) ; java . lang . String hash = revisionParts [ 1 ] ; java . lang . String constructedRevision = ( ( number + 1 ) + ":" ) + hash ; try { mr . getHistory ( root , constructedRevision ) ; org . junit . Assert . fail ( "getHistory()<sp>should<sp>have<sp>failed" ) ; } }
public class aTest{ @Test public void testExecuteCommandWithContextOnAbortedProcess ( ) { long processInstanceId = processService . startProcess ( deploymentUnit . getIdentifier ( ) , org . jbpm . kie . services . test . ProcessServiceImplPerProcessInstanceTest . PROCESS_ID_HUMAN_TASK ) ; org . junit . Assert . assertNotNull ( processInstanceId ) ; processService . abortProcessInstance ( processInstanceId ) ; try { processService . execute ( deploymentUnit . getIdentifier ( ) , org . kie . internal . runtime . manager . context . ProcessInstanceIdContext . get ( processInstanceId ) , new org . drools . core . command . runtime . process . GetProcessInstanceCommand ( processInstanceId ) ) ; org . junit . Assert . fail ( "Executing<sp>command<sp>with<sp>context<sp>on<sp>already<sp>aborted<sp>process<sp>instance<sp>should<sp>throw<sp>ProcessInstanceNotFoundException." ) ; } }
public class aTest{ @Test public void shouldReturnPropertyValueIfPropertyValueIsNotNull ( ) { java . lang . String propertyValue = "test" ; java . lang . String defaultValue = "default" ; java . lang . String context = "mik" 0 ; ch . puzzle . itc . mobiliar . business . property . entity . ResourceEditProperty resourceEditProperty = new ch . puzzle . itc . mobiliar . business . property . entity . ResourceEditProperty ( "technicalKey" , "displayName" , propertyValue , "exampleValue" , defaultValue , "propertyComment" , true , true , false , null , true , "validationLogic" , "mik" , null , null , null , "propContName" , "typeContName" , null , null , null , null , null , null , null , null , null , null , null , null , null ) ; dto = new ch . mobi . itc . mobiliar . rest . dtos . PropertyDTO ( resourceEditProperty , context ) ; org . junit . Assert . assertThat ( dto . getValue ( ) , org . hamcrest . CoreMatchers . is ( propertyValue ) ) ; } }
public class aTest{ @Test public void testValidateListBadSubscription ( ) { java . util . List < tigase . xml . Element > items = new java . util . ArrayList < tigase . xml . Element > ( ) ; tigase . xmpp . Authorization result = null ; items . add ( new tigase . xml . Element ( "item" , new java . lang . String [ ] { "type" , "value" , "action" , "order" } , new java . lang . String [ ] { "subscription" , "or" , "item" 0 , "10" } ) ) ; items . add ( new tigase . xml . Element ( "item" , new java . lang . String [ ] { "action" , "order" } , new java . lang . String [ ] { "deny" , "15" } ) ) ; result = tigase . xmpp . impl . JabberIqPrivacy . validateList ( null , items ) ; org . junit . Assert . assertEquals ( Authorization . BAD_REQUEST , result ) ; } }
public class aTest{ @Test public void testOfClass_Instantiator ( ) { org . openscience . cdk . DynamicFactory factory = new org . openscience . cdk . DynamicFactory ( 5 ) ; factory . register ( org . openscience . cdk . DynamicFactory . key ( org . openscience . cdk . interfaces . IAtom . class ) , new org . openscience . cdk . DynamicFactory . BasicCreator < org . openscience . cdk . interfaces . IAtom > ( null ) { @ java . lang . Override public org . openscience . cdk . interfaces . IAtom create ( java . lang . Object [ ] objects ) { return mock ( . class ) ; } } ) ; org . junit . Assert . assertNotNull ( factory . ofClass ( org . openscience . cdk . interfaces . IAtom . class ) ) ; } }
public class aTest{ @Test public void testLabeledNotCharSet ( ) { org . antlr . tool . Grammar g = new org . antlr . tool . Grammar ( ( "lexer<sp>grammar<sp>P;\n" + "A<sp>:<sp>t=~\'3\'<sp>;\n" ) ) ; java . lang . String expecting = ".s0->.s1\n" + ( ( ( ".s1->.s2\n" + ".s2-{\'\\u0000\'..\'2\',<sp>\'4\'..\'\\uFFFF\'}->.s3\n" ) + ".s3->:s4\n" ) + ":s4-<EOT>->.s5\n" ) ; checkRule ( g , "A<sp>:<sp>t=~<sp>\'3\'<sp>;\n" 0 , expecting ) ; java . lang . String expectingGrammarStr = "1:7:<sp>lexer<sp>grammar<sp>P;\n" + ( "A<sp>:<sp>t=~<sp>\'3\'<sp>;\n" + "Tokens<sp>:<sp>A<sp>;" ) ; org . junit . Assert . assertEquals ( expectingGrammarStr , g . toString ( ) ) ; } }
public class aTest{ @Test public void testBadOption ( io . vertx . ext . unit . TestContext ) { async = context . async ( ) ; java . lang . String [ ] args = new java . lang . String [ ] { "-bad-option" } ; org . folio . okapi . MainDeploy d = new org . folio . okapi . MainDeploy ( ) ; d . init ( args , ( res ) -> { vertx = ( res . succeeded ( ) ) ? res . result ( ) : null ; org . junit . Assert . assertFalse ( res . succeeded ( ) ) ; async . complete ( ) ; } }
public class aTest{ @Test public void testListChildren ( ) { org . qualipso . factory . client . test . sb . BrowserServiceSBTest . logger . debug ( "testing<sp>list<sp>children" ) ; try { javax . security . auth . login . LoginContext lc = new javax . security . auth . login . LoginContext ( "qualipso" , new org . jboss . security . auth . callback . UsernamePasswordHandler ( "root" , org . qualipso . factory . client . test . AllTests . ROOT_ACCOUNT_PASS ) ) ; lc . login ( ) ; java . lang . String [ ] childs = browser . listChildren ( "/testfolder" ) ; org . junit . Assert . assertTrue ( ( ( childs . length ) == 8 ) ) ; lc . logout ( ) ; } }
public class aTest{ @Test public void testListenerStorageAfterDeserialization ( ) { com . orsoncharts . Chart3D c1 = com . orsoncharts . Chart3DFactory . createPieChart ( "title" , "subtitle" , createPieDataset ( ) ) ; com . orsoncharts . Chart3D c2 = ( ( com . orsoncharts . Chart3D ) ( com . orsoncharts . TestUtils . serialized ( c1 ) ) ) ; org . junit . Assert . assertEquals ( c1 , c2 ) ; c2 . addChangeListener ( new com . orsoncharts . Chart3DChangeListener ( ) { @ com . orsoncharts . Override public void chartChanged ( com . orsoncharts . Chart3DChangeEvent event ) { throw new java . lang . UnsupportedOperationException ( "Not<sp>supported<sp>yet." ) ; } } }
public class aTest{ @Test public void testSpecifyStreamHandler ( ) { java . io . File tmp = null ; try { tmp = java . io . File . createTempFile ( ( "testFile_" + ( java . lang . System . currentTimeMillis ( ) ) ) , "txt" ) ; tmp . deleteOnExit ( ) ; final java . lang . String text = "The<sp>answer<sp>is<sp>42" ; java . io . BufferedWriter out = new java . io . BufferedWriter ( new java . io . FileWriter ( tmp ) ) ; out . write ( text ) ; out . flush ( ) ; out . close ( ) ; final java . lang . String fileName = tmp . getAbsolutePath ( ) ; java . util . concurrent . Future < ? > future = org . evosuite . runtime . sandbox . MSecurityManagerTest . executor . submit ( new java . lang . Runnable ( ) { @ org . evosuite . runtime . sandbox . Override public void run ( ) { try { java . net . URL url = new java . net . URL ( ( "file:" + fileName ) ) ; java . io . BufferedReader in = new java . io . BufferedReader ( new java . io . InputStreamReader ( url . openStream ( ) ) ) ; java . lang . String input = in . readLine ( ) ; org . junit . Assert . assertEquals ( text , input ) ; in . close ( ) ; } }
public class aTest{ @Test public void testParseURISegmentInvalidIndex00 ( ) { try { parser = new org . apache . olingo . odata2 . jpa . processor . core . ODataEntityParser ( mock ( "JPATypeMock(2)" ) ) ; org . apache . olingo . odata2 . api . uri . UriInfo uriInfo = parser . parseURISegment ( 0 , 0 ) ; org . junit . Assert . assertNull ( uriInfo ) ; } }
public class aTest{ @Test public void testRetryIoExceptionFromExecute ( ) { java . io . IOException ioException = new java . io . IOException ( "BOOM" ) ; when ( abortableCallable . call ( ) ) . thenThrow ( ioException ) ; software . amazon . awssdk . core . http . ExecutionContext context = software . amazon . awssdk . core . internal . http . timers . ClientExecutionAndRequestTimerTestUtils . executionContext ( null ) ; try { client . requestExecutionBuilder ( ) . request ( utils . ValidSdkObjects . sdkHttpFullRequest ( ) . build ( ) ) . originalRequest ( software . amazon . awssdk . core . http . NoopTestRequest . builder ( ) . build ( ) ) . executionContext ( context ) . execute ( ) ; org . junit . Assert . fail ( "No<sp>exception<sp>when<sp>request<sp>repeatedly<sp>fails!" ) ; } catch ( software . amazon . awssdk . core . exception . SdkClientException e ) { org . junit . Assert . assertSame ( ioException , e . getCause ( ) ) ; } }
public class aTest{ @Test public void testPersist ( ) { System . out . println ( "Testing<sp>CKANBackendImpl.persist" ) ; try { backend . setCache ( mockCache ) ; backend . setHttpClient ( mockHttpClient ) ; backend . persist ( orgName , pkgName , resName , data , true ) ; } catch ( java . lang . Exception e ) { org . junit . Assert . fail ( e . getMessage ( ) ) ; } finally { org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void getBigDecimalFromResultSet ( ) { java . lang . String [ ] expectedNames = new java . lang . String [ ] { "Decimal" , "double" , "float" , "real" , "numeric" , "Null" } ; java . lang . String [ ] realValues = new java . lang . String [ ] { "1.1" , "numeric" 0 , "3.3" , "4.4" , "numeric" 1 , null } ; java . lang . String [ ] expectedValues = new java . lang . String [ ] { "1.1" , "numeric" 0 , "3.3" , "4.4" , "numeric" 1 , "" } ; int [ ] expectedTypes = new int [ ] { Types . DECIMAL , Types . DOUBLE , Types . FLOAT , Types . REAL , Types . NUMERIC , Types . DECIMAL } ; au . com . bytecode . opencsv . ResultSetMetaData metaData = au . com . bytecode . opencsv . MockResultSetMetaDataBuilder . buildMetaData ( expectedNames , expectedTypes ) ; au . com . bytecode . opencsv . ResultSet resultSet = au . com . bytecode . opencsv . MockResultSetBuilder . buildResultSet ( metaData , realValues , expectedTypes ) ; au . com . bytecode . opencsv . ResultSetHelperService service = new au . com . bytecode . opencsv . ResultSetHelperService ( ) ; java . lang . String [ ] columnValues = service . getColumnValues ( resultSet ) ; org . junit . Assert . assertArrayEquals ( expectedValues , columnValues ) ; } }
public class aTest{ @Test public void toEnrichedRst ( ) { final org . apache . kafka . common . config . ConfigDef def = new org . apache . kafka . common . config . ConfigDef ( ) . define ( "<sp>*<sp>Dependents:<sp>``some.option``\n" 3 , Type . STRING , "<sp>*<sp>Dependents:<sp>``some.option``\n" 1 , org . apache . kafka . common . config . ConfigDef . ValidString . in ( "<sp>*<sp>Dependents:<sp>``some.option``\n" 1 , "b" , "Doc<sp>doc<sp>doc." 3 ) , Importance . HIGH , "Doc<sp>doc." , "<sp>*<sp>Dependents:<sp>``some.option``\n" 9 , 0 , Width . NONE , "^^^^^^^^^\n" 7 , java . util . Collections . < java . lang . String > emptyList ( ) ) . define ( "<sp>*<sp>Dependents:<sp>``some.option``\n" 4 , Type . INT , ConfigDef . NO_DEFAULT_VALUE , Importance . MEDIUM , "Doc<sp>doc<sp>doc." , "<sp>*<sp>Dependents:<sp>``some.option``\n" 9 , 1 , Width . NONE , "^^^^^^^^^\n" 7 , java . util . Arrays . asList ( "<sp>*<sp>Dependents:<sp>``some.option``\n" 0 , "Doc<sp>doc<sp>doc." 5 ) ) . define ( "Doc<sp>doc<sp>doc." 8 , Type . BOOLEAN , false , Importance . HIGH , "^^^^^^^^^\n" 6 , "^^^^^^^^^\n" 4 , 1 , Width . NONE , "^^^^^^^^^\n" 7 , java . util . Collections . < java . lang . String > emptyList ( ) ) . define ( "Doc<sp>doc<sp>doc." 2 , Type . BOOLEAN , false , Importance . HIGH , "^^^^^^^^^\n" 8 , "^^^^^^^^^\n" 4 , 0 , Width . NONE , "^^^^^^^^^\n" 7 , java . util . Collections . singletonList ( "some.option" ) ) . define ( "poor.opt" , Type . STRING , "foo" , Importance . HIGH , "^^^^^^^^^\n" 6 ) ; final java . lang . String expectedRst = "^^^^^^^^^\n" 5 + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "<sp>*<sp>Dependents:<sp>``some.option``\n" 2 + "Doc<sp>doc<sp>doc." 9 ) + "some.option" 2 ) + "^^^^^^^^^\n" 3 ) + "some.option" 0 ) + "some.option" 3 ) + "some.option" 2 ) + "<sp>*<sp>Dependents:<sp>``some.option``\n" 5 ) + "^^^^^^^^^\n" ) + "some.option" 2 ) + "``opt1.of.group1``\n" ) + "Doc<sp>doc<sp>doc." 0 ) + "some.option" 2 ) + "^^^^^^^^^\n" 3 ) + "<sp>*<sp>Default:<sp>a\n" ) + "Doc<sp>doc<sp>doc." 4 ) + "some.option" 3 ) + "some.option" 2 ) + "Doc<sp>doc<sp>doc." 6 ) + "<sp>*<sp>Dependents:<sp>``some.option``\n" 7 ) + "some.option" 2 ) + "^^^^^^^^^\n" 0 ) + "Doc<sp>doc<sp>doc." 1 ) + "<sp>*<sp>Dependents:<sp>``some.option``\n" 8 ) + "some.option" 2 ) + "Doc<sp>doc<sp>doc." 7 ) + "^^^^^^^^^\n" ) + "some.option" 2 ) + "<sp>*<sp>Dependents:<sp>``some.option``\n" 6 ) + "^^^^^^^^^\n" 1 ) + "some.option" 2 ) + "some.option" 1 ) + "^^^^^^^^^\n" 9 ) + "some.option" 3 ) + "<sp>*<sp>Dependents:<sp>``some.option``\n" ) + "some.option" 2 ) + "^^^^^^^^^\n" 2 ) + "Doc<sp>doc<sp>doc." 9 ) + "some.option" 2 ) + "some.option" 1 ) + "^^^^^^^^^\n" 9 ) + "some.option" 3 ) + "some.option" 2 ) ; org . junit . Assert . assertEquals ( expectedRst , def . toEnrichedRst ( ) ) ; } }
public class aTest{ @Test public void testWriteJsonForPredictions ( ) { final java . lang . String expected = com . google . common . io . Files . toString ( jsonFile , Charsets . UTF_8 ) ; final java . io . StringWriter output = new java . io . StringWriter ( ) ; final java . io . FileReader tokenizedInput = new java . io . FileReader ( tokenizedFile ) ; final java . io . FileReader feInput = new java . io . FileReader ( frameElementsFile ) ; try { edu . cmu . cs . lti . ark . fn . evaluation . PrepareFullAnnotationJson . writeJsonForPredictions ( tokenizedInput , feInput , output ) ; org . junit . Assert . assertEquals ( expected , output . toString ( ) ) ; } }
public class aTest{ @Test public void testXPathHelperWithNamespaceTextPath ( ) { try { java . lang . String xmlString = getFileContentsAsString ( ( ( ddf . catalog . impl . XPathHelperTest . TEST_DATA_PATH ) + ( ddf . catalog . impl . XPathHelperTest . INPUT_FILE ) ) ) ; ddf . util . XPathHelper xHelper = new ddf . util . XPathHelper ( xmlString ) ; org . w3c . dom . NodeList nodeList = ( ( org . w3c . dom . NodeList ) ( xHelper . evaluate ( "//abc:fileTitle" , XPathConstants . NODESET , new ddf . catalog . impl . MockNamespaceResolver ( ) ) ) ) ; ddf . catalog . impl . XPathHelperTest . LOGGER . debug ( "testXPathHelper_WithNamespaceTextPath()<sp>-<sp>nodeList<sp>length<sp>=<sp>{}" , nodeList . getLength ( ) ) ; org . junit . Assert . assertEquals ( 1 , nodeList . getLength ( ) ) ; } }
public class aTest{ @Test public void testAnalyzeTarBz2 ( ) { org . owasp . dependencycheck . analyzer . ArchiveAnalyzer instance = new org . owasp . dependencycheck . analyzer . ArchiveAnalyzer ( ) ; instance . initialize ( getSettings ( ) ) ; instance . accept ( new java . io . File ( "zip" ) ) ; try ( org . owasp . dependencycheck . Engine engine = new org . owasp . dependencycheck . Engine ( getSettings ( ) ) ) { instance . prepare ( null ) ; java . io . File file = org . owasp . dependencycheck . BaseTest . getResourceAsFile ( this , "file.tar.bz2" ) ; getSettings ( ) . setBoolean ( Settings . KEYS . AUTO_UPDATE , false ) ; getSettings ( ) . setBoolean ( Settings . KEYS . ANALYZER_NEXUS_ENABLED , false ) ; getSettings ( ) . setBoolean ( Settings . KEYS . ANALYZER_CENTRAL_ENABLED , false ) ; int initial_size = engine . getDependencies ( ) . length ; engine . scan ( file ) ; engine . analyzeDependencies ( ) ; int ending_size = engine . getDependencies ( ) . length ; org . junit . Assert . assertTrue ( ( initial_size < ending_size ) ) ; } }
public class aTest{ @Test public void testTranslation ( ) { org . inchain . crypto . ECKey key = org . inchain . crypto . ECKey . fromPrivate ( new java . math . BigInteger ( "61914497277584841097702477783063064420681667313180238384957944936487927892583" ) ) ; org . inchain . transaction . business . RegConsensusTransaction regConsensusTransaction = new org . inchain . transaction . business . RegConsensusTransaction ( network , 1L , 1478070769L , null ) ; org . inchain . account . Address address = org . inchain . account . AccountTool . newAddress ( network , key ) ; org . inchain . account . Account account = new org . inchain . account . Account ( network ) ; account . setAddress ( address ) ; account . setEcKey ( key ) ; regConsensusTransaction . sign ( account ) ; regConsensusTransaction . verify ( ) ; regConsensusTransaction . verifyScript ( ) ; org . inchain . transaction . business . RegConsensusTransaction regConsensusTransactionTemp = new org . inchain . transaction . business . RegConsensusTransaction ( network , regConsensusTransaction . baseSerialize ( ) , 0 ) ; org . junit . Assert . assertEquals ( regConsensusTransaction . getHash ( ) , regConsensusTransactionTemp . getHash ( ) ) ; try { java . lang . Thread . sleep ( 3000L ) ; peerKit . broadcastMessage ( regConsensusTransactionTemp ) ; java . lang . Thread . sleep ( 10000L ) ; } }
public class aTest{ @Test public void testXTimes0Matrix ( ) { org . ujmp . core . Matrix m1 = createMatrixWithAnnotation ( 5 , 7 ) ; org . ujmp . core . Matrix m2 = createMatrixWithAnnotation ( 5 , 7 ) ; m1 . randn ( Ret . ORIG ) ; org . ujmp . core . Matrix m3 = m1 . times ( m2 ) ; org . junit . Assert . assertTrue ( getLabel ( ) , m3 . isEmpty ( ) ) ; if ( m1 instanceof org . ujmp . core . interfaces . Erasable ) { ( ( org . ujmp . core . interfaces . Erasable ) ( m1 ) ) . erase ( ) ; } }
public class aTest{ @Test public void testSerializeMappedBitmap ( ) { java . nio . file . Path file = org . roaringbitmap . buffer . TestSerializationViaByteBuffer . dir . resolve ( java . util . UUID . randomUUID ( ) . toString ( ) ) ; java . nio . file . Files . createFile ( file ) ; try ( org . roaringbitmap . buffer . RandomAccessFile raf = new org . roaringbitmap . buffer . RandomAccessFile ( file . toFile ( ) , "rw" ) ; java . nio . channels . FileChannel channel = raf . getChannel ( ) ) { java . nio . ByteBuffer buffer = channel . map ( org . roaringbitmap . buffer . READ_WRITE , 0 , serialised . length ) ; buffer . put ( serialised ) ; buffer . flip ( ) ; org . roaringbitmap . buffer . ImmutableRoaringBitmap bitmap = new org . roaringbitmap . buffer . ImmutableRoaringBitmap ( buffer ) ; java . nio . ByteBuffer buf = java . nio . ByteBuffer . allocate ( bitmap . serializedSizeInBytes ( ) ) ; bitmap . serialize ( buf ) ; buf . flip ( ) ; org . roaringbitmap . buffer . ImmutableRoaringBitmap deserialised = new org . roaringbitmap . buffer . ImmutableRoaringBitmap ( buf ) ; org . junit . Assert . assertEquals ( bitmap , deserialised ) ; } }
public class aTest{ @Test public void testWerkbijOverlijdenMetNullOverlijden ( ) { nl . bzk . brp . model . objecttype . operationeel . ActieModel actie = maakActie ( SoortActie . AANGIFTE_OVERLIJDEN ) ; nl . bzk . brp . model . objecttype . operationeel . PersoonModel persoon = em . find ( nl . bzk . brp . model . objecttype . operationeel . PersoonModel . class , 1 ) ; org . junit . Assert . assertNotNull ( persoon ) ; try { persoonRepository . werkbijOverlijden ( persoon , null , null , actie , new nl . bzk . brp . model . attribuuttype . Datum ( 20120708 ) ) ; } }
public class aTest{ @Test public void test_value_loaded_with_source_supporting_requested_format ( ) { java . lang . String bundleName = "folder.bundle" ; java . lang . String objectName = "folder/bundle.properties" ; com . github . resource4j . ResourceKey key = com . github . resource4j . ResourceKey . key ( bundleName , "value" ) ; java . time . Clock clock = java . time . Clock . systemUTC ( ) ; com . github . resource4j . resources . context . ResourceResolutionContext ctx = in ( "ctx" ) ; com . github . resource4j . ResourceObject bundleSlowCtx = given ( aPropertiesBundle ( objectName , "folder/bundle-ctx.properties" ) . with ( "value" , "slow-ctx" ) ) ; com . github . resource4j . ResourceObject bundleSlowCommon = given ( aPropertiesBundle ( objectName , objectName ) . with ( "value" , "slow" ) ) ; com . github . resource4j . objects . providers . mutable . HeapResourceObjectRepository pSlow = new com . github . resource4j . objects . providers . mutable . HeapResourceObjectRepository ( clock ) ; pSlow . put ( objectName , ctx , bundleSlowCtx :: asStream ) ; pSlow . put ( objectName , withoutContext ( ) , bundleSlowCommon :: asStream ) ; com . github . resource4j . ResourceObject bundleFastCommon = given ( aPropertiesBundle ( ) . with ( "value" , "fast" ) ) ; com . github . resource4j . objects . providers . mutable . HeapResourceObjectRepository pFast = new com . github . resource4j . objects . providers . mutable . HeapResourceObjectRepository ( clock ) ; pFast . put ( objectName , ctx , bundleFastCommon :: asStream ) ; com . github . resource4j . objects . providers . ExpensiveResourceObjectProvider mSlow = com . github . resource4j . resources . RefreshableResourcesTest . managed ( "slow" , pSlow ) ; mSlow . whenRequested ( bundleName , ctx ) . sleep ( 1000 ) ; com . github . resource4j . objects . providers . ExpensiveResourceObjectProvider mFast = com . github . resource4j . resources . RefreshableResourcesTest . managed ( "fast" , pFast ) ; com . github . resource4j . resources . RefreshableResources resources = new com . github . resource4j . resources . RefreshableResources ( com . github . resource4j . resources . ResourcesConfigurationBuilder . configure ( ) . sources ( mSlow . objectsLike ( name ( ".\\.xml$" ) ) , mFast . objectsLike ( name ( "value" 0 ) ) ) . formats ( org . junit . Assert . format ( propertyMap ( ) , ".properties" ) ) . get ( ) ) ; com . github . resource4j . OptionalString value = resources . get ( key , ctx ) ; org . junit . Assert . assertEquals ( "fast" , value . asIs ( ) ) ; } }
public class aTest{ @Test public void testStringFieldWithToError ( ) { com . streamsets . pipeline . stage . processor . fieldhasher . HasherConfig hasherConfig = com . streamsets . pipeline . stage . processor . fieldhasher . TestFieldHasherProcessor . createInPlaceHasherProcessor ( com . google . common . collect . ImmutableList . of ( "/name" ) , hashType ) ; com . streamsets . pipeline . stage . processor . fieldhasher . FieldHasherProcessor processor = new com . streamsets . pipeline . stage . processor . fieldhasher . FieldHasherProcessor ( hasherConfig , com . streamsets . pipeline . config . OnStagePreConditionFailure . TO_ERROR ) ; com . streamsets . pipeline . sdk . ProcessorRunner runner = new com . streamsets . pipeline . sdk . ProcessorRunner . Builder ( com . streamsets . pipeline . stage . processor . fieldhasher . FieldHasherDProcessor . class , processor ) . setOnRecordError ( OnRecordError . TO_ERROR ) . addOutputLane ( "a" ) . build ( ) ; runner . runInit ( ) ; try { java . util . Map < java . lang . String , com . streamsets . pipeline . api . Field > map = new java . util . LinkedHashMap ( ) ; com . streamsets . pipeline . api . Record record = com . streamsets . pipeline . sdk . RecordCreator . create ( "s" , "s:1" ) ; record . set ( com . streamsets . pipeline . api . Field . create ( map ) ) ; runner . runProcess ( com . google . common . collect . ImmutableList . of ( record ) ) ; org . junit . Assert . assertEquals ( 1 , runner . getErrorRecords ( ) . size ( ) ) ; } }
public class aTest{ @Test public void testActionEvalsAtCorrectIndex ( ) { java . lang . String grammar = "lexer<sp>grammar<sp>L;\n" + ( "I<sp>:<sp>[0-9]<sp>{System.out.println(\"2nd<sp>char:<sp>\"+(char)_input.LA(1));}<sp>[0-9]+<sp>;\n" + "+(char)_input.LA(1));}<sp>[0-9]+<sp>;\n" 1 ) ; java . lang . String found = execLexer ( "L.g4" , grammar , "L" , "123<sp>45" ) ; java . lang . String expecting = "2nd<sp>char:<sp>2\n" + ( ( ( "2nd<sp>char:<sp>5\n" + "[@0,0:2=\'123\',<1>,1:0]\n" ) + "[@1,4:5=\'45\',<1>,1:4]\n" ) + "+(char)_input.LA(1));}<sp>[0-9]+<sp>;\n" 0 ) ; org . junit . Assert . assertEquals ( expecting , found ) ; } }
public class aTest{ @Test public void removeProperty ( ) { javax . jcr . Node parentNode = getNode ( org . apache . jackrabbit . oak . jcr . RepositoryTest . TEST_PATH ) ; parentNode . setProperty ( "newProperty" , "some<sp>value" ) ; parentNode . getSession ( ) . save ( ) ; javax . jcr . Session session2 = createAdminSession ( ) ; try { session2 . getProperty ( ( ( org . apache . jackrabbit . oak . jcr . RepositoryTest . TEST_PATH ) + "/newProperty" ) ) . remove ( ) ; session2 . save ( ) ; } finally { session2 . logout ( ) ; } javax . jcr . Session session3 = createAnonymousSession ( ) ; try { org . junit . Assert . assertFalse ( session3 . propertyExists ( ( ( org . apache . jackrabbit . oak . jcr . RepositoryTest . TEST_PATH ) + "/newProperty" ) ) ) ; } }
public class aTest{ @Test public void testSimpleEncrypt ( ) { java . lang . String data = "<ID>some<sp>sensitive<sp>data</ID><sp>and<sp>then<sp>some<sp>data<sp>and<sp>then<sp><CARD>some<sp>card<sp>data</CARD><sp>and<sp>some<sp>more<sp>data<sp>and<sp>then<sp><CARD>some<sp>other<sp>card<sp>data</CARD><sp>and<sp>then<sp>finally<sp><ID>some<sp>other<sp>sensitive<sp>data</ID><sp>and<sp>the<sp>something." ; java . lang . String responseBody = client . target ( ( ( "http://localhost:" + ( com . kunai . keyvault . IntegrationTest . RULE . getLocalPort ( ) ) ) + "/proxy" ) ) . request ( ) . header ( "proxy-url" , "http://httpbin.org/put" ) . header ( "encryption-regex0" , "(?<=<ID>).*?(?=</ID>)" ) . header ( "proxy-url" 0 , "test" ) . header ( "encryption-regex1" , "proxy-url" 1 ) . header ( "encryption-type1" , "test" ) . put ( javax . ws . rs . client . Entity . text ( data ) ) . readEntity ( java . lang . String . class ) ; org . junit . Assert . assertTrue ( responseBody . contains ( "proxy-url" 2 ) ) ; } }
public class aTest{ @Test public void testUpdateProductAttributeMedia ( ) { try { java . lang . Boolean result = runFlowAndGetPayload ( "update-product-attribute-media" ) ; org . junit . Assert . assertTrue ( result ) ; } }
public class aTest{ @Test public void testSlidingTupleTsTopology ( ) { backtype . storm . topology . TopologyBuilder topologyBuilder = new backtype . storm . topology . TopologyBuilder ( ) ; backtype . storm . topology . base . BaseWindowedBolt bolt = new com . jstorm . example . unittests . window . SlidingTupleTestBolt ( ) . withWindow ( new backtype . storm . topology . base . BaseWindowedBolt . Duration ( com . jstorm . example . unittests . window . SlidingTupleTsTopologyTest . WINDOW_LENGTH_SEC , java . util . concurrent . TimeUnit . SECONDS ) , new backtype . storm . topology . base . BaseWindowedBolt . Duration ( com . jstorm . example . unittests . window . SlidingTupleTsTopologyTest . WINDOW_SLIDE_SEC , java . util . concurrent . TimeUnit . SECONDS ) ) . withTimestampField ( "ts" ) . withLag ( new backtype . storm . topology . base . BaseWindowedBolt . Duration ( com . jstorm . example . unittests . window . SlidingTupleTsTopologyTest . WINDOW_LAG_SEC , java . util . concurrent . TimeUnit . SECONDS ) ) ; topologyBuilder . setSpout ( "spout" , new com . jstorm . example . unittests . window . SlidingTupleTestRandomSpout ( com . jstorm . example . unittests . window . SlidingTupleTsTopologyTest . SPOUT_LIMIT ) , 1 ) ; topologyBuilder . setBolt ( "sum" , bolt , 1 ) . shuffleGrouping ( "spout" ) ; com . jstorm . example . unittests . window . Map config = new com . jstorm . example . unittests . window . HashMap ( ) ; config . put ( Config . TOPOLOGY_NAME , "SlidingTupleTsTopologyTest" ) ; com . jstorm . example . unittests . window . Set < java . lang . String > userDefineMetrics = new com . jstorm . example . unittests . window . HashSet < java . lang . String > ( ) ; userDefineMetrics . add ( "SlidingTupleTsTopologyTest.SpoutSum" ) ; userDefineMetrics . add ( "SlidingTupleTsTopologyTest.BoltSum" ) ; com . jstorm . example . unittests . utils . JStormUnitTestValidator validator = new com . jstorm . example . unittests . utils . JStormUnitTestMetricValidator ( userDefineMetrics ) { @ com . jstorm . example . unittests . window . Override public boolean validateMetrics ( com . jstorm . example . unittests . window . Map < java . lang . String , java . lang . Double > metrics ) { int spoutSum = ( ( int ) ( metrics . get ( "SlidingTupleTsTopologyTest.SpoutSum" ) . doubleValue ( ) ) ) ; int boltSum = ( ( int ) ( metrics . get ( "SlidingTupleTsTopologyTest.BoltSum" ) . doubleValue ( ) ) ) ; org . junit . Assert . assertEquals ( spoutSum , boltSum ) ; return true ; } } }
public class aTest{ @Test public void testServerNameMismatch ( ) { final javax . security . sasl . SaslClientFactory clientFactory = obtainSaslClientFactory ( org . wildfly . security . sasl . entity . EntitySaslClientFactory . class ) ; org . junit . Assert . assertNotNull ( clientFactory ) ; final javax . security . sasl . SaslServer saslServer = createSaslServer ( SaslMechanismInformation . Names . IEC_ISO_9798_M_RSA_SHA1_ENC , "testserver1.example.com" , getX509KeyManager ( serverKeyStore , org . wildfly . security . sasl . entity . EntityTest . KEYSTORE_PASSWORD ) , serverTrustStore ) ; final java . lang . String [ ] mechanisms = new java . lang . String [ ] { SaslMechanismInformation . Names . IEC_ISO_9798_M_RSA_SHA1_ENC } ; javax . security . auth . callback . CallbackHandler cbh = createClientCallbackHandler ( mechanisms , clientKeyStore , org . wildfly . security . sasl . entity . EntityTest . CLIENT_KEYSTORE_ALIAS , org . wildfly . security . sasl . entity . EntityTest . KEYSTORE_PASSWORD , getX509TrustManager ( clientTrustStore ) ) ; final javax . security . sasl . SaslClient saslClient = clientFactory . createSaslClient ( mechanisms , null , "test" , "anotherserver.example.com" , java . util . Collections . < java . lang . String , java . lang . Object > emptyMap ( ) , cbh ) ; byte [ ] message = saslServer . evaluateResponse ( new byte [ 0 ] ) ; try { saslClient . evaluateChallenge ( message ) ; org . junit . Assert . fail ( "Expected<sp>SaslException<sp>not<sp>thrown" ) ; } }
public class aTest{ @Test public void testReject ( ) { org . societies . api . identity . Requestor provider ; org . societies . api . internal . security . policynegotiator . INegotiationCallback callback ; provider = mock ( org . societies . api . identity . Requestor . class ) ; callback = new org . societies . api . internal . security . policynegotiator . INegotiationCallback ( ) { @ org . societies . security . policynegotiator . requester . Override public void onNegotiationComplete ( java . lang . String agreementKey , java . util . List < java . net . URI > fileUris ) { org . junit . Assert . assertNull ( agreementKey ) ; } }
public class aTest{ @Test public void testGetObjectIdExisting ( ) { final java . util . concurrent . atomic . AtomicReference < java . math . BigInteger > id = new java . util . concurrent . atomic . AtomicReference < java . math . BigInteger > ( ) ; com . sun . sgs . test . impl . service . data . TestDataServiceImpl . txnScheduler . runTask ( new com . sun . sgs . test . impl . service . data . TestDataServiceImpl . InitialTestRunnable ( ) { public void run ( ) throws java . lang . Exception { super . run ( ) ; id . set ( com . sun . sgs . test . impl . service . data . TestDataServiceImpl . service . getObjectId ( dummy ) ) ; } } , com . sun . sgs . test . impl . service . data . TestDataServiceImpl . taskOwner ) ; com . sun . sgs . test . impl . service . data . TestDataServiceImpl . txnScheduler . runTask ( new com . sun . sgs . test . util . TestAbstractKernelRunnable ( ) { public void run ( ) { com . sun . sgs . test . util . DummyManagedObject dummy = ( ( com . sun . sgs . test . util . DummyManagedObject ) ( com . sun . sgs . test . impl . service . data . TestDataServiceImpl . service . getBinding ( "dummy" ) ) ) ; org . junit . Assert . assertEquals ( id . get ( ) , com . sun . sgs . test . impl . service . data . TestDataServiceImpl . service . getObjectId ( dummy ) ) ; } } }
public class aTest{ @Test public void testReverseXmlRoundTrip ( ) { org . apache . hadoop . test . GenericTestUtils . setLogLevel ( OfflineImageReconstructor . LOG , Level . TRACE ) ; java . io . File reverseImageXml = new java . io . File ( org . apache . hadoop . hdfs . tools . offlineImageViewer . TestOfflineImageViewer . tempDir , "reverseImage.xml" ) ; java . io . File reverseImage = new java . io . File ( org . apache . hadoop . hdfs . tools . offlineImageViewer . TestOfflineImageViewer . tempDir , "reverseImage" ) ; java . io . File reverseImage2Xml = new java . io . File ( org . apache . hadoop . hdfs . tools . offlineImageViewer . TestOfflineImageViewer . tempDir , "reverseImage2.xml" ) ; org . apache . hadoop . hdfs . tools . offlineImageViewer . TestOfflineImageViewer . LOG . info ( ( ( ( ( ( "reverseImage.xml" 3 + ( reverseImageXml . getAbsolutePath ( ) ) ) + ",<sp>reverseImage=" ) + ( reverseImage . getAbsolutePath ( ) ) ) + "reverseImage.xml" 0 ) + ( reverseImage2Xml . getAbsolutePath ( ) ) ) ) ; if ( ( org . apache . hadoop . hdfs . tools . offlineImageViewer . OfflineImageViewerPB . run ( new java . lang . String [ ] { "reverseImage.xml" 5 , "XML" , "-i" , org . apache . hadoop . hdfs . tools . offlineImageViewer . TestOfflineImageViewer . originalFsimage . getAbsolutePath ( ) , "-o" , reverseImageXml . getAbsolutePath ( ) } ) ) != 0 ) { throw new java . io . IOException ( "oiv<sp>returned<sp>failure<sp>creating<sp>first<sp>XML<sp>file." ) ; } if ( ( org . apache . hadoop . hdfs . tools . offlineImageViewer . OfflineImageViewerPB . run ( new java . lang . String [ ] { "reverseImage.xml" 5 , "reverseImage.xml" 4 , "-i" , reverseImageXml . getAbsolutePath ( ) , "-o" , reverseImage . getAbsolutePath ( ) } ) ) != 0 ) { throw new java . io . IOException ( "oiv<sp>returned<sp>failure<sp>recreating<sp>fsimage<sp>file." ) ; } if ( ( org . apache . hadoop . hdfs . tools . offlineImageViewer . OfflineImageViewerPB . run ( new java . lang . String [ ] { "reverseImage.xml" 5 , "XML" , "-i" , reverseImage . getAbsolutePath ( ) , "-o" , reverseImage2Xml . getAbsolutePath ( ) } ) ) != 0 ) { throw new java . io . IOException ( ( "oiv<sp>returned<sp>failure<sp>creating<sp>second<sp>" + "reverseImage.xml" 1 ) ) ; } org . junit . Assert . assertEquals ( "reverseImage.xml" 2 , org . apache . hadoop . test . GenericTestUtils . getFilesDiff ( reverseImageXml , reverseImage2Xml ) ) ; } }
public class aTest{ @Test public void testElseBatch ( ) { final org . jboss . as . cli . CommandContext ctx = org . jboss . as . test . integration . management . util . CLITestUtil . getCommandContext ( cliOut ) ; try { ctx . connectController ( ) ; ctx . handle ( getAddPropertyReq ( "1" ) ) ; ctx . handle ( ( "if<sp>result.value==\"3\"<sp>of<sp>" + ( getReadPropertyReq ( ) ) ) ) ; ctx . handle ( "else" ) ; ctx . handle ( "batch" ) ; ctx . handle ( getWritePropertyReq ( "2" ) ) ; ctx . handle ( getReadNonexistingPropReq ( ) ) ; ctx . handle ( "run-batch" ) ; ctx . handle ( "end-if" ) ; org . junit . Assert . fail ( "expected<sp>exception" ) ; } catch ( org . jboss . as . cli . CommandLineException e ) { cliOut . reset ( ) ; ctx . handle ( getReadPropertyReq ( ) ) ; org . junit . Assert . assertEquals ( "1" , getValue ( ) ) ; } }
public class aTest{ @Test public void testEncodeDecodeSpecific ( ) { java . io . ByteArrayOutputStream bos = new java . io . ByteArrayOutputStream ( ) ; org . apache . avro . Schema aSchema = org . apache . avro . Schema . createArray ( DemoRecordInfo . SCHEMA . ) ; org . spf4j . avro . csv . CsvEncoder csvEncoder = new org . spf4j . avro . csv . CsvEncoder ( Csv . CSV . writer ( new java . io . OutputStreamWriter ( bos , java . nio . charset . StandardCharsets . UTF_8 ) ) , aSchema ) ; csvEncoder . writeHeader ( ) ; org . apache . avro . io . DatumWriter writer = new org . apache . avro . generic . GenericDatumWriter ( aSchema ) ; java . util . List < org . spf4j . demo . avro . DemoRecordInfo > testRecords = org . spf4j . avro . csv . CsvEncoderTest . testRecords ( ) ; writer . write ( testRecords , csvEncoder ) ; csvEncoder . flush ( ) ; org . spf4j . avro . csv . CsvEncoderTest . LOG . debug ( "serialized" , bos . toString ( "UTF8" ) ) ; java . io . ByteArrayInputStream bis = new java . io . ByteArrayInputStream ( bos . toByteArray ( ) ) ; org . spf4j . avro . DecodedSchema ds = org . spf4j . avro . csv . CsvDecoder . tryDecodeSchema ( bis , aSchema ) ; org . apache . avro . io . Decoder decoder = ds . getDecoder ( ) ; org . apache . avro . specific . SpecificDatumReader reader = new org . apache . avro . specific . SpecificDatumReader ( ds . getSchema ( ) ) ; java . lang . Object read = reader . read ( reader , decoder ) ; org . spf4j . avro . csv . CsvEncoderTest . LOG . debug ( "deserialized" , read ) ; org . junit . Assert . assertEquals ( testRecords , read ) ; } }
public class aTest{ @Test public void testRPOPLPUSH ( ) { doCmdTest ( new org . cyy . fw . nedis . test . cmd . TestAction ( ) { @ org . cyy . fw . nedis . test . cmd . Override public void doTest ( ) throws org . cyy . fw . nedis . test . cmd . InterruptedException , org . cyy . fw . nedis . util . NedisException { client . flushAll ( null ) ; java . lang . Thread . sleep ( org . cyy . fw . nedis . test . cmd . CMD_PAUSE_TIME ) ; client . lPush ( null , "llkey1" , "llvalue1" ) ; java . lang . Thread . sleep ( org . cyy . fw . nedis . test . cmd . CMD_PAUSE_TIME ) ; client . lPush ( null , "llkey2" , "llvalue2" , "llvalue3" ) ; java . lang . Thread . sleep ( org . cyy . fw . nedis . test . cmd . CMD_PAUSE_TIME ) ; client . rPopLPush ( new org . cyy . fw . nedis . ResponseCallback < java . lang . String > ( ) { @ org . cyy . fw . nedis . test . cmd . Override public void done ( java . lang . String result ) { org . junit . Assert . assertEquals ( "llvalue1" , result ) ; controller . countDown ( ) ; } }
public class aTest{ @Test public void testDescribeDBInstances ( ) { try { com . fit2cloud . aliyun . rds . model . request . DescribeDBInstancesRequest request = new com . fit2cloud . aliyun . rds . model . request . DescribeDBInstancesRequest ( ) ; request . setDBInstanceType ( DBInstanceType . ALL ) ; request . setEngine ( EngineType . MYSQL ) ; request . setRegionId ( region ) ; com . fit2cloud . aliyun . Response response = client . describeDBInstances ( request ) ; System . out . println ( ( "testDescribeDBInstances<sp>::<sp>" + ( new com . google . gson . Gson ( ) . toJson ( response ) ) ) ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void checkPersistence ( ) { org . eclipse . ice . datastructures . ICEObject . ICEJAXBHandler xmlHandler = new org . eclipse . ice . datastructures . ICEObject . ICEJAXBHandler ( ) ; java . util . ArrayList < java . lang . Class > classList = new java . util . ArrayList < java . lang . Class > ( ) ; classList . add ( org . eclipse . ice . datastructures . form . Material . class ) ; org . eclipse . ice . datastructures . ICEObject . ICEJAXBHandler jaxbHandler = new org . eclipse . ice . datastructures . ICEObject . ICEJAXBHandler ( ) ; org . eclipse . ice . datastructures . form . Material material = org . eclipse . ice . tests . datastructures . TestMaterialFactory . createCO2 ( ) ; try { java . io . ByteArrayOutputStream outputStream = new java . io . ByteArrayOutputStream ( ) ; jaxbHandler . write ( material , classList , outputStream ) ; java . io . ByteArrayInputStream inputStream = new java . io . ByteArrayInputStream ( outputStream . toByteArray ( ) ) ; org . eclipse . ice . datastructures . form . Material readMaterial = ( ( org . eclipse . ice . datastructures . form . Material ) ( jaxbHandler . read ( classList , inputStream ) ) ) ; org . junit . Assert . assertTrue ( readMaterial . equals ( material ) ) ; } }
public class aTest{ @Test public void testConsumeAfterRestart ( ) { org . apache . activemq . artemis . api . core . client . ClientSession session = null ; org . apache . activemq . artemis . tests . integration . client . InterruptedLargeMessageTest . LargeMessageTestInterceptorIgnoreLastPacket . clearInterrupt ( ) ; org . apache . activemq . artemis . core . server . ActiveMQServer server = createServer ( true , isNetty ( ) ) ; server . start ( ) ; org . apache . activemq . artemis . core . server . QueueFactory original = server . getQueueFactory ( ) ; locator . setBlockOnNonDurableSend ( true ) . setBlockOnDurableSend ( true ) ; org . apache . activemq . artemis . api . core . client . ClientSessionFactory sf = createSessionFactory ( locator ) ; session = sf . createSession ( false , true , true ) ; session . createQueue ( org . apache . activemq . artemis . tests . integration . client . ADDRESS , org . apache . activemq . artemis . tests . integration . client . ADDRESS , true ) ; org . apache . activemq . artemis . api . core . client . ClientProducer producer = session . createProducer ( org . apache . activemq . artemis . tests . integration . client . ADDRESS ) ; for ( int i = 0 ; i < 10 ; i ++ ) { org . apache . activemq . artemis . api . core . Message clientFile = createLargeClientMessageStreaming ( session , LARGE_MESSAGE_SIZE , true ) ; producer . send ( clientFile ) ; } session . commit ( ) ; session . close ( ) ; sf . close ( ) ; server . stop ( ) ; server . start ( ) ; sf = createSessionFactory ( locator ) ; session = sf . createSession ( false , false ) ; org . apache . activemq . artemis . api . core . client . ClientConsumer cons = session . createConsumer ( org . apache . activemq . artemis . tests . integration . client . ADDRESS ) ; session . start ( ) ; for ( int i = 0 ; i < 10 ; i ++ ) { org . apache . activemq . artemis . api . core . client . ClientMessage msg = cons . receive ( 5000 ) ; org . junit . Assert . assertNotNull ( msg ) ; msg . saveToOutputStream ( new java . io . OutputStream ( ) { @ org . apache . activemq . artemis . tests . integration . client . Override public void write ( int b ) throws java . io . IOException { } } ) ; msg . acknowledge ( ) ; session . commit ( ) ; } }
public class aTest{ @Test public void iterator_Empty ( ) { java . util . Iterator < Cache . Entry < java . lang . Long , java . lang . String > > iterator = cache . iterator ( ) ; org . junit . Assert . assertFalse ( iterator . hasNext ( ) ) ; try { iterator . remove ( ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void test2_4_SequenceOfMappings ( ) { net . openhft . chronicle . bytes . Bytes b = net . openhft . chronicle . bytes . Bytes . elasticByteBuffer ( ) ; try { java . io . InputStream is = net . openhft . chronicle . wire . YamlSpecTest . class . getResourceAsStream ( ( ( net . openhft . chronicle . wire . YamlSpecTest . DIR ) + "2_4_SequenceOfMappings.yaml" ) ) ; java . lang . Object o = net . openhft . chronicle . wire . Marshallable . fromString ( is ) ; java . lang . String actual = o . toString ( ) ; org . junit . Assert . assertEquals ( "" , actual ) ; } }
public class aTest{ @Test public void should_map_double_to_double ( ) { com . github . erchu . beancp . commons . NumberConvertersTest . Source sourceInstance = new com . github . erchu . beancp . commons . NumberConvertersTest . Source ( ) ; sourceInstance . setDoubleValue ( ( ( double ) ( 8 ) ) ) ; com . github . erchu . beancp . Mapper mapper = new com . github . erchu . beancp . MapperBuilder ( ) . addConverter ( com . github . erchu . beancp . commons . NumberConverters . get ( ) ) . addMap ( com . github . erchu . beancp . commons . NumberConvertersTest . Source . class , com . github . erchu . beancp . commons . NumberConvertersTest . Destination . class , ( config , source , destination ) -> config . mapInner ( source :: getDoubleValue , destination :: setDoubleValue , . class ) ) . buildMapper ( ) ; com . github . erchu . beancp . commons . NumberConvertersTest . Destination result = mapper . map ( sourceInstance , com . github . erchu . beancp . commons . NumberConvertersTest . Destination . class ) ; org . junit . Assert . assertEquals ( ( ( double ) ( 8 ) ) , result . getDoubleValue ( ) , 0.0 ) ; } }
public class aTest{ @Test public void testConcatSingleNumberedFiles ( ) { setMockups ( ) ; java . lang . String targetTags = "engage,rss" ; java . util . Map < java . lang . String , java . lang . String > configurations = new java . util . HashMap < java . lang . String , java . lang . String > ( ) ; configurations . put ( "source-flavor-numbered-files" , "presenter/source" ) ; configurations . put ( "target-flavor" , "presenter/concat" ) ; configurations . put ( "target-tags" , targetTags ) ; configurations . put ( "encoding-profile" , "concat" ) ; configurations . put ( "output-resolution" , "1900x1080" ) ; org . opencastproject . workflow . api . WorkflowOperationResult result = getWorkflowOperationResult ( mp , configurations ) ; org . opencastproject . mediapackage . MediaPackage mpNew = result . getMediaPackage ( ) ; org . opencastproject . mediapackage . Track [ ] tracks = mpNew . getTracks ( org . opencastproject . mediapackage . MediaPackageElementFlavor . parseFlavor ( "presenter/concat" ) ) ; org . opencastproject . mediapackage . Track trackEncoded = tracks [ 0 ] ; org . junit . Assert . assertArrayEquals ( org . apache . commons . lang3 . StringUtils . split ( targetTags , "target-flavor" 0 ) , trackEncoded . getTags ( ) ) ; } }
public class aTest{ @Test public void testSubmitQueryInvalid ( ) { final java . lang . String query = "SELECT<sp>?p<sp>WHERE<sp>{<sp>?s<sp><INVALID><sp>?o<sp>}" ; org . apache . commons . httpclient . methods . PostMethod post = new org . apache . commons . httpclient . methods . PostMethod ( aseBaseUrl ) ; post . addParameter ( Protocol . QUERY_PARAM_NAME , java . net . URLEncoder . encode ( query , "UTF-8" ) ) ; final int code = client . executeMethod ( post ) ; if ( code == ( org . apache . commons . httpclient . HttpStatus . SC_OK ) ) { final java . lang . String json = post . getResponseBodyAsString ( ) ; java . lang . String ref = "{\"status\":\"ERROR\"," + ( "\"message\":\"org.openrdf.repository.http.HTTPQueryEvaluationException:<sp>" + "org.openrdf.query.MalformedQueryException:<sp>Not<sp>a<sp>valid<sp>(absolute)<sp>URI:<sp>INVALID\"}" ) ; org . junit . Assert . assertEquals ( ref , json . toString ( ) ) ; } }
public class aTest{ @Test public void payloadSplitBetweenBuffers ( ) { deframer . request ( 1 ) ; fakeClock . forwardTime ( 10 , TimeUnit . MILLISECONDS ) ; deframer . deframe ( io . grpc . internal . MessageDeframerTest . buffer ( new byte [ ] { 0 , 0 , 0 , 0 , 7 , 3 , 14 , 1 , 5 , 9 } ) ) ; verify ( listener , atLeastOnce ( ) ) . bytesRead ( anyInt ( ) ) ; verifyNoMoreInteractions ( listener ) ; deframer . deframe ( io . grpc . internal . MessageDeframerTest . buffer ( new byte [ ] { 2 , 6 } ) ) ; verify ( listener ) . messagesAvailable ( producer . capture ( ) ) ; org . junit . Assert . assertEquals ( com . google . common . primitives . Bytes . asList ( new byte [ ] { 3 , 14 , 1 , 5 , 9 , 2 , 6 } ) , io . grpc . internal . MessageDeframerTest . bytes ( producer . getValue ( ) . next ( ) ) ) ; verify ( listener , atLeastOnce ( ) ) . bytesRead ( anyInt ( ) ) ; verifyNoMoreInteractions ( listener ) ; if ( useGzipInflatingBuffer ) { + 2 ) , 7 ) ; } }
public class aTest{ @Test public void testHttpHeaderAuthenticationFilterInvalidateSessionOnWrongHeader ( ) { modifyPropertySourceInEnvironment ( getDefaultSecurityEnvironmentVariables ( ) ) ; try { org . springframework . mock . web . MockHttpServletRequest request = getRequestWithHeaders ( org . finra . herd . app . security . USER_ID , "testFirstName" , "testLastName" , "testEmail" , "testRole" , "Wed,<sp>11<sp>Mar<sp>2015<sp>10:24:09" ) ; invalidateApplicationUser ( request ) ; httpHeaderAuthenticationFilter . init ( new org . springframework . mock . web . MockFilterConfig ( ) ) ; httpHeaderAuthenticationFilter . doFilter ( request , new org . springframework . mock . web . MockHttpServletResponse ( ) , new org . springframework . mock . web . MockFilterChain ( ) ) ; validateHttpHeaderApplicationUser ( org . finra . herd . app . security . USER_ID , "testFirstName" , "testLastName" , "testEmail" , "testRole" , "Wed,<sp>11<sp>Mar<sp>2015<sp>10:24:09" , null , null ) ; httpHeaderAuthenticationFilter . doFilter ( new org . springframework . mock . web . MockHttpServletRequest ( ) , new org . springframework . mock . web . MockHttpServletResponse ( ) , new org . springframework . mock . web . MockFilterChain ( ) ) ; org . springframework . security . core . Authentication authentication = org . springframework . security . core . context . SecurityContextHolder . getContext ( ) . getAuthentication ( ) ; org . junit . Assert . assertNull ( authentication ) ; } }
public class aTest{ @Test public void testMove ( ) { connection . set ( "foo" , "bar" ) ; actual . add ( connection . move ( "foo" , 1 ) ) ; verifyResults ( java . util . Arrays . asList ( new java . lang . Object [ ] { true } ) ) ; org . springframework . data . redis . connection . jredis . JredisConnectionFactory factory2 = new org . springframework . data . redis . connection . jredis . JredisConnectionFactory ( ) ; factory2 . setDatabase ( 1 ) ; factory2 . afterPropertiesSet ( ) ; org . springframework . data . redis . connection . StringRedisConnection conn2 = new org . springframework . data . redis . connection . DefaultStringRedisConnection ( factory2 . getConnection ( ) ) ; try { org . junit . Assert . assertEquals ( "bar" , conn2 . get ( "foo" ) ) ; } }
public class aTest{ @Test public void getUrlTwinSucceeds ( ) { final java . lang . String iotHubName = "b.c.d" ; final java . lang . String hostName = "HOSTNAME." + iotHubName ; final java . lang . String sharedAccessKeyName = "ACCESSKEYNAME" ; final java . lang . String policyName = "HostName=" 0 ; final java . lang . String sharedAccessKey = "1234567890abcdefghijklmnopqrstvwxyz=" ; final java . lang . String connectionString = ( ( ( ( ( ( "HostName=" + hostName ) + ";SharedAccessKeyName=" ) + sharedAccessKeyName ) + ";" ) + policyName ) + "=" ) + sharedAccessKey ; final java . lang . String deviceId = "testDevice" ; final com . microsoft . azure . sdk . iot . service . IotHubConnectionString iotHubConnectionString = com . microsoft . azure . sdk . iot . service . IotHubConnectionStringBuilder . createConnectionString ( connectionString ) ; final java . lang . String expected = "https://HOSTNAME.b.c.d/twins/testDevice?" + ( tests . unit . com . microsoft . azure . sdk . iot . service . IotHubConnectionStringTest . URL_API_VERSION ) ; java . lang . String actual = iotHubConnectionString . getUrlTwin ( deviceId ) . toString ( ) ; org . junit . Assert . assertTrue ( actual . equals ( expected ) ) ; } }
public class aTest{ @Test public void testConcurrentClose ( ) { java . nio . channels . FileChannel spyChannel = spy ( java . nio . channels . FileChannel . class ) ; org . terracotta . offheapstore . disk . paging . MappedPageSource source = new org . terracotta . offheapstore . disk . paging . MappedPageSource ( dataFile ) { @ org . terracotta . offheapstore . disk . storage . Override public java . nio . channels . FileChannel getReadableChannel ( ) { return spyChannel ; } } ; org . terracotta . offheapstore . disk . storage . FileBackedStorageEngine < byte [ ] , byte [ ] > engine = new org . terracotta . offheapstore . disk . storage . FileBackedStorageEngine ( source , Long . MAX_VALUE , org . terracotta . offheapstore . util . MemoryUnit . BYTES , org . terracotta . offheapstore . disk . storage . portability . PersistentByteArrayPortability . INSTANCE , org . terracotta . offheapstore . disk . storage . portability . PersistentByteArrayPortability . INSTANCE ) ; when ( spyChannel . read ( notNull ( ) , anyLong ( ) ) ) . thenAnswer ( ( o ) -> { engine . close ( ) ; throw new java . nio . channels . ClosedChannelException ( ) ; } ) ; try { byte [ ] buffer = new byte [ 10 ] ; long p = engine . writeMapping ( new byte [ 0 ] , buffer , 0 , 0 ) ; org . junit . Assert . assertTrue ( ( p >= 0 ) ) ; engine . flush ( ) ; engine . readValue ( 0 ) ; } }
public class aTest{ @Test public void should_map_float_to_float ( ) { com . github . erchu . beancp . commons . NumberConvertersTest . Source sourceInstance = new com . github . erchu . beancp . commons . NumberConvertersTest . Source ( ) ; sourceInstance . setFloatValue ( ( ( float ) ( 8 ) ) ) ; com . github . erchu . beancp . Mapper mapper = new com . github . erchu . beancp . MapperBuilder ( ) . addConverter ( com . github . erchu . beancp . commons . NumberConverters . get ( ) ) . addMap ( com . github . erchu . beancp . commons . NumberConvertersTest . Source . class , com . github . erchu . beancp . commons . NumberConvertersTest . Destination . class , ( config , source , destination ) -> config . mapInner ( source :: getFloatValue , destination :: setFloatValue , . class ) ) . buildMapper ( ) ; com . github . erchu . beancp . commons . NumberConvertersTest . Destination result = mapper . map ( sourceInstance , com . github . erchu . beancp . commons . NumberConvertersTest . Destination . class ) ; org . junit . Assert . assertEquals ( ( ( float ) ( 8 ) ) , result . getFloatValue ( ) , 0.0 ) ; } }
public class aTest{ @Test public void testRemoveEntities_19 ( ) { try { java . io . StringReader reader = new java . io . StringReader ( "&#x" ) ; java . io . StringWriter writer = new java . io . StringWriter ( ) ; org . milyn . xml . XmlUtil . removeEntities ( reader , writer ) ; org . junit . Assert . assertEquals ( "&#x" , writer . toString ( ) ) ; } }
public class aTest{ @Test public void copy ( com . airhacks . IndexPage ) { java . lang . String expected = "duke" ; java . lang . String actual = page . copy ( expected ) ; org . junit . Assert . assertThat ( actual , org . hamcrest . CoreMatchers . is ( expected ) ) ; } }
public class aTest{ @Test public void testGetNamedParameters ( ) { dispatcher . registerResponse ( "GET" , "14916" 0 ) . code ( 200 ) . content ( "{<sp>\"data\":<sp>[{<sp>\"id\":<sp>\"1\"<sp>}]}" ) ; java . util . Collection < com . asana . models . Task > result = client . tasks . findAll ( ) . query ( "workspace" , "14916" ) . query ( "assignee" , "me" ) . execute ( ) ; org . junit . Assert . assertEquals ( result . iterator ( ) . next ( ) . id , "1" ) ; } }
public class aTest{ @Test public void ADStrue ( ) { com . fujitsu . dc . core . model . impl . es . repair . RepairAds repair = com . fujitsu . dc . core . model . impl . es . repair . RepairAds . getInstance ( ) ; com . fujitsu . dc . common . ads . AdsWriteFailureLogWriter writer = com . fujitsu . dc . common . ads . AdsWriteFailureLogWriter . getInstance ( "./" , com . fujitsu . dc . test . unit . core . model . impl . es . repair . RepairAdsCompleteTest . PIO_VERSION_DUMMY , true ) ; java . io . File dir = new java . io . File ( com . fujitsu . dc . test . unit . core . model . impl . es . repair . RepairAdsCompleteTest . TEST_ADS_LOGDIR ) ; try { java . lang . Class < ? > clazz = com . fujitsu . dc . common . ads . AbstractAdsWriteFailureLog . class ; java . lang . reflect . Field baseDir = clazz . getDeclaredField ( "baseDir" ) ; baseDir . setAccessible ( true ) ; baseDir . set ( writer , com . fujitsu . dc . test . unit . core . model . impl . es . repair . RepairAdsCompleteTest . TEST_ADS_LOGDIR ) ; if ( ! ( dir . mkdir ( ) ) ) { org . junit . Assert . fail ( ( "mkdir<sp>failed(environment<sp>error):<sp>" + ( dir . getAbsolutePath ( ) ) ) ) ; } } catch ( java . lang . Exception e ) { e . printStackTrace ( ) ; org . junit . Assert . fail ( "configuration<sp>failed." ) ; } java . io . File file = null ; java . lang . String fileName = ( java . lang . String . format ( AbstractAdsWriteFailureLog . LOGNAME_FORMAT_ROTATE , com . fujitsu . dc . test . unit . core . model . impl . es . repair . RepairAdsCompleteTest . PIO_VERSION_DUMMY , java . lang . System . currentTimeMillis ( ) ) ) + ( com . fujitsu . dc . common . ads . AbstractAdsWriteFailureLog . LOGICAL_DELETED_LOGNAME_SUFFIX ) ; java . io . File rotated = new java . io . File ( dir , fileName ) ; try { java . lang . Class < ? > clazz = com . fujitsu . dc . core . model . impl . es . repair . RepairAds . class ; java . lang . reflect . Field baseDir = clazz . getDeclaredField ( "adsLogBaseDir" ) ; baseDir . setAccessible ( true ) ; baseDir . set ( repair , new java . io . File ( com . fujitsu . dc . test . unit . core . model . impl . es . repair . RepairAdsCompleteTest . TEST_ADS_LOGDIR ) ) ; java . lang . reflect . Field version = clazz . getDeclaredField ( "pcsVersion" ) ; version . setAccessible ( true ) ; version . set ( repair , com . fujitsu . dc . test . unit . core . model . impl . es . repair . RepairAdsCompleteTest . PIO_VERSION_DUMMY ) ; java . lang . reflect . Method method = clazz . getDeclaredMethod ( "isRepairCompleted" ) ; method . setAccessible ( true ) ; rotated . createNewFile ( ) ; file = getAdsWriteFailureLog ( writer ) ; org . junit . Assert . assertTrue ( ( ( java . lang . Boolean ) ( method . invoke ( repair ) ) ) ) ; } }
public class aTest{ @Test public void testRun ( ) { final sqlite . feature . typeadapter . kripton180 . Employee bean = new sqlite . feature . typeadapter . kripton180 . Employee ( ) ; bean . birthDate = new java . sql . Date ( new java . util . Date ( ) . getTime ( ) ) ; bean . fieldBoolean = "true" ; bean . fieldByte = "42" ; bean . fieldByteArray = "42" ; bean . fieldCharacter = "a" ; bean . fieldDouble = "120" ; bean . fieldFloat = "120" ; bean . fieldInteger = "11" ; bean . fieldLong = "13" ; bean . fieldShort = "2" ; bean . fieldString = "a" ; bean . hireDate = new java . sql . Date ( new java . util . Date ( ) . getTime ( ) ) ; sqlite . feature . typeadapter . kripton180 . bean . insertselect . BindKripton180BeanInsertSelectDataSource dataSource = sqlite . feature . typeadapter . kripton180 . bean . insertselect . BindKripton180BeanInsertSelectDataSource . getInstance ( ) ; dataSource . execute ( new sqlite . feature . typeadapter . kripton180 . bean . insertselect . BindKripton180BeanInsertSelectDataSource . Transaction ( ) { @ sqlite . feature . typeadapter . kripton180 . Override public com . abubusoft . kripton . android . sqlite . TransactionResult onExecute ( sqlite . feature . typeadapter . kripton180 . bean . insertselect . BindKripton180BeanInsertSelectDaoFactory daoFactory ) { sqlite . feature . typeadapter . kripton180 . bean . insertselect . EmployeeBeanInsertSelectDaoImpl dao = daoFactory . getEmployeeBeanInsertSelectDao ( ) ; dao . insertJQL ( bean ) ; org . junit . Assert . assertTrue ( ( ( bean . id ) > 0 ) ) ; return com . abubusoft . kripton . android . sqlite . TransactionResult . ROLLBACK ; } } }
public class aTest{ @Test public void debieraCrearAlmacen ( ) { mx . edu . um . mateo . general . model . Organizacion organizacion = new mx . edu . um . mateo . general . model . Organizacion ( "tst-01" , "almacen.creado.message" 5 , "almacen.creado.message" 5 ) ; currentSession ( ) . save ( organizacion ) ; mx . edu . um . mateo . contabilidad . model . EjercicioPK ejercicioPK = new mx . edu . um . mateo . contabilidad . model . EjercicioPK ( "almacen.creado.message" 3 , organizacion ) ; java . lang . Byte x = new java . lang . Byte ( "almacen.creado.message" 2 ) ; mx . edu . um . mateo . contabilidad . model . Ejercicio ejercicio = new mx . edu . um . mateo . contabilidad . model . Ejercicio ( ejercicioPK , "almacen.creado.message" 3 , "almacen.creado.message" 0 , org . apache . commons . lang . StringUtils . EMPTY , org . apache . commons . lang . StringUtils . EMPTY , org . apache . commons . lang . StringUtils . EMPTY , org . apache . commons . lang . StringUtils . EMPTY , x , x ) ; currentSession ( ) . save ( ejercicio ) ; mx . edu . um . mateo . general . model . Empresa empresa = new mx . edu . um . mateo . general . model . Empresa ( "tst-01" , "almacen.creado.message" 5 , "almacen.creado.message" 5 , "000000000001" , organizacion ) ; currentSession ( ) . save ( empresa ) ; mx . edu . um . mateo . general . model . Rol rol = new mx . edu . um . mateo . general . model . Rol ( "almacen.creado.message" 7 ) ; currentSession ( ) . save ( rol ) ; java . util . Set < mx . edu . um . mateo . general . model . Rol > roles = new java . util . HashSet ( ) ; roles . add ( rol ) ; mx . edu . um . mateo . inventario . model . Almacen almacen = new mx . edu . um . mateo . inventario . model . Almacen ( "TST" , "almacen.creado.message" 3 , empresa ) ; currentSession ( ) . save ( almacen ) ; mx . edu . um . mateo . general . model . Usuario usuario = new mx . edu . um . mateo . general . model . Usuario ( "bugs@um.edu.mx" , "apPaterno" , "apMaterno" , "TEST-01" , "TEST-01" ) ; usuario . setEmpresa ( empresa ) ; usuario . setAlmacen ( almacen ) ; usuario . setRoles ( roles ) ; usuario . setEjercicio ( ejercicio ) ; currentSession ( ) . save ( usuario ) ; java . lang . Long id = usuario . getId ( ) ; org . junit . Assert . assertNotNull ( id ) ; this . authenticate ( usuario , usuario . getPassword ( ) , new java . util . ArrayList < org . springframework . security . core . GrantedAuthority > ( usuario . getRoles ( ) ) ) ; this . mockMvc . perform ( post ( "almacen.creado.message" 8 ) . param ( "almacen.creado.message" 4 , "TST-01" ) . param ( "almacen.creado.message" 1 , "almacen.creado.message" 6 ) ) . andExpect ( status ( ) . isOk ( ) ) . andExpect ( flash ( ) . attributeExists ( "message" ) ) . andExpect ( flash ( ) . attribute ( "message" , "almacen.creado.message" ) ) ; } }
public class aTest{ @Test public void testLoginSuccess ( ) { json . put ( "username" , "billaras" ) ; json . put ( "password" , "user" 0 ) ; com . tech . models . entities . user . User tempUser = new com . tech . models . entities . user . User ( 4L , "billaras" , "user" 0 , true , true ) ; com . tech . models . entities . user . UserInfo tempUserInfo = new com . tech . models . entities . user . UserInfo ( 4L , "bill54greek@gmail.com" , "nothing" , "hi<sp>there" , "iwannidis" , new java . util . Date ( ) , "Giannitsa" , "vasilis" ) ; when ( userService . validateUser ( "billaras" , "user" 0 ) ) . thenReturn ( true ) ; when ( userService . getUserByUsername ( "billaras" ) ) . thenReturn ( tempUser ) ; when ( userInfoService . getUserInfoByUserId ( tempUser . getId ( ) ) ) . thenReturn ( tempUserInfo ) ; when ( userRoleService . getRoleByUserID ( tempUser . getId ( ) ) ) . thenReturn ( "user" ) ; org . springframework . test . web . servlet . MvcResult result = mvc . perform ( org . springframework . test . web . servlet . request . MockMvcRequestBuilders . post ( uri ) . content ( json . toString ( ) ) . contentType ( MediaType . APPLICATION_JSON ) ) . andReturn ( ) ; int status = result . getResponse ( ) . getStatus ( ) ; verify ( userService , times ( 1 ) ) . getUserByUsername ( "billaras" ) ; verify ( userService , times ( 1 ) ) . validateUser ( "billaras" , "user" 0 ) ; verify ( userInfoService , times ( 1 ) ) . getUserInfoByUserId ( tempUser . getId ( ) ) ; verify ( userRoleService , times ( 1 ) ) . getRoleByUserID ( tempUser . getId ( ) ) ; org . junit . Assert . assertEquals ( "user" 1 , 200 , status ) ; } }
public class aTest{ @Test public void testNestedRVCBasic ( ) { long ts = nextTimestamp ( ) ; java . lang . String tenantId = getOrganizationId ( ) ; initATableValues ( tenantId , getDefaultSplits ( tenantId ) , null , ts ) ; java . lang . String [ ] queries = new java . lang . String [ ] { "SELECT<sp>organization_id,<sp>entity_id,<sp>a_string<sp>FROM<sp>aTable<sp>WHERE<sp>((organization_id,<sp>entity_id),<sp>a_string)<sp>>=<sp>((?,<sp>?),<sp>?)" , "SELECT<sp>organization_id,<sp>entity_id,<sp>a_string<sp>FROM<sp>aTable<sp>WHERE<sp>(organization_id,<sp>entity_id,<sp>a_string)<sp>>=<sp>(?,<sp>?,<sp>?)" , "SELECT<sp>organization_id,<sp>entity_id,<sp>a_string<sp>FROM<sp>aTable<sp>WHERE<sp>(organization_id,<sp>(entity_id,<sp>a_string))<sp>>=<sp>(?,<sp>(?,<sp>?))" } ; java . util . Properties props = new java . util . Properties ( TEST_PROPERTIES ) ; props . setProperty ( PhoenixRuntime . CURRENT_SCN_ATTRIB , java . lang . Long . toString ( ( ts + 2 ) ) ) ; java . sql . Connection conn = java . sql . DriverManager . getConnection ( com . salesforce . phoenix . end2end . PHOENIX_JDBC_URL , props ) ; java . sql . PreparedStatement statement = null ; try { try { for ( int i = 0 ; i <= 2 ; i ++ ) { statement = conn . prepareStatement ( queries [ i ] ) ; statement . setString ( 1 , tenantId ) ; statement . setString ( 2 , com . salesforce . phoenix . end2end . ROW1 ) ; statement . setString ( 3 , "a" ) ; java . sql . ResultSet rs = statement . executeQuery ( ) ; int count = 0 ; while ( rs . next ( ) ) { count ++ ; } org . junit . Assert . assertEquals ( 9 , count ) ; } } }
public class aTest{ @Test public void testIsEncodeEquals ( ) { final java . lang . String [ ] [ ] data = new java . lang . String [ ] [ ] { new java . lang . String [ ] { "Muller" , "Müller" } , new java . lang . String [ ] { "Muller" 0 , "Muller" 1 } , new java . lang . String [ ] { "house" , "house" } , new java . lang . String [ ] { "House" , "house" } , new java . lang . String [ ] { "Haus" , "house" } , new java . lang . String [ ] { "ganz" , "Gans" } , new java . lang . String [ ] { "ganz" , "Gänse" } , new java . lang . String [ ] { "Miyagi" , "Miyako" } } ; for ( final java . lang . String [ ] element : data ) { final boolean encodeEqual = this . getStringEncoder ( ) . isEncodeEqual ( element [ 1 ] , element [ 0 ] ) ; org . junit . Assert . assertTrue ( ( ( ( element [ 1 ] ) + "Muller" 2 ) + ( element [ 0 ] ) ) , encodeEqual ) ; } } }
public class aTest{ @Test public void testBasicStackUnstackDebug ( ) { org . nd4j . linalg . factory . Nd4j . getRandom ( ) . setSeed ( 12345 ) ; org . deeplearning4j . nn . conf . ComputationGraphConfiguration conf = new org . deeplearning4j . nn . conf . NeuralNetConfiguration . Builder ( ) . seed ( 12345 ) . optimizationAlgo ( OptimizationAlgorithm . STOCHASTIC_GRADIENT_DESCENT ) . dist ( new org . deeplearning4j . nn . conf . distribution . NormalDistribution ( 0 , 1 ) ) . activation ( Activation . TANH ) . updater ( new org . nd4j . linalg . learning . config . NoOp ( ) ) . graphBuilder ( ) . addInputs ( "in1" , "in1" 1 ) . addLayer ( "d0" , new org . deeplearning4j . gradientcheck . DenseLayer . Builder ( ) . nIn ( 2 ) . nOut ( 2 ) . build ( ) , "in1" ) . addLayer ( "d1" , new org . deeplearning4j . gradientcheck . DenseLayer . Builder ( ) . nIn ( 2 ) . nOut ( 2 ) . build ( ) , "in1" 1 ) . addVertex ( "in1" 0 , new org . deeplearning4j . gradientcheck . StackVertex ( ) , "d0" , "d1" ) . addVertex ( "u0" , new org . deeplearning4j . gradientcheck . UnstackVertex ( 0 , 2 ) , "in1" 0 ) . addVertex ( "u1" , new org . deeplearning4j . gradientcheck . UnstackVertex ( 1 , 2 ) , "in1" 0 ) . addLayer ( "out1" , new org . deeplearning4j . gradientcheck . OutputLayer . Builder ( ) . lossFunction ( LossFunctions . LossFunction . L2 ) . nIn ( 2 ) . nOut ( 2 ) . activation ( Activation . IDENTITY ) . build ( ) , "u0" ) . addLayer ( "out2" , new org . deeplearning4j . gradientcheck . OutputLayer . Builder ( ) . lossFunction ( LossFunctions . LossFunction . L2 ) . nIn ( 2 ) . nOut ( 2 ) . activation ( Activation . IDENTITY ) . build ( ) , "u1" ) . setOutputs ( "out1" , "out2" ) . build ( ) ; org . deeplearning4j . nn . graph . ComputationGraph graph = new org . deeplearning4j . nn . graph . ComputationGraph ( conf ) ; graph . init ( ) ; org . nd4j . linalg . factory . Nd4j . getRandom ( ) . setSeed ( 12345 ) ; long nParams = graph . numParams ( ) ; org . nd4j . linalg . api . ndarray . INDArray newParams = org . nd4j . linalg . factory . Nd4j . rand ( new long [ ] { 1 , nParams } ) ; graph . setParams ( newParams ) ; int [ ] mbSizes = new int [ ] { 1 , 3 , 10 } ; for ( int minibatch : mbSizes ) { org . nd4j . linalg . api . ndarray . INDArray in1 = org . nd4j . linalg . factory . Nd4j . rand ( minibatch , 2 ) ; org . nd4j . linalg . api . ndarray . INDArray in2 = org . nd4j . linalg . factory . Nd4j . rand ( minibatch , 2 ) ; org . nd4j . linalg . api . ndarray . INDArray labels1 = org . nd4j . linalg . factory . Nd4j . rand ( minibatch , 2 ) ; org . nd4j . linalg . api . ndarray . INDArray labels2 = org . nd4j . linalg . factory . Nd4j . rand ( minibatch , 2 ) ; java . lang . String testName = "testBasicStackUnstack()<sp>-<sp>minibatch<sp>=<sp>" + minibatch ; if ( org . deeplearning4j . gradientcheck . GradientCheckTestsComputationGraph . PRINT_RESULTS ) { System . out . println ( testName ) ; for ( int j = 0 ; j < ( graph . getNumLayers ( ) ) ; j ++ ) System . out . println ( ( ( ( "Layer<sp>" + j ) + "<sp>#<sp>params:<sp>" ) + ( graph . getLayer ( j ) . numParams ( ) ) ) ) ; } boolean gradOK = org . deeplearning4j . gradientcheck . GradientCheckUtil . checkGradients ( graph , org . deeplearning4j . gradientcheck . GradientCheckTestsComputationGraph . DEFAULT_EPS , org . deeplearning4j . gradientcheck . GradientCheckTestsComputationGraph . DEFAULT_MAX_REL_ERROR , org . deeplearning4j . gradientcheck . GradientCheckTestsComputationGraph . DEFAULT_MIN_ABS_ERROR , org . deeplearning4j . gradientcheck . GradientCheckTestsComputationGraph . PRINT_RESULTS , org . deeplearning4j . gradientcheck . GradientCheckTestsComputationGraph . RETURN_ON_FIRST_FAILURE , new org . nd4j . linalg . api . ndarray . INDArray [ ] { in1 , in2 } , new org . nd4j . linalg . api . ndarray . INDArray [ ] { labels1 , labels2 } ) ; org . junit . Assert . assertTrue ( testName , gradOK ) ; org . deeplearning4j . TestUtils . testModelSerialization ( graph ) ; } } }
public class aTest{ @Test public void testGeneratingBaseConusmerScenario ( ) { java . io . File wsdl = getCodegenQEDataFileInput ( "Vanilla-Codegen/ServiceInputFiles" 4 ) ; java . io . File consumerProps = createPropertyFile ( destDir . getAbsolutePath ( ) , CONSUMER_PROPERTIES ) ; fillProperties ( consumerProper , consumerProps ) ; java . lang . String [ ] testArgs1 = new java . lang . String [ ] { "Vanilla-Codegen/ServiceInputFiles" 0 , "Vanilla-Codegen/ServiceInputFiles" 8 , "-genType" , "Consumer" , "-wsdl" , wsdl . getAbsolutePath ( ) , "-dest" , destDir . getAbsolutePath ( ) , "Vanilla-Codegen/ServiceInputFiles" 7 , ( destDir . getAbsolutePath ( ) ) + "/src" , "Vanilla-Codegen/ServiceInputFiles" 3 , "Vanilla-Codegen/ServiceInputFiles" 9 , "Vanilla-Codegen/ServiceInputFiles" 2 , "COMMON" , "-bin" , binDir . getAbsolutePath ( ) , "Vanilla-Codegen/ServiceInputFiles" 5 , destDir . getAbsolutePath ( ) , "-cn" , "Vanilla-Codegen/ServiceInputFiles" 6 } ; performDirectCodeGen ( testArgs1 , binDir ) ; baseConsumer = ( destDir . getAbsolutePath ( ) ) + "/src/org/ebayopensource/turmeric/common/v1/services/gen/BaseNewService123Consumer.java" ; baseConsumerClass = new java . io . File ( baseConsumer ) ; org . junit . Assert . assertTrue ( baseConsumerClass . exists ( ) ) ; } }
public class aTest{ @Test public void testYankingRegionFromUnderIt ( ) { final org . apache . hadoop . hbase . Server server = new org . apache . hadoop . hbase . util . MockServer ( org . apache . hadoop . hbase . regionserver . handler . TestOpenRegionHandler . HTU ) ; final org . apache . hadoop . hbase . regionserver . RegionServerServices rss = new org . apache . hadoop . hbase . util . MockRegionServerServices ( ) ; org . apache . hadoop . hbase . HTableDescriptor htd = org . apache . hadoop . hbase . regionserver . handler . TestOpenRegionHandler . TEST_HTD ; final org . apache . hadoop . hbase . HRegionInfo hri = TEST_HRI ; org . apache . hadoop . hbase . regionserver . HRegion region = org . apache . hadoop . hbase . regionserver . HRegion . createHRegion ( hri , org . apache . hadoop . hbase . regionserver . handler . TestOpenRegionHandler . HTU . getDataTestDir ( ) , org . apache . hadoop . hbase . regionserver . handler . TestOpenRegionHandler . HTU . getConfiguration ( ) , htd ) ; org . junit . Assert . assertNotNull ( region ) ; try { org . apache . hadoop . hbase . regionserver . handler . OpenRegionHandler handler = new org . apache . hadoop . hbase . regionserver . handler . OpenRegionHandler ( server , rss , hri , htd ) { org . apache . hadoop . hbase . regionserver . HRegion openRegion ( ) { org . apache . hadoop . hbase . regionserver . HRegion region = super . openRegion ( ) ; org . apache . hadoop . hbase . zookeeper . ZooKeeperWatcher zkw = this . server . getZooKeeper ( ) ; java . lang . String node = org . apache . hadoop . hbase . zookeeper . ZKAssign . getNodeName ( zkw , hri . getEncodedName ( ) ) ; try { org . apache . hadoop . hbase . zookeeper . ZKUtil . deleteNodeFailSilent ( zkw , node ) ; } }
public class aTest{ @Test public void testCorruptedSnapshot ( ) { org . apache . hadoop . hbase . snapshot . SnapshotTestingUtils . corruptSnapshot ( org . apache . hadoop . hbase . client . TEST_UTIL , snapshotName0 ) ; org . apache . hadoop . hbase . TableName cloneName = org . apache . hadoop . hbase . TableName . valueOf ( ( ( ( getValidMethodName ( ) ) + "-" ) + ( java . lang . System . currentTimeMillis ( ) ) ) ) ; try { admin . cloneSnapshot ( snapshotName0 , cloneName ) ; org . junit . Assert . fail ( "Expected<sp>CorruptedSnapshotException,<sp>got<sp>succeeded<sp>cloneSnapshot()" ) ; } catch ( org . apache . hadoop . hbase . snapshot . CorruptedSnapshotException e ) { org . junit . Assert . assertFalse ( admin . tableExists ( cloneName ) ) ; } }
public class aTest{ @Test public void testSupplierOrdering ( ) { org . osgi . framework . BundleContext bc = org . osgi . framework . FrameworkUtil . getBundle ( getClass ( ) ) . getBundleContext ( ) ; org . eclipse . e4 . core . di . suppliers . ExtendedObjectSupplier supplier = new org . eclipse . e4 . core . di . suppliers . ExtendedObjectSupplier ( ) { @ org . eclipse . e4 . core . internal . tests . di . extensions . Override public java . lang . Object get ( org . eclipse . e4 . core . di . suppliers . IObjectDescriptor descriptor , org . eclipse . e4 . core . di . suppliers . IRequestor requestor , boolean track , boolean group ) { return null ; } } ; java . util . Dictionary < java . lang . String , java . lang . Object > properties = new java . util . Hashtable ( ) ; properties . put ( ExtendedObjectSupplier . SERVICE_CONTEXT_KEY , org . eclipse . e4 . core . di . extensions . EventTopic . class . getName ( ) ) ; properties . put ( org . osgi . framework . Constants . SERVICE_RANKING , 100 ) ; org . osgi . framework . ServiceRegistration < ? > sr = bc . registerService ( ExtendedObjectSupplier . SERVICE_NAME , supplier , properties ) ; try { org . junit . Assert . assertEquals ( supplier , org . eclipse . e4 . core . internal . di . osgi . ProviderHelper . findProvider ( org . eclipse . e4 . core . di . extensions . EventTopic . class . getName ( ) , null ) ) ; } }
public class aTest{ @Test public void sendEventsFullBatchWithAppPropsTest ( ) { final java . util . concurrent . CompletableFuture < java . lang . Void > validator = new java . util . concurrent . CompletableFuture ( ) ; final com . microsoft . azure . eventhubs . sendrecv . PartitionReceiver receiver = com . microsoft . azure . eventhubs . sendrecv . EventDataBatchAPITest . ehClient . createReceiverSync ( com . microsoft . azure . eventhubs . sendrecv . EventDataBatchAPITest . cgName , com . microsoft . azure . eventhubs . sendrecv . EventDataBatchAPITest . partitionId , com . microsoft . azure . eventhubs . sendrecv . EventPosition . fromEndOfStream ( ) ) ; receiver . setReceiveTimeout ( java . time . Duration . ofSeconds ( 5 ) ) ; try { final com . microsoft . azure . eventhubs . sendrecv . EventDataBatch batchEvents = com . microsoft . azure . eventhubs . sendrecv . EventDataBatchAPITest . sender . createBatch ( ) ; int count = 0 ; while ( true ) { final com . microsoft . azure . eventhubs . sendrecv . EventData eventData = com . microsoft . azure . eventhubs . sendrecv . EventData . create ( new java . lang . String ( new char [ 50000 ] ) . replace ( " " , "a" ) . getBytes ( ) ) ; for ( int i = 0 ; i < ( new java . util . Random ( ) . nextInt ( 20 ) ) ; i ++ ) eventData . getProperties ( ) . put ( ( "somekey" + i ) , "somevalue" ) ; if ( batchEvents . tryAdd ( eventData ) ) count ++ ; else break ; } org . junit . Assert . assertEquals ( count , batchEvents . getSize ( ) ) ; receiver . setReceiveHandler ( new com . microsoft . azure . eventhubs . sendrecv . EventDataBatchAPITest . CountValidator ( validator , count ) ) ; com . microsoft . azure . eventhubs . sendrecv . EventDataBatchAPITest . sender . sendSync ( batchEvents ) ; validator . get ( 100 , TimeUnit . SECONDS ) ; receiver . setReceiveHandler ( null ) ; } }
public class aTest{ @Test public void testMerge ( ) { try ( com . questdb . store . JournalWriter < com . questdb . model . Quote > w = getFactory ( ) . writer ( com . questdb . model . Quote . class ) ) { com . questdb . test . tools . TestUtils . generateQuoteData ( w , 100000 , com . questdb . std . time . DateFormatUtils . parseDateTime ( "2014-02-11T00:00:00.000Z" ) , 10 ) ; com . questdb . ql . RowSource srcA = new com . questdb . ql . latest . KvIndexSymLookupRowSource ( "sym" , "BP.L" , true ) ; com . questdb . ql . RowSource srcB = new com . questdb . ql . latest . KvIndexSymLookupRowSource ( "sym" , "WTB.L" , true ) ; try ( com . questdb . ql . RecordSource rs = new com . questdb . ql . JournalRecordSource ( new com . questdb . ql . JournalPartitionSource ( w . getMetadata ( ) , true ) , new com . questdb . ql . latest . MergingRowSource ( srcA , srcB ) ) ) { long last = 0 ; com . questdb . store . RecordCursor c = rs . prepareCursor ( getFactory ( ) ) ; try { int ts = rs . getMetadata ( ) . getColumnIndex ( "timestamp" ) ; while ( c . hasNext ( ) ) { long r = c . next ( ) . getDate ( ts ) ; org . junit . Assert . assertTrue ( ( r > last ) ) ; last = r ; } } }
public class aTest{ @Test public void testPosixError ( ) { java . util . Map < jnr . ffi . LibraryOption , java . lang . Object > options = new java . util . HashMap < jnr . ffi . LibraryOption , java . lang . Object > ( ) ; options . put ( LibraryOption . TypeMapper , jnr . ffi . ResultConverterTest . posixTypeMapper ) ; jnr . ffi . ResultConverterTest . Posix posix = jnr . ffi . TstUtil . loadTestLib ( jnr . ffi . ResultConverterTest . Posix . class , options ) ; org . junit . Assert . assertEquals ( 1 , posix . ret_int32_t ( 1 ) ) ; try { posix . ret_int32_t ( ( - 1 ) ) ; } }
public class aTest{ @Test public void be_alerted_of_process_end ( ) { org . terracotta . ipceventbus . proc . EventJavaProcess process = org . terracotta . ipceventbus . proc . EventJavaProcess . newBuilder ( ) . mainClass ( org . terracotta . ipceventbus . proc . EchoEvent2 . class ) . pipeStdout ( ) . pipeStderr ( ) . debug ( ) . build ( ) ; org . junit . Assert . assertTrue ( process . isEventBusConnected ( ) ) ; final java . util . concurrent . CountDownLatch latch = new java . util . concurrent . CountDownLatch ( 3 ) ; process . on ( "process.exiting" , new org . terracotta . ipceventbus . event . EventListener ( ) { @ org . terracotta . ipceventbus . proc . Override public void onEvent ( org . terracotta . ipceventbus . event . Event e ) throws java . lang . Throwable { System . out . println ( "Exiting..." ) ; latch . countDown ( ) ; } } }
public class aTest{ @Test public void testGetAllSubsections ( ) { config . put ( "section1.subsection1.int" , 1 ) ; config . put ( "section1.subsection2.string" , "2" ) ; config . put ( "4" 0 , 1 ) ; config . put ( "section1.subsection2.subsub2.string" , "2" ) ; config . put ( "section2.subsection3.int" , 3 ) ; config . put ( "section2.subsection4.string" , "4" ) ; java . util . List < java . lang . String > allSubsections = config . getAllSubsections ( "4" 1 ) ; java . util . Set < java . lang . String > expected = com . google . common . collect . ImmutableSet . of ( "4" 2 , "subsection2" , "subsection1.subsub1" , "subsection2.subsub2" ) ; org . junit . Assert . assertEquals ( expected , new java . util . HashSet < java . lang . String > ( allSubsections ) ) ; } }
public class aTest{ @Test public void callsCanReturnNullAkaInvalidObject ( ) { try { com . aldebaran . qi . AnyObject anyObject = proxy . < com . aldebaran . qi . AnyObject > call ( "createNullObject" ) . get ( ) ; org . junit . Assert . assertNull ( anyObject ) ; } }
public class aTest{ @Test public void testGetContentRootDoc ( ) { try { org . odftoolkit . simple . Document odt = org . odftoolkit . simple . Document . loadDocument ( org . odftoolkit . simple . utils . ResourceUtilities . getAbsolutePath ( org . odftoolkit . simple . DocumentTest . TEST_FILE_WITHOUT_OPT ) ) ; org . odftoolkit . odfdom . dom . OdfContentDom odfcon = odt . getContentDom ( ) ; org . junit . Assert . assertNotNull ( odfcon ) ; } }
public class aTest{ @Test public void testStmExecConnectFail ( ) { ch . rgw . tools . JdbcLink link = new ch . rgw . tools . JdbcLink ( "org.h2.Driver" , "jdbc:h2:mem:test_mem" , "" ) ; link . connect ( "" , "" ) ; ch . rgw . tools . JdbcLink . Stm stm = link . getStatement ( ) ; org . junit . Assert . assertNotNull ( stm ) ; link . disconnect ( ) ; stm . delete ( ) ; try { stm . exec ( "" ) ; org . junit . Assert . fail ( "Expected<sp>Exception<sp>not<sp>thrown!" ) ; } }
public class aTest{ @Test public void testBindSimple ( ) { java . net . ServerSocket socket = new java . net . ServerSocket ( ) ; java . net . InetSocketAddress address = new java . net . InetSocketAddress ( "0.0.0.0" , 0 ) ; org . apache . hadoop . ipc . Server . bind ( socket , address , 10 ) ; try { org . junit . Assert . assertTrue ( socket . isBound ( ) ) ; } }
public class aTest{ @Test public void testSkipNotEnough ( ) { org . apache . hadoop . hdfs . util . ExactSizeInputStream s = new org . apache . hadoop . hdfs . util . ExactSizeInputStream ( org . apache . hadoop . hdfs . util . TestExactSizeInputStream . byteStream ( "he" ) , 5 ) ; org . junit . Assert . assertEquals ( 2 , s . skip ( 3 ) ) ; try { s . skip ( 1 ) ; org . junit . Assert . fail ( "Skip<sp>when<sp>should<sp>be<sp>out<sp>of<sp>data" ) ; } }
public class aTest{ @Test public void assertItemRegistryIsThreadSafe ( ) { java . util . concurrent . atomic . AtomicInteger numberOfSuccessfulGetItemCalls = new java . util . concurrent . atomic . AtomicInteger ( 0 ) ; for ( int i = 0 ; i < 10 ; i ++ ) { new java . lang . Thread ( ( ) -> { for ( int j = 0 ; j < 10 ; j ++ ) { try { itemRegistry . getItem ( org . eclipse . smarthome . core . items . ItemRegistryImplTest . ITEM_NAME ) ; numberOfSuccessfulGetItemCalls . incrementAndGet ( ) ; } catch ( e ) { } } } ) . start ( ) ; } waitFor ( ( ) -> ( numberOfSuccessfulGetItemCalls . get ( ) ) >= 100 ) ; org . junit . Assert . assertThat ( numberOfSuccessfulGetItemCalls . get ( ) , org . hamcrest . CoreMatchers . is ( 100 ) ) ; } }
public class aTest{ @Test public void importHechingers ( ) { java . lang . String [ ] datasetsUsingHechingerFormat = new java . lang . String [ ] { "thieltges2011" , "preston2012" , "zander2011" , "mouritsen2011" } ; int studyCount = 0 ; for ( java . lang . String dataset : datasetsUsingHechingerFormat ) { try { org . eol . globi . data . StudyImporterForRegistry importer = createImporter ( ) ; importer . importData ( ( "globalbioticinteractions/" + dataset ) ) ; java . util . List < org . eol . globi . domain . StudyNode > allStudies = org . eol . globi . util . NodeUtil . findAllStudies ( getGraphDb ( ) ) ; org . junit . Assert . assertThat ( allStudies . size ( ) , org . hamcrest . core . Is . is ( ( studyCount + 1 ) ) ) ; studyCount += 1 ; } }
public class aTest{ @Test public void testPaginatedQueryWithoutCriterionWithLimit ( ) { int numberOfElements = 283 ; java . lang . String identifier = "testPaginatedQueryWithoutCriterionWithLimit" ; generateDataTest ( identifier , numberOfElements ) ; ch . epfl . gsn . beans . DataField [ ] structure = dataField . toArray ( new ch . epfl . gsn . beans . DataField [ ] { } ) ; ch . epfl . gsn . storage . hibernate . HibernateStorage storage = ch . epfl . gsn . storage . hibernate . HibernateStorage . newInstance ( ch . epfl . gsn . storage . hibernate . TestHibernateStorage . dbInfo , identifier , dataField . toArray ( new ch . epfl . gsn . beans . DataField [ ] { } ) , false ) ; org . junit . Assert . assertNotNull ( storage ) ; int [ ] pageSizes = new int [ ] { 1 , 11 , numberOfElements / 2 , numberOfElements / 3 , ( numberOfElements / 3 ) - 1 , ( numberOfElements / 3 ) + 1 , numberOfElements - 1 , numberOfElements + 1 , numberOfElements * 2 } ; for ( int pageSize : pageSizes ) { System . out . println ( ( "testPaginatedQueryWithoutCriterionWithLimit<sp>ASC<sp>with<sp>pageSize:<sp>" + pageSize ) ) ; checkQueryResult ( storage . getStreamElements ( pageSize , org . hibernate . criterion . Order . asc ( "timed" ) , new org . hibernate . criterion . Criterion [ ] { } , 1 ) , 1 , 1 ) ; checkQueryResult ( storage . getStreamElements ( pageSize , org . hibernate . criterion . Order . asc ( "timed" ) , new org . hibernate . criterion . Criterion [ ] { } , 11 ) , 1 , 11 ) ; checkQueryResult ( storage . getStreamElements ( pageSize , org . hibernate . criterion . Order . asc ( "timed" ) , new org . hibernate . criterion . Criterion [ ] { } , ( ( numberOfElements / 10 ) + 1 ) ) , 1 , ( ( numberOfElements / 10 ) + 1 ) ) ; checkQueryResult ( storage . getStreamElements ( pageSize , org . hibernate . criterion . Order . asc ( "timed" ) , new org . hibernate . criterion . Criterion [ ] { } , ( ( numberOfElements / 10 ) - 1 ) ) , 1 , ( ( numberOfElements / 10 ) - 1 ) ) ; checkQueryResult ( storage . getStreamElements ( pageSize , org . hibernate . criterion . Order . asc ( "timed" ) , new org . hibernate . criterion . Criterion [ ] { } , 0 ) , ( - 1 ) , ( - 1 ) ) ; } }
public class aTest{ @Test public void testSendSuccessCompressed ( ) { try ( org . apache . nifi . remote . client . SiteToSiteClient client = getDefaultBuilder ( ) . portName ( "input-running" ) . useCompression ( true ) . build ( ) ) { final org . apache . nifi . remote . Transaction transaction = client . createTransaction ( TransferDirection . SEND ) ; org . junit . Assert . assertNotNull ( transaction ) ; org . apache . nifi . remote . client . http . TestHttpClient . serverChecksum = "1071206772" ; for ( int i = 0 ; i < 20 ; i ++ ) { org . apache . nifi . remote . protocol . DataPacket packet = new org . apache . nifi . remote . client . http . TestHttpClient . DataPacketBuilder ( ) . contents ( "Example<sp>contents<sp>from<sp>client." ) . attr ( "Client<sp>attr<sp>1" , "Client<sp>attr<sp>1<sp>value" ) . attr ( "Client<sp>attr<sp>2" , "Client<sp>attr<sp>2<sp>value" ) . build ( ) ; transaction . send ( packet ) ; long written = ( ( org . apache . nifi . remote . Peer ) ( transaction . getCommunicant ( ) ) ) . getCommunicationsSession ( ) . getBytesWritten ( ) ; org . apache . nifi . remote . client . http . TestHttpClient . logger . info ( "{}:<sp>{}<sp>bytes<sp>have<sp>been<sp>written." , i , written ) ; } }
public class aTest{ @Test public void shouldNotMergeWithWrongUser ( ) { updateViewHook . setViewToMerge ( viewToMerge ) ; updateViewHook . setWithOpAuth ( com . google . common . collect . Sets . newHashSet ( "opA" ) ) ; opChain = new uk . gov . gchq . gaffer . operation . OperationChain ( new uk . gov . gchq . gaffer . operation . impl . get . GetAllElements ( ) ) ; updateViewHook . preExecute ( opChain , new uk . gov . gchq . gaffer . store . Context ( new uk . gov . gchq . gaffer . user . User ( ) ) ) ; java . lang . Object op = opChain . getOperations ( ) . get ( 0 ) ; if ( op instanceof uk . gov . gchq . gaffer . operation . graph . OperationView ) { uk . gov . gchq . gaffer . operation . graph . OperationView opView = ( ( uk . gov . gchq . gaffer . operation . graph . OperationView ) ( op ) ) ; org . junit . Assert . assertNull ( opView . getView ( ) ) ; } }
public class aTest{ @Test public void testSplitTreeSerialization ( ) { com . sun . sgs . test . app . util . TestScalableHashMap . txnScheduler . runTask ( new com . sun . sgs . test . util . TestAbstractKernelRunnable ( ) { public void run ( ) throws com . sun . sgs . test . app . util . Exception { java . util . Map < java . lang . Integer , java . lang . Integer > test = new com . sun . sgs . app . util . ScalableHashMap < java . lang . Integer , java . lang . Integer > ( 16 ) ; java . util . Map < java . lang . Integer , java . lang . Integer > control = new java . util . HashMap < java . lang . Integer , java . lang . Integer > ( ) ; int [ ] a = new int [ 100 ] ; for ( int i = 0 ; i < ( a . length ) ; i ++ ) { int j = com . sun . sgs . test . app . util . TestScalableHashMap . RANDOM . nextInt ( ) ; test . put ( j , j ) ; control . put ( j , j ) ; a [ i ] = j ; } java . io . ByteArrayOutputStream baos = new java . io . ByteArrayOutputStream ( ) ; java . io . ObjectOutputStream oos = new java . io . ObjectOutputStream ( baos ) ; oos . writeObject ( test ) ; byte [ ] serializedForm = baos . toByteArray ( ) ; java . io . ByteArrayInputStream bais = new java . io . ByteArrayInputStream ( serializedForm ) ; java . io . ObjectInputStream ois = new java . io . ObjectInputStream ( bais ) ; com . sun . sgs . app . util . ScalableHashMap < java . lang . Integer , java . lang . Integer > m = ( ( com . sun . sgs . app . util . ScalableHashMap < java . lang . Integer , java . lang . Integer > ) ( ois . readObject ( ) ) ) ; org . junit . Assert . assertEquals ( control , m ) ; } } }
public class aTest{ @Test public void mockFieldWithTwoInterfaces ( ) { new mockit . Expectations ( ) { { multiMock . doSomething ( false ) ; result = "test" ; } } ; multiMock . run ( ) ; org . junit . Assert . assertEquals ( "test" , multiMock . doSomething ( false ) ) ; new mockit . Verifications ( ) { { multiMock . run ( ) ; } } }
public class aTest{ @Test public void testSelectStatementForArrayTypes ( ) { java . sql . Connection conn = java . sql . DriverManager . getConnection ( getUrl ( ) , org . apache . phoenix . util . PropertiesUtil . deepCopy ( TestUtil . TEST_PROPERTIES ) ) ; final java . lang . String tableName = "TEST_TABLE" ; try { java . lang . String ddl = ( "CREATE<sp>TABLE<sp>" + tableName ) + "<sp>(ID<sp>BIGINT<sp>NOT<sp>NULL<sp>PRIMARY<sp>KEY,<sp>VCARRAY<sp>VARCHAR[])\n" ; conn . createStatement ( ) . execute ( ddl ) ; final org . apache . hadoop . conf . Configuration configuration = new org . apache . hadoop . conf . Configuration ( ) ; configuration . set ( HConstants . ZOOKEEPER_QUORUM , getUrl ( ) ) ; org . apache . phoenix . mapreduce . util . PhoenixConfigurationUtil . setSelectColumnNames ( configuration , new java . lang . String [ ] { "ID" , "VCARRAY" } ) ; org . apache . phoenix . mapreduce . util . PhoenixConfigurationUtil . setSchemaType ( configuration , SchemaType . QUERY ) ; org . apache . phoenix . mapreduce . util . PhoenixConfigurationUtil . setInputTableName ( configuration , tableName ) ; final java . lang . String selectStatement = org . apache . phoenix . mapreduce . util . PhoenixConfigurationUtil . getSelectStatement ( configuration ) ; final java . lang . String expectedSelectStatement = "SELECT<sp>\"ID\"<sp>,<sp>\"0\".\"VCARRAY\"<sp>FROM<sp>" + tableName ; org . junit . Assert . assertEquals ( expectedSelectStatement , selectStatement ) ; } }
public class aTest{ @Test public void testConvertBasic ( ) { java . util . List < org . apache . commons . lang3 . tuple . Pair < java . lang . Integer , java . util . List < java . lang . String > > > fixedValues = new java . util . ArrayList ( ) ; fixedValues . add ( org . apache . commons . lang3 . tuple . Pair . of ( 1 , listOf ( "0/0" , "S5" 2 ) ) ) ; fixedValues . add ( org . apache . commons . lang3 . tuple . Pair . of ( 3 , listOf ( "0/1" , "S5" 2 ) ) ) ; org . opencb . biodata . models . variant . StudyEntry s = converter . convert ( fixedValues , java . util . Collections . emptyList ( ) , new org . opencb . biodata . models . variant . Variant ( "S5" 4 ) , 1 ) ; org . opencb . biodata . models . variant . StudyEntry expected = new org . opencb . biodata . models . variant . StudyEntry ( "1" , java . util . Collections . emptyList ( ) , listOf ( "GT" , "S5" 0 ) ) . addSampleData ( "S5" 3 , listOf ( "0/0" , "S5" 2 ) ) . addSampleData ( "S2" , listOf ( "?/?" , "S5" 1 ) ) . addSampleData ( "S3" , listOf ( "0/1" , "S5" 2 ) ) . addSampleData ( "S4" , listOf ( "?/?" , "S5" 1 ) ) . addSampleData ( "S5" , listOf ( "?/?" , "S5" 1 ) ) . addSampleData ( "S6" , listOf ( "?/?" , "S5" 1 ) ) ; org . junit . Assert . assertEquals ( s . toString ( ) , expected , s ) ; } }
public class aTest{ @Test public void testZoneOffsetDeserFromEmpty ( ) { org . junit . Assert . assertNull ( MAPPER . readValue ( quote ( "<sp>" ) , java . time . ZoneOffset . class ) ) ; try { MAPPER . readerFor ( java . time . ZoneOffset . class ) . without ( ALLOW_COERCION_OF_SCALARS ) . readValue ( quote ( "<sp>" ) ) ; org . junit . Assert . fail ( "Should<sp>not<sp>pass" ) ; } }
public class aTest{ @Test public void testValidSubjectWithAuthInfo ( ) { org . apache . jackrabbit . oak . api . AuthInfo info = new org . apache . jackrabbit . oak . spi . security . authentication . AuthInfoImpl ( "testUserId" , java . util . Collections . < java . lang . String , java . lang . Object > emptyMap ( ) , java . util . Collections . < java . security . Principal > emptySet ( ) ) ; java . util . Set < org . apache . jackrabbit . oak . api . AuthInfo > publicCreds = java . util . Collections . singleton ( info ) ; final javax . security . auth . Subject subject = new javax . security . auth . Subject ( false , java . util . Collections . singleton ( new org . apache . jackrabbit . oak . security . authentication . PreAuthTest . TestPrincipal ( ) ) , publicCreds , java . util . Collections . < java . lang . Object > emptySet ( ) ) ; org . apache . jackrabbit . oak . api . ContentSession cs = javax . security . auth . Subject . doAsPrivileged ( subject , new java . security . PrivilegedAction < org . apache . jackrabbit . oak . api . ContentSession > ( ) { @ org . apache . jackrabbit . oak . security . authentication . Override public org . apache . jackrabbit . oak . api . ContentSession run ( ) { try { return login ( null ) ; } catch ( java . lang . Exception e ) { return null ; } } } , null ) ; try { org . junit . Assert . assertSame ( info , cs . getAuthInfo ( ) ) ; } }
public class aTest{ @Test public void testSolutionRightSide ( ) { org . apache . commons . math3 . analysis . UnivariateFunction f = new org . apache . commons . math3 . analysis . function . Sin ( ) ; org . apache . commons . math3 . analysis . solvers . UnivariateSolver solver = getSolver ( ) ; double left = - 1.5 ; double right = 0.05 ; for ( int i = 0 ; i < 10 ; i ++ ) { double solution = getSolution ( solver , 100 , f , left , right , AllowedSolution . RIGHT_SIDE ) ; if ( ! ( java . lang . Double . isNaN ( solution ) ) ) { org . junit . Assert . assertTrue ( ( solution >= 0.0 ) ) ; } }
public class aTest{ @Test public void testOnConnectionChangedAddedPreWithAggTypeError ( ) { org . o3project . odenos . core . manager . system . ComponentConnection prev = new org . o3project . odenos . core . manager . system . ComponentConnectionLogicAndNetwork ( "obj_id" , "original" , "initializing" , "logic_id" , "add" 2 ) ; org . o3project . odenos . core . manager . system . ComponentConnection curr = org . powermock . api . mockito . PowerMockito . spy ( new org . o3project . odenos . core . manager . system . ComponentConnectionLogicAndNetwork ( "obj_id" , "add" 0 , "running" , "logic_id" , "add" 2 ) ) ; when ( curr . getObjectType ( ) ) . thenReturn ( "add" 1 ) ; when ( curr . getConnectionType ( ) ) . thenReturn ( "AGGREGATED" ) ; when ( target . getObjectId ( ) ) . thenReturn ( "ComponentConnectionLogicAndNetwork.LOGIC_ID" ) ; org . o3project . odenos . core . manager . system . event . ComponentConnectionChanged msg = new org . o3project . odenos . core . manager . system . event . ComponentConnectionChanged ( "add" , prev , curr ) ; org . o3project . odenos . core . component . ConversionTable conversionTable = org . powermock . api . mockito . PowerMockito . spy ( new org . o3project . odenos . core . component . ConversionTable ( ) ) ; org . powermock . api . mockito . PowerMockito . doReturn ( true ) . when ( conversionTable , "isConnectionType" , "AGGREGATED" ) ; org . powermock . api . mockito . PowerMockito . doReturn ( conversionTable ) . when ( target , "conversionTable" ) ; org . junit . Assert . assertThat ( target . onConnectionChangedAddedPre ( msg ) , org . hamcrest . CoreMatchers . is ( false ) ) ; } }
public class aTest{ @Test public void testConnectToWrongTransaction ( ) { try { byte [ ] blockID = org . apache . commons . codec . binary . Hex . decodeHex ( "000000000000307b75c9b213f61b2a0c429a34b41b628daae9774cb9b5ff1059" . toCharArray ( ) ) ; byte [ ] transactionID = org . apache . commons . codec . binary . Hex . decodeHex ( "b6b55f7b4d004a788c751f3f8fc881f96c7b647ae06eb9a720bddc924e6f9147" . toCharArray ( ) ) ; org . provebit . proof . TransactionMerkleSerializer tm = new org . provebit . proof . TransactionMerkleSerializer ( ) ; byte [ ] serial = tm . SerializedPathUpMerkle ( transactionID , blockID ) ; org . junit . Assert . assertTrue ( ( serial == null ) ) ; } }
public class aTest{ @Test public void testInvalidJcrPaths ( ) { java . lang . String [ ] paths = new java . lang . String [ ] { "//" , "/foo//" , "/..//" , "/.." , "/foo/../.." , "/..//" 0 , "foo:bar:baz" , "foo:bar]baz" , "foo:bar[baz" , "foo:bar|baz" , "foo:bar*baz" } ; org . apache . jackrabbit . oak . namepath . NamePathMapper [ ] mappers = new org . apache . jackrabbit . oak . namepath . NamePathMapper [ ] { npMapper , new org . apache . jackrabbit . oak . namepath . impl . NamePathMapperImpl ( new org . apache . jackrabbit . oak . namepath . impl . LocalNameMapper ( org . apache . jackrabbit . oak . namepath . impl . NamePathMapperImplTest . GLOBAL , java . util . Collections . < java . lang . String , java . lang . String > emptyMap ( ) ) ) } ; for ( org . apache . jackrabbit . oak . namepath . NamePathMapper mapper : mappers ) { for ( java . lang . String path : paths ) { org . junit . Assert . assertNull ( path , mapper . getOakPath ( path ) ) ; } } } }
public class aTest{ @Test public void testHandleInError ( ) { org . mockito . Mockito . when ( invocation . getMicroserviceName ( ) ) . thenReturn ( "testHandleInError" ) ; org . mockito . Mockito . when ( invocation . getOperationMeta ( ) ) . thenReturn ( org . mockito . Mockito . mock ( org . apache . servicecomb . core . definition . OperationMeta . class ) ) ; org . mockito . Mockito . when ( invocation . getOperationMeta ( ) . getMicroserviceQualifiedName ( ) ) . thenReturn ( "testHandleInError" ) ; org . apache . servicecomb . bizkeeper . FallbackPolicy policy = org . mockito . Mockito . mock ( org . apache . servicecomb . bizkeeper . FallbackPolicy . class ) ; org . mockito . Mockito . when ( policy . name ( ) ) . thenReturn ( "throwException" ) ; org . mockito . Mockito . when ( policy . getFallbackResponse ( org . mockito . Mockito . any ( org . apache . servicecomb . core . Invocation . class ) ) ) . thenThrow ( new java . lang . RuntimeException ( ) ) ; org . apache . servicecomb . bizkeeper . FallbackPolicyManager . addPolicy ( policy ) ; java . lang . System . setProperty ( "servicecomb.fallbackpolicy.groupname.testHandleInError.policy" , "throwException" ) ; org . mockito . Mockito . doAnswer ( new org . mockito . stubbing . Answer < java . lang . Void > ( ) { @ org . apache . servicecomb . bizkeeper . Override public org . apache . servicecomb . bizkeeper . Void answer ( org . mockito . invocation . InvocationOnMock invocation ) { org . apache . servicecomb . swagger . invocation . AsyncResponse asyncRsp = invocation . getArgumentAt ( 0 , org . apache . servicecomb . swagger . invocation . AsyncResponse . class ) ; asyncRsp . fail ( InvocationType . CONSUMER , new java . lang . Exception ( "testHandleInError" ) ) ; return null ; } } ) . when ( invocation ) . next ( org . mockito . Mockito . any ( org . apache . servicecomb . swagger . invocation . AsyncResponse . class ) ) ; bizkeeperHandler . handle ( invocation , ( f ) -> { org . junit . Assert . assertTrue ( f . isFailed ( ) ) ; } }
public class aTest{ @Test public void testMutationsLine10 ( ) { java . lang . reflect . Method m2 = clazz . getMethod ( "m2" , double . class , double . class ) ; java . util . List < de . unisb . cs . st . javalanche . mutation . results . Mutation > mutations = de . unisb . cs . st . javalanche . mutation . results . persistence . QueryManager . getMutations ( className , MutationType . ARITHMETIC_REPLACE , 10 ) ; org . junit . Assert . assertEquals ( 6 , mutations . size ( ) ) ; for ( de . unisb . cs . st . javalanche . mutation . results . Mutation mutation : mutations ) { int addInfo = java . lang . Integer . parseInt ( mutation . getOperatorAddInfo ( ) ) ; if ( addInfo == ( org . objectweb . asm . Opcodes . DADD ) ) { java . lang . Double [ ] input = new java . lang . Double [ ] { 2.0 , 2.0 } ; checkUnmutated ( input , 0.0 , m2 , clazz ) ; checkMutation ( mutation , input , 4.0 , m2 , clazz ) ; } }
public class aTest{ @Test public void testEmptyIterableSideInput ( ) { final org . apache . beam . sdk . values . PCollectionView < java . lang . Iterable < java . lang . Integer > > view = pipeline . apply ( "CreateEmptyView" , org . apache . beam . sdk . transforms . Create . empty ( org . apache . beam . sdk . coders . VarIntCoder . of ( ) ) ) . apply ( org . apache . beam . sdk . transforms . View . asIterable ( ) ) ; org . apache . beam . sdk . values . PCollection < java . lang . Integer > results = pipeline . apply ( "Create1" , org . apache . beam . sdk . transforms . Create . of ( 1 ) ) . apply ( "OutputSideInputs" , org . apache . beam . sdk . transforms . ParDo . of ( new org . apache . beam . sdk . transforms . DoFn < java . lang . Integer , java . lang . Integer > ( ) { @ org . apache . beam . sdk . transforms . ProcessElement public void processElement ( org . apache . beam . sdk . transforms . ProcessContext c ) { org . junit . Assert . assertFalse ( c . sideInput ( view ) . iterator ( ) . hasNext ( ) ) ; c . output ( 1 ) ; } } }
public class aTest{ @Test public void testReductionGradientsSimple ( ) { org . nd4j . linalg . factory . Nd4j . getRandom ( ) . setSeed ( 12345 ) ; for ( int i = 0 ; i < 12 ; i ++ ) { org . nd4j . autodiff . samediff . SameDiff sd = org . nd4j . autodiff . samediff . SameDiff . create ( ) ; boolean skipBackward = false ; int nOut = 4 ; int minibatch = 10 ; org . nd4j . autodiff . samediff . SDVariable input = sd . var ( "<sp>-<sp>" 5 , new int [ ] { - 1 , nOut } ) ; org . nd4j . autodiff . samediff . SDVariable loss ; java . lang . String name ; switch ( i ) { case 0 : loss = sd . mean ( "loss" , input ) ; name = "<sp>-<sp>" 0 ; break ; case 1 : loss = sd . sum ( "loss" , input ) ; name = "sum" ; break ; case 2 : loss = sd . standardDeviation ( "loss" , input , true ) ; name = "<sp>-<sp>" 2 ; break ; case 3 : loss = sd . min ( "loss" , input ) ; name = "min" ; break ; case 4 : loss = sd . max ( "loss" , input ) ; name = "max" ; break ; case 5 : loss = sd . variance ( "loss" , input , true ) ; name = "variance" ; break ; case 6 : loss = sd . prod ( "loss" , input ) ; name = "<sp>-<sp>" 3 ; break ; case 7 : loss = sd . norm1 ( "loss" , input ) ; name = "<sp>-<sp>" 4 ; break ; case 8 : loss = sd . norm2 ( "loss" , input ) ; name = "norm2" ; break ; case 9 : loss = sd . normmax ( "loss" , input ) ; name = "normmax" ; break ; case 10 : loss = sd . countNonZero ( "loss" , input ) ; name = "<sp>-<sp>" 1 ; skipBackward = true ; break ; case 11 : loss = sd . countZero ( "loss" , input ) ; name = "countZero" ; skipBackward = true ; break ; default : throw new java . lang . RuntimeException ( ) ; } java . lang . String msg = ( ( "test:<sp>" + i ) + "<sp>-<sp>" ) + name ; log . info ( ( "<sp>-<sp>" 6 + msg ) ) ; org . nd4j . linalg . api . ndarray . INDArray inputArr = org . nd4j . linalg . factory . Nd4j . randn ( minibatch , nOut ) . muli ( 100 ) ; sd . associateArrayWithVariable ( inputArr , input ) ; if ( ! skipBackward ) { boolean ok = org . nd4j . autodiff . gradcheck . GradCheckUtil . checkGradients ( sd ) ; org . junit . Assert . assertTrue ( msg , ok ) ; } } } }
public class aTest{ @Test public void simpleParseWithTail ( ) { ws . prova . kernel2 . ProvaKnowledgeBase kb = new ws . prova . reference2 . ProvaKnowledgeBaseImpl ( ) ; ws . prova . kernel2 . ProvaResultSet resultSet = new ws . prova . reference2 . ProvaResultSetImpl ( ) ; java . io . StringReader sr = new java . io . StringReader ( ( ":-solve(a([X,Y|Z])).\n" + ( ( ( ( ( "a([X,Y]):-b(X),!,d(Y).\n" + "b(X):-c(X).\n" ) + "c(1).\n" ) + "c(2).\n" ) + "d(3).\n" ) + "d(4)." ) ) ) ; java . io . BufferedReader in = new java . io . BufferedReader ( sr ) ; ws . prova . parser2 . ProvaParserImpl parser = new ws . prova . parser2 . ProvaParserImpl ( "inline1" , new java . lang . Object [ ] { } ) ; try { java . util . List < ws . prova . kernel2 . ProvaRule > rules = parser . parse ( kb , resultSet , in ) ; for ( ws . prova . kernel2 . ProvaRule rule : rules ) { if ( ( rule . getHead ( ) ) == null ) { ws . prova . kernel2 . ProvaResolutionInferenceEngine engine = new ws . prova . reference2 . ProvaResolutionInferenceEngineImpl ( kb , rule ) ; engine . run ( ) ; org . junit . Assert . assertEquals ( resultSet . getSolutions ( ) . size ( ) , 2 ) ; } } } }
public class aTest{ @Test public void getProcessDefinitionIdFromActivityInstanceId ( ) { final org . bonitasoft . engine . bpm . process . DesignProcessDefinition designProcessDefinition = org . bonitasoft . engine . test . BuildTestUtil . buildProcessDefinitionWithHumanAndAutomaticSteps ( java . util . Arrays . asList ( "step1" , "step2" ) , java . util . Arrays . asList ( true , true ) ) ; final org . bonitasoft . engine . bpm . process . ProcessDefinition processDefinition = deployAndEnableProcessWithActor ( designProcessDefinition , org . bonitasoft . engine . process . ACTOR_NAME , user ) ; final org . bonitasoft . engine . bpm . process . ProcessInstance pi0 = getProcessAPI ( ) . startProcess ( processDefinition . getId ( ) ) ; final java . util . List < org . bonitasoft . engine . bpm . flownode . ActivityInstance > activityInstances = getProcessAPI ( ) . getActivities ( pi0 . getId ( ) , 0 , 10 ) ; for ( final org . bonitasoft . engine . bpm . flownode . ActivityInstance activityInstance : activityInstances ) { final long processDefinitionId = getProcessAPI ( ) . getProcessDefinitionIdFromActivityInstanceId ( activityInstance . getId ( ) ) ; org . junit . Assert . assertEquals ( processDefinition . getId ( ) , processDefinitionId ) ; } }
public class aTest{ @Test public void testStreamWithoutMaxResult ( ) { java . util . List < com . vladmihalcea . book . hpjp . hibernate . fetching . PostgreSQLScrollableResultsStreamingTest . Post > posts = doInJPA ( ( entityManager ) -> { try ( Stream < com . vladmihalcea . book . hpjp . hibernate . fetching . Post > postStream = entityManager . createQuery ( ( "select<sp>p<sp>" + ( "from<sp>Post<sp>p<sp>" + "order<sp>by<sp>p.createdOn<sp>desc" ) ) , . class ) . unwrap ( . class ) . stream ( ) ) { return postStream . limit ( 50 ) . collect ( java . util . stream . Collectors . toList ( ) ) ; } } ) ; org . junit . Assert . assertEquals ( 50 , posts . size ( ) ) ; } }
public class aTest{ @Test public void makeSureAllImportsArePresent ( ) { final com . speedment . common . codegen . model . Import [ ] expected = new com . speedment . common . codegen . model . Import [ ] { com . speedment . common . codegen . model . Import . of ( java . util . Objects . class ) , com . speedment . common . codegen . model . Import . of ( java . util . Objects . class ) . static_ ( ) . setStaticMember ( "requireNonNull" ) , com . speedment . common . codegen . model . Import . of ( java . util . Objects . class ) . static_ ( ) . setStaticMember ( "equals" ) } ; final com . speedment . common . codegen . model . Import [ ] actual = file . getImports ( ) . stream ( ) . toArray ( com . speedment . common . codegen . model . Import [ ] :: new ) ; org . junit . Assert . assertArrayEquals ( expected , actual ) ; } }
public class aTest{ @Test public void doit ( ) { java . io . File base = new java . io . File ( ".." ) ; com . predic8 . membrane . examples . env . ConsistentVersionNumbers . validateBase ( base ) ; com . predic8 . membrane . examples . env . ConsistentVersionNumbers . handler = new com . predic8 . membrane . examples . env . ConsistentVersionNumbers . Handler ( ) { @ com . predic8 . membrane . examples . env . Override public java . lang . String handle ( java . io . File file , java . lang . String old ) { if ( ( version ) == null ) version = old ; else { try { org . junit . Assert . assertEquals ( version , old ) ; } }
public class aTest{ @Test public void testTruncDivFractional ( ) { java . util . Map < java . lang . String , java . lang . Object > context = com . google . common . collect . Maps . newHashMap ( ) ; context . put ( "dividend" , 5.0 ) ; context . put ( "divisor" , 2 ) ; context . put ( "negativeDividend" , ( - 5.0 ) ) ; context . put ( "{%<sp>set<sp>x<sp>=<sp>5.0<sp>//<sp>divisor<sp>%}{{x}}" 1 , ( - 2 ) ) ; java . lang . String [ ] [ ] testCases = new java . lang . String [ ] [ ] { new java . lang . String [ ] { "{%<sp>set<sp>x<sp>=<sp>5.0<sp>//<sp>divisor<sp>%}{{x}}" 0 , "2.0" } , new java . lang . String [ ] { "{%<sp>set<sp>x<sp>=<sp>5.0<sp>//<sp>2<sp>%}{{x}}" , "2.0" } , new java . lang . String [ ] { "{%<sp>set<sp>x<sp>=<sp>dividend<sp>//<sp>2<sp>%}{{x}}" , "2.0" } , new java . lang . String [ ] { "{%<sp>set<sp>x<sp>=<sp>5.0<sp>//<sp>divisor<sp>%}{{x}}" , "2.0" } , new java . lang . String [ ] { "{%<sp>set<sp>x<sp>=<sp>5.0<sp>//<sp>divisor<sp>%}{{x}}" 2 , "-3.0" } , new java . lang . String [ ] { "{%<sp>set<sp>x<sp>=<sp>5.0<sp>//<sp>divisor<sp>%}{{x}}" 5 , "-3.0" } , new java . lang . String [ ] { "{%<sp>set<sp>x<sp>=<sp>dividend<sp>//<sp>negativeDivisor<sp>%}{{x}}" , "-3.0" } , new java . lang . String [ ] { "{%<sp>set<sp>x<sp>=<sp>5.0<sp>//<sp>-2<sp>%}{{x}}" , "-3.0" } , new java . lang . String [ ] { "{%<sp>set<sp>x<sp>=<sp>5.0<sp>//<sp>divisor<sp>%}{{x}}" 3 , "2.0" } , new java . lang . String [ ] { "{%<sp>set<sp>x<sp>=<sp>5.0<sp>//<sp>divisor<sp>%}{{x}}" 4 , "2.0" } } ; for ( java . lang . String [ ] testCase : testCases ) { java . lang . String template = testCase [ 0 ] ; java . lang . String expected = testCase [ 1 ] ; java . lang . String rendered = jinja . render ( template , context ) ; org . junit . Assert . assertEquals ( expected , rendered ) ; } } }
public class aTest{ @Test public void sendMessageAndGetDeliveryHashSendFails ( ) { tests . unit . com . microsoft . azure . sdk . iot . device . transport . amqps . AmqpsDeviceOperations amqpsDeviceOperations = mockit . Deencapsulation . newInstance ( tests . unit . com . microsoft . azure . sdk . iot . device . transport . amqps . AmqpsDeviceOperations . class , mockDeviceClientConfig , "" , "" , "" , "" , "" , "" ) ; final byte [ ] msgData = new byte [ 1 ] ; final int offset = 0 ; final int length = 1 ; final byte [ ] deliveryTag = new byte [ 1 ] ; mockit . Deencapsulation . setField ( amqpsDeviceOperations , "senderLink" , mockSender ) ; new mockit . NonStrictExpectations ( ) { { mockSender . delivery ( deliveryTag ) ; result = mockDelivery ; mockSender . send ( msgData , offset , length ) ; result = new java . lang . Exception ( ) ; } } ; tests . unit . com . microsoft . azure . sdk . iot . device . transport . amqps . AmqpsSendReturnValue amqpsSendReturnValue = mockit . Deencapsulation . invoke ( amqpsDeviceOperations , "sendMessageAndGetDeliveryHash" , MessageType . DEVICE_TELEMETRY , msgData , offset , length , deliveryTag ) ; int deliveryHash = mockit . Deencapsulation . invoke ( amqpsSendReturnValue , "getDeliveryHash" ) ; org . junit . Assert . assertTrue ( ( deliveryHash == ( - 1 ) ) ) ; new mockit . Verifications ( ) { { mockSender . advance ( ) ; times = 1 ; mockDelivery . free ( ) ; times = 1 ; } } }
public class aTest{ @Test public void captureClassWhichImplementsCapturedBaseInterfaceAndExtendsUnrelatedBase ( mockit . CapturingImplementationsTest$Interface2 ) { int i = new mockit . CapturingImplementationsTest . ClassImplementingSubInterfaceAndExtendingUnrelatedBase ( ) . doSomething ( ) ; org . junit . Assert . assertEquals ( 0 , i ) ; } }
public class aTest{ @Test public void testIDnotEnum ( ) { java . lang . String grammar = "lexer<sp>grammar<sp>L;\n" + ( ( "ENUM<sp>:<sp>[a-z]+<sp>{false}?<sp>;\n" + "ID<sp>:<sp>[a-z]+<sp>;\n" ) + "[@1,5:7=\'abc\',<2>,1:5]\n" 1 ) ; java . lang . String found = execLexer ( "L.g4" , grammar , "L" , "[@1,5:7=\'abc\',<2>,1:5]\n" 0 , true ) ; java . lang . String expecting = "[@0,0:3=\'enum\',<2>,1:0]\n" + ( ( ( "[@1,5:7=\'abc\',<2>,1:5]\n" + "[@2,9:12=\'enum\',<2>,1:9]\n" ) + "[@3,13:12=\'<EOF>\',<-1>,1:13]\n" ) + "s0-\'<sp>\'->:s2=>3\n" ) ; org . junit . Assert . assertEquals ( expecting , found ) ; } }
public class aTest{ @Test public void testMultiOutputParDoWithSideInputs ( ) { org . apache . beam . runners . apex . ApexPipelineOptions options = org . apache . beam . sdk . options . PipelineOptionsFactory . create ( ) . as ( org . apache . beam . runners . apex . ApexPipelineOptions . class ) ; options . setRunner ( org . apache . beam . runners . apex . ApexRunner . class ) ; org . apache . beam . sdk . Pipeline pipeline = org . apache . beam . sdk . Pipeline . create ( options ) ; java . util . List < java . lang . Integer > inputs = java . util . Arrays . asList ( 3 , ( - 42 ) , 666 ) ; final org . apache . beam . sdk . values . TupleTag < java . lang . String > mainOutputTag = new org . apache . beam . sdk . values . TupleTag ( "main" ) ; final org . apache . beam . sdk . values . TupleTag < java . lang . Void > additionalOutputTag = new org . apache . beam . sdk . values . TupleTag ( "output" ) ; org . apache . beam . sdk . values . PCollectionView < java . lang . Integer > sideInput1 = pipeline . apply ( "CreateSideInput1" , org . apache . beam . sdk . transforms . Create . of ( 11 ) ) . apply ( "ViewSideInput1" , org . apache . beam . sdk . transforms . View . asSingleton ( ) ) ; org . apache . beam . sdk . values . PCollectionView < java . lang . Integer > sideInputUnread = pipeline . apply ( "processing:<sp>-42:<sp>[11,<sp>222]" 1 , org . apache . beam . sdk . transforms . Create . of ( ( - 3333 ) ) ) . apply ( "ViewSideInputUnread" , org . apache . beam . sdk . transforms . View . asSingleton ( ) ) ; org . apache . beam . sdk . values . PCollectionView < java . lang . Integer > sideInput2 = pipeline . apply ( "processing:<sp>-42:<sp>[11,<sp>222]" 0 , org . apache . beam . sdk . transforms . Create . of ( 222 ) ) . apply ( "ViewSideInput2" , org . apache . beam . sdk . transforms . View . asSingleton ( ) ) ; org . apache . beam . sdk . values . PCollectionTuple outputs = pipeline . apply ( org . apache . beam . sdk . transforms . Create . of ( inputs ) ) . apply ( org . apache . beam . sdk . transforms . ParDo . of ( new org . apache . beam . runners . apex . translation . ParDoTranslatorTest . TestMultiOutputWithSideInputsFn ( java . util . Arrays . asList ( sideInput1 , sideInput2 ) , java . util . Arrays . asList ( ) ) ) . withSideInputs ( sideInput1 ) . withSideInputs ( sideInputUnread ) . withSideInputs ( sideInput2 ) . withOutputTags ( mainOutputTag , org . apache . beam . sdk . values . TupleTagList . of ( additionalOutputTag ) ) ) ; outputs . get ( mainOutputTag ) . apply ( org . apache . beam . sdk . transforms . ParDo . of ( new org . apache . beam . runners . apex . translation . ParDoTranslatorTest . EmbeddedCollector ( ) ) ) ; outputs . get ( additionalOutputTag ) . setCoder ( org . apache . beam . sdk . coders . VoidCoder . of ( ) ) ; org . apache . beam . runners . apex . ApexRunnerResult result = ( ( org . apache . beam . runners . apex . ApexRunnerResult ) ( pipeline . run ( ) ) ) ; java . util . HashSet < java . lang . String > expected = org . apache . beam . vendor . guava . v20_0 . com . google . common . collect . Sets . newHashSet ( "processing:<sp>3:<sp>[11,<sp>222]" , "processing:<sp>-42:<sp>[11,<sp>222]" , "processing:<sp>666:<sp>[11,<sp>222]" ) ; long timeout = ( java . lang . System . currentTimeMillis ( ) ) + ( org . apache . beam . runners . apex . translation . ParDoTranslatorTest . TIMEOUT_MILLIS ) ; while ( ( java . lang . System . currentTimeMillis ( ) ) < timeout ) { if ( org . apache . beam . runners . apex . translation . ParDoTranslatorTest . EmbeddedCollector . RESULTS . containsAll ( expected ) ) { break ; } org . apache . beam . runners . apex . translation . ParDoTranslatorTest . LOG . info ( "Waiting<sp>for<sp>expected<sp>results." ) ; java . lang . Thread . sleep ( org . apache . beam . runners . apex . translation . ParDoTranslatorTest . SLEEP_MILLIS ) ; } result . cancel ( ) ; org . junit . Assert . assertEquals ( org . apache . beam . vendor . guava . v20_0 . com . google . common . collect . Sets . newHashSet ( expected ) , org . apache . beam . runners . apex . translation . ParDoTranslatorTest . EmbeddedCollector . RESULTS ) ; } }
public class aTest{ @Test public void testSAML2SubjectLocalityOutbound ( ) { java . io . ByteArrayOutputStream baos = new java . io . ByteArrayOutputStream ( ) ; { org . apache . wss4j . stax . ext . WSSSecurityProperties securityProperties = new org . apache . wss4j . stax . ext . WSSSecurityProperties ( ) ; java . util . List < org . apache . wss4j . stax . ext . WSSConstants . Action > actions = new java . util . ArrayList ( ) ; actions . add ( WSSConstants . SAML_TOKEN_UNSIGNED ) ; securityProperties . setActions ( actions ) ; org . apache . wss4j . stax . test . saml . SAMLCallbackHandlerImpl callbackHandler = new org . apache . wss4j . stax . test . saml . SAMLCallbackHandlerImpl ( ) ; callbackHandler . setStatement ( SAMLCallbackHandlerImpl . Statement . AUTHN ) ; callbackHandler . setIssuer ( "www.example.com" ) ; callbackHandler . setSignAssertion ( false ) ; callbackHandler . setSamlVersion ( Version . SAML_20 ) ; callbackHandler . setSubjectLocality ( org . apache . wss4j . stax . test . saml . SAMLTokenTest . IP_ADDRESS , "test-dns" ) ; securityProperties . setSamlCallbackHandler ( callbackHandler ) ; org . apache . wss4j . stax . setup . OutboundWSSec wsSecOut = org . apache . wss4j . stax . setup . WSSec . getOutboundWSSec ( securityProperties ) ; javax . xml . stream . XMLStreamWriter xmlStreamWriter = wsSecOut . processOutMessage ( baos , StandardCharsets . UTF_8 . name ( ) , new java . util . ArrayList < org . apache . xml . security . stax . securityEvent . SecurityEvent > ( ) ) ; javax . xml . stream . XMLStreamReader xmlStreamReader = xmlInputFactory . createXMLStreamReader ( this . getClass ( ) . getClassLoader ( ) . getResourceAsStream ( "testdata/plain-soap-1.1.xml" ) ) ; org . apache . wss4j . stax . test . utils . XmlReaderToWriter . writeAll ( xmlStreamReader , xmlStreamWriter ) ; xmlStreamWriter . close ( ) ; org . w3c . dom . Document document = documentBuilderFactory . newDocumentBuilder ( ) . parse ( new java . io . ByteArrayInputStream ( baos . toByteArray ( ) ) ) ; org . w3c . dom . NodeList nodeList = document . getElementsByTagNameNS ( WSSConstants . TAG_dsig_Signature . getNamespaceURI ( ) , WSSConstants . TAG_dsig_Signature . getLocalPart ( ) ) ; org . junit . Assert . assertEquals ( nodeList . getLength ( ) , 0 ) ; } }
public class aTest{ @Test public void checkPersistence ( ) { org . eclipse . ice . datastructures . ICEObject . ICEJAXBHandler xmlHandler = new org . eclipse . ice . datastructures . ICEObject . ICEJAXBHandler ( ) ; java . util . ArrayList < java . lang . Class > classList = new java . util . ArrayList < java . lang . Class > ( ) ; classList . add ( org . eclipse . ice . datastructures . form . Material . class ) ; classList . add ( org . eclipse . ice . datastructures . form . MaterialStack . class ) ; org . eclipse . ice . datastructures . ICEObject . ICEJAXBHandler jaxbHandler = new org . eclipse . ice . datastructures . ICEObject . ICEJAXBHandler ( ) ; org . eclipse . ice . datastructures . form . Material material = org . eclipse . ice . tests . datastructures . TestMaterialFactory . createCO2 ( ) ; org . eclipse . ice . datastructures . form . MaterialStack stack = new org . eclipse . ice . datastructures . form . MaterialStack ( ) ; stack . setMaterial ( material ) ; try { java . io . ByteArrayOutputStream outputStream = new java . io . ByteArrayOutputStream ( ) ; jaxbHandler . write ( stack , classList , outputStream ) ; java . io . ByteArrayInputStream inputStream = new java . io . ByteArrayInputStream ( outputStream . toByteArray ( ) ) ; org . eclipse . ice . datastructures . form . MaterialStack readMaterialStack = ( ( org . eclipse . ice . datastructures . form . MaterialStack ) ( jaxbHandler . read ( classList , inputStream ) ) ) ; org . junit . Assert . assertTrue ( readMaterialStack . equals ( stack ) ) ; } }
public class aTest{ @Test public void Select_with_Bindings_no_Match ( ) { com . mysema . rdfbean . model . SPARQLQuery query = session . createQuery ( QueryLanguage . SPARQL , "SELECT<sp>?s<sp>?p<sp>?o<sp>WHERE<sp>{?s<sp>?p<sp>?o}<sp>LIMIT<sp>10" ) ; query . setBinding ( "p" , new com . mysema . rdfbean . model . UID ( com . mysema . rdfbean . TEST . NS , ( "p" + ( java . lang . System . currentTimeMillis ( ) ) ) ) ) ; com . mysema . commons . lang . CloseableIterator < java . util . Map < java . lang . String , com . mysema . rdfbean . model . NODE > > rows = query . getTuples ( ) ; try { org . junit . Assert . assertFalse ( rows . hasNext ( ) ) ; } }
public class aTest{ @Test public void testMixedStrategyWithOneTable ( ) { createOneTable ( "SUS1" ) ; setStrategy ( org . apache . cayenne . access . dbsync . ThrowOnPartialOrCreateSchemaStrategy . class ) ; java . lang . String template = "SELECT<sp>#result('ARTIST_ID'<sp>'int')<sp>FROM<sp>ARTIST<sp>ORDER<sp>BY<sp>ARTIST_ID" ; org . apache . cayenne . query . SQLTemplate query = new org . apache . cayenne . query . SQLTemplate ( java . lang . Object . class , template ) ; try { node . performQueries ( java . util . Collections . singletonList ( query ) , mock ( org . apache . cayenne . access . OperationObserver . class ) ) ; org . junit . Assert . assertEquals ( 1 , existingTables ( ) . size ( ) ) ; org . junit . Assert . fail ( "Must<sp>have<sp>thrown<sp>on<sp>partial<sp>schema" ) ; } }
public class aTest{ @Test public void testSamplesFile ( ) { java . io . File samplesOutFile = folder . newFile ( "samples-testSamplesFile.out" ) ; samplesOutFile . delete ( ) ; java . lang . String [ ] args = new java . lang . String [ ] { "-c" 2 , "LOWE_KEYPOINT_ASCII" , "-k" , "-c" 0 , "-d" , "1" , "-s" , "5" , "-sf" , samplesOutFile . getAbsolutePath ( ) , keyFiles [ 0 ] , keyFiles [ 1 ] , keyFiles [ 2 ] } ; org . openimaj . tools . clusterquantiser . ClusterQuantiserOptions options = new org . openimaj . tools . clusterquantiser . ClusterQuantiserOptions ( args ) ; options . prepare ( ) ; byte [ ] [ ] samples = org . openimaj . tools . clusterquantiser . ClusterQuantiser . do_getSamples ( options ) ; args = new java . lang . String [ ] { "-c" 1 , "HKMEANS" , "-k" , "-c" 0 , "-d" , "1" , "-c" , folder . newFile ( "-c" 3 ) . getAbsolutePath ( ) , "-sf" , samplesOutFile . getAbsolutePath ( ) } ; options = new org . openimaj . tools . clusterquantiser . ClusterQuantiserOptions ( args ) ; options . prepare ( ) ; options . loadSamplesFile ( ) ; byte [ ] [ ] gotSamples = options . getSampleKeypoints ( ) ; for ( int i = 0 ; i < ( samples . length ) ; i ++ ) { for ( int j = 0 ; j < ( samples [ i ] . length ) ; j ++ ) { org . junit . Assert . assertTrue ( ( ( samples [ i ] [ j ] ) == ( gotSamples [ i ] [ j ] ) ) ) ; } } } }
public class aTest{ @Test public void testDynamicDeleteEntity ( ) { org . apache . ibatis . session . SqlSession sqlSession = tk . mybatis . mapper . mapper . MybatisHelper . getSqlSession ( ) ; try { tk . mybatis . mapper . mapper . CountryMapper mapper = sqlSession . getMapper ( tk . mybatis . mapper . mapper . CountryMapper . class ) ; tk . mybatis . mapper . model . Country country = new tk . mybatis . mapper . model . Country ( ) ; country . setId ( 100 ) ; org . junit . Assert . assertEquals ( 1 , mapper . deleteByPrimaryKey ( country ) ) ; } }
public class aTest{ @Test public void test_diskVmulti ( ) { org . terrier . utility . ApplicationSetup . setProperty ( "termpipelines" , "" ) ; org . terrier . utility . ApplicationSetup . setProperty ( "five" 1 , "filename" ) ; org . terrier . utility . ApplicationSetup . setProperty ( "five" 3 , "100" ) ; org . terrier . utility . ApplicationSetup . setProperty ( "indexer.meta.reverse.keys" , "filename" ) ; org . terrier . structures . Index disk = org . terrier . indexing . IndexTestUtils . makeIndex ( new java . lang . String [ ] { "five" 0 , "B" } , new java . lang . String [ ] { "five" 2 , "three<sp>four<sp>five" } ) ; org . terrier . structures . Index disk1 = org . terrier . indexing . IndexTestUtils . makeIndex ( new java . lang . String [ ] { "five" 0 } , new java . lang . String [ ] { "five" 2 } ) ; org . terrier . structures . Index disk2 = org . terrier . indexing . IndexTestUtils . makeIndex ( new java . lang . String [ ] { "B" } , new java . lang . String [ ] { "three<sp>four<sp>five" } ) ; org . terrier . structures . Index multi = new org . terrier . realtime . multi . MultiIndex ( new org . terrier . structures . Index [ ] { disk1 , disk2 } ) ; org . junit . Assert . assertNotNull ( multi ) ; org . terrier . realtime . TestUtils . compareIndices ( disk , multi ) ; org . terrier . realtime . TestUtils . compareProperties ( disk , multi ) ; org . terrier . realtime . TestUtils . compareRetrieval ( "one" , disk , multi ) ; org . terrier . realtime . TestUtils . compareRetrieval ( "three" , disk , multi ) ; org . terrier . realtime . TestUtils . compareRetrieval ( "five" , disk , multi ) ; org . terrier . realtime . TestUtils . checkContents ( disk , "one" , 1 , new int [ ] { 0 } , new int [ ] { 1 } , new int [ ] { 3 } ) ; org . terrier . realtime . TestUtils . checkContents ( multi , "one" , 1 , new int [ ] { 0 } , new int [ ] { 1 } , new int [ ] { 3 } ) ; org . terrier . realtime . TestUtils . checkContents ( disk , "three" , 2 , new int [ ] { 0 , 1 } , new int [ ] { 1 , 1 } , new int [ ] { 3 , 3 } ) ; org . terrier . realtime . TestUtils . checkContents ( multi , "three" , 2 , new int [ ] { 0 , 1 } , new int [ ] { 1 , 1 } , new int [ ] { 3 , 3 } ) ; org . terrier . realtime . TestUtils . checkContents ( disk , "five" , 1 , new int [ ] { 1 } , new int [ ] { 1 } , new int [ ] { 3 } ) ; org . terrier . realtime . TestUtils . checkContents ( multi , "five" , 1 , new int [ ] { 1 } , new int [ ] { 1 } , new int [ ] { 3 } ) ; } }
public class aTest{ @Test public void testWsadl ( ) { org . glassfish . tyrus . server . Server server = startServer ( org . glassfish . tyrus . test . e2e . non_deployable . WsadlTest . NoopEndpoint . class ) ; try { boolean found = false ; java . net . URLConnection urlConnection = getURI ( "/application.wsadl" , "http" ) . toURL ( ) . openConnection ( ) ; urlConnection . connect ( ) ; System . out . println ( urlConnection . getContentLength ( ) ) ; java . io . InputStream inputStream = urlConnection . getInputStream ( ) ; java . io . BufferedReader in = new java . io . BufferedReader ( new java . io . InputStreamReader ( inputStream ) ) ; java . lang . String line ; while ( ( line = in . readLine ( ) ) != null ) { if ( line . contains ( org . glassfish . tyrus . test . e2e . non_deployable . WsadlTest . NoopEndpoint . class . getAnnotation ( javax . websocket . server . ServerEndpoint . class ) . value ( ) ) ) { found = true ; } System . out . println ( ( "###<sp>" + line ) ) ; } org . junit . Assert . assertTrue ( found ) ; } }
public class aTest{ @Test public void testExtFun17 ( ) { java . lang . String init = "prefix<sp>ex:<sp><http://example.org/><sp>" + ( ( "mapevery(lambda(?t)<sp>{<sp>?t<sp>=<sp>?t<sp>},<sp>query(construct<sp>where<sp>{?x<sp>?p<sp>?y}))" 1 + "ex:John<sp>rdf:value<sp>1<sp>;<sp>rdfs:label<sp>2" ) + "}" ) ; java . lang . String q = "mapevery(lambda(?t)<sp>{<sp>?t<sp>=<sp>?t<sp>},<sp>query(construct<sp>where<sp>{?x<sp>?p<sp>?y}))" 2 + ( ( ( ( ( ( ( ( "@event<sp>" + "select<sp>(us:foo()<sp>as<sp>?t)<sp>" ) + "mapevery(lambda(?t)<sp>{<sp>?t<sp>=<sp>?t<sp>},<sp>query(construct<sp>where<sp>{?x<sp>?p<sp>?y}))" 0 ) + "function<sp>us:foo()<sp>{" ) + "mapevery(lambda(?t)<sp>{<sp>?t<sp>=<sp>?t<sp>},<sp>query(construct<sp>where<sp>{?x<sp>?p<sp>?y}))" ) + "}" ) + "@type<sp>dt:triple<sp>function<sp>us:eq(?t1,<sp>?t2)<sp>{" ) + "<sp>mapevery(lambda(?x,<sp>?y)<sp>{<sp>xt:print(?x,<sp>?y)<sp>;<sp>?x<sp>=<sp>?y<sp>},<sp>?t1,<sp>?t2)" ) + "}" ) ; fr . inria . corese . core . Graph g = createGraph ( ) ; fr . inria . corese . core . query . QueryProcess exec = fr . inria . corese . core . query . QueryProcess . create ( g ) ; exec . query ( init ) ; fr . inria . corese . kgram . core . Mappings map = exec . query ( q ) ; fr . inria . corese . sparql . api . IDatatype dt = ( ( fr . inria . corese . sparql . api . IDatatype ) ( map . getValue ( "?t" ) ) ) ; org . junit . Assert . assertEquals ( true , dt . booleanValue ( ) ) ; } }
public class aTest{ @Test public void testConfigureEnableLowercaseAlwaysTrue ( ) { System . out . println ( ( ( getTestTraceHead ( "[NGSICartoDBSink.configure]" ) ) + "--------<sp>Independently<sp>of<sp>the<sp>configured<sp>value,<sp>enable_lowercase<sp>is<sp>always<sp>'true'<sp>by<sp>default" ) ) ; java . lang . String apiKey = "1234567890abcdef" ; java . lang . String backendMaxConns = null ; java . lang . String backendMaxConnsPerRoute = null ; java . lang . String batchSize = null ; java . lang . String batchTimeout = null ; java . lang . String batchTTL = null ; java . lang . String dataModel = null ; java . lang . String enableDistanceHistoric = null ; java . lang . String enableGrouping = null ; java . lang . String enableLowercase = "false" ; java . lang . String enableRawHistoric = null ; java . lang . String enableRawSnapshot = null ; java . lang . String swapCoordinates = null ; java . lang . String keysConfFile = "/keys.conf" ; com . telefonica . iot . cygnus . sinks . NGSICartoDBSink sink = new com . telefonica . iot . cygnus . sinks . NGSICartoDBSink ( ) ; sink . configure ( createContext ( apiKey , backendMaxConns , backendMaxConnsPerRoute , batchSize , batchTimeout , batchTTL , dataModel , enableDistanceHistoric , enableGrouping , enableLowercase , enableRawHistoric , enableRawSnapshot , swapCoordinates , keysConfFile ) ) ; try { org . junit . Assert . assertTrue ( sink . enableLowercase ) ; System . out . println ( ( ( getTestTraceHead ( "[NGSICartoDBSink.configure]" ) ) + "-<sp>OK<sp>-<sp>'enable_lowercase=false'<sp>was<sp>configured,<sp>nevertheless<sp>it<sp>is<sp>always<sp>true<sp>by<sp>default" ) ) ; } }
public class aTest{ @Test public void testBroadcastVariableNotFound ( ) { try { org . apache . flink . api . common . functions . util . RuntimeUDFContext ctx = new org . apache . flink . api . common . functions . util . RuntimeUDFContext ( taskInfo , getClass ( ) . getClassLoader ( ) , new org . apache . flink . api . common . ExecutionConfig ( ) , new java . util . HashMap ( ) , new java . util . HashMap ( ) , new org . apache . flink . metrics . groups . UnregisteredMetricsGroup ( ) ) ; org . junit . Assert . assertFalse ( ctx . hasBroadcastVariable ( "some<sp>name" ) ) ; try { ctx . getBroadcastVariable ( "some<sp>name" ) ; org . junit . Assert . fail ( "should<sp>throw<sp>an<sp>exception" ) ; } }
public class aTest{ @Test public void testAddingConnectionFactories ( ) { final java . lang . String poolName = "JMSConnFactory" + ( generateRandomString ( ) ) ; final java . lang . String description = "Test<sp>Pool<sp>-<sp>" + poolName ; org . glassfish . admingui . devtests . StandaloneTest standaloneTest = new org . glassfish . admingui . devtests . StandaloneTest ( ) ; org . glassfish . admingui . devtests . ClusterTest clusterTest = new org . glassfish . admingui . devtests . ClusterTest ( ) ; standaloneTest . deleteAllStandaloneInstances ( ) ; clusterTest . deleteAllCluster ( ) ; clickAndWait ( "form:propertySheet:generalPropertySheet:jndiProp:jndiProp" 6 ) ; clickAndWait ( "propertyForm:resourcesTable:topActionsGroup1:newButton" ) ; setFieldValue ( "form:propertySheet:generalPropertySheet:jndiProp:jndiProp" , poolName ) ; org . openqa . selenium . support . ui . Select select = new org . openqa . selenium . support . ui . Select ( driver . findElement ( org . openqa . selenium . By . id ( "form:propertySheet:generalPropertySheet:resTyped:resType" ) ) ) ; select . selectByVisibleText ( "form:propertySheet:generalPropertySheet:jndiProp:jndiProp" 0 ) ; setFieldValue ( "form:propertySheet:generalPropertySheet:descProp:descProp" , description ) ; org . openqa . selenium . support . ui . Select select1 = new org . openqa . selenium . support . ui . Select ( driver . findElement ( org . openqa . selenium . By . id ( "form:propertySheet:poolPropertySheet:transprop:trans" ) ) ) ; select1 . selectByVisibleText ( "LocalTransaction" ) ; clickAndWait ( "form:propertySheet:generalPropertySheet:jndiProp:jndiProp" 1 ) ; java . lang . String prefix = getTableRowByValue ( "form:propertySheet:generalPropertySheet:jndiProp:jndiProp" 2 , poolName , "colName" ) ; org . junit . Assert . assertEquals ( poolName , getText ( ( prefix + "form:propertySheet:generalPropertySheet:jndiProp:jndiProp" 8 ) ) ) ; java . lang . String selectId = prefix + "colSelect:select" ; clickByIdAction ( selectId ) ; clickAndWait ( "form:propertySheet:generalPropertySheet:jndiProp:jndiProp" 4 ) ; waitforBtnDisable ( "form:propertySheet:generalPropertySheet:jndiProp:jndiProp" 3 ) ; clickByIdAction ( selectId ) ; clickAndWait ( "form:propertySheet:generalPropertySheet:jndiProp:jndiProp" 3 ) ; waitforBtnDisable ( "form:propertySheet:generalPropertySheet:jndiProp:jndiProp" 7 ) ; clickByIdAction ( selectId ) ; clickAndWait ( "form:propertySheet:generalPropertySheet:jndiProp:jndiProp" 7 ) ; closeAlertAndGetItsText ( ) ; waitForAlertProcess ( "form:propertySheet:generalPropertySheet:jndiProp:jndiProp" 5 ) ; } }
public class aTest{ @Test public void testInsertAsyncMulti ( ) { com . pardot . rhombus . functional . AsyncExecITCase . logger . debug ( "Starting<sp>testInsertAsync" ) ; com . pardot . rhombus . ConnectionManager cm = getConnectionManager ( ) ; com . pardot . rhombus . cobject . CKeyspaceDefinition definition = com . pardot . rhombus . util . JsonUtil . objectFromJsonResource ( com . pardot . rhombus . cobject . CKeyspaceDefinition . class , this . getClass ( ) . getClassLoader ( ) , "AuditKeyspace.js" ) ; org . junit . Assert . assertNotNull ( definition ) ; cm . buildKeyspace ( definition , true ) ; com . pardot . rhombus . functional . AsyncExecITCase . logger . debug ( "Built<sp>keyspace:<sp>{}" , definition . getName ( ) ) ; cm . setDefaultKeyspace ( definition ) ; final com . pardot . rhombus . ObjectMapper om = cm . getObjectMapper ( ) ; final int numThreads = 10 ; final java . util . concurrent . ExecutorService executorService = java . util . concurrent . Executors . newFixedThreadPool ( numThreads ) ; java . util . List < java . util . Map < java . lang . String , java . lang . Object > > values = com . pardot . rhombus . util . JsonUtil . rhombusMapFromResource ( this . getClass ( ) . getClassLoader ( ) , "DateRangeQueryTestData.js" ) ; final com . pardot . rhombus . cobject . CDefinition objectAuditDef = definition . getDefinitions ( ) . get ( "object_audit" ) ; long startTime = java . lang . System . currentTimeMillis ( ) ; for ( int i = 0 ; i < 100 ; i ++ ) { insertObjectSetAsync ( numThreads , executorService , om , values , objectAuditDef ) ; } }
public class aTest{ @Test public void testSysPurposePoolPriorityUseCaseRoleMismatchOutweighsABunchOfOtherMismatches ( ) { org . candlepin . model . Product product69 = new org . candlepin . model . Product ( ) ; product69 . setId ( "RH00009" 3 ) ; consumer . setRole ( "RHEL<sp>Server" ) ; consumer . setServiceLevel ( "Premium" ) ; consumer . setUsage ( "Production" ) ; java . util . Set < java . lang . String > addons = new java . util . HashSet ( ) ; addons . add ( "RH00009" 1 ) ; consumer . setAddOns ( addons ) ; org . candlepin . model . ConsumerInstalledProduct consumerInstalledProduct = new org . candlepin . model . ConsumerInstalledProduct ( product69 ) ; consumer . addInstalledProduct ( consumerInstalledProduct ) ; org . candlepin . model . Product prodRH00009 = createSysPurposeProduct ( null , "RHEL<sp>Server" , null , "Standard" , "Development" ) ; org . candlepin . model . Pool RH00009 = org . candlepin . test . TestUtil . createPool ( owner , prodRH00009 ) ; RH00009 . setId ( "RH00009" ) ; RH00009 . addProvidedProduct ( product69 ) ; org . candlepin . model . Product prodRH00008 = createSysPurposeProduct ( null , "RH00009" 2 , "RH00009" 1 , "Premium" , "Production" ) ; org . candlepin . model . Pool RH00008 = org . candlepin . test . TestUtil . createPool ( owner , prodRH00008 ) ; RH00008 . setId ( "RH00008" ) ; RH00008 . addProvidedProduct ( product69 ) ; jsRules . reinitTo ( "RH00009" 5 ) ; org . candlepin . policy . js . JsonJsContext args = new org . candlepin . policy . js . JsonJsContext ( mapper ) ; args . put ( "RH00009" 0 , org . candlepin . policy . AutobindRulesTest . log , false ) ; args . put ( "consumer" , consumer ) ; args . put ( "compliance" , compliance ) ; args . put ( "pool" , RH00009 ) ; java . lang . Double RH00009Priority = jsRules . invokeMethod ( "RH00009" 6 , args ) ; args . put ( "pool" , RH00008 ) ; java . lang . Double RH00008Priority = jsRules . invokeMethod ( "RH00009" 6 , args ) ; org . junit . Assert . assertTrue ( "RH00009" 4 , ( RH00009Priority > RH00008Priority ) ) ; } }
public class aTest{ @Test public void testWriteBeyondFileSize ( ) { final java . nio . channels . ReadableByteChannel channel = new org . apache . hc . core5 . http . ReadableByteChannelMock ( new java . lang . String [ ] { "a" } , java . nio . charset . StandardCharsets . US_ASCII ) ; final org . apache . hc . core5 . http . nio . SessionInputBuffer inbuf = new org . apache . hc . core5 . http . impl . nio . SessionInputBufferImpl ( 1024 , 256 , 0 , java . nio . charset . StandardCharsets . US_ASCII ) ; final org . apache . hc . core5 . http . impl . BasicHttpTransportMetrics metrics = new org . apache . hc . core5 . http . impl . BasicHttpTransportMetrics ( ) ; final org . apache . hc . core5 . http . impl . nio . LengthDelimitedDecoder decoder = new org . apache . hc . core5 . http . impl . nio . LengthDelimitedDecoder ( channel , inbuf , metrics , 1 ) ; createTempFile ( ) ; final java . io . RandomAccessFile testfile = new java . io . RandomAccessFile ( this . tmpfile , "rw" ) ; try { final java . nio . channels . FileChannel fchannel = testfile . getChannel ( ) ; org . junit . Assert . assertEquals ( 0 , testfile . length ( ) ) ; try { decoder . transfer ( fchannel , 5 , 10 ) ; org . junit . Assert . fail ( "IOException<sp>should<sp>have<sp>been<sp>thrown" ) ; } }
public class aTest{ @Test public void whenReplaceKeyInLineWithProperKeyMapThenResultLineWithReplacedWord ( ) { final java . lang . String lineToReplace = "How<sp>${are}<sp>you<sp>${are}?<sp>${g}" ; final java . lang . String expectedResult = "How<sp>do<sp>you<sp>do?<sp>good" ; try { java . lang . String result = generator . generate ( lineToReplace , keyMap ) ; org . junit . Assert . assertThat ( result , org . hamcrest . core . Is . is ( expectedResult ) ) ; } }
public class aTest{ @Test public void testUpdateWorkflowVersionOld ( ) { final io . dockstore . common . CommonTestUtilities . TestingPostgres testingPostgres = getTestingPostgres ( ) ; runOldDockstoreClient ( io . dockstore . client . cli . GeneralWorkflowRegressionIT . dockstore , new java . lang . String [ ] { "select<sp>count(*)<sp>from<sp>workflowversion<sp>where<sp>name<sp>=<sp>'master'<sp>and<sp>hidden<sp>=<sp>'t'<sp>and<sp>workflowpath<sp>=<sp>'/Dockstore2.wdl'" 0 , io . dropwizard . testing . ResourceHelpers . resourceFilePath ( "select<sp>count(*)<sp>from<sp>workflowversion<sp>where<sp>name<sp>=<sp>'master'<sp>and<sp>hidden<sp>=<sp>'t'<sp>and<sp>workflowpath<sp>=<sp>'/Dockstore2.wdl'" 6 ) , "--hidden" 7 , "--hidden" 0 , "--repository" , "hello-dockstore-workflow" , "select<sp>count(*)<sp>from<sp>workflowversion<sp>where<sp>name<sp>=<sp>'master'<sp>and<sp>hidden<sp>=<sp>'t'<sp>and<sp>workflowpath<sp>=<sp>'/Dockstore2.wdl'" 1 , "DockstoreTestUser2" , "--hidden" 1 , "--hidden" 9 , "select<sp>count(*)<sp>from<sp>workflowversion<sp>where<sp>name<sp>=<sp>'master'<sp>and<sp>hidden<sp>=<sp>'t'<sp>and<sp>workflowpath<sp>=<sp>'/Dockstore2.wdl'" 4 , "select<sp>count(*)<sp>from<sp>workflowversion<sp>where<sp>name<sp>=<sp>'master'<sp>and<sp>hidden<sp>=<sp>'t'<sp>and<sp>workflowpath<sp>=<sp>'/Dockstore2.wdl'" 2 , "--hidden" 5 , "/Dockstore.wdl" , "select<sp>count(*)<sp>from<sp>workflowversion<sp>where<sp>name<sp>=<sp>'master'<sp>and<sp>hidden<sp>=<sp>'t'<sp>and<sp>workflowpath<sp>=<sp>'/Dockstore2.wdl'" 3 , "select<sp>count(*)<sp>from<sp>workflowversion<sp>where<sp>name<sp>=<sp>'master'<sp>and<sp>hidden<sp>=<sp>'t'<sp>and<sp>workflowpath<sp>=<sp>'/Dockstore2.wdl'" 5 , "--script" } ) ; runOldDockstoreClient ( io . dockstore . client . cli . GeneralWorkflowRegressionIT . dockstore , new java . lang . String [ ] { "select<sp>count(*)<sp>from<sp>workflowversion<sp>where<sp>name<sp>=<sp>'master'<sp>and<sp>hidden<sp>=<sp>'t'<sp>and<sp>workflowpath<sp>=<sp>'/Dockstore2.wdl'" 0 , io . dropwizard . testing . ResourceHelpers . resourceFilePath ( "select<sp>count(*)<sp>from<sp>workflowversion<sp>where<sp>name<sp>=<sp>'master'<sp>and<sp>hidden<sp>=<sp>'t'<sp>and<sp>workflowpath<sp>=<sp>'/Dockstore2.wdl'" 6 ) , "--hidden" 7 , "--hidden" 3 , "--hidden" 4 , ( SourceControl . GITHUB . toString ( ) ) + "--hidden" 6 , "--name" , "master" , "--hidden" 5 , "/Dockstore2.wdl" , "--hidden" , "--hidden" 2 , "--script" } ) ; final long count = testingPostgres . runSelectStatement ( "select<sp>count(*)<sp>from<sp>workflowversion<sp>where<sp>name<sp>=<sp>'master'<sp>and<sp>hidden<sp>=<sp>'t'<sp>and<sp>workflowpath<sp>=<sp>'/Dockstore2.wdl'" , new org . apache . commons . dbutils . handlers . ScalarHandler ( ) ) ; org . junit . Assert . assertEquals ( ( "--hidden" 8 + count ) , 1 , count ) ; } }
public class aTest{ @Test public void testDisableRule ( ) { java . lang . String grammar = "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" 3 + ( ( ( "E1<sp>:<sp>\'enum\'<sp>{false}?<sp>;\n" + "E2<sp>:<sp>\'enum\'<sp>{true}?<sp>;\n" ) + "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" ) + "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" 7 ) ; java . lang . String found = execLexer ( "L.g4" , grammar , "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" 4 , "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" 2 , true ) ; java . lang . String expecting = "[@0,0:3=\'enum\',<2>,1:0]\n" + ( ( ( ( ( ( ( ( "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" 5 + "[@2,8:7=\'<EOF>\',<-1>,1:8]\n" ) + "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" 0 ) + "s0-\'a\'->:s6=>3\n" ) + "s0-\'e\'->:s1=>3\n" ) + ":s1=>3-\'n\'->:s2=>3\n" ) + "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" 1 ) + "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" 6 ) + ":s6=>3-\'c\'->:s6=>3\n" ) ; org . junit . Assert . assertEquals ( expecting , found ) ; } }
public class aTest{ @Test public void test_evaluate_array_strings ( ) { org . junit . Assume . assumeFalse ( webkit1SkipMsg ( ) , isWebkit1 ) ; final java . util . concurrent . atomic . AtomicReferenceArray < java . lang . String > atomicStringArray = new java . util . concurrent . atomic . AtomicReferenceArray ( 3 ) ; atomicStringArray . set ( 0 , "executing" ) ; browser . addProgressListener ( org . eclipse . swt . browser . ProgressListener . completedAdapter ( ( event ) -> { java . lang . Object [ ] evalResult = ( ( java . lang . Object [ ] ) ( browser . evaluate ( "return<sp>new<sp>Array(\"str1\",<sp>\"str2\",<sp>\"str3\")" ) ) ) ; atomicStringArray . set ( 0 , ( ( java . lang . String ) ( evalResult [ 0 ] ) ) ) ; atomicStringArray . set ( 1 , ( ( java . lang . String ) ( evalResult [ 1 ] ) ) ) ; atomicStringArray . set ( 2 , ( ( java . lang . String ) ( evalResult [ 2 ] ) ) ) ; if ( debug_verbose_output ) System . out . println ( ( "executing" 3 + evalResult ) ) ; } ) ) ; browser . setText ( "executing" 2 ) ; shell . open ( ) ; java . util . concurrent . atomic . AtomicReference < java . lang . String > additionalErrorInfo = new java . util . concurrent . atomic . AtomicReference ( "" ) ; boolean passed = waitForPassCondition ( ( ) -> { if ( ! ( "executing" . equals ( atomicStringArray . get ( 0 ) ) ) ) { if ( ( ( atomicStringArray . get ( 0 ) . equals ( "str1" ) ) && ( atomicStringArray . get ( 1 ) . equals ( "executing" 1 ) ) ) && ( atomicStringArray . get ( 2 ) . equals ( "str3" ) ) ) { return true ; } else additionalErrorInfo . set ( "Resulting<sp>strings<sp>in<sp>array<sp>are<sp>not<sp>as<sp>expected" ) ; } return false ; } ) ; java . lang . String message = ( "" . equals ( additionalErrorInfo . get ( ) ) ) ? "executing" 0 : ( ( "Received<sp>a<sp>callback<sp>from<sp>javascript,<sp>but:<sp>" + ( additionalErrorInfo . get ( ) ) ) + "<sp>:<sp>" ) + ( atomicStringArray . toString ( ) ) ; org . junit . Assert . assertTrue ( message , passed ) ; } }
public class aTest{ @Test public void testRead ( ) { com . github . mygreen . supercsv . io . CsvAnnotationBeanReader < com . github . mygreen . supercsv . builder . CallbackMethodTest . TestCsv > csvReader = new com . github . mygreen . supercsv . io . CsvAnnotationBeanReader ( com . github . mygreen . supercsv . builder . CallbackMethodTest . TestCsv . class , new java . io . StringReader ( com . github . mygreen . supercsv . builder . CallbackMethodTest . TEST_CSV ) , org . supercsv . prefs . CsvPreference . STANDARD_PREFERENCE , com . github . mygreen . supercsv . builder . CallbackMethodTest . Group1 . class , com . github . mygreen . supercsv . builder . CallbackMethodTest . Group2 . class ) ; java . util . List < com . github . mygreen . supercsv . builder . CallbackMethodTest . TestCsv > list = csvReader . readAll ( false ) ; for ( com . github . mygreen . supercsv . builder . CallbackMethodTest . TestCsv record : list ) { org . junit . Assert . assertThat ( record . messages ) . containsExactly ( "method::handlePreRead-null" , "listener::handlePreRead-null" , ( "validate::validate-" + ( record . getId ( ) ) ) , ( "method::handlePostRead-" + ( record . getId ( ) ) ) , ( "listener::handlePostRead-" + ( record . getId ( ) ) ) ) ; } }
public class aTest{ @Test public void testHiveConfigClusterUpdateSpecifyingHostNamesHiveServer2HA ( ) { final java . lang . String expectedHostGroupName = "host_group_1" ; final java . lang . String expectedMetaStoreURIs = "thrift://c6401.ambari.apache.org:9083,thrift://c6402.ambari.apache.org:9083" ; java . util . Map < java . lang . String , java . util . Map < java . lang . String , java . lang . String > > configProperties = new java . util . HashMap ( ) ; java . util . Map < java . lang . String , java . lang . String > hiveEnvProperties = new java . util . HashMap ( ) ; java . util . Map < java . lang . String , java . lang . String > hiveSiteProperties = new java . util . HashMap ( ) ; configProperties . put ( "hive-env" , hiveEnvProperties ) ; configProperties . put ( "hive-site" , hiveSiteProperties ) ; hiveSiteProperties . put ( "hive.server2.support.dynamic.service.discovery" , "true" ) ; hiveSiteProperties . put ( "hive.metastore.uris" , expectedMetaStoreURIs ) ; org . apache . ambari . server . topology . Configuration clusterConfig = new org . apache . ambari . server . topology . Configuration ( configProperties , java . util . Collections . emptyMap ( ) ) ; java . util . Collection < java . lang . String > hgComponents = new java . util . HashSet ( ) ; hgComponents . add ( "HIVE_SERVER" ) ; org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup group1 = new org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup ( expectedHostGroupName , hgComponents , java . util . Collections . singleton ( "some-host" ) ) ; java . util . Collection < java . lang . String > hgComponents2 = new java . util . HashSet ( ) ; hgComponents2 . add ( "HIVE_SERVER" ) ; org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup group2 = new org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup ( "HIVE_SERVER" 1 , hgComponents2 , java . util . Collections . singleton ( "some-host2" ) ) ; java . util . Collection < org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup > hostGroups = new java . util . HashSet ( ) ; hostGroups . add ( group1 ) ; hostGroups . add ( group2 ) ; expect ( stack . getCardinality ( "HIVE_SERVER" ) ) . andReturn ( new org . apache . ambari . server . topology . Cardinality ( "HIVE_SERVER" 2 ) ) . anyTimes ( ) ; org . apache . ambari . server . topology . ClusterTopology topology = createClusterTopology ( bp , clusterConfig , hostGroups ) ; org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessor updater = new org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessor ( topology ) ; updater . doUpdateForClusterCreate ( ) ; org . junit . Assert . assertEquals ( "HIVE_SERVER" 0 , expectedMetaStoreURIs , hiveSiteProperties . get ( "hive.metastore.uris" ) ) ; } }
public class aTest{ @Test public void testCopy ( ) { try { this . fileSystem . copy ( "fail" , gov . redhawk . sca . efs . server . tests . FileSystemImplTest . TEMP_FILE_NAME ) ; org . junit . Assert . fail ( "Should<sp>raise<sp>file<sp>exception." ) ; } catch ( final CF . InvalidFileName e ) { org . junit . Assert . fail ( "Should<sp>raise<sp>file<sp>exception." ) ; } catch ( final CF . FileException e ) { } try { this . fileSystem . copy ( "testFile.txt" , "testFile.txt" ) ; org . junit . Assert . fail ( "Should<sp>raise<sp>invalid<sp>file<sp>name." ) ; } catch ( final CF . InvalidFileName e ) { } catch ( final CF . FileException e ) { org . junit . Assert . fail ( "Should<sp>raise<sp>invalid<sp>file<sp>name." ) ; } try { this . fileSystem . copy ( "testFile.txt" , gov . redhawk . sca . efs . server . tests . FileSystemImplTest . TEMP_FILE_NAME ) ; org . junit . Assert . assertTrue ( new java . io . File ( gov . redhawk . sca . efs . server . tests . FileSystemImplTest . root , gov . redhawk . sca . efs . server . tests . FileSystemImplTest . TEMP_FILE_NAME ) . exists ( ) ) ; } }
public class aTest{ @Test public void testProcessNonEmptyBatch ( ) { com . streamsets . pipeline . sdk . TestTargetRunner . DummyTarget stage = new com . streamsets . pipeline . sdk . TestTargetRunner . DummyTarget ( ) ; com . streamsets . pipeline . sdk . TargetRunner . Builder builder = new com . streamsets . pipeline . sdk . TargetRunner . Builder ( com . streamsets . pipeline . sdk . TestTargetRunner . DummyTarget . class , stage ) ; com . streamsets . pipeline . sdk . TargetRunner runner = builder . build ( ) ; try { runner . runInit ( ) ; runner . runWrite ( com . google . common . collect . ImmutableList . of ( com . streamsets . pipeline . sdk . RecordCreator . create ( ) ) ) ; org . junit . Assert . assertTrue ( stage . write ) ; } }
public class aTest{ @Test public void testNodeDecomissionWithOverreplicationRespectsRackPolicy ( ) { org . apache . hadoop . conf . Configuration conf = getConf ( ) ; short REPLICATION_FACTOR = 5 ; final org . apache . hadoop . fs . Path filePath = new org . apache . hadoop . fs . Path ( "/testFile" ) ; org . apache . hadoop . fs . FileSystem localFileSys = org . apache . hadoop . fs . FileSystem . getLocal ( conf ) ; org . apache . hadoop . fs . Path workingDir = localFileSys . getWorkingDirectory ( ) ; org . apache . hadoop . fs . Path dir = new org . apache . hadoop . fs . Path ( workingDir , "build/test/data/temp/decommission" ) ; org . apache . hadoop . fs . Path excludeFile = new org . apache . hadoop . fs . Path ( dir , "exclude" ) ; org . apache . hadoop . fs . Path includeFile = new org . apache . hadoop . fs . Path ( dir , "include" ) ; org . junit . Assert . assertTrue ( localFileSys . mkdirs ( dir ) ) ; org . apache . hadoop . hdfs . DFSTestUtil . writeFile ( localFileSys , excludeFile , "" ) ; org . apache . hadoop . hdfs . DFSTestUtil . writeFile ( localFileSys , includeFile , "" ) ; conf . set ( DFSConfigKeys . DFS_HOSTS , includeFile . toUri ( ) . getPath ( ) ) ; conf . set ( DFSConfigKeys . DFS_HOSTS_EXCLUDE , excludeFile . toUri ( ) . getPath ( ) ) ; java . lang . String [ ] racks = new java . lang . String [ ] { "/rack1" , "/rack2" , "/rack1" , "/rack1" , "/rack1" } ; org . apache . hadoop . hdfs . MiniDFSCluster cluster = new org . apache . hadoop . hdfs . MiniDFSCluster . Builder ( conf ) . numDataNodes ( racks . length ) . racks ( racks ) . build ( ) ; final org . apache . hadoop . hdfs . server . namenode . FSNamesystem ns = cluster . getNameNode ( ) . getNamesystem ( ) ; try { final org . apache . hadoop . fs . FileSystem fs = cluster . getFileSystem ( ) ; org . apache . hadoop . hdfs . DFSTestUtil . createFile ( fs , filePath , 1L , REPLICATION_FACTOR , 1L ) ; org . apache . hadoop . hdfs . protocol . ExtendedBlock b = org . apache . hadoop . hdfs . DFSTestUtil . getFirstBlock ( fs , filePath ) ; org . apache . hadoop . hdfs . DFSTestUtil . waitForReplication ( cluster , b , 2 , REPLICATION_FACTOR , 0 ) ; REPLICATION_FACTOR = 2 ; fs . setReplication ( filePath , REPLICATION_FACTOR ) ; org . apache . hadoop . fs . BlockLocation [ ] locs = fs . getFileBlockLocations ( fs . getFileStatus ( filePath ) , 0 , Long . MAX_VALUE ) ; for ( java . lang . String top : locs [ 0 ] . getTopologyPaths ( ) ) { if ( ! ( top . startsWith ( "/rack2" ) ) ) { java . lang . String name = top . substring ( ( ( "/rack1" . length ( ) ) + 1 ) ) ; org . apache . hadoop . hdfs . DFSTestUtil . writeFile ( localFileSys , excludeFile , name ) ; ns . getBlockManager ( ) . getDatanodeManager ( ) . refreshNodes ( conf ) ; org . apache . hadoop . hdfs . DFSTestUtil . waitForDecommission ( fs , name ) ; break ; } } }
public class aTest{ @Test public void testNotInstalledRecursiveCase ( ) { final org . w3c . dom . Document doc = javax . xml . parsers . DocumentBuilderFactory . newInstance ( ) . newDocumentBuilder ( ) . newDocument ( ) ; final org . jboss . errai . forge . facet . resource . List < org . w3c . dom . Node > nodes = org . jboss . errai . forge . facet . resource . Arrays . asList ( new org . w3c . dom . Node [ ] { doc . createElement ( "first" ) , doc . createElement ( "first" ) } ) ; nodes . get ( 0 ) . appendChild ( doc . createElement ( "second" ) ) . appendChild ( doc . createElement ( "third" ) ) ; ( ( org . w3c . dom . Element ) ( nodes . get ( 0 ) . getFirstChild ( ) ) ) . setAttribute ( "foo" , "bar" ) ; nodes . get ( 0 ) . getFirstChild ( ) . appendChild ( doc . createElement ( "fourth" ) ) ; nodes . get ( 0 ) . appendChild ( doc . createElement ( "fifth" ) ) ; ( ( org . w3c . dom . Element ) ( nodes . get ( 1 ) . appendChild ( doc . createElement ( "second" ) ) ) ) . setAttribute ( "name" , "test" ) ; final org . jboss . errai . forge . facet . resource . Map < java . lang . String , org . jboss . errai . forge . facet . resource . Collection < org . w3c . dom . Node > > insertMap = new org . jboss . errai . forge . facet . resource . HashMap < java . lang . String , org . jboss . errai . forge . facet . resource . Collection < org . w3c . dom . Node > > ( 1 ) ; insertMap . put ( "bar" 0 , nodes ) ; final org . jboss . errai . forge . facet . resource . Map < java . lang . String , org . w3c . dom . Node > empty = new org . jboss . errai . forge . facet . resource . HashMap < java . lang . String , org . w3c . dom . Node > ( 0 ) ; final org . jboss . forge . addon . projects . Project project = initializeJavaProject ( ) ; final org . jboss . errai . forge . facet . resource . AbstractXmlResourceFacetTest . TestXmlResourceFacet testFacet = new org . jboss . errai . forge . facet . resource . AbstractXmlResourceFacetTest . TestXmlResourceFacet ( writeResourceToFile ( "org/jboss/errai/forge/facet/resource/AbstractXmlResourceFacetTest-1.xml" ) , insertMap , empty , empty ) ; testFacet . setFaceted ( project ) ; org . junit . Assert . assertFalse ( testFacet . isInstalled ( ) ) ; } }
public class aTest{ @Test public void encrypt ( ) { java . io . ByteArrayOutputStream bos = new java . io . ByteArrayOutputStream ( ) ; try { try ( org . apache . poi . hssf . usermodel . HSSFWorkbook hwb = org . apache . poi . hssf . HSSFTestDataSamples . openSampleWorkbook ( "SampleSS.xls" ) ) { org . apache . poi . hssf . record . crypto . Biff8EncryptionKey . setCurrentUserPassword ( "abc" ) ; hwb . getInternalWorkbook ( ) . getWorkbookRecordList ( ) . add ( 1 , new org . apache . poi . hssf . record . FilePassRecord ( EncryptionMode . xor ) ) ; hwb . write ( bos ) ; } try ( org . apache . poi . hssf . usermodel . HSSFWorkbook hwb = new org . apache . poi . hssf . usermodel . HSSFWorkbook ( new java . io . ByteArrayInputStream ( bos . toByteArray ( ) ) ) ) { org . junit . Assert . assertEquals ( 3 , hwb . getNumberOfSheets ( ) ) ; } } }
public class aTest{ @Test public void testSubruleWithRewrite ( ) { java . lang . String grammar = "grammar<sp>T;\n" + ( ( ( ( ( ( ( "options<sp>{output=AST;}\n" + "tokens<sp>{BLOCK;}\n" ) + "TParser" 4 ) + "TParser" 2 ) + "<sp>;\n" ) + "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" ) + "INT<sp>:<sp>\'0\'..\'9\'+;\n" ) + "WS<sp>:<sp>(\'<sp>\'|\'\\n\')<sp>{$channel=HIDDEN;}<sp>;\n" ) ; java . lang . String found = execParser ( "TParser" 3 , grammar , "TParser" , "TParser" 0 , "a" , "a<sp>1<sp>2<sp>3" , debug ) ; org . junit . Assert . assertEquals ( "TParser" 1 , found ) ; } }
public class aTest{ @Test public void testExtractColumnNames ( ) { java . util . List < de . metanome . backend . results_db . Input > inputs = new java . util . ArrayList ( ) ; java . lang . String pathToCsvFolder = java . lang . Thread . currentThread ( ) . getContextClassLoader ( ) . getResource ( "inputC.tsv" 1 ) . getPath ( ) ; inputs . add ( new de . metanome . backend . results_db . FileInput ( ( ( pathToCsvFolder + ( java . io . File . separator ) ) + "inputC.tsv" ) ) . setSeparator ( "inputC.tsv" 7 ) ) ; java . util . List < de . metanome . algorithm_integration . ColumnIdentifier > expectedColumnNames = new java . util . ArrayList ( ) ; expectedColumnNames . add ( new de . metanome . algorithm_integration . ColumnIdentifier ( "inputC.tsv" , "inputC.tsv" 6 ) ) ; expectedColumnNames . add ( new de . metanome . algorithm_integration . ColumnIdentifier ( "inputC.tsv" , "inputC.tsv" 5 ) ) ; expectedColumnNames . add ( new de . metanome . algorithm_integration . ColumnIdentifier ( "inputC.tsv" , "002" ) ) ; expectedColumnNames . add ( new de . metanome . algorithm_integration . ColumnIdentifier ( "inputC.tsv" , "003" ) ) ; expectedColumnNames . add ( new de . metanome . algorithm_integration . ColumnIdentifier ( "inputC.tsv" , "004" ) ) ; expectedColumnNames . add ( new de . metanome . algorithm_integration . ColumnIdentifier ( "inputC.tsv" , "005" ) ) ; expectedColumnNames . add ( new de . metanome . algorithm_integration . ColumnIdentifier ( "inputC.tsv" , "inputC.tsv" 2 ) ) ; expectedColumnNames . add ( new de . metanome . algorithm_integration . ColumnIdentifier ( "inputC.tsv" , "inputC.tsv" 3 ) ) ; expectedColumnNames . add ( new de . metanome . algorithm_integration . ColumnIdentifier ( "inputC.tsv" , "008" ) ) ; expectedColumnNames . add ( new de . metanome . algorithm_integration . ColumnIdentifier ( "inputC.tsv" , "009" ) ) ; expectedColumnNames . add ( new de . metanome . algorithm_integration . ColumnIdentifier ( "inputC.tsv" , "010" ) ) ; expectedColumnNames . add ( new de . metanome . algorithm_integration . ColumnIdentifier ( "inputC.tsv" , "inputC.tsv" 0 ) ) ; expectedColumnNames . add ( new de . metanome . algorithm_integration . ColumnIdentifier ( "inputC.tsv" , "012" ) ) ; expectedColumnNames . add ( new de . metanome . algorithm_integration . ColumnIdentifier ( "inputC.tsv" , "013" ) ) ; expectedColumnNames . add ( new de . metanome . algorithm_integration . ColumnIdentifier ( "inputC.tsv" , "inputC.tsv" 4 ) ) ; java . util . List < de . metanome . algorithm_integration . ColumnIdentifier > actualColumnNames = de . metanome . backend . algorithm_execution . AlgorithmExecution . extractColumnNames ( inputs ) ; org . junit . Assert . assertEquals ( expectedColumnNames , actualColumnNames ) ; } }
public class aTest{ @Test public void testCopyRequestHeaders ( ) { org . apache . shindig . gadgets . http . HttpRequest origRequest = new org . apache . shindig . gadgets . http . HttpRequest ( org . apache . shindig . common . uri . Uri . parse ( "http://www.example.org/data.html" ) ) ; java . util . Map < java . lang . String , java . util . List < java . lang . String > > addedHeaders = com . google . common . collect . ImmutableMap . < java . lang . String , java . util . List < java . lang . String > > builder ( ) . put ( "h1" , com . google . common . collect . ImmutableList . of ( "http://www.example.org/data.html" 3 , "http://www.example.org/data.html" 2 ) ) . put ( "http://www.example.org/data.html" 7 , com . google . common . collect . ImmutableList . of ( "v3" , "v4" ) ) . put ( "http://www.example.org/data.html" 4 , com . google . common . collect . ImmutableList . of ( "http://www.example.org/data.html" 5 , "http://www.example.org/data.html" 1 ) ) . put ( "unchanged_header" , com . google . common . collect . ImmutableList . < java . lang . String > of ( ) ) . put ( "http://www.example.org/data.html" 0 , com . google . common . collect . ImmutableList . of ( "50" , "100" ) ) . build ( ) ; origRequest . addAllHeaders ( addedHeaders ) ; org . apache . shindig . gadgets . http . HttpRequest req = new org . apache . shindig . gadgets . http . HttpRequest ( org . apache . shindig . common . uri . Uri . parse ( "http://www.example.org/data.html" ) ) ; req . removeHeader ( HttpRequest . DOS_PREVENTION_HEADER ) ; req . addHeader ( "h1" , "hello" ) ; req . addHeader ( "http://www.example.org/data.html" 0 , "10" ) ; req . addHeader ( "unchanged_header" , "original_value" ) ; org . apache . shindig . gadgets . uri . UriUtils . copyRequestHeaders ( origRequest , req , UriUtils . DisallowedHeaders . POST_INCOMPATIBLE_DIRECTIVES ) ; java . util . Map < java . lang . String , java . util . List < java . lang . String > > headers = com . google . common . collect . ImmutableMap . < java . lang . String , java . util . List < java . lang . String > > builder ( ) . put ( "h1" , com . google . common . collect . ImmutableList . of ( "http://www.example.org/data.html" 3 , "http://www.example.org/data.html" 2 ) ) . put ( "http://www.example.org/data.html" 7 , com . google . common . collect . ImmutableList . of ( "v3" , "v4" ) ) . put ( "unchanged_header" , com . google . common . collect . ImmutableList . of ( "original_value" ) ) . put ( "http://www.example.org/data.html" 0 , com . google . common . collect . ImmutableList . of ( "10" ) ) . put ( HttpRequest . DOS_PREVENTION_HEADER , com . google . common . collect . ImmutableList . of ( "http://www.example.org/data.html" 6 ) ) . build ( ) ; org . junit . Assert . assertEquals ( headers , req . getHeaders ( ) ) ; } }
public class aTest{ @Test public void receiveMoreThanBufferSize ( ) { alluxio . grpc . WriteResponse [ ] responses = java . util . stream . Stream . generate ( ( ) -> alluxio . grpc . WriteResponse . newBuilder ( ) . build ( ) ) . limit ( ( ( alluxio . client . block . stream . GrpcBlockingStreamTest . BUFFER_SIZE ) * 2 ) ) . toArray ( alluxio . grpc . WriteResponse [ ] :: new ) ; alluxio . client . block . stream . GrpcBlockingStreamTest . EXECUTOR . submit ( ( ) -> { for ( alluxio . grpc . WriteResponse response : responses ) { mResponseObserver . onNext ( response ) ; } } ) ; java . lang . Thread . sleep ( alluxio . client . block . stream . GrpcBlockingStreamTest . SHORT_TIMEOUT ) ; for ( alluxio . grpc . WriteResponse response : responses ) { alluxio . grpc . WriteResponse actualResponse = mStream . receive ( alluxio . client . block . stream . GrpcBlockingStreamTest . TIMEOUT ) ; org . junit . Assert . assertEquals ( response , actualResponse ) ; } } }
public class aTest{ @Test public void testGroupOnRowPosition5 ( ) { java . lang . String [ ] bindingNameGroup = new java . lang . String [ 1 ] ; bindingNameGroup [ 0 ] = "row.__rownum" 3 ; org . eclipse . birt . data . engine . api . IBaseExpression [ ] bindingExprGroup = new org . eclipse . birt . data . engine . api . IBaseExpression [ 1 ] ; bindingExprGroup [ 0 ] = new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( "dataSetRow.ID" ) ; org . eclipse . birt . data . engine . api . querydefn . GroupDefinition [ ] groupDefn = new org . eclipse . birt . data . engine . api . querydefn . GroupDefinition [ ] { new org . eclipse . birt . data . engine . api . querydefn . GroupDefinition ( "row.__rownum" 5 ) } ; groupDefn [ 0 ] . setKeyExpression ( "row.__rownum" ) ; groupDefn [ 0 ] . setInterval ( IGroupDefinition . NUMERIC_INTERVAL ) ; groupDefn [ 0 ] . setIntervalRange ( 5 ) ; java . lang . String [ ] bindingNameRow = new java . lang . String [ 4 ] ; bindingNameRow [ 0 ] = "ROW_ID" ; bindingNameRow [ 1 ] = "ROW_rowPosition" ; bindingNameRow [ 2 ] = "ROW_AMOUT1" ; bindingNameRow [ 3 ] = "ROW_AMOUT2" ; org . eclipse . birt . data . engine . api . IBaseExpression [ ] bindingExprRow = new org . eclipse . birt . data . engine . api . IBaseExpression [ 4 ] ; bindingExprRow [ 0 ] = new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( "dataSetRow.ID" ) ; bindingExprRow [ 1 ] = new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( "row.__rownum" 2 ) ; bindingExprRow [ 2 ] = new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( "dataSetRow.AMOUNT1" ) ; bindingExprRow [ 3 ] = new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( "dataSetRow.AMOUNT2" ) ; java . lang . String [ ] columnStr = new java . lang . String [ ] { "row.__rownum" 1 , "_rowPosition" , "amount1" , "row.__rownum" 4 } ; try { org . eclipse . birt . data . engine . api . querydefn . QueryDefinition qd = this . createQuery ( bindingNameGroup , bindingExprGroup , groupDefn , null , null , null , null , null , null , bindingNameRow , bindingExprRow ) ; java . lang . String outputStr = getOutputStrForGroupTest ( 15 , qd , groupDefn . length , bindingNameRow , columnStr ) ; testPrint ( outputStr ) ; this . checkOutputFile ( ) ; } catch ( org . eclipse . birt . core . exception . BirtException be ) { org . junit . Assert . assertEquals ( be . getErrorCode ( ) , "row.__rownum" 0 ) ; } } }
public class aTest{ @Test public void test ( ) { java . io . File baseDir = getExampleDir ( "throttle" ) ; com . predic8 . membrane . examples . Process2 sl = new com . predic8 . membrane . examples . Process2 . Builder ( ) . in ( baseDir ) . script ( "service-proxy" ) . waitForMembrane ( ) . start ( ) ; try { getAndAssert200 ( "http://localhost:2000/" ) ; long start = java . lang . System . currentTimeMillis ( ) ; getAndAssert200 ( "http://localhost:2000/" ) ; long elapsedMillis = ( java . lang . System . currentTimeMillis ( ) ) - start ; org . junit . Assert . assertTrue ( ( elapsedMillis >= 1000 ) ) ; } }
public class aTest{ @Test public void testViews ( ) { class MockConsumer { private int i = 0 ; private void consume ( io . datakernel . bytebuf . ByteBuf buf ) { org . junit . Assert . assertEquals ( ( "Test<sp>message<sp>" + ( ( i ) ++ ) ) , buf . toString ( ) ) ; buf . recycle ( ) ; } } }
public class aTest{ @Test public void testCreateTable1 ( ) { try { org . odftoolkit . odfdom . pkg . OdfFileDom doc = org . odftoolkit . odfdom . doc . OdfDocument . loadDocument ( org . odftoolkit . odfdom . utils . ResourceUtilities . getAbsolutePath ( "empty.odt" ) ) . getContentDom ( ) ; org . w3c . dom . NodeList lst = doc . getElementsByTagNameNS ( TextPElement . ELEMENT_NAME . getUri ( ) , TextPElement . ELEMENT_NAME . getLocalName ( ) ) ; org . junit . Assert . assertTrue ( ( ( lst . getLength ( ) ) > 0 ) ) ; org . odftoolkit . odfdom . incubator . doc . text . OdfTextParagraph p0 = ( ( org . odftoolkit . odfdom . incubator . doc . text . OdfTextParagraph ) ( lst . item ( ( ( lst . getLength ( ) ) - 1 ) ) ) ) ; org . odftoolkit . odfdom . dom . element . table . TableTableElement table = doc . newOdfElement ( org . odftoolkit . odfdom . dom . element . table . TableTableElement . class ) ; org . odftoolkit . odfdom . dom . element . table . TableTableRowElement tr = ( ( org . odftoolkit . odfdom . dom . element . table . TableTableRowElement ) ( table . appendChild ( doc . newOdfElement ( org . odftoolkit . odfdom . dom . element . table . TableTableRowElement . class ) ) ) ) ; org . odftoolkit . odfdom . dom . element . table . TableTableCellElement td1 = ( ( org . odftoolkit . odfdom . dom . element . table . TableTableCellElement ) ( tr . appendChild ( doc . newOdfElement ( org . odftoolkit . odfdom . dom . element . table . TableTableCellElement . class ) ) ) ) ; org . odftoolkit . odfdom . incubator . doc . text . OdfTextParagraph p1 = doc . newOdfElement ( org . odftoolkit . odfdom . incubator . doc . text . OdfTextParagraph . class ) ; p1 . appendChild ( doc . createTextNode ( "content<sp>1" ) ) ; td1 . appendChild ( p1 ) ; org . odftoolkit . odfdom . dom . element . table . TableTableCellElement td2 = ( ( org . odftoolkit . odfdom . dom . element . table . TableTableCellElement ) ( tr . appendChild ( doc . newOdfElement ( org . odftoolkit . odfdom . dom . element . table . TableTableCellElement . class ) ) ) ) ; org . odftoolkit . odfdom . incubator . doc . text . OdfTextParagraph p2 = doc . newOdfElement ( org . odftoolkit . odfdom . incubator . doc . text . OdfTextParagraph . class ) ; p2 . appendChild ( doc . createTextNode ( "cell<sp>2" ) ) ; td2 . appendChild ( p2 ) ; org . odftoolkit . odfdom . dom . element . table . TableTableCellElement td3 = ( ( org . odftoolkit . odfdom . dom . element . table . TableTableCellElement ) ( tr . appendChild ( doc . newOdfElement ( org . odftoolkit . odfdom . dom . element . table . TableTableCellElement . class ) ) ) ) ; org . odftoolkit . odfdom . incubator . doc . text . OdfTextParagraph p3 = doc . newOdfElement ( org . odftoolkit . odfdom . incubator . doc . text . OdfTextParagraph . class ) ; p3 . appendChild ( doc . createTextNode ( "content<sp>1" 0 ) ) ; td3 . appendChild ( p3 ) ; p0 . getParentNode ( ) . insertBefore ( table , p0 ) ; table . setProperty ( StyleTablePropertiesElement . Width , "12cm" ) ; table . setProperty ( StyleTablePropertiesElement . Align , "content<sp>1" 2 ) ; td1 . setProperty ( StyleTableColumnPropertiesElement . ColumnWidth , "2cm" ) ; td2 . setProperty ( StyleTableColumnPropertiesElement . ColumnWidth , "4cm" ) ; td3 . setProperty ( StyleTableColumnPropertiesElement . ColumnWidth , "6cm" ) ; doc . getDocument ( ) . save ( org . odftoolkit . odfdom . utils . ResourceUtilities . newTestOutputFile ( "tabletest.odt" ) ) ; } }
public class aTest{ @Test public void updateTest ( ) { com . jfireframework . sql . test . entity . User user = new com . jfireframework . sql . test . entity . User ( ) ; user . setId ( 1 ) ; user . setName ( "" ) ; user . setPassword ( "weadasda" ) ; user . setAge ( 15 ) ; user . setBirthday ( "2015-05-06<sp>12:12:12" ) ; com . jfireframework . baseutil . time . Timewatch timewatch = new com . jfireframework . baseutil . time . Timewatch ( ) ; timewatch . start ( ) ; session . save ( user ) ; timewatch . end ( ) ; logger . debug ( "{}" , timewatch . getTotal ( ) ) ; try { java . sql . PreparedStatement pstat = connection . prepareStatement ( "select<sp>username<sp>from<sp>user<sp>where<sp>userid=1" ) ; java . sql . ResultSet resultSet = pstat . executeQuery ( ) ; resultSet . next ( ) ; java . lang . String username = resultSet . getString ( 1 ) ; org . junit . Assert . assertEquals ( user . getName ( ) , username ) ; } }
public class aTest{ @Test public void testFiltering ( ) { org . apache . lucene . analysis . TokenStream stream = new org . apache . lucene . analysis . core . WhitespaceTokenizer ( org . apache . lucene . util . Version . LUCENE_47 , new java . io . StringReader ( "0.10<sp>0.20<sp>0.30<sp>0.40" ) ) ; org . apache . jackrabbit . oak . plugins . index . lucene . util . fv . TruncateTokenFilter filter = new org . apache . jackrabbit . oak . plugins . index . lucene . util . fv . TruncateTokenFilter ( stream , 3 ) ; filter . reset ( ) ; java . util . List < java . lang . String > expectedTokens = new java . util . LinkedList ( ) ; expectedTokens . add ( "0.1" ) ; expectedTokens . add ( "0.2" ) ; expectedTokens . add ( "0.3" ) ; expectedTokens . add ( "0.4" ) ; int i = 0 ; while ( filter . incrementToken ( ) ) { org . apache . lucene . analysis . tokenattributes . CharTermAttribute charTermAttribute = filter . getAttribute ( org . apache . lucene . analysis . tokenattributes . CharTermAttribute . class ) ; java . lang . String token = new java . lang . String ( charTermAttribute . buffer ( ) , 0 , charTermAttribute . length ( ) ) ; org . junit . Assert . assertEquals ( expectedTokens . get ( i ) , token ) ; i ++ ; } }
public class aTest{ @Test public void testAddRemoveFlattenedRelationship1 ( ) { create1Artist1ArtGroupDataSet ( ) ; org . apache . cayenne . testdo . testmap . Artist a1 = org . apache . cayenne . Cayenne . objectForPK ( context , org . apache . cayenne . testdo . testmap . Artist . class , 33001 ) ; org . apache . cayenne . query . SelectQuery q = new org . apache . cayenne . query . SelectQuery ( org . apache . cayenne . testdo . testmap . ArtGroup . class , org . apache . cayenne . exp . ExpressionFactory . matchExp ( "name" , "g1" ) ) ; java . util . List < ? > results = context . performQuery ( q ) ; org . junit . Assert . assertEquals ( 1 , results . size ( ) ) ; org . apache . cayenne . testdo . testmap . ArtGroup group = ( ( org . apache . cayenne . testdo . testmap . ArtGroup ) ( results . get ( 0 ) ) ) ; a1 . addToGroupArray ( group ) ; group . removeFromArtistArray ( a1 ) ; queryInterceptor . runWithQueriesBlocked ( new org . apache . cayenne . unit . di . UnitTestClosure ( ) { public void execute ( ) { context . commitChanges ( ) ; } } }
public class aTest{ @Test public void should_map_Float_to_Float ( ) { com . github . erchu . beancp . commons . NumberConvertersTest . Source sourceInstance = new com . github . erchu . beancp . commons . NumberConvertersTest . Source ( ) ; sourceInstance . setFloatWrapperValue ( ( ( float ) ( 8 ) ) ) ; com . github . erchu . beancp . Mapper mapper = new com . github . erchu . beancp . MapperBuilder ( ) . addConverter ( com . github . erchu . beancp . commons . NumberConverters . get ( ) ) . addMap ( com . github . erchu . beancp . commons . NumberConvertersTest . Source . class , com . github . erchu . beancp . commons . NumberConvertersTest . Destination . class , ( config , source , destination ) -> config . mapInner ( source :: getFloatWrapperValue , destination :: setFloatWrapperValue , . class ) ) . buildMapper ( ) ; com . github . erchu . beancp . commons . NumberConvertersTest . Destination result = mapper . map ( sourceInstance , com . github . erchu . beancp . commons . NumberConvertersTest . Destination . class ) ; org . junit . Assert . assertEquals ( ( ( float ) ( 8 ) ) , result . getFloatWrapperValue ( ) , 0.0 ) ; } }
public class aTest{ @Test public void testThreads ( ) { final java . lang . Thread t1 = new org . apache . commons . lang3 . ThreadUtilsTest . TestThread ( "thread1_XXOOLL__" ) ; final java . lang . Thread t2 = new org . apache . commons . lang3 . ThreadUtilsTest . TestThread ( "thread2_XXOOLL__" ) ; try { t1 . start ( ) ; t2 . start ( ) ; org . junit . Assert . assertEquals ( 1 , org . apache . commons . lang3 . ThreadUtils . findThreadsByName ( "thread2_XXOOLL__" ) . size ( ) ) ; } }
public class aTest{ @Test public void test_byteArrayToWritable ( ) { org . apache . hadoop . io . Text testInputText = new org . apache . hadoop . io . Text ( "test<sp>writable" ) ; org . apache . hadoop . io . Text testOutputText = new org . apache . hadoop . io . Text ( ) ; try { byte [ ] byteArray = org . goldenorb . zookeeper . ZookeeperUtils . writableToByteArray ( testInputText ) ; testOutputText = ( ( org . apache . hadoop . io . Text ) ( org . goldenorb . zookeeper . ZookeeperUtils . byteArrayToWritable ( byteArray , testOutputText ) ) ) ; org . junit . Assert . assertEquals ( testInputText , testOutputText ) ; } }
public class aTest{ @Test public void getPaymentHistory_NoPSPAccount ( ) { runTX ( new java . util . concurrent . Callable < java . lang . Void > ( ) { @ org . oscm . paymentservice . retrieval . Override public org . oscm . paymentservice . retrieval . Void call ( ) throws org . oscm . paymentservice . retrieval . Exception { deleteDbEntries ( org . oscm . domobjects . PSPAccountHistory . class ) ; org . oscm . paymentservice . data . PaymentHistoryData data = reader . getPaymentHistory ( subscriptionKey ) ; org . junit . Assert . assertNull ( data . getPspAccountHistory ( ) ) ; return null ; } } }
public class aTest{ @Test public void testCalculateCountSpatialFilter ( ) { try { java . lang . String typeName = org . geotools . arcsde . data . ArcSDEJavaApiTest . testData . getTempTableName ( ) ; java . lang . String where = null ; int expCount = 4 ; int actualCount ; com . esri . sde . sdk . client . SeExtent extent = new com . esri . sde . sdk . client . SeExtent ( ( - 180 ) , ( - 90 ) , ( - 170 ) , ( - 80 ) ) ; com . esri . sde . sdk . client . SeLayer layer = session . getLayer ( typeName ) ; com . esri . sde . sdk . client . SeShape filterShape = new com . esri . sde . sdk . client . SeShape ( layer . getCoordRef ( ) ) ; filterShape . generateRectangle ( extent ) ; com . esri . sde . sdk . client . SeShapeFilter bboxFilter = new com . esri . sde . sdk . client . SeShapeFilter ( typeName , layer . getSpatialColumn ( ) , filterShape , com . esri . sde . sdk . client . SeFilter . METHOD_ENVP , true ) ; com . esri . sde . sdk . client . SeFilter [ ] spatFilters = new com . esri . sde . sdk . client . SeFilter [ ] { bboxFilter } ; expCount = 2 ; actualCount = org . geotools . arcsde . data . ArcSDEJavaApiTest . getTempTableCount ( session , typeName , where , spatFilters , null ) ; org . junit . Assert . assertEquals ( expCount , actualCount ) ; } }
public class aTest{ @Test public void testMainForArchName ( ) { java . io . PrintStream out = System . out ; try { java . io . ByteArrayOutputStream buf = new java . io . ByteArrayOutputStream ( ) ; java . io . PrintStream tmpOut = new java . io . PrintStream ( buf ) ; java . lang . System . setOut ( tmpOut ) ; org . sqlite . OSInfo . main ( new java . lang . String [ ] { "--arch" } ) ; org . junit . Assert . assertEquals ( org . sqlite . OSInfo . getArchName ( ) , buf . toString ( ) ) ; } }
public class aTest{ @Test public void testParseNumberInputWidth3 ( ) { for ( int j = 0 ; j < ( org . zkoss . zss . engine . impl . ToGeneralTextTest . VAL3 . length ) ; ++ j ) { try { java . lang . String result = org . zkoss . poi . ss . util . NumberToGeneralTextConverter . toGeneralText ( ( ( java . lang . Number ) ( org . zkoss . zss . engine . impl . ToGeneralTextTest . VAL3 [ j ] [ 0 ] ) ) . doubleValue ( ) , Locale . US , 3 ) ; org . junit . Assert . assertEquals ( ( ( ( org . zkoss . zss . engine . impl . ToGeneralTextTest . VAL3 [ j ] [ 0 ] ) + "->" ) + j ) , org . zkoss . zss . engine . impl . ToGeneralTextTest . VAL3 [ j ] [ 1 ] , result ) ; } }
public class aTest{ @Test public void testUnmarshal ( ) { org . apache . camel . CamelContext camelctx = new org . apache . camel . impl . DefaultCamelContext ( ) ; camelctx . addRoutes ( new org . apache . camel . builder . RouteBuilder ( ) { @ org . wildfly . camel . test . lzf . Override public void configure ( ) throws org . wildfly . camel . test . lzf . Exception { from ( "direct:unmarshalTextToLzf" ) . marshal ( ) . lzf ( ) . unmarshal ( ) . lzf ( ) ; } } ) ; camelctx . start ( ) ; try { org . apache . camel . ProducerTemplate producer = camelctx . createProducerTemplate ( ) ; java . lang . String result = producer . requestBody ( "direct:unmarshalTextToLzf" , org . wildfly . camel . test . lzf . LzfDataFormatTest . TEXT . getBytes ( "UTF-8" ) , java . lang . String . class ) ; org . junit . Assert . assertEquals ( org . wildfly . camel . test . lzf . LzfDataFormatTest . TEXT , result ) ; } }
public class aTest{ @Test public void testUnresolvedHostWithFragmentCycle ( ) { org . eclipse . osgi . tests . container . dummys . DummyContainerAdaptor adaptor = createDummyAdaptor ( ) ; org . eclipse . osgi . container . ModuleContainer container = adaptor . getContainer ( ) ; java . util . Map < java . lang . String , java . lang . String > hostManifest = new java . util . HashMap < java . lang . String , java . lang . String > ( ) ; hostManifest . put ( org . osgi . framework . Constants . BUNDLE_MANIFESTVERSION , "1.0" 0 ) ; hostManifest . put ( org . osgi . framework . Constants . BUNDLE_SYMBOLICNAME , "host" ) ; hostManifest . put ( org . osgi . framework . Constants . BUNDLE_VERSION , "1.0" ) ; hostManifest . put ( org . osgi . framework . Constants . EXPORT_PACKAGE , "host" ) ; hostManifest . put ( org . osgi . framework . Constants . IMPORT_PACKAGE , "host.impl" ) ; installDummyModule ( hostManifest , "host10" , container ) ; hostManifest . put ( org . osgi . framework . Constants . BUNDLE_VERSION , "1.1" ) ; installDummyModule ( hostManifest , "host11" , container ) ; hostManifest . put ( org . osgi . framework . Constants . BUNDLE_VERSION , "1.2" ) ; installDummyModule ( hostManifest , "1.0" 4 , container ) ; java . util . Map < java . lang . String , java . lang . String > hostImplManifest = new java . util . HashMap < java . lang . String , java . lang . String > ( ) ; hostImplManifest . put ( org . osgi . framework . Constants . BUNDLE_MANIFESTVERSION , "1.0" 0 ) ; hostImplManifest . put ( org . osgi . framework . Constants . BUNDLE_SYMBOLICNAME , "host.impl" ) ; hostImplManifest . put ( org . osgi . framework . Constants . EXPORT_PACKAGE , "host.impl" ) ; hostImplManifest . put ( org . osgi . framework . Constants . IMPORT_PACKAGE , "host" ) ; hostImplManifest . put ( org . osgi . framework . Constants . FRAGMENT_HOST , "host" ) ; installDummyModule ( hostImplManifest , "hostImpl" , container ) ; java . util . Map < java . lang . String , java . lang . String > hostImporterManifest = new java . util . HashMap < java . lang . String , java . lang . String > ( ) ; hostImporterManifest . put ( org . osgi . framework . Constants . BUNDLE_MANIFESTVERSION , "1.0" 0 ) ; hostImporterManifest . put ( org . osgi . framework . Constants . BUNDLE_SYMBOLICNAME , "host.importer" ) ; hostImporterManifest . put ( org . osgi . framework . Constants . IMPORT_PACKAGE , "host" ) ; org . eclipse . osgi . container . Module hostImporter = installDummyModule ( hostImporterManifest , "hostImporter" , container ) ; org . eclipse . osgi . report . resolution . ResolutionReport report = container . resolve ( java . util . Arrays . asList ( hostImporter ) , true ) ; org . junit . Assert . assertNull ( "1.0" 2 , report . getResolutionException ( ) ) ; } }
public class aTest{ @Test public void metadata_xmlComplexTypeNamefalse ( ) { final java . lang . String entryName = "bar/90_contents/odatacol1/00_$metadata.xml" ; final java . lang . String filename = "/00_$metadata_complex_name_attr_notexist.xml" ; java . net . URL fileUrl = java . lang . ClassLoader . getSystemResource ( ( ( com . fujitsu . dc . test . unit . core . bar . BarFileValidateTest . RESOURCE_PATH ) + filename ) ) ; java . io . File file = new java . io . File ( fileUrl . getPath ( ) ) ; java . io . FileInputStream fis = null ; try { fis = new java . io . FileInputStream ( file ) ; com . fujitsu . dc . test . unit . core . bar . BarFileValidateTest . TestBarRunner testBarRunner = new com . fujitsu . dc . test . unit . core . bar . BarFileValidateTest . TestBarRunner ( ) ; boolean res = testBarRunner . registUserSchema ( entryName , fis , null ) ; org . junit . Assert . assertFalse ( res ) ; return ; } }
public class aTest{ @Test public void testPathToEAPZipSpecifiedWithSystemProperty_file ( ) { java . lang . System . setProperty ( "pathToEAPZip" , "/home/someUser/somePath/eap.zip" ) ; try { setupMockVersion ( org . richfaces . tests . qa . plugin . properties . eap . SimpleEAPPropertiesTest . version631 ) ; props = new org . richfaces . tests . qa . plugin . properties . eap . SimpleEAPProperties ( providerLinux ) ; org . junit . Assert . assertEquals ( new java . io . File ( "/home/someUser/somePath/eap.zip" ) , getZipFile ( ) ) ; } }
public class aTest{ @Test public void testBuildWithOrderBy ( ) { org . lnu . is . domain . asset . Asset context = new org . lnu . is . domain . asset . Asset ( ) ; org . lnu . is . pagination . OrderBy orderBy1 = new org . lnu . is . pagination . OrderBy ( "name" , org . lnu . is . pagination . OrderByType . ASC ) ; org . lnu . is . pagination . OrderBy orderBy2 = new org . lnu . is . pagination . OrderBy ( "invnum" , org . lnu . is . pagination . OrderByType . ASC ) ; org . lnu . is . pagination . OrderBy orderBy3 = new org . lnu . is . pagination . OrderBy ( "serialnum" , org . lnu . is . pagination . OrderByType . ASC ) ; org . lnu . is . pagination . OrderBy orderBy4 = new org . lnu . is . pagination . OrderBy ( "proddate" , org . lnu . is . pagination . OrderByType . ASC ) ; org . lnu . is . pagination . OrderBy orderBy5 = new org . lnu . is . pagination . OrderBy ( "begdate" , org . lnu . is . pagination . OrderByType . ASC ) ; org . lnu . is . pagination . OrderBy orderBy6 = new org . lnu . is . pagination . OrderBy ( "enddate" , org . lnu . is . pagination . OrderByType . ASC ) ; org . lnu . is . pagination . OrderBy orderBy7 = new org . lnu . is . pagination . OrderBy ( "begdate" 0 , org . lnu . is . pagination . OrderByType . ASC ) ; org . lnu . is . pagination . OrderBy orderBy8 = new org . lnu . is . pagination . OrderBy ( "amount" , org . lnu . is . pagination . OrderByType . ASC ) ; org . lnu . is . pagination . OrderBy orderBy9 = new org . lnu . is . pagination . OrderBy ( "suma" , org . lnu . is . pagination . OrderByType . ASC ) ; org . lnu . is . pagination . OrderBy orderBy10 = new org . lnu . is . pagination . OrderBy ( "description" , org . lnu . is . pagination . OrderByType . ASC ) ; java . util . List < org . lnu . is . pagination . OrderBy > orders = java . util . Arrays . asList ( orderBy1 , orderBy2 , orderBy3 , orderBy4 , orderBy5 , orderBy6 , orderBy7 , orderBy8 , orderBy9 , orderBy10 ) ; java . lang . String expectedSql = "SELECT<sp>e<sp>FROM<sp>Asset<sp>e<sp>WHERE<sp>e.status=:status<sp>AND<sp>e.crtUserGroup<sp>IN<sp>(:userGroups)<sp>ORDER<sp>BY<sp>e.name<sp>ASC,<sp>e.invnum<sp>ASC,<sp>e.serialnum<sp>ASC,<sp>e.proddate<sp>ASC,<sp>e.begdate<sp>ASC,<sp>e.enddate<sp>ASC,<sp>e.price<sp>ASC,<sp>e.amount<sp>ASC,<sp>e.suma<sp>ASC,<sp>e.description<sp>ASC" ; org . lnu . is . pagination . MultiplePagedSearch < org . lnu . is . domain . asset . Asset > pagedSearch = new org . lnu . is . pagination . MultiplePagedSearch ( ) ; pagedSearch . setEntity ( context ) ; pagedSearch . setOrders ( orders ) ; java . lang . String actualQuery = unit . build ( pagedSearch ) ; org . junit . Assert . assertEquals ( expectedSql , actualQuery ) ; } }
public class aTest{ @Test public void testReplaceDatanodeOnFailure ( ) { final org . apache . hadoop . conf . Configuration conf = new org . apache . hadoop . hdfs . HdfsConfiguration ( ) ; ReplaceDatanodeOnFailure . ALWAYS . write ( conf ) ; final java . lang . String [ ] racks = new java . lang . String [ org . apache . hadoop . hdfs . TestReplaceDatanodeOnFailure . REPLICATION ] ; java . util . Arrays . fill ( racks , org . apache . hadoop . hdfs . TestReplaceDatanodeOnFailure . RACK0 ) ; final org . apache . hadoop . hdfs . MiniDFSCluster cluster = new org . apache . hadoop . hdfs . MiniDFSCluster . Builder ( conf ) . racks ( racks ) . numDataNodes ( org . apache . hadoop . hdfs . TestReplaceDatanodeOnFailure . REPLICATION ) . build ( ) ; try { final org . apache . hadoop . hdfs . DistributedFileSystem fs = ( ( org . apache . hadoop . hdfs . DistributedFileSystem ) ( cluster . getFileSystem ( ) ) ) ; final org . apache . hadoop . fs . Path dir = new org . apache . hadoop . fs . Path ( org . apache . hadoop . hdfs . TestReplaceDatanodeOnFailure . DIR ) ; final org . apache . hadoop . hdfs . TestReplaceDatanodeOnFailure . SlowWriter [ ] slowwriters = new org . apache . hadoop . hdfs . TestReplaceDatanodeOnFailure . SlowWriter [ 10 ] ; for ( int i = 1 ; i <= ( slowwriters . length ) ; i ++ ) { slowwriters [ ( i - 1 ) ] = new org . apache . hadoop . hdfs . TestReplaceDatanodeOnFailure . SlowWriter ( fs , new org . apache . hadoop . fs . Path ( dir , ( "file" + i ) ) , ( i * 200L ) ) ; } for ( org . apache . hadoop . hdfs . TestReplaceDatanodeOnFailure . SlowWriter s : slowwriters ) { s . start ( ) ; } org . apache . hadoop . hdfs . TestReplaceDatanodeOnFailure . sleepSeconds ( 1 ) ; cluster . startDataNodes ( conf , 2 , true , null , new java . lang . String [ ] { org . apache . hadoop . hdfs . TestReplaceDatanodeOnFailure . RACK1 , org . apache . hadoop . hdfs . TestReplaceDatanodeOnFailure . RACK1 } ) ; cluster . stopDataNode ( org . apache . hadoop . hdfs . AppendTestUtil . nextInt ( org . apache . hadoop . hdfs . TestReplaceDatanodeOnFailure . REPLICATION ) ) ; org . apache . hadoop . hdfs . TestReplaceDatanodeOnFailure . sleepSeconds ( 5 ) ; for ( org . apache . hadoop . hdfs . TestReplaceDatanodeOnFailure . SlowWriter s : slowwriters ) { s . checkReplication ( ) ; s . interruptRunning ( ) ; } for ( org . apache . hadoop . hdfs . TestReplaceDatanodeOnFailure . SlowWriter s : slowwriters ) { s . joinAndClose ( ) ; } org . apache . hadoop . hdfs . TestReplaceDatanodeOnFailure . LOG . info ( "Verify<sp>the<sp>file" ) ; for ( int i = 0 ; i < ( slowwriters . length ) ; i ++ ) { org . apache . hadoop . hdfs . TestReplaceDatanodeOnFailure . LOG . info ( ( ( ( slowwriters [ i ] . filepath ) + ":<sp>length=" ) + ( fs . getFileStatus ( slowwriters [ i ] . filepath ) . getLen ( ) ) ) ) ; org . apache . hadoop . fs . FSDataInputStream in = null ; try { in = fs . open ( slowwriters [ i ] . filepath ) ; for ( int j = 0 , x ; ( x = in . read ( ) ) != ( - 1 ) ; j ++ ) { org . junit . Assert . assertEquals ( j , x ) ; } } }
public class aTest{ @Test public void testGrandParentSensitiveColumnNames ( ) { try { java . lang . String [ ] columnNames = getSensitiveColumnNames ( grandParentClass ) ; java . util . List < java . lang . String > actualColumnNames = java . util . Arrays . asList ( columnNames ) ; java . util . Collections . sort ( actualColumnNames ) ; java . util . List < java . lang . String > expectedColumnNames = getExpectedSensitiveColumnNamesOfGrandParent ( ) ; java . util . Collections . sort ( expectedColumnNames ) ; org . junit . Assert . assertEquals ( actualColumnNames , expectedColumnNames ) ; } }
public class aTest{ @Test public void testOpenSinglePartition ( ) { de . hub . cs . dbis . aeolus . spouts . TestOrderedFileInputSpout spout = new de . hub . cs . dbis . aeolus . spouts . TestOrderedFileInputSpout ( ) ; backtype . storm . Config conf = new backtype . storm . Config ( ) ; conf . put ( TestOrderedFileInputSpout . INPUT_FILE_NAME , "dummyFileName" ) ; java . io . FileReader fileReaderMock = org . powermock . api . mockito . PowerMockito . mock ( java . io . FileReader . class ) ; org . powermock . api . mockito . PowerMockito . whenNew ( java . io . FileReader . class ) . withArguments ( "dummyFileName" ) . thenReturn ( fileReaderMock ) ; java . io . BufferedReader bufferedReaderMock = org . powermock . api . mockito . PowerMockito . mock ( java . io . BufferedReader . class ) ; org . powermock . api . mockito . PowerMockito . whenNew ( java . io . BufferedReader . class ) . withArguments ( fileReaderMock ) . thenReturn ( bufferedReaderMock ) ; spout . open ( conf , mock ( backtype . storm . task . TopologyContext . class ) , mock ( backtype . storm . spout . SpoutOutputCollector . class ) ) ; org . junit . Assert . assertTrue ( spout . closePartition ( new java . lang . Integer ( 0 ) ) ) ; try { spout . closePartition ( new java . lang . Integer ( 1 ) ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void testSignatureForXtendField ( ) { try { org . eclipse . xtend2 . lib . StringConcatenation _builder = new org . eclipse . xtend2 . lib . StringConcatenation ( ) ; _builder . append ( "package<sp>testPackage" ) ; _builder . newLine ( ) ; _builder . append ( "import<sp>java.util.Collections" ) ; _builder . newLine ( ) ; _builder . append ( "import<sp>com.google.inject.Inject" ) ; _builder . newLine ( ) ; _builder . newLine ( ) ; _builder . append ( "class<sp>Foo<sp>{" ) ; _builder . newLine ( ) ; _builder . append ( "\t" ) ; _builder . append ( "@Inject" ) ; _builder . newLine ( ) ; _builder . append ( "\t" ) ; _builder . append ( "Collections<sp>collections" ) ; _builder . newLine ( ) ; _builder . append ( "\t" ) ; _builder . newLine ( ) ; _builder . append ( "\t" ) ; _builder . append ( "def<sp>bar(String<sp>a)<sp>throws<sp>NullPointerException" ) ; _builder . newLine ( ) ; _builder . append ( "}" ) ; _builder . newLine ( ) ; final org . eclipse . xtend . core . xtend . XtendFile xtendFile = this . parseHelper . parse ( _builder , this . getResourceSet ( ) ) ; final org . eclipse . xtend . core . xtend . XtendClass clazz = org . eclipse . xtext . xbase . lib . IterableExtensions . < org . eclipse . xtend . core . xtend . XtendClass > head ( com . google . common . collect . Iterables . < org . eclipse . xtend . core . xtend . XtendClass > filter ( xtendFile . getXtendTypes ( ) , org . eclipse . xtend . core . xtend . XtendClass . class ) ) ; org . eclipse . xtend . core . xtend . XtendMember _get = clazz . getMembers ( ) . get ( 0 ) ; final org . eclipse . xtend . core . xtend . XtendField xtendField = ( ( org . eclipse . xtend . core . xtend . XtendField ) ( _get ) ) ; final java . lang . String signature = this . signatureProvider . getSignature ( xtendField ) ; org . junit . Assert . assertEquals ( "Collections<sp>collections" , signature ) ; } }
public class aTest{ @Test public void testPingUDP ( ) { net . tomp2p . p2p . Peer sender = null ; net . tomp2p . p2p . Peer recv1 = null ; net . tomp2p . connection . ChannelCreator cc = null ; try { sender = new net . tomp2p . p2p . PeerBuilder ( new net . tomp2p . peers . Number160 ( "0x9876" ) ) . p2pId ( 55 ) . ports ( 2424 ) . start ( ) ; recv1 = new net . tomp2p . p2p . PeerBuilder ( new net . tomp2p . peers . Number160 ( "0x1234" ) ) . p2pId ( 55 ) . ports ( 8088 ) . start ( ) ; net . tomp2p . rpc . PingRPC handshake = new net . tomp2p . rpc . PingRPC ( sender . peerBean ( ) , sender . connectionBean ( ) ) ; net . tomp2p . futures . FutureChannelCreator fcc = recv1 . connectionBean ( ) . reservation ( ) . create ( 1 , 0 ) ; fcc . awaitUninterruptibly ( ) ; cc = fcc . channelCreator ( ) ; net . tomp2p . futures . FutureResponse fr = handshake . pingUDP ( recv1 . peerAddress ( ) , cc , new net . tomp2p . connection . DefaultConnectionConfiguration ( ) ) ; fr . awaitUninterruptibly ( ) ; org . junit . Assert . assertEquals ( true , fr . isSuccess ( ) ) ; } }
public class aTest{ @Test public void testAutoWildcard2 ( ) { java . lang . String grammar = "grammar<sp>T;\n" + ( ( ( ( "options<sp>{output=AST;}\n" + "a<sp>:<sp>ID<sp>INT<sp>-><sp>^(ID<sp>INT);\n" ) + "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" ) + "a" 0 ) + "a" 1 ) ; java . lang . String treeGrammar = "tree<sp>grammar<sp>TP;\n" + ( ( "options<sp>{output=AST;<sp>ASTLabelType=CommonTree;<sp>tokenVocab=T;}\n" + "a" 2 ) + "<sp>;\n" ) ; java . lang . String found = execTreeParser ( "a" 7 , grammar , "TParser" , "a" 6 , treeGrammar , "TP" , "a" 3 , "a" , "a" , "a" 4 ) ; org . junit . Assert . assertEquals ( "a" 5 , found ) ; } }
public class aTest{ @Test public void testGetValues ( ) { com . google . common . collect . ImmutableSet < java . lang . @ org . eclipse . jdt . annotation . NonNull String > values = com . google . common . collect . ImmutableSet . of ( org . eclipse . tracecompass . tmf . core . tests . analysis . requirements . AnalysisRequirementTest . VALUE_A , org . eclipse . tracecompass . tmf . core . tests . analysis . requirements . AnalysisRequirementTest . VALUE_B ) ; org . eclipse . tracecompass . tmf . core . analysis . requirements . TmfAbstractAnalysisRequirement requirement = new org . eclipse . tracecompass . tmf . tests . stubs . analysis . AnalysisRequirementFactory . TmfRequirementStub ( values , org . eclipse . tracecompass . tmf . core . analysis . requirements . TmfAbstractAnalysisRequirement . PriorityLevel . OPTIONAL ) ; org . junit . Assert . assertEquals ( values , requirement . getValues ( ) ) ; } }
public class aTest{ @Test public void testParse ( ) { try { java . lang . String s = "47.421217,<sp>10.986314" ; de . nx42 . maps4cim . util . gis . Coordinate expResult = de . nx42 . maps4cim . util . gis . CoordinateTest . instance ; de . nx42 . maps4cim . util . gis . Coordinate result = de . nx42 . maps4cim . util . gis . Coordinate . parse ( s ) ; org . junit . Assert . assertEquals ( expResult , result ) ; } }
public class aTest{ @Test public void testCreateProductAttributeMedia ( ) { try { java . io . InputStream is = this . getClass ( ) . getClassLoader ( ) . getResourceAsStream ( "img.gif" ) ; upsertPayloadContentOnTestRunMessage ( is ) ; java . lang . String newImageFilename = runFlowAndGetPayload ( "create-product-attribute-media" ) ; org . junit . Assert . assertNotNull ( newImageFilename ) ; } }
public class aTest{ @Test public void OAuthImplicitGrantForDesktopMobile_GetAuthorizationUrl_ReturnsCorrectUrl_WithEmptyState ( ) { com . microsoft . bingads . OAuthDesktopMobileImplicitGrant auth = com . microsoft . bingads . internal . OAuthImplicitGrantForDesktopMobileAppTest . CreateAuth ( "test_id" ) ; auth . setState ( "" ) ; java . net . URL authorizationUrl = auth . getAuthorizationEndpoint ( ) ; try { java . net . URL expectedUrl = new java . net . URL ( ( "https://login.live.com/oauth20_authorize.srf?" + ( ( ( "scope=bingads.manage&" + "response_type=token&" ) + "redirect_uri=https%3A%2F%2Flogin.live.com%2Foauth20_desktop.srf&" ) + "client_id=test_id" ) ) ) ; org . junit . Assert . assertEquals ( expectedUrl , authorizationUrl ) ; } }
public class aTest{ @Test public void testLabeledNotBlockSet ( ) { org . antlr . tool . Grammar g = new org . antlr . tool . Grammar ( ( "lexer<sp>grammar<sp>P;\n" + ".s0->.s1\n" 0 ) ) ; java . lang . String expecting = ".s0->.s1\n" + ( ( ( ".s1->.s2\n" + ".s2-{\'\\u0000\'..\'2\',<sp>\'4\'..\'a\',<sp>\'c\'..\'\\uFFFF\'}->.s3\n" ) + ".s3->:s4\n" ) + ":s4-<EOT>->.s5\n" ) ; checkRule ( g , "A" , expecting ) ; java . lang . String expectingGrammarStr = "1:7:<sp>lexer<sp>grammar<sp>P;\n" + ( "A<sp>:<sp>t=~<sp>(<sp>\'3\'<sp>|<sp>\'b\'<sp>)<sp>;\n" + "Tokens<sp>:<sp>A<sp>;" ) ; org . junit . Assert . assertEquals ( expectingGrammarStr , g . toString ( ) ) ; } }
public class aTest{ @Test public void testIsLastSubscription_NegativeTwoTechnicalProducts ( ) { runTX ( new java . util . concurrent . Callable < java . lang . Void > ( ) { @ org . oscm . domobjects . Override public org . oscm . domobjects . Void call ( ) throws org . oscm . domobjects . Exception { supplier = org . oscm . test . data . Organizations . createOrganization ( mgr , OrganizationRoleType . TECHNOLOGY_PROVIDER , OrganizationRoleType . SUPPLIER ) ; org . oscm . domobjects . TechnicalProduct tProd1 = org . oscm . test . data . TechnicalProducts . createTechnicalProduct ( mgr , supplier , "tp1" , false , ServiceAccessType . LOGIN ) ; tProd1 . setAllowingOnBehalfActing ( true ) ; org . oscm . domobjects . Product prod1 = createProduct ( tProd1 , "prod1" ) ; createSubscription ( prod1 , "Subscription1" , supplier ) ; org . oscm . domobjects . TechnicalProduct tProd2 = org . oscm . test . data . TechnicalProducts . createTechnicalProduct ( mgr , supplier , "tp2" , false , ServiceAccessType . LOGIN ) ; tProd2 . setAllowingOnBehalfActing ( true ) ; org . oscm . domobjects . Product prod2 = createProduct ( tProd2 , "prod2" ) ; createSubscription ( prod2 , "Subscription2" , supplier ) ; return null ; } } ) ; runTX ( new java . util . concurrent . Callable < java . lang . Void > ( ) { @ org . oscm . domobjects . Override public org . oscm . domobjects . Void call ( ) throws org . oscm . domobjects . Exception { javax . persistence . Query query = mgr . createNamedQuery ( "Subscription.hasSubscriptionsBasedOnOnBehalfServicesForTp" ) ; query . setParameter ( "tpOrgKey" , java . lang . Long . valueOf ( supplier . getKey ( ) ) ) ; java . lang . Object result = query . getSingleResult ( ) ; org . junit . Assert . assertEquals ( java . lang . Long . valueOf ( 2 ) , result ) ; return null ; } } }
public class aTest{ @Test public void testStandbyTriggersLogRollsWhenTailInProgressEdits ( ) { final int standbyCatchupWaitTime = 2 ; final int noLogRollWaitTime = 2 ; final int logRollWaitTime = 3 ; org . apache . hadoop . conf . Configuration conf = org . apache . hadoop . hdfs . server . namenode . ha . TestEditLogTailer . getConf ( ) ; conf . setInt ( DFSConfigKeys . DFS_HA_LOGROLL_PERIOD_KEY , ( ( standbyCatchupWaitTime + noLogRollWaitTime ) + 1 ) ) ; conf . setInt ( DFSConfigKeys . DFS_HA_TAILEDITS_PERIOD_KEY , 1 ) ; conf . setBoolean ( DFSConfigKeys . DFS_HA_TAILEDITS_INPROGRESS_KEY , true ) ; org . apache . hadoop . hdfs . MiniDFSCluster cluster = org . apache . hadoop . hdfs . server . namenode . ha . TestEditLogTailer . createMiniDFSCluster ( conf , 2 ) ; if ( cluster == null ) { org . junit . Assert . fail ( "failed<sp>to<sp>start<sp>mini<sp>cluster." ) ; } try { int activeIndex = ( new java . util . Random ( ) . nextBoolean ( ) ) ? 1 : 0 ; int standbyIndex = ( activeIndex == 0 ) ? 1 : 0 ; cluster . transitionToActive ( activeIndex ) ; org . apache . hadoop . hdfs . server . namenode . NameNode active = cluster . getNameNode ( activeIndex ) ; org . apache . hadoop . hdfs . server . namenode . NameNode standby = cluster . getNameNode ( standbyIndex ) ; long origTxId = active . getNamesystem ( ) . getFSImage ( ) . getEditLog ( ) . getCurSegmentTxId ( ) ; for ( int i = 0 ; i < ( ( org . apache . hadoop . hdfs . server . namenode . ha . TestEditLogTailer . DIRS_TO_MAKE ) / 2 ) ; i ++ ) { org . apache . hadoop . hdfs . server . namenode . NameNodeAdapter . mkdirs ( active , org . apache . hadoop . hdfs . server . namenode . ha . TestEditLogTailer . getDirPath ( i ) , new org . apache . hadoop . fs . permission . PermissionStatus ( "test" , "test" , new org . apache . hadoop . fs . permission . FsPermission ( ( ( short ) ( 493 ) ) ) ) , true ) ; } long activeTxId = active . getNamesystem ( ) . getFSImage ( ) . getEditLog ( ) . getLastWrittenTxId ( ) ; org . apache . hadoop . hdfs . server . namenode . ha . TestEditLogTailer . waitForStandbyToCatchUpWithInProgressEdits ( standby , activeTxId , standbyCatchupWaitTime ) ; for ( int i = ( org . apache . hadoop . hdfs . server . namenode . ha . TestEditLogTailer . DIRS_TO_MAKE ) / 2 ; i < ( org . apache . hadoop . hdfs . server . namenode . ha . TestEditLogTailer . DIRS_TO_MAKE ) ; i ++ ) { org . apache . hadoop . hdfs . server . namenode . NameNodeAdapter . mkdirs ( active , org . apache . hadoop . hdfs . server . namenode . ha . TestEditLogTailer . getDirPath ( i ) , new org . apache . hadoop . fs . permission . PermissionStatus ( "test" , "test" , new org . apache . hadoop . fs . permission . FsPermission ( ( ( short ) ( 493 ) ) ) ) , true ) ; } boolean exceptionThrown = false ; try { org . apache . hadoop . hdfs . server . namenode . ha . TestEditLogTailer . checkForLogRoll ( active , origTxId , noLogRollWaitTime ) ; } catch ( java . util . concurrent . TimeoutException e ) { exceptionThrown = true ; } org . junit . Assert . assertTrue ( exceptionThrown ) ; org . apache . hadoop . hdfs . server . namenode . ha . TestEditLogTailer . checkForLogRoll ( active , origTxId , logRollWaitTime ) ; } }
public class aTest{ @Test public void test_withJniInchi ( ) { java . lang . String mdlInput = "<sp>4<sp>6<sp>2<sp>0<sp>0<sp>0<sp>0\n" 9 + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "<sp>4<sp>5<sp>1<sp>0<sp>0<sp>0<sp>0\n" 1 + "<sp>4<sp>6<sp>2<sp>0<sp>0<sp>0<sp>0\n" 9 ) + "<sp>4<sp>6<sp>2<sp>0<sp>0<sp>0<sp>0\n" 4 ) + "<sp>4<sp>6<sp>2<sp>0<sp>0<sp>0<sp>0\n" 0 ) + "<sp>4<sp>6<sp>2<sp>0<sp>0<sp>0<sp>0\n" 5 ) + "<sp>0.7145<sp>-0.4125<sp>0.0000<sp>C<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0\n" ) + "<sp>4<sp>6<sp>2<sp>0<sp>0<sp>0<sp>0\n" 3 ) + "<sp>-0.7145<sp>-0.4125<sp>0.0000<sp>N<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0\n" ) + "<sp>0.0000<sp>0.8250<sp>0.0000<sp>N<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0\n" ) + "<sp>0.7145<sp>1.2375<sp>0.0000<sp>C<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0<sp>0\n" ) + "<sp>4<sp>6<sp>2<sp>0<sp>0<sp>0<sp>0\n" 2 ) + "<sp>4<sp>6<sp>2<sp>0<sp>0<sp>0<sp>0\n" 1 ) + "<sp>4<sp>6<sp>2<sp>0<sp>0<sp>0<sp>0\n" 6 ) + "<sp>4<sp>6<sp>2<sp>0<sp>0<sp>0<sp>0\n" 8 ) + "<sp>3<sp>4<sp>1<sp>0<sp>0<sp>0<sp>0\n" ) + "<sp>4<sp>5<sp>1<sp>0<sp>0<sp>0<sp>0\n" ) + "<sp>4<sp>6<sp>2<sp>0<sp>0<sp>0<sp>0\n" ) + "<sp>4<sp>6<sp>2<sp>0<sp>0<sp>0<sp>0\n" 7 ) + "<sp>7<sp>8<sp>1<sp>0<sp>0<sp>0<sp>0\n" ) + "<sp>7<sp>9<sp>2<sp>0<sp>0<sp>0<sp>0\n" ) + "<sp>4<sp>5<sp>1<sp>0<sp>0<sp>0<sp>0\n" 0 ) + "M<sp>END\n" ) ; org . openscience . cdk . io . MDLV2000Reader reader = new org . openscience . cdk . io . MDLV2000Reader ( new java . io . StringReader ( mdlInput ) ) ; org . openscience . cdk . interfaces . IAtomContainer molecule = reader . read ( new org . openscience . cdk . AtomContainer ( ) ) ; org . openscience . cdk . tools . manipulator . AtomContainerManipulator . percieveAtomTypesAndConfigureAtoms ( molecule ) ; org . openscience . cdk . tools . CDKHydrogenAdder hAdder = org . openscience . cdk . tools . CDKHydrogenAdder . getInstance ( molecule . getBuilder ( ) ) ; hAdder . addImplicitHydrogens ( molecule ) ; java . util . List < org . openscience . cdk . interfaces . IAtomContainer > tautomers = tautomerGenerator . getTautomers ( molecule ) ; org . junit . Assert . assertEquals ( 5 , tautomers . size ( ) ) ; } }
public class aTest{ @Test public void testAppendLessThanChecksumChunk ( ) { final byte [ ] buf = new byte [ 1024 ] ; final org . apache . hadoop . hdfs . MiniDFSCluster cluster = new org . apache . hadoop . hdfs . MiniDFSCluster . Builder ( new org . apache . hadoop . hdfs . HdfsConfiguration ( ) ) . numDataNodes ( 1 ) . build ( ) ; cluster . waitActive ( ) ; try ( org . apache . hadoop . hdfs . DistributedFileSystem fs = cluster . getFileSystem ( ) ) { final int len1 = 200 ; final int len2 = 300 ; final org . apache . hadoop . fs . Path p = new org . apache . hadoop . fs . Path ( "/foo" ) ; org . apache . hadoop . fs . FSDataOutputStream out = fs . create ( p ) ; out . write ( buf , 0 , len1 ) ; out . close ( ) ; out = fs . append ( p ) ; out . write ( buf , 0 , len2 ) ; out . hflush ( ) ; org . apache . hadoop . fs . FSDataInputStream in = fs . open ( p ) ; final int length = in . read ( 0 , buf , 0 , ( len1 + len2 ) ) ; org . junit . Assert . assertTrue ( ( length > 0 ) ) ; in . close ( ) ; out . close ( ) ; } }
public class aTest{ @Test public void thenSend ( ) { final io . trane . ndbc . proto . ClientMessage msg = new io . trane . ndbc . proto . ClientMessage ( ) { } ; final io . trane . ndbc . proto . Channel channel = new io . trane . ndbc . proto . ExchangeTest . TestChannel ( ) { @ io . trane . ndbc . proto . Override public < T extends io . trane . ndbc . proto . ClientMessage > io . trane . future . Future < java . lang . Void > send ( final io . trane . ndbc . proto . Marshaller < T > marshaller , final T m ) { org . junit . Assert . assertEquals ( msg , m ) ; return io . trane . future . Future . VOID ; } } }
public class aTest{ @Test public void testHornerForm1 ( ) { org . apache . commons . math3 . random . RandomGenerator rnd = getRandom ( ) ; org . apache . commons . math3 . random . RandomDataGenerator rndd = getRandomData ( ) ; cc . redberry . rings . Ring < cc . redberry . rings . bigint . BigInteger > ring = cc . redberry . rings . Rings . Zp ( 17 ) ; org . apache . commons . math3 . stat . descriptive . DescriptiveStatistics hornerStat = new org . apache . commons . math3 . stat . descriptive . DescriptiveStatistics ( ) ; org . apache . commons . math3 . stat . descriptive . DescriptiveStatistics hornerCreateStat = new org . apache . commons . math3 . stat . descriptive . DescriptiveStatistics ( ) ; org . apache . commons . math3 . stat . descriptive . DescriptiveStatistics plainStat = new org . apache . commons . math3 . stat . descriptive . DescriptiveStatistics ( ) ; long start ; long elapsed ; int nIterations = 100 ; int nEvaluations = its ( 10 , 10 ) ; int nVars = 5 ; int minDeg = 3250 ; int minSize = 1000 ; int [ ] varsSeq = cc . redberry . rings . util . ArraysUtil . sequence ( 0 , nVars ) ; for ( int i = 0 ; i < nIterations ; ++ i ) { if ( i == ( nIterations / 10 ) ) java . util . Arrays . asList ( hornerStat , hornerCreateStat , plainStat ) . forEach ( DescriptiveStatistics :: clear ) ; cc . redberry . rings . poly . multivar . MultivariatePolynomial . MultivariatePolynomial < cc . redberry . rings . bigint . BigInteger > p = cc . redberry . rings . poly . multivar . RandomMultivariatePolynomials . randomPolynomial ( nVars , rndd . nextInt ( minDeg , ( 2 * minDeg ) ) , rndd . nextInt ( minSize , ( 2 * minSize ) ) , ring , MonomialOrder . DEFAULT , rnd ) ; int [ ] variables = new int [ 3 + ( rnd . nextInt ( ( ( p . nVariables ) - 3 ) ) ) ] ; cc . redberry . rings . bigint . BigInteger [ ] values = new cc . redberry . rings . bigint . BigInteger [ variables . length ] ; cc . redberry . rings . util . ArraysUtil . shuffle ( varsSeq , rnd ) ; java . lang . System . arraycopy ( varsSeq , 0 , variables , 0 , values . length ) ; start = java . lang . System . nanoTime ( ) ; cc . redberry . rings . poly . multivar . HornerForm < cc . redberry . rings . bigint . BigInteger > hornerForm = p . getHornerForm ( variables ) ; elapsed = ( java . lang . System . nanoTime ( ) ) - start ; hornerStat . addValue ( elapsed ) ; hornerCreateStat . addValue ( elapsed ) ; for ( int nEval = 0 ; nEval < nEvaluations ; ++ nEval ) { for ( int j = 0 ; j < ( values . length ) ; j ++ ) values [ j ] = ring . randomElement ( rnd ) ; start = java . lang . System . nanoTime ( ) ; cc . redberry . rings . poly . multivar . MultivariatePolynomial . MultivariatePolynomial < cc . redberry . rings . bigint . BigInteger > horner = hornerForm . evaluate ( values ) ; elapsed = ( java . lang . System . nanoTime ( ) ) - start ; hornerStat . addValue ( elapsed ) ; start = java . lang . System . nanoTime ( ) ; cc . redberry . rings . poly . multivar . MultivariatePolynomial . MultivariatePolynomial < cc . redberry . rings . bigint . BigInteger > plain = p . eliminate ( variables , values ) ; elapsed = ( java . lang . System . nanoTime ( ) ) - start ; plainStat . addValue ( elapsed ) ; org . junit . Assert . assertEquals ( plain , horner ) ; } } }
public class aTest{ @Test public void testReadZeroBytesOnEmptyFile ( ) { org . apache . activemq . artemis . jdbc . store . file . JDBCSequentialFile file = ( ( org . apache . activemq . artemis . jdbc . store . file . JDBCSequentialFile ) ( factory . createSequentialFile ( "test.txt" ) ) ) ; file . open ( ) ; try { final java . nio . ByteBuffer readBuffer = java . nio . ByteBuffer . allocate ( 0 ) ; final int bytes = file . read ( readBuffer ) ; org . junit . Assert . assertEquals ( 0 , bytes ) ; } }
public class aTest{ @Test public void testSequentialCreation ( ) { org . drools . builder . KnowledgeBuilder knowledgeBuilder = org . drools . builder . KnowledgeBuilderFactory . newKnowledgeBuilder ( ) ; knowledgeBuilder . add ( new org . drools . io . impl . ClassPathResource ( "fuzzyDL/testLatticeBuilding.drl" ) , ResourceType . DRL ) ; if ( knowledgeBuilder . hasErrors ( ) ) { org . junit . Assert . fail ( knowledgeBuilder . getErrors ( ) . toString ( ) ) ; } org . drools . RuleBaseConfiguration rbC = new org . drools . RuleBaseConfiguration ( ) ; rbC . setAssertBehaviour ( AssertBehaviour . EQUALITY ) ; org . drools . KnowledgeBase knowledgeBase = org . drools . KnowledgeBaseFactory . newKnowledgeBase ( rbC ) ; knowledgeBase . addKnowledgePackages ( knowledgeBuilder . getKnowledgePackages ( ) ) ; org . drools . runtime . StatefulKnowledgeSession kSession = knowledgeBase . newStatefulKnowledgeSession ( ) ; kSession . fireAllRules ( ) ; org . drools . definition . type . FactType type = knowledgeBase . getFactType ( "B" 1 , "SubConceptOf" ) ; java . util . Collection facts = kSession . getObjects ( new org . drools . ClassObjectFilter ( type . getFactClass ( ) ) ) ; org . junit . Assert . assertEquals ( 12 , facts . size ( ) ) ; try { facts . contains ( newSC ( type , "A" , "B" ) ) ; facts . contains ( newSC ( type , "A" , "B" 0 ) ) ; facts . contains ( newSC ( type , "B" , "B" 2 ) ) ; facts . contains ( newSC ( type , "B" 0 , "B" 2 ) ) ; facts . contains ( newSC ( type , "D" , "B" ) ) ; facts . contains ( newSC ( type , "B" 2 , "All" ) ) ; facts . contains ( newSC ( type , "F" , "A" ) ) ; facts . contains ( newSC ( type , "F" , "D" ) ) ; facts . contains ( newSC ( type , "G" , "B" ) ) ; facts . contains ( newSC ( type , "G" , "B" 0 ) ) ; facts . contains ( newSC ( type , "H" , "G" ) ) ; facts . contains ( newSC ( type , "I" , "All" ) ) ; } }
public class aTest{ @Test public void test ( ) { check ( "SQL" , ( ) -> org . pentaho . di . trans . steps . sql . meta . getSql ( ) ) ; check ( "EXECUTE_FOR_EACH_ROW" , ( ) -> org . pentaho . di . trans . steps . sql . meta . isExecutedEachInputRow ( ) ) ; check ( "SQL" 4 , ( ) -> org . pentaho . di . trans . steps . sql . meta . getUpdateField ( ) ) ; check ( "INSERT_STATS_FIELD" , ( ) -> org . pentaho . di . trans . steps . sql . meta . getInsertField ( ) ) ; check ( "DELETE_STATS_FIELD" , ( ) -> org . pentaho . di . trans . steps . sql . meta . getDeleteField ( ) ) ; check ( "READ_STATS_FIELD" , ( ) -> org . pentaho . di . trans . steps . sql . meta . getReadField ( ) ) ; check ( "EXECUTE_AS_SINGLE_STATEMENT" , ( ) -> org . pentaho . di . trans . steps . sql . meta . isSingleStatement ( ) ) ; check ( "REPLACE_VARIABLES" , ( ) -> org . pentaho . di . trans . steps . sql . meta . isReplaceVariables ( ) ) ; check ( "QUOTE_STRINGS" , ( ) -> org . pentaho . di . trans . steps . sql . meta . isQuoteString ( ) ) ; check ( "SQL" 3 , ( ) -> org . pentaho . di . trans . steps . sql . meta . isParams ( ) ) ; check ( "SQL" 0 , ( ) -> org . pentaho . di . trans . steps . sql . meta . getArguments ( ) [ 0 ] ) ; skipPropertyTest ( "CONNECTIONNAME" ) ; final org . pentaho . di . core . database . DatabaseMeta db1 = new org . pentaho . di . core . database . DatabaseMeta ( ) ; db1 . setName ( "SQL" 1 ) ; final org . pentaho . di . core . database . DatabaseMeta db2 = new org . pentaho . di . core . database . DatabaseMeta ( ) ; db2 . setName ( "SQL" 2 ) ; final org . pentaho . di . core . database . DatabaseMeta db3 = new org . pentaho . di . core . database . DatabaseMeta ( ) ; db3 . setName ( "my<sp>connection<sp>3" ) ; final java . util . List < org . pentaho . di . shared . SharedObjectInterface > mockDbs = java . util . Arrays . asList ( new org . pentaho . di . shared . SharedObjectInterface [ ] { db1 , db2 , db3 } ) ; final org . pentaho . di . trans . step . StepMeta parentStepMeta = org . mockito . Mockito . mock ( org . pentaho . di . trans . step . StepMeta . class ) ; final org . pentaho . di . trans . TransMeta parentTransMeta = org . mockito . Mockito . mock ( org . pentaho . di . trans . TransMeta . class ) ; org . mockito . Mockito . doReturn ( mockDbs ) . when ( parentTransMeta ) . getDatabases ( ) ; org . mockito . Mockito . doReturn ( parentTransMeta ) . when ( parentStepMeta ) . getParentTransMeta ( ) ; meta . setParentStepMeta ( parentStepMeta ) ; injector . setProperty ( meta , "CONNECTIONNAME" , setValue ( new org . pentaho . di . core . row . value . ValueMetaString ( "SQL" 2 ) , "SQL" 2 ) , "SQL" 2 ) ; org . junit . Assert . assertEquals ( db2 , meta . getDatabaseMeta ( ) ) ; } }
public class aTest{ @Test public void testAutovivificationOfVariableRestrictions ( ) { final java . lang . String drl = ( ( ( ( ( ( ( ( ( ( "package<sp>org.drools.compiler.integrationtests.drl;\n" + "<sp>);\n" 2 ) + ( org . drools . testcoverage . common . model . Cheese . class . getCanonicalName ( ) ) ) + "<sp>);\n" 0 ) + "<sp>);\n" 4 ) + "\n" ) + "rule<sp>\"autovivification\"\n" ) + "when\n" ) + "<sp>Cheese(<sp>price<sp>><sp>oldPrice,<sp>price<sp>><sp>this.oldPrice<sp>)\n" ) + "<sp>);\n" 1 ) + "<sp>results.add(<sp>\"OK\"<sp>);\n" ) + "end" ; final org . kie . api . KieBase kbase = org . drools . testcoverage . common . util . KieBaseUtil . getKieBaseFromKieModuleFromDrl ( "<sp>);\n" 3 , kieBaseTestConfiguration , drl ) ; final org . kie . api . runtime . KieSession ksession = kbase . newKieSession ( ) ; try { final java . util . List results = new java . util . ArrayList ( ) ; ksession . setGlobal ( "results" , results ) ; final org . drools . testcoverage . common . model . Cheese stilton = new org . drools . testcoverage . common . model . Cheese ( "stilton" ) ; stilton . setPrice ( 10 ) ; stilton . setOldPrice ( 8 ) ; ksession . insert ( stilton ) ; ksession . fireAllRules ( ) ; org . junit . Assert . assertEquals ( 1 , results . size ( ) ) ; } }
public class aTest{ @Test public void shouldSimplifyQuery ( ) { try { java . io . BufferedReader is = org . sourceforge . xsparql . test . Utils . loadReaderFromClasspath ( filename ) ; final org . sourceforge . xsparql . rewriter . XSPARQLLexer lexer = new org . sourceforge . xsparql . rewriter . XSPARQLLexer ( is ) ; final org . antlr . runtime . CommonTokenStream tokenStream = new org . antlr . runtime . CommonTokenStream ( lexer ) ; org . antlr . runtime . tree . CommonTree tree = processor . parse ( tokenStream ) ; org . sourceforge . xsparql . rewriter . Helper . printTree ( tree ) ; org . junit . Assume . assumeTrue ( ( ( processor . getNumberOfSyntaxErrors ( ) ) == 0 ) ) ; tree = processor . rewrite ( tokenStream , tree ) ; org . sourceforge . xsparql . rewriter . Helper . printTree ( tree ) ; org . junit . Assume . assumeTrue ( ( ( processor . getNumberOfSyntaxErrors ( ) ) == 0 ) ) ; org . sourceforge . xsparql . rewriter . XSPARQLProcessorTests . logger . debug ( "Start<sp>Simplifier<sp>for<sp>{}" , filename ) ; processor . setDebug ( true ) ; processor . setVerbose ( true ) ; tree = processor . simplify ( tokenStream , tree ) ; org . junit . Assert . assertEquals ( 0 , processor . getNumberOfSyntaxErrors ( ) ) ; org . sourceforge . xsparql . rewriter . Helper . printTree ( tree ) ; } }
public class aTest{ @Test public void testAnalyzeSelectStatement_OK_WhereCondition_Priority ( ) { final java . lang . String sql = "select<sp>sum(num_user_the_years),<sp>sum(mobile_idx)<sp>" + ( ( ( ( ( ( ( "from<sp>user_profile<sp>" + "where<sp>user_log_acct<sp>like<sp>'a%'<sp>" ) + "or<sp>user_log_acct<sp>like<sp>'b%'<sp>" ) + "mobile_idx" 3 ) + "or<sp>user_log_acct<sp>like<sp>'d%'<sp>" ) + "mobile_idx" 6 ) + "or<sp>user_log_acct<sp>like<sp>'f%'<sp>" ) + "mobile_idx" 5 ) ; final org . codefamily . crabs . jdbc . lang . Statement actual = org . codefamily . crabs . jdbc . compiler . GrammarAnalyzer . analyze ( sql ) ; final org . codefamily . crabs . jdbc . lang . Statement expected = new org . codefamily . crabs . jdbc . lang . extension . statement . SelectStatement ( new org . codefamily . crabs . jdbc . compiler . SelectClause ( false , null , new org . codefamily . crabs . jdbc . compiler . SelectClause . ResultColumnDeclare ( ( ( java . lang . String ) ( null ) ) , new org . codefamily . crabs . jdbc . compiler . SummaryFunction ( new org . codefamily . crabs . jdbc . lang . expression . Reference ( null , "mobile_idx" 8 ) ) ) , new org . codefamily . crabs . jdbc . compiler . SelectClause . ResultColumnDeclare ( ( ( java . lang . String ) ( null ) ) , new org . codefamily . crabs . jdbc . compiler . SummaryFunction ( new org . codefamily . crabs . jdbc . lang . expression . Reference ( null , "mobile_idx" ) ) ) ) , new org . codefamily . crabs . jdbc . compiler . FromClause ( new org . codefamily . crabs . jdbc . compiler . FromClause . SimpleTableDeclare ( null , "mobile_idx" 4 ) ) , new org . codefamily . crabs . jdbc . compiler . WhereClause ( new org . codefamily . crabs . jdbc . compiler . OrExpression ( new org . codefamily . crabs . jdbc . compiler . OrExpression ( new org . codefamily . crabs . jdbc . compiler . OrExpression ( new org . codefamily . crabs . jdbc . compiler . OrExpression ( new org . codefamily . crabs . jdbc . compiler . OrExpression ( new org . codefamily . crabs . jdbc . compiler . LikeExpression ( new org . codefamily . crabs . jdbc . lang . expression . Reference ( null , "user_log_acct" ) , new org . codefamily . crabs . jdbc . lang . expression . Constant ( "a%" ) ) , new org . codefamily . crabs . jdbc . compiler . LikeExpression ( new org . codefamily . crabs . jdbc . lang . expression . Reference ( null , "user_log_acct" ) , new org . codefamily . crabs . jdbc . lang . expression . Constant ( "mobile_idx" 1 ) ) ) , new org . codefamily . crabs . jdbc . compiler . LikeExpression ( new org . codefamily . crabs . jdbc . lang . expression . Reference ( null , "user_log_acct" ) , new org . codefamily . crabs . jdbc . lang . expression . Constant ( "mobile_idx" 0 ) ) ) , new org . codefamily . crabs . jdbc . compiler . LikeExpression ( new org . codefamily . crabs . jdbc . lang . expression . Reference ( null , "user_log_acct" ) , new org . codefamily . crabs . jdbc . lang . expression . Constant ( "mobile_idx" 2 ) ) ) , new org . codefamily . crabs . jdbc . compiler . LikeExpression ( new org . codefamily . crabs . jdbc . lang . expression . Reference ( null , "user_log_acct" ) , new org . codefamily . crabs . jdbc . lang . expression . Constant ( "mobile_idx" 7 ) ) ) , new org . codefamily . crabs . jdbc . compiler . LikeExpression ( new org . codefamily . crabs . jdbc . lang . expression . Reference ( null , "user_log_acct" ) , new org . codefamily . crabs . jdbc . lang . expression . Constant ( "f%" ) ) ) ) , new org . codefamily . crabs . jdbc . compiler . GroupByClause ( new org . codefamily . crabs . jdbc . lang . expression . Reference ( null , "mobile_idx" 8 ) , new org . codefamily . crabs . jdbc . lang . expression . Reference ( null , "mobile_idx" ) ) , null , null , null ) ; org . junit . Assert . assertEquals ( expected , actual ) ; } }
public class aTest{ @Test public void testEmpty ( ) { org . apache . accumulo . core . client . summary . SummarizerConfiguration sc = org . apache . accumulo . core . client . summary . SummarizerConfiguration . builder ( org . apache . accumulo . core . client . summary . summarizers . EntryLengthSummarizer . class ) . build ( ) ; org . apache . accumulo . core . client . summary . summarizers . EntryLengthSummarizer entrySum = new org . apache . accumulo . core . client . summary . summarizers . EntryLengthSummarizer ( ) ; org . apache . accumulo . core . client . summary . Summarizer . Collector collector = entrySum . collector ( sc ) ; java . util . HashMap < java . lang . String , java . lang . Long > stats = new java . util . HashMap ( ) ; collector . summarize ( stats :: put ) ; java . util . HashMap < java . lang . String , java . lang . Long > expected = new java . util . HashMap ( ) ; expected . put ( "key.min" , 0L ) ; expected . put ( "key.max" , 0L ) ; expected . put ( "key.min" 3 , 0L ) ; expected . put ( "row.min" , 0L ) ; expected . put ( "key.min" 2 , 0L ) ; expected . put ( "row.sum" , 0L ) ; expected . put ( "family.min" , 0L ) ; expected . put ( "key.min" 4 , 0L ) ; expected . put ( "family.sum" , 0L ) ; expected . put ( "qualifier.min" , 0L ) ; expected . put ( "key.min" 1 , 0L ) ; expected . put ( "key.min" 6 , 0L ) ; expected . put ( "key.min" 8 , 0L ) ; expected . put ( "visibility.max" , 0L ) ; expected . put ( "key.min" 0 , 0L ) ; expected . put ( "key.min" 7 , 0L ) ; expected . put ( "key.min" 5 , 0L ) ; expected . put ( "value.sum" , 0L ) ; expected . put ( "total" , 0L ) ; org . junit . Assert . assertEquals ( expected , stats ) ; } }
public class aTest{ @Test public void testGetNegativeTtl ( ) { org . apache . accumulo . core . util . AddressUtilTest . log . info ( "Checking<sp>that<sp>we<sp>can<sp>get<sp>the<sp>ttl<sp>on<sp>dns<sp>failures." ) ; int expectedTtl = 20 ; boolean expectException = false ; try { java . security . Security . setProperty ( "networkaddress.cache.negative.ttl" , java . lang . Integer . toString ( expectedTtl ) ) ; } catch ( java . lang . SecurityException exception ) { org . apache . accumulo . core . util . AddressUtilTest . log . warn ( "We<sp>can't<sp>set<sp>the<sp>DNS<sp>cache<sp>period,<sp>so<sp>we're<sp>only<sp>testing<sp>fetching<sp>the<sp>system<sp>value." ) ; expectedTtl = 10 ; } try { expectedTtl = java . lang . Integer . parseInt ( java . security . Security . getProperty ( "networkaddress.cache.negative.ttl" ) ) ; } catch ( java . lang . SecurityException exception ) { org . apache . accumulo . core . util . AddressUtilTest . log . debug ( "Security<sp>manager<sp>won't<sp>let<sp>us<sp>fetch<sp>the<sp>property,<sp>testing<sp>default<sp>path." ) ; expectedTtl = 10 ; } catch ( java . lang . NumberFormatException exception ) { org . apache . accumulo . core . util . AddressUtilTest . log . debug ( "Checking<sp>that<sp>we<sp>can<sp>get<sp>the<sp>ttl<sp>on<sp>dns<sp>failures." 4 ) ; expectedTtl = 10 ; } if ( ( - 1 ) == expectedTtl ) { org . apache . accumulo . core . util . AddressUtilTest . log . debug ( "property<sp>is<sp>set<sp>to<sp>'forever',<sp>testing<sp>exception<sp>path" ) ; expectException = true ; } if ( 0 > expectedTtl ) { org . apache . accumulo . core . util . AddressUtilTest . log . debug ( "Checking<sp>that<sp>we<sp>can<sp>get<sp>the<sp>ttl<sp>on<sp>dns<sp>failures." 3 ) ; expectedTtl = 10 ; } try { if ( expectException ) { org . apache . accumulo . core . util . AddressUtilTest . log . info ( ( "AddressUtil<sp>is<sp>(hopefully)<sp>going<sp>to<sp>spit<sp>out<sp>an<sp>error<sp>about<sp>DNS<sp>lookups.<sp>" + "Checking<sp>that<sp>we<sp>can<sp>get<sp>the<sp>ttl<sp>on<sp>dns<sp>failures." 2 ) ) ; } int result = org . apache . accumulo . core . util . AddressUtil . getAddressCacheNegativeTtl ( null ) ; if ( expectException ) { org . junit . Assert . fail ( ( "The<sp>JVM<sp>Security<sp>settings<sp>cache<sp>DNS<sp>failures<sp>forever.<sp>" + "In<sp>this<sp>case<sp>we<sp>expect<sp>an<sp>exception<sp>but<sp>didn't<sp>get<sp>one." ) ) ; } org . junit . Assert . assertEquals ( "Checking<sp>that<sp>we<sp>can<sp>get<sp>the<sp>ttl<sp>on<sp>dns<sp>failures." 1 , expectedTtl , result ) ; } }
public class aTest{ @Test public void testBestTimeSerialization ( ) { com . urbanairship . api . schedule . model . BestTime bestTime = com . urbanairship . api . schedule . model . BestTime . newBuilder ( ) . setSendDate ( new org . joda . time . DateTime ( "2013-05-05T00:00:01" , org . joda . time . DateTimeZone . UTC ) ) . build ( ) ; com . urbanairship . api . push . model . PushPayload pushPayload = com . urbanairship . api . push . model . PushPayload . newBuilder ( ) . setAudience ( com . urbanairship . api . push . model . audience . Selectors . tag ( "tag" ) ) . setDeviceTypes ( com . urbanairship . api . push . model . DeviceTypeData . newBuilder ( ) . addDeviceType ( DeviceType . IOS ) . build ( ) ) . setNotification ( com . urbanairship . api . push . model . notification . Notification . newBuilder ( ) . setAlert ( "alert" ) . build ( ) ) . setPushOptions ( com . urbanairship . api . push . model . PushOptions . newBuilder ( ) . build ( ) ) . build ( ) ; com . urbanairship . api . schedule . model . SchedulePayload schedulePayload = com . urbanairship . api . schedule . model . SchedulePayload . newBuilder ( ) . setSchedule ( com . urbanairship . api . schedule . model . Schedule . newBuilder ( ) . setBestTime ( bestTime ) . build ( ) ) . setPushPayload ( pushPayload ) . build ( ) ; java . lang . String json = com . urbanairship . api . schedule . SchedulePayloadSerializerTest . MAPPER . writeValueAsString ( schedulePayload ) ; java . lang . String properJson = "{\"schedule\":{\"best_time\":{\"send_date\":\"2013-05-05\"}},\"push\":{\"audience\":{\"tag\":\"tag\"},\"device_types\":[\"ios\"],\"notification\":{\"alert\":\"alert\"},\"options\"tag" 0 ; org . junit . Assert . assertEquals ( json , properJson ) ; } }
public class aTest{ @Test public void testTypedReturn ( ) { org . junit . Assert . assertEquals ( "foo" , program_1 . returnAsString ( "foo" ) ) ; try { program_1 . returnAsString ( 1 ) ; org . junit . Assert . fail ( "Should<sp>not<sp>reach<sp>here" ) ; } }
public class aTest{ @Test public void testAddOrganizationToRole_addNonSupplier ( ) { final org . oscm . domobjects . Organization org = runTX ( new java . util . concurrent . Callable < org . oscm . domobjects . Organization > ( ) { @ org . oscm . accountservice . bean . Override public org . oscm . domobjects . Organization call ( ) throws org . oscm . accountservice . bean . Exception { return org . oscm . test . data . Organizations . createOrganization ( mgr , OrganizationRoleType . CUSTOMER ) ; } } ) ; runTX ( new java . util . concurrent . Callable < org . oscm . domobjects . Organization > ( ) { @ org . oscm . accountservice . bean . Override public org . oscm . domobjects . Organization call ( ) throws org . oscm . accountservice . bean . Exception { org . oscm . domobjects . Organization updated = accountMgmtLocal . addOrganizationToRole ( org . getOrganizationId ( ) , OrganizationRoleType . TECHNOLOGY_PROVIDER ) ; org . junit . Assert . assertTrue ( updated . getMarketplaceToOrganizations ( ) . isEmpty ( ) ) ; return updated ; } } }
public class aTest{ @Test public void helloRolesAllowed ( ) { try { org . junit . Assert . assertEquals ( "Invalid<sp>return<sp>message" , "SlessEJB.helloRolesAllowed():<sp>Hello<sp>World" , enterprise . security_stateless_ear . SecurityStatelessEarTest . sless . helloRolesAllowed ( ) ) ; } }
public class aTest{ @Test public void testFromConnectSchemaless ( ) { java . lang . Object nullConverted = avroData . fromConnectData ( null , null ) ; org . junit . Assert . assertNull ( nullConverted ) ; checkNonRecordConversionNull ( null ) ; org . apache . avro . generic . GenericRecord avroIntRecord = new org . apache . avro . generic . GenericRecordBuilder ( AvroData . ANYTHING_SCHEMA ) . set ( "int" , 12 ) . build ( ) ; checkNonRecordConversion ( AvroData . ANYTHING_SCHEMA , avroIntRecord , null , ( ( byte ) ( 12 ) ) , avroData ) ; checkNonRecordConversion ( AvroData . ANYTHING_SCHEMA , avroIntRecord , null , ( ( short ) ( 12 ) ) , avroData ) ; checkNonRecordConversion ( AvroData . ANYTHING_SCHEMA , avroIntRecord , null , 12 , avroData ) ; org . apache . avro . generic . GenericRecord avroLongRecord = new org . apache . avro . generic . GenericRecordBuilder ( AvroData . ANYTHING_SCHEMA ) . set ( "long" , 12L ) . build ( ) ; checkNonRecordConversion ( AvroData . ANYTHING_SCHEMA , avroLongRecord , null , 12L , avroData ) ; org . apache . avro . generic . GenericRecord avroFloatRecord = new org . apache . avro . generic . GenericRecordBuilder ( AvroData . ANYTHING_SCHEMA ) . set ( "float" , 12.2F ) . build ( ) ; checkNonRecordConversion ( AvroData . ANYTHING_SCHEMA , avroFloatRecord , null , 12.2F , avroData ) ; org . apache . avro . generic . GenericRecord avroDoubleRecord = new org . apache . avro . generic . GenericRecordBuilder ( AvroData . ANYTHING_SCHEMA ) . set ( "double" , 12.2 ) . build ( ) ; checkNonRecordConversion ( AvroData . ANYTHING_SCHEMA , avroDoubleRecord , null , 12.2 , avroData ) ; org . apache . avro . generic . GenericRecord avroBooleanRecord = new org . apache . avro . generic . GenericRecordBuilder ( AvroData . ANYTHING_SCHEMA ) . set ( "value" 0 , true ) . build ( ) ; checkNonRecordConversion ( AvroData . ANYTHING_SCHEMA , avroBooleanRecord , null , true , avroData ) ; org . apache . avro . generic . GenericRecord avroStringRecord = new org . apache . avro . generic . GenericRecordBuilder ( AvroData . ANYTHING_SCHEMA ) . set ( "string" , "teststring" ) . build ( ) ; checkNonRecordConversion ( AvroData . ANYTHING_SCHEMA , avroStringRecord , null , "teststring" , avroData ) ; org . apache . avro . generic . GenericRecord avroNullRecord = new org . apache . avro . generic . GenericRecordBuilder ( AvroData . ANYTHING_SCHEMA ) . build ( ) ; org . apache . avro . generic . GenericRecord avroArrayRecord = new org . apache . avro . generic . GenericRecordBuilder ( AvroData . ANYTHING_SCHEMA ) . set ( "array" , java . util . Arrays . asList ( avroIntRecord , avroStringRecord , avroNullRecord ) ) . build ( ) ; checkNonRecordConversion ( AvroData . ANYTHING_SCHEMA , avroArrayRecord , null , java . util . Arrays . asList ( 12 , "teststring" , null ) , avroData ) ; org . apache . avro . generic . GenericRecord avroMapEntry = new org . apache . avro . generic . GenericRecordBuilder ( AvroData . ANYTHING_SCHEMA_MAP_ELEMENT ) . set ( "key" , avroIntRecord ) . set ( "value" , avroStringRecord ) . build ( ) ; org . apache . avro . generic . GenericRecord avroMapEntryNull = new org . apache . avro . generic . GenericRecordBuilder ( AvroData . ANYTHING_SCHEMA_MAP_ELEMENT ) . set ( "key" , new org . apache . avro . generic . GenericRecordBuilder ( AvroData . ANYTHING_SCHEMA ) . set ( "int" , 13 ) . build ( ) ) . set ( "value" , avroNullRecord ) . build ( ) ; org . apache . avro . generic . GenericRecord avroMapRecord = new org . apache . avro . generic . GenericRecordBuilder ( AvroData . ANYTHING_SCHEMA ) . set ( "map" , java . util . Arrays . asList ( avroMapEntry , avroMapEntryNull ) ) . build ( ) ; java . util . HashMap < java . lang . Object , java . lang . Object > convertedMap = new java . util . HashMap ( ) ; convertedMap . put ( 12 , "teststring" ) ; convertedMap . put ( 13 , null ) ; checkNonRecordConversion ( AvroData . ANYTHING_SCHEMA , avroMapRecord , null , convertedMap , avroData ) ; } }
public class aTest{ @Test public void testMultiColumn ( ) { org . apache . fluo . accumulo . iterators . TestData input = new org . apache . fluo . accumulo . iterators . TestData ( ) ; input . add ( "19" 3 , "19" 7 ) ; input . add ( "0<sp>f<sp>a<sp>LOCK<sp>19" , "19" 2 ) ; input . add ( "19" 0 , "19" 8 ) ; input . add ( "0<sp>f<sp>b<sp>LOCK<sp>19" , "19" 2 ) ; input . add ( "19" 6 , "16" ) ; input . add ( "0<sp>f<sp>c<sp>TX_DONE<sp>20" , "" ) ; input . add ( "19" 4 , "21" ) ; input . add ( "0<sp>f<sp>d<sp>WRITE<sp>20" , "19" 5 ) ; input . add ( "0<sp>f<sp>d<sp>DATA<sp>21" , "19" ) ; input . add ( "19" 9 , "19" 8 ) ; input . add ( "0<sp>f<sp>e<sp>LOCK<sp>19" , "19" 2 ) ; input . add ( "19" 1 , "16" ) ; org . apache . fluo . accumulo . iterators . TestData output = new org . apache . fluo . accumulo . iterators . TestData ( newGCI ( input , 27 ) ) ; org . apache . fluo . accumulo . iterators . TestData expected = new org . apache . fluo . accumulo . iterators . TestData ( ) ; expected . add ( "0<sp>f<sp>b<sp>LOCK<sp>19" , "19" 2 ) ; expected . add ( "19" 6 , "16" ) ; expected . add ( "0<sp>f<sp>c<sp>TX_DONE<sp>20" , "" ) ; expected . add ( "19" 4 , "21" ) ; expected . add ( "0<sp>f<sp>d<sp>WRITE<sp>20" , "19" 5 ) ; expected . add ( "0<sp>f<sp>d<sp>DATA<sp>21" , "19" ) ; expected . add ( "0<sp>f<sp>e<sp>LOCK<sp>19" , "19" 2 ) ; expected . add ( "19" 1 , "16" ) ; org . junit . Assert . assertEquals ( expected , output ) ; } }
public class aTest{ @Test public void testDoesNotCallCapabilityLocalRepositoryEventListenersWhenTheCapabilityHasNoRepositoryClassName ( ) { java . util . concurrent . atomic . AtomicInteger atomicInteger = new java . util . concurrent . atomic . AtomicInteger ( 0 ) ; com . liferay . registry . Registry registry = com . liferay . registry . RegistryUtil . getRegistry ( ) ; com . liferay . registry . ServiceRegistration < com . liferay . portal . kernel . repository . capabilities . Capability > capabilityServiceRegistration = registry . registerService ( com . liferay . portal . kernel . repository . capabilities . Capability . class , ( ( com . liferay . document . library . repository . capabilities . test . LiferayDynamicCapabilityTest . TestRepositoryEventAwareCapability ) ( ( repositoryEventRegistry ) -> repositoryEventRegistry . registerRepositoryEventListener ( . class , . class , ( fileEntry ) -> atomicInteger . incrementAndGet ( ) ) ) ) , _getCapabilityProperties ( null ) ) ; capabilityServiceRegistration . unregister ( ) ; _addRandomFileEntry ( com . liferay . portal . kernel . test . util . ServiceContextTestUtil . getServiceContext ( _group . getGroupId ( ) ) ) ; org . junit . Assert . assertEquals ( 0 , atomicInteger . get ( ) ) ; } }
public class aTest{ @Test public void lockShouldReturnFalseIfTheRowIsAlreadyLocked ( ) { java . sql . Connection connection = null ; try { lock = createLock ( props ) ; connection = lock ( tableName , clustername ) ; org . junit . Assert . assertFalse ( lock . lock ( ) ) ; } }
public class aTest{ @Test public void testWithTwoCustomFields ( ) { java . lang . Integer numberOfSuggestions = DEFAULT_PATTERN . length ; userData . put ( "id" , "123456" ) ; userData . put ( "faculty" , "car-losalvarez1" 0 ) ; apps . provisioning . server . account . UsernameIterator usernameIterator = new apps . provisioning . server . account . UsernameIterator ( DEFAULT_PATTERN , userData ) ; java . lang . Integer counter = 0 ; java . lang . String [ ] output = new java . lang . String [ numberOfSuggestions ] ; while ( ( usernameIterator . hasNext ( ) ) && ( counter < numberOfSuggestions ) ) { output [ ( counter ++ ) ] = usernameIterator . next ( ) ; } java . lang . String [ ] expectedResult = new java . lang . String [ ] { "car-los.alvarez" , "car-losalvarez1" 1 , "c.alvarez" , "car-losal" , "car-los123456car-los" , "car-losmm" , "car-los123456mmc" , "car-losalvarez1" } ; org . junit . Assert . assertArrayEquals ( expectedResult , output ) ; } }
public class aTest{ @Test public void defaultModelFactoryIsXml ( ) { new ro . isdc . wro . manager . factory . BaseWroManagerFactory ( ) { @ ro . isdc . wro . extensions . manager . factory . Override protected ro . isdc . wro . model . factory . WroModelFactory newModelFactory ( ) { final ro . isdc . wro . model . factory . WroModelFactory modelFactory = super . newModelFactory ( ) ; org . junit . Assert . assertEquals ( ro . isdc . wro . extensions . model . factory . SmartWroModelFactory . class , modelFactory . getClass ( ) ) ; return modelFactory ; } } }
public class aTest{ @Test public void testCreateWithAppInit ( ) { final com . spotify . apollo . httpservice . HttpServiceTest . InstanceWaiter waiter = new com . spotify . apollo . httpservice . HttpServiceTest . InstanceWaiter ( ) ; final java . util . concurrent . CountDownLatch closed = new java . util . concurrent . CountDownLatch ( 1 ) ; final com . spotify . apollo . AppInit appInit = ( env ) -> { com . spotify . apollo . httpservice . HttpServiceTest . counter . incrementAndGet ( ) ; env . closer ( ) . register ( closed :: countDown ) ; } ; new java . lang . Thread ( ( ) -> { try { com . spotify . apollo . httpservice . HttpService . boot ( appInit , "test" , waiter , "run" , "foo" ) ; } catch ( e ) { com . spotify . apollo . httpservice . e . printStackTrace ( ) ; fail ( com . spotify . apollo . httpservice . e . getMessage ( ) ) ; } } ) . start ( ) ; final com . spotify . apollo . core . Service . Instance instance = waiter . waitForInstance ( ) ; instance . getSignaller ( ) . signalShutdown ( ) ; instance . waitForShutdown ( ) ; closed . await ( 5000 , TimeUnit . SECONDS ) ; org . junit . Assert . assertEquals ( 1 , com . spotify . apollo . httpservice . HttpServiceTest . counter . get ( ) ) ; } }
public class aTest{ @Test public void hierarchical_toString ( ) { com . alibaba . citrus . turbine . Context context = new com . alibaba . citrus . turbine . support . MappedContext ( this . context ) ; context . put ( "MappedContext<sp>{\n" 0 , "444" ) ; java . lang . String str = context . toString ( ) ; java . lang . String result = "" ; result = "MappedContext<sp>{\n" ; result += "MappedContext<sp>{\n" 1 ; result += "<sp>aaa<sp>=<sp>111\n" ; result += "<sp>bbb<sp>=<sp>222\n" ; result += "<sp>ccc<sp>=<sp>333\n" ; result += "<sp>}\n" ; result += "<sp>thisContext<sp>=<sp>{\n" ; result += "<sp>ddd<sp>=<sp>444\n" ; result += "<sp>}\n" ; result += "}" ; org . junit . Assert . assertEquals ( result , str ) ; } }
public class aTest{ @Test public void test ( ) { org . zkoss . zats . mimic . DesktopAgent desktop = connect ( ) ; java . util . List < org . zkoss . zats . mimic . ComponentAgent > buttons = desktop . queryAll ( "button" ) ; org . junit . Assert . assertEquals ( 2 , buttons . size ( ) ) ; try { buttons . get ( 0 ) . click ( ) ; buttons . get ( 1 ) . click ( ) ; } }
public class aTest{ @Test public void testStatsDMetersReporting ( ) { org . apache . flink . runtime . metrics . MetricRegistryImpl registry = null ; org . apache . flink . metrics . statsd . StatsDReporterTest . DatagramSocketReceiver receiver = null ; java . lang . Thread receiverThread = null ; long timeout = 5000 ; long joinTimeout = 30000 ; java . lang . String meterName = "meter" ; try { receiver = new org . apache . flink . metrics . statsd . StatsDReporterTest . DatagramSocketReceiver ( ) ; receiverThread = new java . lang . Thread ( receiver ) ; receiverThread . start ( ) ; int port = receiver . getPort ( ) ; org . apache . flink . configuration . Configuration config = new org . apache . flink . configuration . Configuration ( ) ; config . setString ( ( ( ( org . apache . flink . configuration . ConfigConstants . METRICS_REPORTER_PREFIX ) + "test." ) + ( org . apache . flink . configuration . ConfigConstants . METRICS_REPORTER_CLASS_SUFFIX ) ) , org . apache . flink . metrics . statsd . StatsDReporter . class . getName ( ) ) ; config . setString ( ( ( ( org . apache . flink . configuration . ConfigConstants . METRICS_REPORTER_PREFIX ) + "test." ) + ( org . apache . flink . configuration . ConfigConstants . METRICS_REPORTER_INTERVAL_SUFFIX ) ) , "1<sp>SECONDS" ) ; config . setString ( ( ( org . apache . flink . configuration . ConfigConstants . METRICS_REPORTER_PREFIX ) + "test.host" ) , "localhost" ) ; config . setString ( ( ( org . apache . flink . configuration . ConfigConstants . METRICS_REPORTER_PREFIX ) + "test.port" ) , ( "" + port ) ) ; registry = new org . apache . flink . runtime . metrics . MetricRegistryImpl ( org . apache . flink . runtime . metrics . MetricRegistryConfiguration . fromConfiguration ( config ) ) ; org . apache . flink . runtime . metrics . groups . TaskManagerMetricGroup metricGroup = new org . apache . flink . runtime . metrics . groups . TaskManagerMetricGroup ( registry , "localhost" , "tmId" ) ; org . apache . flink . metrics . util . TestMeter meter = new org . apache . flink . metrics . util . TestMeter ( ) ; metricGroup . meter ( meterName , meter ) ; java . lang . String prefix = metricGroup . getMetricIdentifier ( meterName ) ; java . util . Set < java . lang . String > expectedLines = new java . util . HashSet ( ) ; expectedLines . add ( ( prefix + ".rate:5.0|g" ) ) ; expectedLines . add ( ( prefix + ".count:100|g" ) ) ; receiver . waitUntilNumLines ( expectedLines . size ( ) , timeout ) ; java . util . Set < java . lang . String > lines = receiver . getLines ( ) ; org . junit . Assert . assertEquals ( expectedLines , lines ) ; } }
public class aTest{ @Test public void testBasic ( ) { final java . lang . String [ ] visibilities = new java . lang . String [ ] { "public" , "private" , "package" , "protected" , "" } ; final java . lang . String [ ] staticNonStatics = new java . lang . String [ ] { "private" 0 , "non_static" , "" } ; final java . lang . String [ ] nativeNonNatives = new java . lang . String [ ] { "private" 6 , "private" 4 , "" } ; final java . lang . String [ ] returnTypesOrNews = new java . lang . String [ ] { "java.lang.String" , "private" 5 , "<sp>" 7 , "private" 1 } ; final java . lang . String [ ] fqClassNames = new java . lang . String [ ] { "<sp>" 2 , "<sp>" 3 , "<sp>" 7 , "private" 1 } ; final java . lang . String [ ] operationNames = new java . lang . String [ ] { "doIt" , "<sp>" 1 , "<sp>" 7 } ; final java . lang . String [ ] paramLists = new java . lang . String [ ] { "" , "<sp>" 7 , "<sp>" 4 , "<sp>" 0 } ; final java . lang . String [ ] whites = new java . lang . String [ ] { "<sp>" 5 , "<sp>" , "\t" } ; final java . lang . String [ ] whiteAndNoWhite = new java . lang . String [ ] { "<sp>" 5 , "<sp>" , "\t" , "" } ; final java . lang . String signature01 = "<sp>" 8 ; final boolean [ ] visibilityMatches = new boolean [ ] { true , false , false , false , true } ; final boolean [ ] staticNonStaticMatches = new boolean [ ] { true , false , true } ; final boolean [ ] nativeNonNativeMatches = new boolean [ ] { true , false , true } ; final boolean [ ] returnTypeOrNewMatches = new boolean [ ] { false , false , true , true } ; final boolean [ ] fqClassNameMatches = new boolean [ ] { false , false , false , true } ; final boolean [ ] operationNameMatches = new boolean [ ] { false , false , true } ; final boolean [ ] paramListMatches = new boolean [ ] { true , false , false , true } ; for ( int visibilityIdy = 0 ; visibilityIdy < ( visibilities . length ) ; visibilityIdy ++ ) { for ( int staticNonStaticIdx = 0 ; staticNonStaticIdx < ( staticNonStatics . length ) ; staticNonStaticIdx ++ ) { for ( int nativeNonNativeIdx = 0 ; nativeNonNativeIdx < ( nativeNonNatives . length ) ; nativeNonNativeIdx ++ ) { for ( int returnTypeOrNewIdx = 0 ; returnTypeOrNewIdx < ( returnTypesOrNews . length ) ; returnTypeOrNewIdx ++ ) { for ( int fqClassNameIdx = 0 ; fqClassNameIdx < ( fqClassNames . length ) ; fqClassNameIdx ++ ) { for ( int operationNameIdx = 0 ; operationNameIdx < ( operationNames . length ) ; operationNameIdx ++ ) { for ( int paramListIdx = 0 ; paramListIdx < ( paramLists . length ) ; paramListIdx ++ ) { for ( final java . lang . String white : whites ) { for ( final java . lang . String whiteOrEmpty : whiteAndNoWhite ) { final java . lang . StringBuilder signatureBuilder = new java . lang . StringBuilder ( ) ; signatureBuilder . append ( whiteOrEmpty ) ; if ( ( visibilities [ visibilityIdy ] . length ( ) ) > 0 ) { signatureBuilder . append ( visibilities [ visibilityIdy ] ) . append ( white ) ; } if ( ( staticNonStatics [ staticNonStaticIdx ] . length ( ) ) > 0 ) { signatureBuilder . append ( staticNonStatics [ staticNonStaticIdx ] ) . append ( white ) ; } if ( ( nativeNonNatives [ nativeNonNativeIdx ] . length ( ) ) > 0 ) { signatureBuilder . append ( nativeNonNatives [ nativeNonNativeIdx ] ) . append ( white ) ; } signatureBuilder . append ( returnTypesOrNews [ returnTypeOrNewIdx ] ) . append ( white ) . append ( fqClassNames [ fqClassNameIdx ] ) . append ( '.' ) . append ( operationNames [ operationNameIdx ] ) . append ( whiteOrEmpty ) . append ( '(' ) . append ( whiteOrEmpty ) ; if ( ( paramLists [ paramListIdx ] . length ( ) ) > 0 ) { signatureBuilder . append ( paramLists [ paramListIdx ] ) . append ( whiteOrEmpty ) ; } signatureBuilder . append ( ')' ) . append ( whiteOrEmpty ) ; final java . lang . String signature = signatureBuilder . toString ( ) ; final boolean expected = ( ( ( ( ( ( visibilityMatches [ visibilityIdy ] ) && ( staticNonStaticMatches [ staticNonStaticIdx ] ) ) && ( nativeNonNativeMatches [ nativeNonNativeIdx ] ) ) && ( returnTypeOrNewMatches [ returnTypeOrNewIdx ] ) ) && ( fqClassNameMatches [ fqClassNameIdx ] ) ) && ( operationNameMatches [ operationNameIdx ] ) ) && ( paramListMatches [ paramListIdx ] ) ; this . checkCombination ( signature , visibilities [ visibilityIdy ] , staticNonStatics [ staticNonStaticIdx ] , nativeNonNatives [ nativeNonNativeIdx ] , returnTypesOrNews [ returnTypeOrNewIdx ] , fqClassNames [ fqClassNameIdx ] , operationNames [ operationNameIdx ] , paramLists [ paramListIdx ] ) ; final java . util . regex . Pattern pattern = kieker . monitoring . core . signaturePattern . PatternParser . parseToPattern ( signature ) ; final java . util . regex . Matcher m = pattern . matcher ( signature01 ) ; final boolean result = m . matches ( ) ; org . junit . Assert . assertEquals ( ( ( ( "private" 3 + expected ) + "<sp>" 9 ) + result ) , expected , result ) ; } } } } } } } } } } }
public class aTest{ @Test public void testServerStartNoJVMOptionsETCFailover ( ) { com . ibm . websphere . simplicity . log . Log . entering ( com . ibm . ws . kernel . boot . ServerStartJVMOptionsTest . c , testName . getMethodName ( ) ) ; java . lang . String [ ] parms = new java . lang . String [ 2 ] ; parms [ 0 ] = "The<sp>jvm<sp>option<sp>was<sp>not<sp>found" 7 ; parms [ 1 ] = com . ibm . ws . kernel . boot . ServerStartJVMOptionsTest . SERVER_NAME ; java . util . Properties envVars = new java . util . Properties ( ) ; envVars . put ( "The<sp>jvm<sp>option<sp>was<sp>not<sp>found" 6 , "The<sp>jvm<sp>option<sp>was<sp>not<sp>found" 4 ) ; initialize ( ) ; java . io . Writer isw = new java . io . OutputStreamWriter ( new java . io . FileOutputStream ( com . ibm . ws . kernel . boot . ServerStartJVMOptionsTest . etcjvmoptions ) , "The<sp>jvm<sp>option<sp>was<sp>not<sp>found" 0 ) ; java . io . BufferedWriter bw = new java . io . BufferedWriter ( isw ) ; bw . write ( "-DTest1=Test1" ) ; bw . close ( ) ; com . ibm . websphere . simplicity . ProgramOutput po = com . ibm . ws . kernel . boot . ServerStartJVMOptionsTest . server . getMachine ( ) . execute ( com . ibm . ws . kernel . boot . ServerStartJVMOptionsTest . serverCommand , parms , com . ibm . ws . kernel . boot . ServerStartJVMOptionsTest . executionDir , envVars ) ; com . ibm . websphere . simplicity . log . Log . info ( com . ibm . ws . kernel . boot . ServerStartJVMOptionsTest . c , testName . getMethodName ( ) , ( "server<sp>start<sp>stdout<sp>=<sp>" + ( po . getStdout ( ) ) ) ) ; com . ibm . websphere . simplicity . log . Log . info ( com . ibm . ws . kernel . boot . ServerStartJVMOptionsTest . c , testName . getMethodName ( ) , ( "server<sp>start<sp>stderr<sp>=<sp>" + ( po . getStderr ( ) ) ) ) ; com . ibm . ws . kernel . boot . ServerStartJVMOptionsTest . server . waitForStringInLog ( "CWWKF0011I" ) ; com . ibm . ws . kernel . boot . ServerStartJVMOptionsTest . server . resetStarted ( ) ; com . ibm . ws . kernel . boot . ServerStartJVMOptionsTest . server . serverDump ( ) ; java . io . File [ ] filesAfterDump = new java . io . File ( ( ( ( com . ibm . ws . kernel . boot . ServerStartJVMOptionsTest . executionDir ) + "/usr/servers/" ) + ( com . ibm . ws . kernel . boot . ServerStartJVMOptionsTest . SERVER_NAME ) ) ) . listFiles ( ) ; java . io . File dumpFile = new java . io . File ( "The<sp>jvm<sp>option<sp>was<sp>not<sp>found" 1 ) ; for ( java . io . File f : filesAfterDump ) { java . lang . String fileName = f . getName ( ) ; com . ibm . websphere . simplicity . log . Log . info ( com . ibm . ws . kernel . boot . ServerStartJVMOptionsTest . c , testName . getMethodName ( ) , ( "The<sp>jvm<sp>option<sp>was<sp>not<sp>found" 2 + fileName ) ) ; if ( ( fileName . startsWith ( ( ( com . ibm . ws . kernel . boot . ServerStartJVMOptionsTest . SERVER_NAME ) + ".dump" ) ) ) && ( fileName . endsWith ( ".zip" ) ) ) { dumpFile = f ; break ; } } if ( ( dumpFile . getPath ( ) . compareTo ( "The<sp>jvm<sp>option<sp>was<sp>not<sp>found" 1 ) ) == 0 ) { org . junit . Assert . fail ( "The<sp>jvm<sp>option<sp>was<sp>not<sp>found" 3 ) ; } java . util . zip . ZipFile zipFile = new java . util . zip . ZipFile ( dumpFile ) ; boolean foundTest1 = false ; for ( java . util . Enumeration < ? extends java . util . zip . ZipEntry > en = zipFile . entries ( ) ; en . hasMoreElements ( ) ; ) { java . util . zip . ZipEntry entry = en . nextElement ( ) ; java . lang . String entryName = entry . getName ( ) ; if ( entryName . endsWith ( "JavaRuntimeInformation.txt" ) ) { java . io . InputStream inputstream = zipFile . getInputStream ( entry ) ; java . io . BufferedReader reader = new java . io . BufferedReader ( new java . io . InputStreamReader ( inputstream ) ) ; java . lang . String line ; int i = 0 ; while ( ( line = reader . readLine ( ) ) != null ) { com . ibm . websphere . simplicity . log . Log . info ( com . ibm . ws . kernel . boot . ServerStartJVMOptionsTest . c , testName . getMethodName ( ) , ( ( ( "Run" + i ) + "The<sp>jvm<sp>option<sp>was<sp>not<sp>found" 5 ) + line ) ) ; if ( line . contains ( "-DTest1=Test1" ) ) { foundTest1 = true ; break ; } i ++ ; } reader . close ( ) ; inputstream . close ( ) ; } } zipFile . close ( ) ; dumpFile . delete ( ) ; org . junit . Assert . assertTrue ( "The<sp>jvm<sp>option<sp>was<sp>not<sp>found" , foundTest1 ) ; com . ibm . ws . kernel . boot . ServerStartJVMOptionsTest . server . stopServer ( ) ; } }
public class aTest{ @Test public void testJira1718 ( ) { org . eclipse . core . runtime . IPath p2 = env . addPackage ( project . getFolder ( "<sp>if<sp>(htmlRenderer<sp>==<sp>null)<sp>{\n" 1 ) . getFullPath ( ) , "<sp>if<sp>(htmlRenderer<sp>==<sp>null)<sp>{\n" 2 ) ; env . addGroovyClass ( p2 , "<sp>if<sp>(htmlRenderer<sp>==<sp>null)<sp>{\n" 5 , ( "<sp>if<sp>(htmlRenderer<sp>==<sp>null)<sp>{\n" 9 + ( ( ( "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 8 + "<sp>if<sp>(htmlRenderer<sp>==<sp>null)<sp>{\n" 3 ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 0 ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 1 ) ) ) ; env . addGroovyClass ( p2 , "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 6 , ( "<sp>if<sp>(htmlRenderer<sp>==<sp>null)<sp>{\n" 9 + ( ( ( ( ( ( ( "abstract<sp>class<sp>AbstractRenderer<T><sp>implements<sp>Renderer<T><sp>{\n" + "<sp>if<sp>(htmlRenderer<sp>==<sp>null)<sp>{\n" 6 ) + "<sp>DefaultRendererRegistry<sp>registry<sp>=<sp>new<sp>DefaultRendererRegistry()\n" 0 ) + "<sp>return<sp>null\n" ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 2 ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 3 ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 2 ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 1 ) ) ) ; env . addGroovyClass ( p2 , "<sp>if<sp>(htmlRenderer<sp>==<sp>null)<sp>{\n" 7 , ( "<sp>if<sp>(htmlRenderer<sp>==<sp>null)<sp>{\n" 9 + ( ( ( ( ( ( ( ( ( ( "<sp>if<sp>(htmlRenderer<sp>==<sp>null)<sp>{\n" 0 + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 5 ) + "<sp>if<sp>(htmlRenderer<sp>==<sp>null)<sp>{\n" 4 ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 7 ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 2 ) + "<sp>DefaultRendererRegistry<sp>registry<sp>=<sp>new<sp>DefaultRendererRegistry()\n" 0 ) + "<sp>return<sp>null\n" ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 2 ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 3 ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 2 ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 1 ) ) ) ; env . addGroovyClass ( p2 , "RendererRegistry" , ( "<sp>if<sp>(htmlRenderer<sp>==<sp>null)<sp>{\n" 9 + ( ( "interface<sp>RendererRegistry<sp>{\n" + "<sp>DefaultRendererRegistry<sp>registry<sp>=<sp>new<sp>DefaultRendererRegistry()\n" 3 ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 1 ) ) ) ; env . addGroovyClass ( p2 , "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 9 , ( "<sp>if<sp>(htmlRenderer<sp>==<sp>null)<sp>{\n" 9 + ( ( ( ( "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" + "<sp>def<sp><T><sp>Renderer<T><sp>findRenderer(String<sp>contentType,<sp>T<sp>object)<sp>{\n" ) + "<sp>return<sp>null\n" ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 2 ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 1 ) ) ) ; org . eclipse . core . runtime . IPath path = env . addGroovyClass ( p2 , "LinkingRenderer" , ( "<sp>if<sp>(htmlRenderer<sp>==<sp>null)<sp>{\n" 9 + ( ( ( ( ( ( ( ( ( ( "@groovy.transform.CompileStatic\n" + "<sp>DefaultRendererRegistry<sp>registry<sp>=<sp>new<sp>DefaultRendererRegistry()\n" 1 ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 3 ) + "<sp>DefaultRendererRegistry<sp>registry<sp>=<sp>new<sp>DefaultRendererRegistry()\n" ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 4 ) + "<sp>if<sp>(htmlRenderer<sp>==<sp>null)<sp>{\n" ) + "<sp>DefaultRendererRegistry<sp>registry<sp>=<sp>new<sp>DefaultRendererRegistry()\n" 4 ) + "<sp>if<sp>(htmlRenderer<sp>==<sp>null)<sp>{\n" 8 ) + "<sp>DefaultRendererRegistry<sp>registry<sp>=<sp>new<sp>DefaultRendererRegistry()\n" 2 ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 2 ) + "class<sp>DefaultRendererRegistry<sp>implements<sp>RendererRegistry<sp>{\n" 1 ) ) ) ; org . eclipse . core . resources . IFile file = org . eclipse . core . resources . ResourcesPlugin . getWorkspace ( ) . getRoot ( ) . getFile ( path ) ; java . util . Set < org . eclipse . jdt . core . compiler . IProblem > problems = org . eclipse . jdt . core . groovy . tests . ReconcilerUtils . reconcile ( org . eclipse . jdt . core . JavaCore . createCompilationUnitFrom ( file ) ) ; org . junit . Assert . assertTrue ( problems . isEmpty ( ) ) ; } }
public class aTest{ @Test public void operationDoubleCompletion ( ) { java . util . concurrent . atomic . AtomicInteger completionCount = new java . util . concurrent . atomic . AtomicInteger ( ) ; com . vmware . xenon . common . Operation . CompletionHandler c = ( o , e ) -> { completionCount . incrementAndGet ( ) ; } ; int count = 100 ; this . host . toggleNegativeTestMode ( true ) ; this . host . testStart ( count ) ; com . vmware . xenon . common . Operation op = com . vmware . xenon . common . Operation . createGet ( this . host . getUri ( ) ) . setCompletion ( c ) ; for ( int i = 0 ; i < count ; i ++ ) { this . host . run ( ( ) -> { op . complete ( ) ; op . fail ( new java . lang . Exception ( ) ) ; try { java . lang . Thread . sleep ( 1 ) ; } catch ( e1 ) { } this . host . completeIteration ( ) ; } ) ; } this . host . testWait ( ) ; this . host . toggleNegativeTestMode ( false ) ; org . junit . Assert . assertTrue ( ( ( completionCount . get ( ) ) == 1 ) ) ; } }
public class aTest{ @Test public void testRowValueConstructorOnRHSWithBuiltInFunctionOperatingOnIntegerLiteralOnLHS ( ) { long ts = nextTimestamp ( ) ; java . lang . String tenantId = getOrganizationId ( ) ; initATableValues ( tenantId , getDefaultSplits ( tenantId ) , null , ts ) ; java . lang . String query = "SELECT<sp>a_integer,<sp>x_integer<sp>FROM<sp>aTable<sp>WHERE<sp>?=organization_id<sp>AND<sp>to_number('7')<sp><=<sp>(a_integer,<sp>x_integer)" ; java . util . Properties props = new java . util . Properties ( TEST_PROPERTIES ) ; props . setProperty ( PhoenixRuntime . CURRENT_SCN_ATTRIB , java . lang . Long . toString ( ( ts + 2 ) ) ) ; java . sql . Connection conn = java . sql . DriverManager . getConnection ( com . salesforce . phoenix . end2end . PHOENIX_JDBC_URL , props ) ; try { java . sql . PreparedStatement statement = conn . prepareStatement ( query ) ; statement . setString ( 1 , tenantId ) ; java . sql . ResultSet rs = statement . executeQuery ( ) ; int count = 0 ; while ( rs . next ( ) ) { count ++ ; } org . junit . Assert . assertEquals ( 3 , count ) ; } }
public class aTest{ @Test public void testLexerUnicodeUnescapedBMPNotSetWithRange ( ) { org . antlr . v4 . tool . LexerGrammar lg = new org . antlr . v4 . tool . LexerGrammar ( ( "2:RULE_STOP<sp>0\n" 2 + "ID<sp>:<sp>~(\'亜\'|\'亝\'|\'江\'|\'た\'..\'ほ\')\n<sp>;" ) ) ; java . lang . String expecting = "max<sp>type<sp>1\n" + ( ( ( ( ( ( ( ( ( ( ( ( "2:RULE_STOP<sp>0\n" 4 + "1:RULE_START<sp>0\n" ) + "2:RULE_STOP<sp>0\n" ) + "2:RULE_STOP<sp>0\n" 0 ) + "4:BASIC<sp>0\n" ) + "rule<sp>0:1<sp>1\n" ) + "mode<sp>0:0\n" ) + "2:RULE_STOP<sp>0\n" 5 ) + "0->1<sp>EPSILON<sp>0,0,0\n" ) + "1->3<sp>EPSILON<sp>0,0,0\n" ) + "2:RULE_STOP<sp>0\n" 1 ) + "2:RULE_STOP<sp>0\n" 3 ) + "0:0\n" ) ; org . antlr . v4 . runtime . atn . ATN atn = createATN ( lg , true ) ; java . lang . String result = org . antlr . v4 . runtime . atn . ATNSerializer . getDecoded ( atn , java . util . Arrays . asList ( lg . getTokenNames ( ) ) ) ; org . junit . Assert . assertEquals ( expecting , result ) ; } }
public class aTest{ @Test public void testDocumentTrackingComplex ( ) { final java . lang . String className = "TestDocumentTrackingComplex" ; com . orientechnologies . orient . core . db . document . ODatabaseDocument db = ru . ydn . wicket . wicketorientdb . orientdb . TestInAppOrientDBCompatibility . wicket . getTester ( ) . getDatabase ( ) ; com . orientechnologies . orient . core . metadata . schema . OSchema schema = db . getMetadata ( ) . getSchema ( ) ; final com . orientechnologies . orient . core . metadata . schema . OClass classA = schema . createClass ( className ) ; classA . createProperty ( "a" , OType . STRING ) ; classA . createProperty ( "b" , OType . STRING ) ; db . registerHook ( new com . orientechnologies . orient . core . hook . ODocumentHookAbstract ( db ) { { setIncludeClasses ( className ) ; } @ ru . ydn . wicket . wicketorientdb . orientdb . Override public com . orientechnologies . orient . core . hook . ORecordHook . DISTRIBUTED_EXECUTION_MODE getDistributedExecutionMode ( ) { return com . orientechnologies . orient . core . hook . ORecordHook . DISTRIBUTED_EXECUTION_MODE . SOURCE_NODE ; } @ ru . ydn . wicket . wicketorientdb . orientdb . Override public void onRecordAfterCreate ( com . orientechnologies . orient . core . record . impl . ODocument iDocument ) { System . out . println ( "onRecordAfterCreate" ) ; iDocument . field ( "a" , "onRecordAfterCreate" ) ; } @ ru . ydn . wicket . wicketorientdb . orientdb . Override public void onRecordAfterUpdate ( com . orientechnologies . orient . core . record . impl . ODocument iDocument ) { System . out . println ( "onRecordAfterUpdate" ) ; iDocument . field ( "a" , "onRecordAfterUpdate" ) ; } @ ru . ydn . wicket . wicketorientdb . orientdb . Override public com . orientechnologies . orient . core . hook . ORecordHook . RESULT onRecordBeforeUpdate ( com . orientechnologies . orient . core . record . impl . ODocument iDocument ) { System . out . println ( ( "onRecordAfterUpdate<sp>before<sp>undo:<sp>" + ( iDocument . field ( "a" ) ) ) ) ; iDocument . undo ( "a" ) ; System . out . println ( ( "onRecordAfterUpdate<sp>after<sp>undo:<sp>" + ( iDocument . field ( "a" ) ) ) ) ; return com . orientechnologies . orient . core . hook . ORecordHook . RESULT . RECORD_CHANGED ; } @ ru . ydn . wicket . wicketorientdb . orientdb . Override public void onRecordAfterRead ( com . orientechnologies . orient . core . record . impl . ODocument iDocument ) { org . junit . Assert . assertEquals ( "original" , iDocument . field ( "a" ) ) ; } } }
public class aTest{ @Test public void instantiationTest ( ) { try { java . text . SimpleDateFormat df = new java . text . SimpleDateFormat ( "EEE<sp>MMM<sp>dd<sp>HH:mm:ss<sp>zzz<sp>yyyy" , java . util . Locale . US ) ; df . setLenient ( true ) ; org . krakenapps . rrd . RrdConfig config ; config = new org . krakenapps . rrd . RrdConfig ( df . parse ( "Fri<sp>Jan<sp>29<sp>22:08:32<sp>KST<sp>2010" ) , 10 ) ; config . addDataSource ( "asdf" , DataSourceType . ABSOLUTE2 , 20 , Double . NaN , Double . NaN ) ; config . addArchive ( ConsolidateFunc . SUM , 0.5 , 6 , 10 ) ; org . krakenapps . rrd . io . FilePersistentLayer persLayer = new org . krakenapps . rrd . io . FilePersistentLayer ( new java . io . File ( "instantiationTest.bin" ) ) ; org . krakenapps . rrd . Rrd rrd = new org . krakenapps . rrd . DefaultRrd ( persLayer , config ) ; updateRrd ( df , rrd ) ; rrd = new org . krakenapps . rrd . DefaultRrd ( new org . krakenapps . rrd . io . FilePersistentLayer ( new java . io . File ( "instantiationTest.bin" ) ) ) ; org . krakenapps . rrd . FetchResult fetch = rrd . fetch ( ConsolidateFunc . SUM , df . parse ( "Fri<sp>Jan<sp>29<sp>22:10:00<sp>KST<sp>2010" ) , df . parse ( "Fri<sp>Jan<sp>29<sp>22:16:00<sp>KST<sp>2010" ) , 60 ) ; for ( org . krakenapps . rrd . FetchRow row : fetch . getRows ( ) ) { org . junit . Assert . assertTrue ( ( ( row . getColumns ( ) [ 0 ] ) == 1.0 ) ) ; } } }
public class aTest{ @Test public void testSelectByIdList ( ) { org . apache . ibatis . session . SqlSession sqlSession = getSqlSession ( ) ; try { tk . mybatis . simple . mapper . UserMapper userMapper = sqlSession . getMapper ( tk . mybatis . simple . mapper . UserMapper . class ) ; java . util . List < java . lang . Long > idList = new java . util . ArrayList < java . lang . Long > ( ) ; idList . add ( 1L ) ; idList . add ( 1001L ) ; java . util . List < tk . mybatis . simple . model . SysUser > userList = userMapper . selectByIdList ( idList ) ; org . junit . Assert . assertEquals ( 2 , userList . size ( ) ) ; } }
public class aTest{ @Test public void testBootstrapDiscover ( ) { final java . util . Random rnd = new java . util . Random ( 42 ) ; net . tomp2p . p2p . Peer master = null ; net . tomp2p . p2p . Peer slave = null ; try { master = new net . tomp2p . p2p . PeerBuilder ( new net . tomp2p . peers . Number160 ( rnd ) ) . ports ( 4001 ) . start ( ) ; slave = new net . tomp2p . p2p . PeerBuilder ( new net . tomp2p . peers . Number160 ( rnd ) ) . ports ( 4002 ) . start ( ) ; net . tomp2p . futures . FutureDiscover fd = master . discover ( ) . peerAddress ( slave . peerAddress ( ) ) . start ( ) ; fd . awaitUninterruptibly ( ) ; System . err . println ( fd . failedReason ( ) ) ; org . junit . Assert . assertEquals ( true , fd . isSuccess ( ) ) ; } }
public class aTest{ @Test public void testEncodeWritable ( ) { java . lang . String [ ] values = new java . lang . String [ ] { "" , "a" , "bb" , "ccc" , "dddd" , "eeeee" , "ffffff" , "ggggggg" , "a" 1 , "a" 0 , "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLM" + "a" 2 } ; org . apache . hadoop . security . token . Token < org . apache . hadoop . security . token . delegation . AbstractDelegationTokenIdentifier > orig ; org . apache . hadoop . security . token . Token < org . apache . hadoop . security . token . delegation . AbstractDelegationTokenIdentifier > copy = new org . apache . hadoop . security . token . Token ( ) ; for ( int i = 0 ; i < ( values . length ) ; ++ i ) { java . lang . String val = values [ i ] ; Token . LOG . info ( "Input<sp>=<sp>{}" , val ) ; orig = new org . apache . hadoop . security . token . Token ( val . getBytes ( ) , val . getBytes ( ) , new org . apache . hadoop . security . token . Text ( val ) , new org . apache . hadoop . security . token . Text ( val ) ) ; java . lang . String encode = orig . encodeToUrlString ( ) ; copy . decodeFromUrlString ( encode ) ; org . junit . Assert . assertEquals ( orig , copy ) ; org . apache . hadoop . security . token . TestToken . checkUrlSafe ( encode ) ; } } }
public class aTest{ @Test public void testMissingSlot ( ) { org . wildfly . swarm . bootstrap . modules . ClasspathModuleFinder finder = new org . wildfly . swarm . bootstrap . modules . ClasspathModuleFinder ( ) ; try { org . jboss . modules . ModuleSpec spec = finder . findModule ( "classpath.module.load.test:missing" , null ) ; org . junit . Assert . assertNull ( spec ) ; } }
public class aTest{ @Test public void testEncodeMySQLUpperCase ( ) { System . out . println ( ( ( com . telefonica . iot . cygnus . utils . CommonUtilsForTests . getTestTraceHead ( "[NGSICharsets.encodeMySQL]" ) ) + "--------<sp>Upper<sp>case<sp>not<sp>accented<sp>characters<sp>are<sp>not<sp>encoded" ) ) ; java . lang . String in = "ABCDEFGHIJKLMNOPQRSTUVWXYZ" ; java . lang . String expected = "ABCDEFGHIJKLMNOPQRSTUVWXYZ" ; java . lang . String out = com . telefonica . iot . cygnus . utils . NGSICharsets . encodeMySQL ( in ) ; try { org . junit . Assert . assertEquals ( expected , out ) ; System . out . println ( ( ( ( ( com . telefonica . iot . cygnus . utils . CommonUtilsForTests . getTestTraceHead ( "[NGSICharsets.encodeMySQL]" ) ) + "-<sp>OK<sp>-<sp>'" ) + in ) + "'<sp>has<sp>not<sp>been<sp>encoded" ) ) ; } }
public class aTest{ @Test public void testUrisWithSpaces ( ) { com . marklogic . client . document . DocumentManager docMgr = Common . client . newDocumentManager ( ) ; java . lang . String [ ] testUris = new java . lang . String [ ] { "/a" , "/a%20b" , "/a+b+c" , "/a%isa#vrybig*andStrangeUr-d/x" , "/.html" , "/" } ; for ( java . lang . String testUri : testUris ) { java . lang . String contents = ( "/a" 0 + testUri ) + "</a>" ; docMgr . write ( testUri , new com . marklogic . client . io . StringHandle ( contents ) ) ; com . marklogic . client . io . StringHandle result = new com . marklogic . client . io . StringHandle ( ) ; docMgr . read ( testUri , result ) ; org . junit . Assert . assertEquals ( contents , result . get ( ) ) ; docMgr . delete ( testUri ) ; try { docMgr . read ( testUri , result ) ; org . junit . Assert . fail ( "Document<sp>was<sp>not<sp>deleted" ) ; } }
public class aTest{ @Test public void actionabilityWorksFusions ( ) { com . hartwig . hmftools . common . actionability . fusion . ActionableFusion fusion = com . hartwig . hmftools . common . actionability . fusion . ImmutableActionableFusion . builder ( ) . fiveGene ( "BCR" ) . threeGene ( "TMPRSS2" 5 ) . cancerType ( "4159" 2 ) . drug ( "TMPRSS2" 3 ) . drugsType ( "BCR-ABL<sp>inhibitor" ) . level ( "TMPRSS2" 0 ) . reference ( "BCR__ABL1" ) . response ( "TMPRSS2" 9 ) . source ( "TMPRSS2" 1 ) . build ( ) ; com . hartwig . hmftools . common . actionability . fusion . ActionablePromiscuousFive five = com . hartwig . hmftools . common . actionability . fusion . ImmutableActionablePromiscuousFive . builder ( ) . cancerType ( "4159" 2 ) . drug ( "FORT-1" ) . drugsType ( "TMPRSS2" 6 ) . gene ( "FGFR1" ) . level ( "B" ) . reference ( "4159" 3 ) . response ( "TMPRSS2" 9 ) . source ( "4159" 0 ) . build ( ) ; com . hartwig . hmftools . common . actionability . fusion . ActionablePromiscuousThree three = com . hartwig . hmftools . common . actionability . fusion . ImmutableActionablePromiscuousThree . builder ( ) . cancerType ( "4159" 2 ) . drug ( "TMPRSS2" 4 ) . drugsType ( "TMPRSS2" 7 ) . gene ( "AKT3__." ) . level ( "TMPRSS2" 2 ) . reference ( "AKT3__." ) . response ( "TMPRSS2" 9 ) . source ( "4159" 1 ) . build ( ) ; com . hartwig . hmftools . common . actionability . fusion . FusionEvidenceAnalyzer analyzer = new com . hartwig . hmftools . common . actionability . fusion . FusionEvidenceAnalyzer ( com . google . common . collect . Lists . newArrayList ( fusion ) , com . google . common . collect . Lists . newArrayList ( five ) , com . google . common . collect . Lists . newArrayList ( three ) ) ; com . hartwig . hmftools . common . actionability . cancertype . CancerTypeAnalyzer cancerTypeAnalyzer = com . hartwig . hmftools . common . actionability . cancertype . CancerTypeAnalyzerTestFactory . buildWithOneCancerTypeMapping ( "Skin" , "4159" ) ; com . hartwig . hmftools . common . variant . structural . annotation . SimpleGeneFusion simpleGeneFusion = com . hartwig . hmftools . common . variant . structural . annotation . ImmutableSimpleGeneFusion . builder ( ) . fiveGene ( "TMPRSS2" ) . threeGene ( "TMPRSS2" 8 ) . build ( ) ; org . junit . Assert . assertTrue ( analyzer . evidenceForFusion ( simpleGeneFusion , "Skin" , cancerTypeAnalyzer ) . isEmpty ( ) ) ; } }
public class aTest{ @Test public void shouldReturnAnEmptyListIfNotResultFounded ( ) { final com . calclab . emite . xep . search . ResultListener < java . util . List < com . calclab . emite . xep . search . SearchResultItem > > result = org . mockito . Mockito . mock ( com . calclab . emite . xep . search . ResultListener . class ) ; manager . search ( new java . util . HashMap < java . lang . String , java . lang . String > ( ) , result ) ; session . answerSuccess ( new com . calclab . emite . core . stanzas . IQ ( com . calclab . emite . base . xml . XMLBuilder . fromXML ( ( "<iq<sp>type='result'<sp>from='characters.shakespeare.lit'<sp>to='romeo@montague.net/home'<sp>id='search2'<sp>xml:lang='en'>" + "<query<sp>xmlns='jabber:iq:search'/></iq>" ) ) ) ) ; org . mockito . Mockito . verify ( result , org . mockito . Mockito . never ( ) ) . onFailure ( org . mockito . Matchers . anyString ( ) ) ; org . mockito . Mockito . verify ( result ) . onSuccess ( org . mockito . Matchers . argThat ( new org . mockito . ArgumentMatcher < java . util . List < com . calclab . emite . xep . search . SearchResultItem > > ( ) { @ com . calclab . emite . xep . search . Override public boolean matches ( final java . lang . Object arg0 ) { final java . util . List < com . calclab . emite . xep . search . SearchResultItem > list = ( ( java . util . List < com . calclab . emite . xep . search . SearchResultItem > ) ( arg0 ) ) ; org . junit . Assert . assertTrue ( list . isEmpty ( ) ) ; return true ; } } }
public class aTest{ @Test public void testStringVarargs ( java . lang . String [ ] ) { for ( java . lang . String s : strings ) { org . junit . Assert . assertNotNull ( s ) ; } } }
public class aTest{ @Test public void testConcurrentLeadershipOperationsBlockingGainLeadership ( ) { final java . util . concurrent . CompletableFuture < java . lang . Exception > suspendFuture = new java . util . concurrent . CompletableFuture ( ) ; final java . util . concurrent . CompletableFuture < org . apache . flink . runtime . messages . Acknowledge > startFuture = new java . util . concurrent . CompletableFuture ( ) ; org . apache . flink . runtime . jobmaster . factories . TestingJobMasterServiceFactory jobMasterServiceFactory = new org . apache . flink . runtime . jobmaster . factories . TestingJobMasterServiceFactory ( ( ) -> new org . apache . flink . runtime . jobmaster . TestingJobMasterService ( "localhost" , ( e ) -> { suspendFuture . complete ( e ) ; return java . util . concurrent . CompletableFuture . completedFuture ( org . apache . flink . runtime . messages . Acknowledge . get ( ) ) ; } , ( ignored ) -> startFuture ) ) ; org . apache . flink . runtime . jobmaster . JobManagerRunner jobManagerRunner = createJobManagerRunner ( jobMasterServiceFactory ) ; jobManagerRunner . start ( ) ; leaderElectionService . isLeader ( java . util . UUID . randomUUID ( ) ) ; leaderElectionService . notLeader ( ) ; org . junit . Assert . assertThat ( suspendFuture . isDone ( ) , org . hamcrest . Matchers . is ( false ) ) ; try { suspendFuture . get ( 1L , TimeUnit . MILLISECONDS ) ; org . junit . Assert . fail ( "Suspended<sp>leadership<sp>even<sp>though<sp>the<sp>JobMaster<sp>has<sp>not<sp>been<sp>started." ) ; } }
public class aTest{ @Test public void testLotsOfContainersRackLocalAllocation ( ) { org . apache . hadoop . yarn . api . records . ResourceBlacklistRequest blacklistRequest = org . apache . hadoop . yarn . api . records . ResourceBlacklistRequest . newInstance ( new java . util . ArrayList ( ) , new java . util . ArrayList ( ) ) ; java . util . List < org . apache . hadoop . yarn . api . records . ResourceRequest > reqs = new java . util . ArrayList ( ) ; for ( int i = 0 ; i < 100 ; i ++ ) { reqs . add ( org . apache . hadoop . yarn . api . records . ResourceRequest . newBuilder ( ) . allocationRequestId ( ( i + 1 ) ) . priority ( org . apache . hadoop . yarn . api . records . Priority . newInstance ( 1 ) ) . resourceName ( "*" ) . capability ( org . apache . hadoop . yarn . util . resource . Resources . createResource ( ( 1 * ( org . apache . hadoop . yarn . server . scheduler . TestOpportunisticContainerAllocator . GB ) ) ) ) . relaxLocality ( true ) . executionType ( ExecutionType . OPPORTUNISTIC ) . build ( ) ) ; reqs . add ( org . apache . hadoop . yarn . api . records . ResourceRequest . newBuilder ( ) . allocationRequestId ( ( i + 1 ) ) . priority ( org . apache . hadoop . yarn . api . records . Priority . newInstance ( 1 ) ) . resourceName ( "h1" ) . capability ( org . apache . hadoop . yarn . util . resource . Resources . createResource ( ( 1 * ( org . apache . hadoop . yarn . server . scheduler . TestOpportunisticContainerAllocator . GB ) ) ) ) . relaxLocality ( true ) . executionType ( ExecutionType . OPPORTUNISTIC ) . build ( ) ) ; reqs . add ( org . apache . hadoop . yarn . api . records . ResourceRequest . newBuilder ( ) . allocationRequestId ( ( i + 1 ) ) . priority ( org . apache . hadoop . yarn . api . records . Priority . newInstance ( 1 ) ) . resourceName ( "/r1" ) . capability ( org . apache . hadoop . yarn . util . resource . Resources . createResource ( ( 1 * ( org . apache . hadoop . yarn . server . scheduler . TestOpportunisticContainerAllocator . GB ) ) ) ) . relaxLocality ( true ) . executionType ( ExecutionType . OPPORTUNISTIC ) . build ( ) ) ; } org . apache . hadoop . yarn . api . records . ApplicationAttemptId appAttId = org . apache . hadoop . yarn . api . records . ApplicationAttemptId . newInstance ( org . apache . hadoop . yarn . api . records . ApplicationId . newInstance ( 0L , 1 ) , 1 ) ; oppCntxt . updateNodeList ( java . util . Arrays . asList ( org . apache . hadoop . yarn . server . api . protocolrecords . RemoteNode . newInstance ( org . apache . hadoop . yarn . api . records . NodeId . newInstance ( "h3" , 1234 ) , "h5:1234" 0 , "/r2" ) , org . apache . hadoop . yarn . server . api . protocolrecords . RemoteNode . newInstance ( org . apache . hadoop . yarn . api . records . NodeId . newInstance ( "h5:1234" 2 , 1234 ) , "h5:1234" 1 , "/r1" ) , org . apache . hadoop . yarn . server . api . protocolrecords . RemoteNode . newInstance ( org . apache . hadoop . yarn . api . records . NodeId . newInstance ( "h5" , 1234 ) , "h5:1234" , "/r1" ) , org . apache . hadoop . yarn . server . api . protocolrecords . RemoteNode . newInstance ( org . apache . hadoop . yarn . api . records . NodeId . newInstance ( "h4" , 1234 ) , "h4:1234" , "/r2" ) ) ) ; java . util . List < org . apache . hadoop . yarn . api . records . Container > containers = new java . util . ArrayList ( ) ; for ( int i = 0 ; i < 25 ; i ++ ) { containers . addAll ( allocator . allocateContainers ( blacklistRequest , reqs , appAttId , oppCntxt , 1L , "luser" ) ) ; } org . junit . Assert . assertEquals ( 100 , containers . size ( ) ) ; } }
public class aTest{ @Test public void testIngestAndQueryGeneralGpx ( ) { org . locationtech . geowave . test . TestUtils . deleteAll ( dataStorePluginOptions ) ; org . locationtech . geowave . test . mapreduce . MapReduceTestUtils . testMapReduceIngest ( dataStorePluginOptions , DimensionalityType . SPATIAL , org . locationtech . geowave . test . mapreduce . BasicMapReduceIT . GENERAL_GPX_INPUT_GPX_DIR ) ; final java . io . File gpxInputDir = new java . io . File ( org . locationtech . geowave . test . mapreduce . BasicMapReduceIT . GENERAL_GPX_INPUT_GPX_DIR ) ; final java . io . File expectedResultsDir = new java . io . File ( org . locationtech . geowave . test . mapreduce . BasicMapReduceIT . GENERAL_GPX_EXPECTED_RESULTS_DIR ) ; final java . util . List < java . net . URL > expectedResultsResources = new java . util . ArrayList ( ) ; final java . util . Map < java . lang . String , java . net . URL > baseNameToExpectedResultURL = new java . util . HashMap ( ) ; for ( final java . io . File file : expectedResultsDir . listFiles ( new java . io . FileFilter ( ) { @ org . locationtech . geowave . test . mapreduce . Override public boolean accept ( final java . io . File pathname ) { final java . util . Map < java . lang . String , java . lang . Object > map = new java . util . HashMap ( ) ; try { map . put ( "url" , pathname . toURI ( ) . toURL ( ) ) ; return ( org . geotools . data . DataStoreFinder . getDataStore ( map ) ) != null ; } catch ( final java . io . IOException e ) { org . locationtech . geowave . test . mapreduce . BasicMapReduceIT . LOGGER . warn ( "Cannot<sp>read<sp>file<sp>as<sp>GeoTools<sp>data<sp>store" , e ) ; } return false ; } } ) ) { baseNameToExpectedResultURL . put ( org . apache . commons . io . FilenameUtils . getBaseName ( file . getName ( ) ) . replaceAll ( "_filtered" , "" ) , file . toURI ( ) . toURL ( ) ) ; } for ( final java . lang . String filename : gpxInputDir . list ( new java . io . FilenameFilter ( ) { @ org . locationtech . geowave . test . mapreduce . Override public boolean accept ( final java . io . File dir , final java . lang . String name ) { return org . apache . commons . io . FilenameUtils . isExtension ( name , new org . locationtech . geowave . format . gpx . GpxIngestPlugin ( ) . getFileExtensionFilters ( ) ) ; } } ) ) { final java . net . URL url = baseNameToExpectedResultURL . get ( org . apache . commons . io . FilenameUtils . getBaseName ( filename ) ) ; org . junit . Assert . assertNotNull ( url ) ; expectedResultsResources . add ( url ) ; } }
public class aTest{ @Test public void dispatch ( ) { org . apache . jackrabbit . oak . plugins . document . DocumentNodeStore ns = builderProvider . newBuilder ( ) . getNodeStore ( ) ; org . apache . jackrabbit . oak . plugins . document . RevisionVector from = ns . getHeadRevision ( ) ; org . apache . jackrabbit . oak . spi . state . NodeBuilder builder = ns . getRoot ( ) . builder ( ) ; builder . child ( "test" ) ; org . apache . jackrabbit . oak . plugins . document . DocumentNodeStoreTest . merge ( ns , builder ) ; org . apache . jackrabbit . oak . plugins . document . RevisionVector to = ns . getHeadRevision ( ) ; org . apache . jackrabbit . oak . plugins . document . DiffCache . Entry entry = ns . getDiffCache ( ) . newEntry ( from , to , true ) ; entry . append ( Path . ROOT , "-\"foo\"" ) ; entry . done ( ) ; ns . compare ( ns . getRoot ( ) , ns . getRoot ( from ) , new org . apache . jackrabbit . oak . spi . state . DefaultNodeStateDiff ( ) { @ org . apache . jackrabbit . oak . plugins . document . Override public boolean childNodeDeleted ( java . lang . String name , org . apache . jackrabbit . oak . spi . state . NodeState before ) { org . junit . Assert . assertNotNull ( before ) ; return true ; } } }
public class aTest{ @Test public void testSendAnalytics ( ) { com . liferay . analytics . model . AnalyticsEventsMessage . Builder analyticsEventsMessageBuilder = com . liferay . analytics . model . AnalyticsEventsMessage . builder ( "ApplicationKey" , "UserId" ) ; analyticsEventsMessageBuilder . contextProperty ( "languageId" , "en_US" ) ; analyticsEventsMessageBuilder . contextProperty ( "url" , "http://www.liferay.com" ) ; com . liferay . analytics . model . AnalyticsEventsMessage . Event . Builder eventBuilder = AnalyticsEventsMessage . Event . builder ( "ApplicationId" , "View" ) ; eventBuilder . property ( "elementId" , "http://www.liferay.com" 0 ) ; analyticsEventsMessageBuilder . event ( eventBuilder . build ( ) ) ; analyticsEventsMessageBuilder . protocolVersion ( "1.0" ) ; java . lang . String response = _analyticsClientImpl . sendAnalytics ( analyticsEventsMessageBuilder . build ( ) ) ; org . junit . Assert . assertNull ( response ) ; } }
public class aTest{ @Test public void test11 ( ) { final com . persistit . Key key1 = new com . persistit . Key ( _persistit ) ; final com . persistit . Key key2 = new com . persistit . Key ( _persistit ) ; System . out . print ( "test11<sp>" ) ; for ( int i1 = 0 ; i1 < ( com . persistit . unit . KeyTest1 . TEST_LONGS . length ) ; i1 ++ ) { for ( int i2 = 0 ; i2 < ( com . persistit . unit . KeyTest1 . TEST_LONGS . length ) ; i2 ++ ) { for ( int s = 0 ; s < 4 ; s ++ ) { lv1 = com . persistit . unit . KeyTest1 . TEST_LONGS [ i1 ] ; lv2 = com . persistit . unit . KeyTest1 . TEST_LONGS [ i2 ] ; if ( ( s & 1 ) != 0 ) { lv1 = - ( lv1 ) ; } if ( ( s & 2 ) != 0 ) { lv2 = - ( lv2 ) ; } final java . math . BigInteger biv1 = java . math . BigInteger . valueOf ( lv1 ) ; final java . math . BigInteger biv2 = java . math . BigInteger . valueOf ( lv2 ) ; key1 . clear ( ) ; key1 . append ( biv1 ) ; key2 . clear ( ) ; key2 . append ( biv2 ) ; final int compare = key1 . compareTo ( key2 ) ; final boolean result = ( ( ( compare == 0 ) && ( ( lv1 ) == ( lv2 ) ) ) || ( ( compare > 0 ) && ( ( lv1 ) > ( lv2 ) ) ) ) || ( ( compare < 0 ) && ( ( lv1 ) < ( lv2 ) ) ) ; org . junit . Assert . assertTrue ( result ) ; } } } }
public class aTest{ @Test public void getBooks ( ) { log . info ( "...<sp>getBooks<sp>..." ) ; javax . persistence . EntityManager em = emf . createEntityManager ( ) ; em . getTransaction ( ) . begin ( ) ; javax . persistence . criteria . CriteriaBuilder cb = em . getCriteriaBuilder ( ) ; javax . persistence . criteria . CriteriaQuery < org . thoughts . on . java . model . Book > cq = cb . createQuery ( org . thoughts . on . java . model . Book . class ) ; javax . persistence . criteria . Root < org . thoughts . on . java . model . Book > root = cq . from ( org . thoughts . on . java . model . Book . class ) ; javax . persistence . criteria . SetJoin < org . thoughts . on . java . model . Book , org . thoughts . on . java . model . Author > authors = root . join ( Book_ . authors ) ; javax . persistence . criteria . ParameterExpression < java . lang . String > paramFirstName = cb . parameter ( java . lang . String . class ) ; javax . persistence . criteria . ParameterExpression < java . lang . String > paramLastName = cb . parameter ( java . lang . String . class ) ; cq . where ( cb . and ( cb . equal ( authors . get ( Author_ . firstName ) , paramFirstName ) , cb . equal ( authors . get ( Author_ . lastName ) , paramLastName ) ) ) ; javax . persistence . TypedQuery < org . thoughts . on . java . model . Book > query = em . createQuery ( cq ) ; query . setParameter ( paramFirstName , "Thorben" ) ; query . setParameter ( paramLastName , "Janssen" ) ; java . util . List < org . thoughts . on . java . model . Book > books = query . getResultList ( ) ; org . junit . Assert . assertEquals ( 1 , books . size ( ) ) ; for ( org . thoughts . on . java . model . Book b : books ) { log . info ( b ) ; } }
public class aTest{ @Test public void testDestroyProcessTree ( ) { java . lang . String pid = "100" ; java . io . File procfsRootDir = new java . io . File ( org . apache . hadoop . yarn . util . TestProcfsBasedProcessTree . TEST_ROOT_DIR , "proc" ) ; try { org . apache . hadoop . yarn . util . TestProcfsBasedProcessTree . setupProcfsRootDir ( procfsRootDir ) ; createProcessTree ( pid , procfsRootDir . getAbsolutePath ( ) , org . apache . hadoop . yarn . util . SystemClock . getInstance ( ) ) ; org . junit . Assert . assertTrue ( org . apache . hadoop . yarn . util . ProcfsBasedProcessTree . checkPidPgrpidForMatch ( pid , procfsRootDir . getAbsolutePath ( ) ) ) ; } }
public class aTest{ @Test public void testKinesisOperations ( ) { java . lang . String streamName = "java-test-stream-" + ( java . lang . System . currentTimeMillis ( ) ) ; boolean created = false ; try { System . out . println ( "Creating<sp>Stream..." ) ; client . createStream ( software . amazon . awssdk . services . kinesis . model . CreateStreamRequest . builder ( ) . streamName ( streamName ) . shardCount ( 1 ) . build ( ) ) ; System . out . println ( "<sp>OK" ) ; created = true ; findStreamInList ( streamName ) ; System . out . println ( "Waiting<sp>for<sp>stream<sp>to<sp>become<sp>active..." ) ; java . util . List < software . amazon . awssdk . services . kinesis . model . Shard > shards = waitForStream ( streamName ) ; System . out . println ( "<sp>OK" ) ; org . junit . Assert . assertEquals ( 1 , shards . size ( ) ) ; software . amazon . awssdk . services . kinesis . model . Shard shard = shards . get ( 0 ) ; java . lang . Thread . sleep ( 5000 ) ; testPuts ( streamName , shard ) ; java . lang . Thread . sleep ( 5000 ) ; System . out . println ( "Reading..." ) ; testGets ( streamName , shard ) ; System . out . println ( "<sp>OK" ) ; } }
public class aTest{ @Test public void buildKind ( ) { com . vmware . xenon . common . CommandLineArgumentParser . parseFromProperties ( this ) ; java . lang . String kind = com . vmware . xenon . common . Utils . buildKind ( com . vmware . xenon . services . common . ExampleService . ExampleServiceState . class ) ; long s = ( java . lang . System . nanoTime ( ) ) / 1000 ; for ( int i = 0 ; i < ( this . iterationCount ) ; i ++ ) { java . lang . String k = com . vmware . xenon . common . Utils . buildKind ( com . vmware . xenon . services . common . ExampleService . ExampleServiceState . class ) ; org . junit . Assert . assertTrue ( ( ( kind . hashCode ( ) ) == ( k . hashCode ( ) ) ) ) ; } }
public class aTest{ @Test public void testExecuteWithRetry ( ) { java . lang . String className = "testExecuteWithRetry" ; db . createClass ( className ) ; final com . orientechnologies . orient . core . record . OElement v = db . newInstance ( className ) ; v . setProperty ( "count" , 0 ) ; v . save ( ) ; int nThreads = 4 ; java . util . List < java . lang . Thread > threads = new java . util . ArrayList ( ) ; for ( int i = 0 ; i < nThreads ; i ++ ) { java . lang . Thread thread = new java . lang . Thread ( ) { @ com . orientechnologies . orient . core . db . document . Override public void run ( ) { com . orientechnologies . orient . core . db . document . ODatabaseDocumentTx dbCopy = db . copy ( ) ; dbCopy . activateOnCurrentThread ( ) ; dbCopy . executeWithRetry ( 10 , ( db ) -> { com . orientechnologies . orient . core . record . OElement vCopy = ( ( com . orientechnologies . orient . core . record . OElement ) ( db . load ( v . getIdentity ( ) ) ) ) ; try { java . lang . Thread . sleep ( 1000 ) ; } catch ( e ) { } vCopy . setProperty ( "count" , ( ( ( int ) ( vCopy . getProperty ( "count" ) ) ) + 1 ) ) ; db . save ( vCopy ) ; return vCopy ; } ) ; dbCopy . close ( ) ; } } ; threads . add ( thread ) ; thread . start ( ) ; } for ( java . lang . Thread t : threads ) { t . join ( ) ; } v . reload ( ) ; org . junit . Assert . assertEquals ( nThreads , ( ( int ) ( v . getProperty ( "count" ) ) ) ) ; } }
public class aTest{ @Test public void testChecked ( ) { org . apache . tuscany . sca . binding . jms . ExceptionService service = org . apache . tuscany . sca . binding . jms . ExceptionsTestCase . node . getService ( org . apache . tuscany . sca . binding . jms . ExceptionService . class , "ExceptionServiceClient" ) ; try { service . throwChecked ( ) ; org . junit . Assert . fail ( ) ; } catch ( org . apache . tuscany . sca . binding . jms . CheckedExcpetion e ) { org . junit . Assert . assertEquals ( "foo" , e . getMessage ( ) ) ; } }
public class aTest{ @Test public void testGroupEntries ( ) { try ( org . nuxeo . ecm . automation . OperationContext ctx = new org . nuxeo . ecm . automation . OperationContext ( session ) ) { java . util . Map < java . lang . String , java . lang . String > params = new java . util . HashMap ( ) ; params . put ( "searchType" , "displayLabel" 3 ) ; params . put ( "prefixed_id" 0 , "prefixed_id" 1 ) ; org . nuxeo . ecm . core . api . Blob result = ( ( org . nuxeo . ecm . core . api . Blob ) ( automationService . run ( ctx , SuggestUserEntries . ID , params ) ) ) ; org . junit . Assert . assertNotNull ( result ) ; org . nuxeo . ecm . core . io . marshallers . json . JsonAssert json = org . nuxeo . ecm . core . io . marshallers . json . JsonAssert . on ( result . getString ( ) ) . isArray ( ) . length ( 1 ) ; org . nuxeo . ecm . core . io . marshallers . json . JsonAssert entry = json . get ( 0 ) ; entry . has ( "type" ) . isEquals ( "displayLabel" 3 ) ; entry . has ( "prefixed_id" 7 ) . isEquals ( "prefixed_id" 8 ) ; entry . has ( "prefixed_id" ) . isEquals ( "group:administrators" ) ; entry . has ( "groupname" ) . isEquals ( "prefixed_id" 8 ) ; entry . has ( "prefixed_id" 5 ) . isEquals ( "prefixed_id" 6 ) ; entry . has ( "prefixed_id" 9 ) . isEquals ( "prefixed_id" 2 ) ; entry . has ( "displayLabel" 1 ) . isNull ( ) ; entry . has ( "displayLabel" 2 ) . isTrue ( ) ; entry . has ( "displayLabel" ) . isEquals ( "prefixed_id" 6 ) ; entry . has ( "members" ) . isArray ( ) . length ( 0 ) ; entry . has ( "parentGroups" ) . isArray ( ) . length ( 0 ) ; entry . has ( "prefixed_id" 4 ) . isArray ( ) . length ( 0 ) ; params . put ( "prefixed_id" 0 , "foo" ) ; result = ( ( org . nuxeo . ecm . core . api . Blob ) ( automationService . run ( ctx , SuggestUserEntries . ID , params ) ) ) ; json = org . nuxeo . ecm . core . io . marshallers . json . JsonAssert . on ( result . getString ( ) ) . length ( 0 ) ; params . put ( "prefixed_id" 0 , "m" ) ; result = ( ( org . nuxeo . ecm . core . api . Blob ) ( automationService . run ( ctx , SuggestUserEntries . ID , params ) ) ) ; json = org . nuxeo . ecm . core . io . marshallers . json . JsonAssert . on ( result . getString ( ) ) . length ( 2 ) ; json . childrenContains ( "prefixed_id" 7 , "members" , "displayLabel" 4 ) ; params . put ( "prefixed_id" 0 , "displayLabel" 0 ) ; result = ( ( org . nuxeo . ecm . core . api . Blob ) ( automationService . run ( ctx , SuggestUserEntries . ID , params ) ) ) ; json = org . nuxeo . ecm . core . io . marshallers . json . JsonAssert . on ( result . getString ( ) ) . length ( 1 ) ; json . childrenContains ( "prefixed_id" 7 , "members" ) ; params . put ( "prefixed_id" 0 , "prefixed_id" 3 ) ; result = ( ( org . nuxeo . ecm . core . api . Blob ) ( automationService . run ( ctx , SuggestUserEntries . ID , params ) ) ) ; json = org . nuxeo . ecm . core . io . marshallers . json . JsonAssert . on ( result . getString ( ) ) . length ( 1 ) ; json . childrenContains ( "prefixed_id" 7 , "members" ) ; } } }
public class aTest{ @Test public void verifySerialization2 ( ) { try ( org . neo4j . graphdb . Transaction tx = database . beginTx ( ) ) { com . graphaware . common . description . relationship . RelationshipDescription description = com . graphaware . common . description . relationship . RelationshipDescriptionFactory . literal ( database . getRelationshipById ( 0 ) , database . getNodeById ( 0 ) ) . with ( "k5" 0 , equalTo ( "k6" 0 ) ) . with ( "k5" 3 , equalTo ( "v2" ) ) . with ( "k3" , equalTo ( "k5" 9 ) ) . with ( "k4" , equalTo ( "k5" 2 ) ) . with ( "k5" , equalTo ( "v5" ) ) . with ( "k6" , equalTo ( "k5" 5 ) ) . with ( "k5" 1 , equalTo ( new java . lang . String [ ] { "k5" 6 , "test2" , "k5" 8 } ) ) . with ( "k8" , equalTo ( new java . lang . String [ ] { "k5" 6 , "test2" , "k5" 8 } ) ) . with ( "k5" 7 , equalTo ( new java . lang . String [ ] { "k5" 6 , "test2" , "k5" 8 } ) ) . with ( "k5" 4 , equalTo ( new java . lang . String [ ] { "k5" 6 , "test2" , "k5" 8 } ) ) . with ( "k12" , equalTo ( new java . lang . String [ ] { "k5" 6 , "test2" , "k5" 8 } ) ) . with ( "k13" , equalTo ( new java . lang . String [ ] { "k5" 6 , "test2" , "k5" 8 } ) ) ; java . lang . String serialized = com . graphaware . common . serialize . Serializer . toString ( description , "k6" 1 ) ; com . graphaware . common . description . relationship . RelationshipDescription deserialized = com . graphaware . common . serialize . Serializer . fromString ( serialized , "k6" 1 ) ; org . junit . Assert . assertEquals ( deserialized , description ) ; } } }
public class aTest{ @Test public void testQQS ( ) { fr . inria . corese . core . Graph g = createGraph ( ) ; fr . inria . corese . core . query . QueryProcess exec = fr . inria . corese . core . query . QueryProcess . create ( g ) ; java . lang . String init = "insert<sp>data<sp>{<sp>" + ( ( "<John><sp>rdfs:label<sp>'John'<sp>" + "<James><sp>rdfs:label<sp>'James'" ) + "}" ) ; java . lang . String q = "<James><sp>rdfs:label<sp>'James'" 0 + ( ( ( ( ( ( ( ( ( "graph<sp>?g<sp>{" + "{" ) + "?x<sp>rdfs:label<sp>'John'<sp>" ) + "filter<sp>exists<sp>{<sp>select<sp>*<sp>where<sp>{filter(?l<sp>=<sp>'John')<sp>?y<sp>rdfs:label<sp>?l}}<sp>" ) + "}" ) + "union<sp>{filter(?l<sp>=<sp>'John')<sp>?x<sp>rdfs:label<sp>?l}" ) + "}" ) + "" ) + "" ) + "}" ) ; exec . query ( init ) ; fr . inria . corese . kgram . core . Mappings map = exec . query ( q ) ; org . junit . Assert . assertEquals ( 2 , map . size ( ) ) ; } }
public class aTest{ @Test public void testComplexStringsDirecty ( ) { try { java . util . Random rnd = new java . util . Random ( 349712539451944123L ) ; for ( int i = 0 ; i < 10 ; i ++ ) { java . lang . String testString = eu . stratosphere . util . StringUtils . getRandomString ( rnd , 10 , 100 ) ; java . io . ByteArrayOutputStream baos = new java . io . ByteArrayOutputStream ( 512 ) ; { java . io . DataOutputStream dataOut = new java . io . DataOutputStream ( baos ) ; eu . stratosphere . api . avro . DataOutputEncoder encoder = new eu . stratosphere . api . avro . DataOutputEncoder ( ) ; encoder . setOut ( dataOut ) ; encoder . writeString ( testString ) ; dataOut . flush ( ) ; dataOut . close ( ) ; } byte [ ] data = baos . toByteArray ( ) ; { java . io . ByteArrayInputStream bais = new java . io . ByteArrayInputStream ( data ) ; java . io . DataInputStream dataIn = new java . io . DataInputStream ( bais ) ; eu . stratosphere . api . avro . DataInputDecoder decoder = new eu . stratosphere . api . avro . DataInputDecoder ( ) ; decoder . setIn ( dataIn ) ; java . lang . String deserialized = decoder . readString ( ) ; org . junit . Assert . assertEquals ( testString , deserialized ) ; } } } }
public class aTest{ @Test public void test32kBranchLimit ( ) { java . lang . String preamble = "" + ( ( ( ( "run" 0 + "run" 1 ) + "<sp>public<sp>int<sp>run()<sp>{\n" ) + "<sp>int<sp>res<sp>=<sp>0;\n" ) + "<sp>for(int<sp>i<sp>=<sp>0;<sp>i<sp><<sp>2;<sp>++i)<sp>{\n" ) ; java . lang . String middle = "" + "<sp>++res;\n" ; final java . lang . String postamble = "" + ( ( ( "<sp>}\n" + "run" 2 ) + "<sp>}\n" ) + "}" ) ; int [ ] repetitionss = new int [ ] { 1 , 10 , 100 , ( Short . MAX_VALUE ) / 5 , ( Short . MAX_VALUE ) / 4 , ( Short . MAX_VALUE ) / 2 } ; for ( int repetitions : repetitionss ) { java . lang . StringBuilder sb = new java . lang . StringBuilder ( ) ; sb . append ( preamble ) ; for ( int j = 0 ; j < repetitions ; ++ j ) { sb . append ( middle ) ; } sb . append ( postamble ) ; org . codehaus . commons . compiler . ISimpleCompiler sc = this . compilerFactory . newSimpleCompiler ( ) ; sc . cook ( sb . toString ( ) ) ; java . lang . Class < ? > c = sc . getClassLoader ( ) . loadClass ( "test.Test" ) ; java . lang . reflect . Method m = c . getDeclaredMethod ( "run" , new java . lang . Class [ 0 ] ) ; java . lang . Object o = c . newInstance ( ) ; java . lang . Object res = m . invoke ( o , new java . lang . Object [ 0 ] ) ; org . junit . Assert . assertEquals ( ( 2 * repetitions ) , res ) ; } } }
public class aTest{ @Test public void testBrowseUnknownNode ( ) { net . holmes . core . business . configuration . ConfigurationManager configurationManager = createMock ( net . holmes . core . business . configuration . ConfigurationManager . class ) ; net . holmes . core . business . media . MediaManager mediaManager = createMock ( net . holmes . core . business . media . MediaManager . class ) ; net . holmes . core . business . streaming . StreamingManager streamingManager = createMock ( net . holmes . core . business . streaming . StreamingManager . class ) ; org . fourthline . cling . model . profile . RemoteClientInfo remoteClientInfo = createMock ( org . fourthline . cling . model . profile . RemoteClientInfo . class ) ; net . holmes . core . service . upnp . directory . ContentDirectoryService contentDirectoryService = new net . holmes . core . service . upnp . directory . ContentDirectoryService ( ) ; contentDirectoryService . setConfigurationManager ( configurationManager ) ; contentDirectoryService . setMediaManager ( mediaManager ) ; contentDirectoryService . setStreamingManager ( streamingManager ) ; expect ( mediaManager . getNode ( eq ( "0" ) ) ) . andReturn ( java . util . Optional . empty ( ) ) ; replay ( mediaManager , streamingManager , remoteClientInfo , configurationManager ) ; try { org . fourthline . cling . support . model . BrowseResult result = contentDirectoryService . browse ( "0" , BrowseFlag . METADATA , 0 , 100 , remoteClientInfo ) ; org . junit . Assert . assertNotNull ( result ) ; } }
public class aTest{ @Test public void testLeaseTasksOnlyReturnsSpecifiedNumberOfTasks ( ) { com . google . appengine . api . taskqueue . Queue queue = com . google . appengine . api . taskqueue . QueueFactory . getQueue ( "pull-queue" ) ; com . google . appengine . api . taskqueue . TaskHandle th1 = queue . add ( withMethod ( org . jboss . test . capedwarf . tasks . test . PULL ) ) ; com . google . appengine . api . taskqueue . TaskHandle th2 = queue . add ( withMethod ( org . jboss . test . capedwarf . tasks . test . PULL ) ) ; try { int countLimit = 1 ; java . util . List < com . google . appengine . api . taskqueue . TaskHandle > handles = queue . leaseTasks ( 10 , TimeUnit . SECONDS , countLimit ) ; org . junit . Assert . assertEquals ( countLimit , handles . size ( ) ) ; } }
public class aTest{ @Test public void testCleanupUnknownPeerZNode ( ) { org . apache . hadoop . hbase . Server server = new org . apache . hadoop . hbase . replication . regionserver . TestReplicationSourceManager . DummyServer ( "hostname2.example.org" ) ; org . apache . hadoop . hbase . replication . ReplicationQueueStorage rq = org . apache . hadoop . hbase . replication . ReplicationStorageFactory . getReplicationQueueStorage ( server . getZooKeeper ( ) , server . getConfiguration ( ) ) ; java . lang . String group = "testgroup" ; rq . addWAL ( server . getServerName ( ) , "2" , ( group + ".log1" ) ) ; rq . addWAL ( server . getServerName ( ) , "2" , ( group + ".log2" ) ) ; org . apache . hadoop . hbase . replication . regionserver . ReplicationSourceManager . NodeFailoverWorker w1 = org . apache . hadoop . hbase . replication . regionserver . TestReplicationSourceManager . manager . new org . apache . hadoop . hbase . replication . regionserver . ReplicationSourceManager . NodeFailoverWorker ( server . getServerName ( ) ) ; w1 . run ( ) ; for ( java . lang . String peer : org . apache . hadoop . hbase . replication . regionserver . TestReplicationSourceManager . manager . getAllQueues ( ) ) { org . junit . Assert . assertTrue ( peer . startsWith ( "1" ) ) ; } } }
public class aTest{ @Test public void cascadeFromGenericMethodWithClassParameterOfMockedInterface ( mockit . CascadingWithGenericsTest$FactoryInterface ) { mockit . CascadingWithGenericsTest . Foo cascaded = mock . genericWithClass ( mockit . CascadingWithGenericsTest . Foo . class ) ; org . junit . Assert . assertNotNull ( cascaded ) ; } }
public class aTest{ @Test public void testModel ( ) { System . out . println ( "model" ) ; java . util . concurrent . ExecutorService ex = java . util . concurrent . Executors . newFixedThreadPool ( SystemInfo . LogicalCores ) ; for ( int iters = 0 ; iters < 2 ; iters ++ ) { int attempts = 3 ; do { jsat . text . topicmodel . List < jsat . text . topicmodel . Vec > basis = new jsat . text . topicmodel . ArrayList < jsat . text . topicmodel . Vec > ( ) ; for ( int i = 0 ; i < ( jsat . text . topicmodel . OnlineLDAsviTest . rows ) ; i ++ ) { jsat . text . topicmodel . Vec b0 = new jsat . text . topicmodel . SparseVector ( ( ( jsat . text . topicmodel . OnlineLDAsviTest . rows ) * ( jsat . text . topicmodel . OnlineLDAsviTest . rows ) ) ) ; for ( int a = 0 ; a < ( jsat . text . topicmodel . OnlineLDAsviTest . rows ) ; a ++ ) b0 . set ( ( ( i * 5 ) + a ) , 1.0 ) ; jsat . text . topicmodel . Vec b1 = new jsat . text . topicmodel . SparseVector ( ( ( jsat . text . topicmodel . OnlineLDAsviTest . rows ) * ( jsat . text . topicmodel . OnlineLDAsviTest . rows ) ) ) ; for ( int a = 0 ; a < ( jsat . text . topicmodel . OnlineLDAsviTest . rows ) ; a ++ ) b1 . set ( ( ( a * ( jsat . text . topicmodel . OnlineLDAsviTest . rows ) ) + i ) , 1.0 ) ; b0 . mutableDivide ( b0 . sum ( ) ) ; b1 . mutableDivide ( b1 . sum ( ) ) ; basis . add ( b0 ) ; basis . add ( b1 ) ; } double alpha = 0.1 ; jsat . text . topicmodel . List < jsat . classifiers . DataPoint > docs = new jsat . text . topicmodel . ArrayList < jsat . classifiers . DataPoint > ( ) ; jsat . distributions . multivariate . Dirichlet dirichlet = new jsat . distributions . multivariate . Dirichlet ( new jsat . text . topicmodel . ConstantVector ( alpha , basis . size ( ) ) ) ; jsat . text . topicmodel . Random rand = jsat . utils . random . RandomUtil . getRandom ( ) ; for ( jsat . text . topicmodel . Vec topicSample : dirichlet . sample ( 100000 , rand ) ) { jsat . text . topicmodel . Vec doc = new jsat . text . topicmodel . DenseVector ( basis . get ( 0 ) . length ( ) ) ; for ( int i = 0 ; i < 100 ; i ++ ) { double topicRand = rand . nextDouble ( ) ; int topic = 0 ; double sum = topicSample . get ( 0 ) ; while ( sum < topicRand ) { sum += topicSample . get ( ( ++ topic ) ) ; } jsat . text . topicmodel . Vec basisVec = basis . get ( topic ) ; int randBasisWord = rand . nextInt ( basisVec . nnz ( ) ) ; int pos = 0 ; for ( jsat . text . topicmodel . IndexValue iv : basisVec ) { if ( pos == randBasisWord ) { doc . increment ( iv . getIndex ( ) , 1.0 ) ; break ; } pos ++ ; } } docs . add ( new jsat . classifiers . DataPoint ( doc , new int [ 0 ] , new jsat . classifiers . CategoricalData [ 0 ] ) ) ; } jsat . text . topicmodel . OnlineLDAsvi lda = new jsat . text . topicmodel . OnlineLDAsvi ( ) ; lda . setAlpha ( 0.1 ) ; lda . setEta ( ( 1.0 / ( basis . size ( ) ) ) ) ; lda . setKappa ( 0.6 ) ; lda . setMiniBatchSize ( 256 ) ; lda . setTau0 ( 64 ) ; lda . setEpochs ( 1 ) ; if ( iters == 0 ) lda . model ( new jsat . SimpleDataSet ( docs ) , basis . size ( ) ) ; else lda . model ( new jsat . SimpleDataSet ( docs ) , basis . size ( ) , ex ) ; if ( passTest ( lda , basis , dirichlet , rand ) ) break ; } while ( ( attempts -- ) > 0 ) ; org . junit . Assert . assertTrue ( ( attempts > 0 ) ) ; } }
public class aTest{ @Test public void verifyVariablesWithNerdyStuffLikeRecursion ( ) { try { variablesMap . clear ( ) ; variablesMap . put ( "GNU" , "${GNU}'s<sp>Not<sp>UNIX" ) ; java . lang . String one = variablesMap . get ( "GNU" ) ; variablesMap . put ( "whatever" , "we<sp>just<sp>want<sp>to<sp>force<sp>map<sp>to<sp>resolve<sp>variables<sp>again..." ) ; java . lang . String two = variablesMap . get ( "GNU" ) ; org . junit . Assert . assertTrue ( ( ( one . length ( ) ) < ( two . length ( ) ) ) ) ; } }
public class aTest{ @Test public void testLicenseList ( eu . trentorise . opendata . jackan . CkanClient ) { java . util . List < eu . trentorise . opendata . jackan . model . CkanLicense > licenses = client . getLicenseList ( ) ; org . junit . Assert . assertTrue ( ( ( licenses . size ( ) ) > 0 ) ) ; } }
public class aTest{ @Test public void testSourceNotReady ( ) { int retries = 1 ; org . hibernate . cfg . Configuration cfg = new org . hibernate . cfg . Configuration ( ) ; cfg . setProperty ( "hibernate.search.default.sourceBase" , ( ( ( root . toAbsolutePath ( ) ) + ( org . hibernate . search . test . directoryProvider . FSSlaveAndMasterDPTest . masterCopy ) ) + "nooooot" ) ) ; cfg . setProperty ( "Should<sp>be<sp>around<sp>10<sp>seconds:<sp>" 0 , ( ( root . toAbsolutePath ( ) ) + ( org . hibernate . search . test . directoryProvider . FSSlaveAndMasterDPTest . slave ) ) ) ; cfg . setProperty ( "hibernate.search.default.refresh" , "1" ) ; cfg . setProperty ( "hibernate.search.lucene_version" , "LUCENE_CURRENT" ) ; cfg . setProperty ( "hibernate.search.default.directory_provider" , "filesystem-slave" ) ; cfg . setProperty ( "hibernate.search.default.retry_marker_lookup" , java . lang . String . valueOf ( retries ) ) ; cfg . addAnnotatedClass ( org . hibernate . search . test . directoryProvider . SnowStorm . class ) ; long start = java . lang . System . nanoTime ( ) ; try { cfg . buildSessionFactory ( ) ; } catch ( org . hibernate . search . exception . SearchException e ) { final long elapsedTime = TimeUnit . NANOSECONDS . toSeconds ( ( ( java . lang . System . nanoTime ( ) ) - start ) ) ; org . junit . Assert . assertTrue ( ( "Should<sp>be<sp>around<sp>10<sp>seconds:<sp>" + elapsedTime ) , ( elapsedTime > ( ( retries * 5 ) - 1 ) ) ) ; } } }
public class aTest{ @Test public void testDomain5 ( ) { org . springframework . mock . web . MockHttpServletRequest mockHttpServletRequest = new org . springframework . mock . web . MockHttpServletRequest ( ) ; mockHttpServletRequest . setServerName ( "www.liferay.com" ) ; java . lang . reflect . Field field = com . liferay . petra . reflect . ReflectionUtil . getDeclaredField ( com . liferay . portal . kernel . util . CookieKeys . class , "_SESSION_COOKIE_USE_FULL_HOSTNAME" ) ; java . lang . Object value = field . get ( null ) ; try { field . set ( null , Boolean . TRUE ) ; java . lang . String domain = com . liferay . portal . kernel . util . CookieKeys . getDomain ( mockHttpServletRequest ) ; org . junit . Assert . assertEquals ( StringPool . BLANK , domain ) ; } }
public class aTest{ @Test public void applyTest ( ) { java . lang . String [ ] [ ] tests = new java . lang . String [ ] [ ] { new java . lang . String [ ] { "{{<sp>4.5<sp>|<sp>at_least:5<sp>}}" 1 , "5" , "{}" } , new java . lang . String [ ] { "{{<sp>5<sp>|<sp>at_least:5<sp>}}" , "5" , "{}" } , new java . lang . String [ ] { "{{<sp>5<sp>|<sp>at_least:6<sp>}}" , "6" , "{}" } , new java . lang . String [ ] { "{{<sp>4.5<sp>|<sp>at_least:5<sp>}}" , "5" , "{}" } , new java . lang . String [ ] { "{{<sp>width<sp>|<sp>at_least:5<sp>}}" , "6" , "{<sp>\"width\":<sp>6<sp>}" } , new java . lang . String [ ] { "{{<sp>width<sp>|<sp>at_least:5<sp>}}" , "5" , "{<sp>\"width\":<sp>4<sp>}" } , new java . lang . String [ ] { "{{<sp>4.5<sp>|<sp>at_least:5<sp>}}" 0 , "6" , "{<sp>\"width\":<sp>6<sp>}" } } ; for ( java . lang . String [ ] test : tests ) { liqp . Template template = liqp . Template . parse ( test [ 0 ] ) ; java . lang . String rendered = java . lang . String . valueOf ( template . render ( test [ 2 ] ) ) ; org . junit . Assert . assertThat ( rendered , org . hamcrest . CoreMatchers . is ( test [ 1 ] ) ) ; } } }
public class aTest{ @Test public void testDynamicSalienceInStreamMode ( ) { final java . lang . String drl = "<sp>salience1.decrementAndGet();\n" 0 + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "<sp>salience1.decrementAndGet();\n" 3 + "global<sp>AtomicInteger<sp>salience1\n" ) + "<sp>salience1.decrementAndGet();\n" 2 ) + "<sp>@role(event)\n" 2 ) + "<sp>salience1.decrementAndGet();\n" 3 ) + "<sp>@role(event)\n" 4 ) + "<sp>@role(event)\n" ) + "end\n" ) + "<sp>salience1.decrementAndGet();\n" 3 ) + "<sp>@role(event)\n" 9 ) + "<sp>@role(event)\n" 8 ) + "when\n" ) + "<sp>@role(event)\n" 6 ) + "then\n" ) + "<sp>retract($i);\n" ) + "<sp>salience1.decrementAndGet();\n" ) + "<sp>@role(event)\n" 0 ) + "end<sp>\n" ) + "<sp>salience1.decrementAndGet();\n" 3 ) + "<sp>@role(event)\n" 5 ) + "salience<sp>salience2.get()\n" ) + "when\n" ) + "<sp>@role(event)\n" 6 ) + "then\n" ) + "<sp>retract($i);\n" ) + "<sp>@role(event)\n" 7 ) + "<sp>@role(event)\n" 1 ) + "<sp>salience1.decrementAndGet();\n" 5 ) ; final org . kie . api . KieBase kbase = org . drools . testcoverage . common . util . KieBaseUtil . getKieBaseFromKieModuleFromDrl ( "<sp>salience1.decrementAndGet();\n" 1 , kieBaseTestConfiguration , drl ) ; final org . kie . api . runtime . KieSession ksession = kbase . newKieSession ( ) ; try { final java . util . List < java . lang . Integer > list = new java . util . ArrayList ( ) ; ksession . setGlobal ( "<sp>@role(event)\n" 3 , list ) ; ksession . setGlobal ( "<sp>salience1.decrementAndGet();\n" 4 , new java . util . concurrent . atomic . AtomicInteger ( 9 ) ) ; ksession . setGlobal ( "salience2" , new java . util . concurrent . atomic . AtomicInteger ( 10 ) ) ; for ( int i = 0 ; i < 10 ; i ++ ) { ksession . insert ( i ) ; ksession . fireAllRules ( ) ; } org . junit . Assert . assertEquals ( list , java . util . Arrays . asList ( 2 , 1 , 2 , 1 , 2 , 1 , 2 , 1 , 2 , 1 ) ) ; } }
public class aTest{ @Test public void testAddBackendServers ( ) { try { com . fit2cloud . aliyun . slb . model . request . AddBackendServersRequest request = new com . fit2cloud . aliyun . slb . model . request . AddBackendServersRequest ( ) ; request . setLoadBalancerId ( loadBalancerId ) ; java . util . List < com . fit2cloud . aliyun . slb . model . BackendServer > backendServer = new java . util . ArrayList < com . fit2cloud . aliyun . slb . model . BackendServer > ( ) ; com . fit2cloud . aliyun . slb . model . BackendServer s1 = new com . fit2cloud . aliyun . slb . model . BackendServer ( ) ; s1 . setServerId ( serverId ) ; s1 . setWeight ( 100 ) ; backendServer . add ( s1 ) ; java . lang . String backendServers = new com . google . gson . Gson ( ) . toJson ( backendServer ) ; request . setBackendServers ( backendServers ) ; com . fit2cloud . aliyun . Response response = client . addBackendServers ( request ) ; System . out . println ( ( "testAddBackendServers<sp>::<sp>" + ( new com . google . gson . Gson ( ) . toJson ( response ) ) ) ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testDefaultInitialStartLevel ( ) { org . osgi . framework . launch . Framework framework = createFramework ( ) ; try { framework . start ( ) ; org . osgi . framework . startlevel . FrameworkStartLevel startLevel = framework . adapt ( org . osgi . framework . startlevel . FrameworkStartLevel . class ) ; org . junit . Assert . assertEquals ( 1 , startLevel . getStartLevel ( ) ) ; } }
public class aTest{ @Test public void testPriceGets ( ) { int id = 0 ; try { id = _setupTestPrice ( false ) ; _checkPriceIntoDb ( id ) ; } catch ( java . lang . Exception e ) { e . printStackTrace ( ) ; org . junit . Assert . assertEquals ( true , false ) ; } }
public class aTest{ @Test public void testDataSetAfterEachInterval ( ) { final org . drugis . addis . lyndobrien . ScatterPlotDataset set = new org . drugis . addis . lyndobrien . ScatterPlotDataset ( d_model ) ; final int n = 200 ; set . addChangeListener ( new org . jfree . data . general . DatasetChangeListener ( ) { public void datasetChanged ( org . jfree . data . general . DatasetChangeEvent arg0 ) { org . junit . Assert . assertEquals ( n , set . getItemCount ( 0 ) ) ; } } }
public class aTest{ @Test public void testMatchAllQuery ( ) { final java . lang . String EXPECTED_QUERY = cleanJson ( ( "{" + ( ( ( "\"query\"<sp>:<sp>{" + "\"match_all\"<sp>:<sp>{}" ) + "}" ) + "}" ) ) ) ; try { com . stratio . connector . commons . engine . query . ProjectParsed parsedQuery = createBaseParsedQuery ( ) ; org . elasticsearch . action . search . SearchRequestBuilder requestBuilder = ( ( org . elasticsearch . action . search . SearchRequestBuilder ) ( connectorQueryBuilder . buildQuery ( client , parsedQuery ) ) ) ; org . junit . Assert . assertEquals ( EXPECTED_QUERY , cleanJson ( requestBuilder . toString ( ) ) ) ; } }
public class aTest{ @Test public void testGetParametersWithDisabledDefaults ( ) { unit . setActive ( false ) ; unit . setSecurity ( false ) ; java . lang . String docSeries = "docPin" 2 ; java . lang . String docNum = "doc<sp>nu" ; java . util . Date docDate = new java . util . Date ( ) ; java . lang . String docIssued = "doc<sp>issued" ; java . lang . String docPin = "docPin" 3 ; java . lang . Double mark = 2.5 ; java . lang . Integer isChecked = 1 ; java . lang . Integer isForeign = 2 ; java . lang . Long personId = 1L ; org . lnu . is . domain . person . Person person = new org . lnu . is . domain . person . Person ( ) ; person . setId ( personId ) ; java . lang . Long paperTypeId = 2L ; org . lnu . is . domain . paper . type . PaperType paperType = new org . lnu . is . domain . paper . type . PaperType ( ) ; paperType . setId ( paperTypeId ) ; java . lang . Long honorTypeId = 4L ; org . lnu . is . domain . honors . type . HonorType honorsType = new org . lnu . is . domain . honors . type . HonorType ( ) ; honorsType . setId ( honorTypeId ) ; org . lnu . is . domain . person . paper . PersonPaper entity = new org . lnu . is . domain . person . paper . PersonPaper ( ) ; entity . setPerson ( person ) ; entity . setPaperType ( paperType ) ; entity . setHonorsType ( honorsType ) ; entity . setDocSeries ( docSeries ) ; entity . setDocNum ( docNum ) ; entity . setDocDate ( docDate ) ; entity . setDocIssued ( docIssued ) ; entity . setDocPin ( docPin ) ; entity . setMark ( mark ) ; entity . setIsChecked ( isChecked ) ; entity . setIsForeign ( isForeign ) ; java . util . Map < java . lang . String , java . lang . Object > expected = new java . util . HashMap < java . lang . String , java . lang . Object > ( ) ; expected . put ( "docPin" 4 , person ) ; expected . put ( "docPin" 0 , paperType ) ; expected . put ( "docPin" 1 , honorsType ) ; expected . put ( "docSeries" , docSeries ) ; expected . put ( "docNum" , docNum ) ; expected . put ( "docDate" , docDate ) ; expected . put ( "docIssued" , docIssued ) ; expected . put ( "docPin" , docPin ) ; expected . put ( "mark" , mark ) ; expected . put ( "isChecked" , isChecked ) ; expected . put ( "isForeign" , isForeign ) ; when ( personDao . getEntityById ( anyLong ( ) ) ) . thenReturn ( person ) ; when ( paperTypeDao . getEntityById ( anyLong ( ) ) ) . thenReturn ( paperType ) ; when ( honorTypeDao . getEntityById ( anyLong ( ) ) ) . thenReturn ( honorsType ) ; java . util . Map < java . lang . String , java . lang . Object > actual = unit . getParameters ( entity ) ; verify ( personDao ) . getEntityById ( personId ) ; verify ( paperTypeDao ) . getEntityById ( paperTypeId ) ; verify ( honorTypeDao ) . getEntityById ( honorTypeId ) ; org . junit . Assert . assertEquals ( expected , actual ) ; } }
public class aTest{ @Test public void pushInterruptably ( ) { final com . conversantmedia . util . concurrent . ConcurrentStack < java . lang . Integer > iStack = new com . conversantmedia . util . concurrent . ConcurrentStack ( 128 ) ; final java . util . concurrent . atomic . AtomicBoolean expectInterrupt = new java . util . concurrent . atomic . AtomicBoolean ( false ) ; for ( int i = 0 ; i < 128 ; i ++ ) { iStack . pushInterruptibly ( i ) ; } final java . lang . Thread t = new java . lang . Thread ( ( ) -> { try { java . lang . Thread . currentThread ( ) . interrupt ( ) ; iStack . pushInterruptibly ( 129 ) ; } catch ( e ) { expectInterrupt . set ( true ) ; } } ) ; t . start ( ) ; t . join ( ) ; org . junit . Assert . assertEquals ( true , expectInterrupt . get ( ) ) ; } }
public class aTest{ @Test public void testConfigureEnableEncoding ( ) { System . out . println ( ( ( getTestTraceHead ( "[NGSIPostgisSink.configure]" ) ) + "--------<sp>enable_encoding<sp>can<sp>only<sp>be<sp>'true'<sp>or<sp>'false'" ) ) ; java . lang . String attrPersistence = null ; java . lang . String batchSize = null ; java . lang . String batchTime = null ; java . lang . String batchTTL = null ; java . lang . String dataModel = null ; java . lang . String enableEncoding = "falso" ; java . lang . String enableGrouping = null ; java . lang . String enableLowercase = null ; java . lang . String host = null ; java . lang . String password = null ; java . lang . String port = null ; java . lang . String username = null ; java . lang . String cache = null ; com . telefonica . iot . cygnus . sinks . NGSIPostgisSink sink = new com . telefonica . iot . cygnus . sinks . NGSIPostgisSink ( ) ; sink . configure ( createContext ( attrPersistence , batchSize , batchTime , batchTTL , dataModel , enableEncoding , enableGrouping , enableLowercase , host , password , port , username , cache ) ) ; try { org . junit . Assert . assertTrue ( sink . getInvalidConfiguration ( ) ) ; System . out . println ( ( ( getTestTraceHead ( "[NGSIPostgisSink.configure]" ) ) + "-<sp>OK<sp>-<sp>'enable_encoding=falso'<sp>was<sp>detected" ) ) ; } }
public class aTest{ @Test public void testRMIMessage ( ) { try { i5 . las2peer . security . BasicAgentStorage storage = new i5 . las2peer . security . BasicAgentStorage ( ) ; i5 . las2peer . security . UserAgentImpl eve = i5 . las2peer . testing . MockAgentFactory . getEve ( ) ; i5 . las2peer . security . ServiceAgentImpl service = i5 . las2peer . security . ServiceAgentImpl . createServiceAgent ( i5 . las2peer . api . p2p . ServiceNameVersion . fromString ( "i5.las2peer.api.TestService@1.0" ) , "a<sp>pass" ) ; storage . registerAgents ( eve , service ) ; eve . unlock ( "evespass" ) ; i5 . las2peer . communication . Message m = new i5 . las2peer . communication . Message ( eve , service , new i5 . las2peer . execution . RMITask ( i5 . las2peer . api . p2p . ServiceNameVersion . fromString ( "i5.las2peer.api.TestService@1.0" ) , "inc" , new java . io . Serializable [ ] { new java . lang . Integer ( 10 ) } ) ) ; java . lang . String xml = m . toXmlString ( ) ; i5 . las2peer . communication . Message back = i5 . las2peer . communication . Message . createFromXml ( xml ) ; service . unlock ( "a<sp>pass" ) ; back . open ( service , storage ) ; i5 . las2peer . execution . RMITask content = ( ( i5 . las2peer . execution . RMITask ) ( back . getContent ( ) ) ) ; org . junit . Assert . assertEquals ( new java . lang . Integer ( 10 ) , content . getParameters ( ) [ 0 ] ) ; } }
public class aTest{ @Test public void test001 ( ) { System . out . println ( "---------Erstelle<sp>Job" ) ; org . kapott . hbci . GV . HBCIJob job = handler . newJob ( "UebSEPA" ) ; job . setParam ( "src" , passport . getAccounts ( ) [ 2 ] ) ; job . setParam ( "dst" , passport . getAccounts ( ) [ 0 ] ) ; job . setParam ( "btg" , new org . kapott . hbci . structures . Value ( 100L , "EUR" ) ) ; job . setParam ( "usage" , "UebSEPA" 3 ) ; System . out . println ( "UebSEPA" 2 ) ; job . addToQueue ( ) ; org . kapott . hbci . status . HBCIExecStatus ret = handler . execute ( ) ; org . kapott . hbci . GV_Result . HBCIJobResult res = job . getJobResult ( ) ; System . out . println ( ( "----------Result:<sp>" + ( res . toString ( ) ) ) ) ; org . junit . Assert . assertEquals ( "UebSEPA" 0 , true , res . isOK ( ) ) ; } }
public class aTest{ @Test public void sendPingTCP ( ) { net . tomp2p . p2p . Peer sender = null ; net . tomp2p . connection . ChannelCreator cc = null ; try { net . tomp2p . peers . PeerAddress pa = net . tomp2p . peers . PeerAddress . create ( Number160 . ZERO , java . net . Inet4Address . getByName ( net . tomp2p . rpc . TestRealPing . IP ) , net . tomp2p . rpc . TestRealPing . PORT , net . tomp2p . rpc . TestRealPing . PORT , ( ( net . tomp2p . rpc . TestRealPing . PORT ) + 1 ) ) ; sender = new net . tomp2p . p2p . PeerBuilder ( new net . tomp2p . peers . Number160 ( "0x9876" ) ) . ports ( net . tomp2p . rpc . TestRealPing . PORT ) . enableMaintenance ( false ) . start ( ) ; net . tomp2p . rpc . PingRPC handshake = new net . tomp2p . rpc . PingRPC ( sender . peerBean ( ) , sender . connectionBean ( ) ) ; net . tomp2p . futures . FutureChannelCreator fcc = sender . connectionBean ( ) . reservation ( ) . create ( 0 , 1 ) ; fcc . awaitUninterruptibly ( ) ; cc = fcc . channelCreator ( ) ; net . tomp2p . futures . FutureResponse fr = handshake . pingTCP ( pa , cc , new net . tomp2p . connection . DefaultConnectionConfiguration ( ) ) ; fr . awaitUninterruptibly ( ) ; org . junit . Assert . assertEquals ( true , fr . isSuccess ( ) ) ; java . lang . Thread . sleep ( net . tomp2p . rpc . TestRealPing . WAIT ) ; } }
public class aTest{ @Test public void testPasteAtEndOf ( ) { java . util . TreeMap < org . odftoolkit . odfdom . dom . style . props . OdfStyleProperty , java . lang . String > searchProps = new java . util . TreeMap < org . odftoolkit . odfdom . dom . style . props . OdfStyleProperty , java . lang . String > ( ) ; searchProps . put ( StyleTextPropertiesElement . FontName , "Times<sp>New<sp>Roman1" ) ; searchProps . put ( StyleTextPropertiesElement . FontSize , "16pt" ) ; search1 = new org . odftoolkit . odfdom . incubator . search . TextStyleNavigation ( searchProps , doc ) ; search2 = new org . odftoolkit . odfdom . incubator . search . TextNavigation ( "delete" , doc ) ; search3 = new org . odftoolkit . odfdom . incubator . search . TextNavigation ( "deleteRoman16<sp>Romanl16" , doc ) ; org . odftoolkit . odfdom . incubator . search . TextSelection itemstyle = null ; if ( search1 . hasNext ( ) ) { itemstyle = ( ( org . odftoolkit . odfdom . incubator . search . TextSelection ) ( search1 . getCurrentItem ( ) ) ) ; org . odftoolkit . odfdom . incubator . search . TextStyleNavigationTest . LOG . info ( itemstyle . toString ( ) ) ; } int i = 0 ; if ( itemstyle != null ) { while ( search2 . hasNext ( ) ) { i ++ ; org . odftoolkit . odfdom . incubator . search . TextSelection itemtext = ( ( org . odftoolkit . odfdom . incubator . search . TextSelection ) ( search2 . getCurrentItem ( ) ) ) ; try { itemstyle . pasteAtEndOf ( itemtext ) ; } catch ( org . odftoolkit . odfdom . incubator . search . InvalidNavigationException e ) { org . junit . Assert . fail ( e . getMessage ( ) ) ; } org . odftoolkit . odfdom . incubator . search . TextStyleNavigationTest . LOG . info ( itemtext . toString ( ) ) ; } } int j = 0 ; while ( search3 . hasNext ( ) ) { j ++ ; } org . junit . Assert . assertTrue ( ( i == j ) ) ; try { doc . save ( org . odftoolkit . odfdom . utils . ResourceUtilities . newTestOutputFile ( org . odftoolkit . odfdom . incubator . search . TextStyleNavigationTest . SAVE_FILE_PAST_END ) ) ; } }
public class aTest{ @Test public void clusterServerRefValid ( ) { com . sun . enterprise . config . serverbeans . Cluster cluster = habitat . getService ( com . sun . enterprise . config . serverbeans . Cluster . class , "clusterA" ) ; org . junit . Assert . assertNotNull ( cluster ) ; com . sun . enterprise . config . serverbeans . ServerRef sref = cluster . getServerRef ( ) . get ( 0 ) ; org . jvnet . hk2 . config . ConfigBean serverConfig = ( ( org . jvnet . hk2 . config . ConfigBean ) ( org . jvnet . hk2 . config . ConfigBean . unwrap ( sref ) ) ) ; java . util . Map < org . jvnet . hk2 . config . ConfigBean , java . util . Map < java . lang . String , java . lang . String > > changes = new java . util . HashMap < org . jvnet . hk2 . config . ConfigBean , java . util . Map < java . lang . String , java . lang . String > > ( ) ; java . util . Map < java . lang . String , java . lang . String > configChanges = new java . util . HashMap < java . lang . String , java . lang . String > ( ) ; configChanges . put ( "ref" , "server" ) ; changes . put ( serverConfig , configChanges ) ; try { org . jvnet . hk2 . config . ConfigSupport cs = getHabitat ( ) . getService ( org . jvnet . hk2 . config . ConfigSupport . class ) ; cs . apply ( changes ) ; } }
public class aTest{ @Test public void decoratedModelShouldBeThreadSafe ( ) { final java . util . List < ro . isdc . wro . util . Transformer < ro . isdc . wro . model . WroModel > > modelTransformers = new java . util . ArrayList < ro . isdc . wro . util . Transformer < ro . isdc . wro . model . WroModel > > ( ) ; modelTransformers . add ( new ro . isdc . wro . model . transformer . WildcardExpanderModelTransformer ( ) ) ; factory = ro . isdc . wro . model . factory . DefaultWroModelFactoryDecorator . decorate ( new ro . isdc . wro . extensions . model . factory . GroovyModelFactory ( ) { @ ro . isdc . wro . extensions . model . factory . Override protected java . io . InputStream getModelResourceAsStream ( ) throws java . io . IOException { return ro . isdc . wro . extensions . model . factory . TestGroovyModelFactory . class . getResourceAsStream ( "wro.groovy" ) ; } } , modelTransformers ) ; ro . isdc . wro . util . WroTestUtils . init ( factory ) ; final ro . isdc . wro . model . WroModel expectedModel = factory . create ( ) ; ro . isdc . wro . util . WroTestUtils . runConcurrently ( new ro . isdc . wro . config . support . ContextPropagatingCallable < java . lang . Void > ( new java . util . concurrent . Callable < java . lang . Void > ( ) { @ ro . isdc . wro . extensions . model . factory . Override public ro . isdc . wro . extensions . model . factory . Void call ( ) throws ro . isdc . wro . extensions . model . factory . Exception { org . junit . Assert . assertEquals ( expectedModel , factory . create ( ) ) ; return null ; } } }
public class aTest{ @Test public void testTableSinkWithMacro ( ) { io . cdap . cdap . api . data . schema . Schema schema = io . cdap . cdap . api . data . schema . Schema . recordOf ( "user" 4 , Schema . Field . of ( "rowkey" , io . cdap . cdap . api . data . schema . Schema . of ( Schema . Type . STRING ) ) , Schema . Field . of ( "user" , io . cdap . cdap . api . data . schema . Schema . of ( Schema . Type . STRING ) ) , Schema . Field . of ( "count" , io . cdap . cdap . api . data . schema . Schema . of ( Schema . Type . INT ) ) ) ; io . cdap . cdap . etl . proto . v2 . ETLPlugin sourceConfig = new io . cdap . cdap . etl . proto . v2 . ETLPlugin ( "user" 7 , io . cdap . cdap . etl . api . batch . BatchSource . PLUGIN_TYPE , com . google . common . collect . ImmutableMap . of ( Properties . BatchReadableWritable . NAME , "user" 1 , Properties . Table . PROPERTY_SCHEMA_ROW_FIELD , "rowkey" , Properties . Table . PROPERTY_SCHEMA , schema . toString ( ) ) , null ) ; io . cdap . cdap . etl . proto . v2 . ETLStage source = new io . cdap . cdap . etl . proto . v2 . ETLStage ( "tableSource" , sourceConfig ) ; io . cdap . cdap . etl . proto . v2 . ETLPlugin sinkConfig = new io . cdap . cdap . etl . proto . v2 . ETLPlugin ( "user" 7 , io . cdap . cdap . etl . api . batch . BatchSink . PLUGIN_TYPE , com . google . common . collect . ImmutableMap . of ( Properties . Table . NAME , "user" 3 , Properties . Table . PROPERTY_SCHEMA_ROW_FIELD , "rowkey" ) , null ) ; io . cdap . cdap . etl . proto . v2 . ETLStage sink = new io . cdap . cdap . etl . proto . v2 . ETLStage ( "tableSinkUnique" , sinkConfig ) ; io . cdap . cdap . etl . proto . v2 . ETLBatchConfig etlConfig = io . cdap . cdap . etl . proto . v2 . ETLBatchConfig . builder ( "user" 5 ) . addStage ( source ) . addStage ( sink ) . addConnection ( source . getName ( ) , sink . getName ( ) ) . build ( ) ; io . cdap . cdap . test . ApplicationManager appManager = deployETL ( etlConfig , "user" 2 ) ; io . cdap . cdap . test . DataSetManager < io . cdap . cdap . api . dataset . table . Table > inputManager = getDataset ( "user" 1 ) ; io . cdap . cdap . api . dataset . table . Table inputTable = inputManager . get ( ) ; io . cdap . cdap . api . dataset . table . Put put = new io . cdap . cdap . api . dataset . table . Put ( io . cdap . cdap . api . common . Bytes . toBytes ( "row1" ) ) ; put . add ( "user" , "samuel" ) ; put . add ( "count" , 5 ) ; put . add ( "user" 6 , 123.45 ) ; put . add ( "item" , "user" 0 ) ; inputTable . put ( put ) ; inputManager . flush ( ) ; java . util . Map < java . lang . String , java . lang . String > runTimeProperties = new java . util . HashMap < java . lang . String , java . lang . String > ( ) ; runTimeProperties . put ( "name" , "tableSinkName" ) ; runETLOnce ( appManager , runTimeProperties ) ; io . cdap . cdap . test . DataSetManager < io . cdap . cdap . api . dataset . table . Table > tableManager = getDataset ( "tableSinkName" ) ; io . cdap . cdap . api . dataset . table . Table table = tableManager . get ( ) ; org . junit . Assert . assertEquals ( "samuel" , table . get ( io . cdap . cdap . api . common . Bytes . toBytes ( "row1" ) ) . getString ( "user" ) ) ; } }
public class aTest{ @Test public void testDropIndex ( ) { java . lang . String className = "testDropIndex" ; java . lang . String propertyName = "foo" ; com . orientechnologies . orient . core . metadata . schema . OSchema schema = com . orientechnologies . orient . core . sql . executor . ODropPropertyStatementExecutionTest . db . getMetadata ( ) . getSchema ( ) ; schema . createClass ( className ) . createProperty ( propertyName , OType . STRING ) . createIndex ( OClass . INDEX_TYPE . NOTUNIQUE ) ; schema . reload ( ) ; org . junit . Assert . assertNotNull ( schema . getClass ( className ) . getProperty ( propertyName ) ) ; try { com . orientechnologies . orient . core . sql . executor . ODropPropertyStatementExecutionTest . db . command ( ( ( ( "drop<sp>property<sp>" + className ) + "." ) + propertyName ) ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void testTrace ( ) { try { try ( org . eclipse . tracecompass . ctf . core . trace . CTFTraceReader reader = new org . eclipse . tracecompass . ctf . core . trace . CTFTraceReader ( trace ) ) { reader . getCurrentEventDef ( ) ; while ( reader . advance ( ) ) { org . junit . Assert . assertNotNull ( reader . getCurrentEventDef ( ) ) ; } }
public class aTest{ @Test public void initial_getRefreshTime ( ) { org . cache2k . Cache < java . lang . Integer , java . lang . Integer > c = target . cache ( ) ; final long t0 = java . lang . System . currentTimeMillis ( ) ; c . invoke ( 1 , new org . cache2k . processor . EntryProcessor < java . lang . Integer , java . lang . Integer , java . lang . Object > ( ) { @ org . cache2k . test . core . Override public java . lang . Object process ( final org . cache2k . processor . MutableCacheEntry < java . lang . Integer , java . lang . Integer > e ) { org . junit . Assert . assertEquals ( 0L , e . getRefreshedTime ( ) ) ; return null ; } } }
public class aTest{ @Test public void testCreateDiffID ( ) { new com . bugquery . serverside . dbparsing . dbupdating . XMLDiffer ( getClass ( ) . getClassLoader ( ) . getResource ( "old.xml" ) . getPath ( ) , getClass ( ) . getClassLoader ( ) . getResource ( "new.xml" ) . getPath ( ) , ( ( getClass ( ) . getClassLoader ( ) . getResource ( "" ) . getPath ( ) ) + "/diff.xml" ) ) . createDiff ( true ) ; try ( java . io . BufferedReader br1 = new java . io . BufferedReader ( new java . io . FileReader ( getClass ( ) . getClassLoader ( ) . getResource ( "diffcomp.xml" ) . getPath ( ) ) ) ; java . io . BufferedReader br2 = new java . io . BufferedReader ( new java . io . FileReader ( ( ( getClass ( ) . getClassLoader ( ) . getResource ( "" ) . getPath ( ) ) + "/diff.xml" ) ) ) ) { for ( java . lang . String line1 , line2 ; ( ( line1 = br1 . readLine ( ) ) != null ) && ( ( line2 = br2 . readLine ( ) ) != null ) ; ) org . junit . Assert . assertEquals ( line1 , line2 ) ; } }
public class aTest{ @Test public void test4 ( ) { final java . io . File tempFile = java . io . File . createTempFile ( "ConcurrentWriteTest_test4" , ".bin" ) ; final long limit = 400000000L ; final long writers = 8L ; final long partition = limit / writers ; final java . io . RandomAccessFile tmpRAF = new tlc2 . util . BufferedRandomAccessFile ( tempFile , "rw" ) ; tmpRAF . setLength ( ( limit * ( Long . BYTES ) ) ) ; final java . nio . channels . FileChannel channel = tmpRAF . getChannel ( ) ; final java . util . Collection < java . util . concurrent . Callable < java . lang . Void > > tasks = new java . util . ArrayList < java . util . concurrent . Callable < java . lang . Void > > ( ( ( int ) ( writers ) ) ) ; for ( long i = 0L ; i < writers ; i ++ ) { final long id = i ; tasks . add ( new java . util . concurrent . Callable < java . lang . Void > ( ) { public tlc2 . tool . fp . Void call ( ) throws tlc2 . tool . fp . Exception { long position = ( id * partition ) * ( Long . BYTES ) ; final java . nio . ByteBuffer buffer = buffer . putLong ( j ) ; buffer . flip ( ) ; channel . write ( buffer , ( position + ( j * ( Long . BYTES ) ) ) ) ; buffer . clear ( ) ; } channel . force ( false ) ; return null ; } } ) ; } final java . util . concurrent . ExecutorService executorService = java . util . concurrent . Executors . newFixedThreadPool ( ( ( int ) ( writers ) ) ) ; executorService . invokeAll ( tasks ) ; executorService . shutdown ( ) ; tmpRAF . close ( ) ; final tlc2 . util . BufferedRandomAccessFile checkRaf = new tlc2 . util . BufferedRandomAccessFile ( tempFile , "r" ) ; for ( long i = 0L ; i < limit ; i ++ ) { org . junit . Assert . assertEquals ( i , checkRaf . readLong ( ) ) ; } }
public class aTest{ @Test public void testStrings ( ) { try { java . lang . String [ ] data = new java . lang . String [ ] { "Oh" , "boy" , "what" , "a" , "show" , "!" } ; org . apache . flink . streaming . api . functions . source . FromElementsFunction < java . lang . String > source = new org . apache . flink . streaming . api . functions . source . FromElementsFunction < java . lang . String > ( BasicTypeInfo . STRING_TYPE_INFO . createSerializer ( new org . apache . flink . api . common . ExecutionConfig ( ) ) , data ) ; java . util . List < java . lang . String > result = new java . util . ArrayList < java . lang . String > ( ) ; source . run ( new org . apache . flink . streaming . api . functions . ListSourceContext < java . lang . String > ( result ) ) ; org . junit . Assert . assertEquals ( java . util . Arrays . asList ( data ) , result ) ; } }
public class aTest{ @Test public void testHandleGetType ( ) { org . zoodb . test . testutil . TestTools . defineSchema ( org . zoodb . test . jdo . TestClassTiny . class ) ; javax . jdo . PersistenceManager pm = org . zoodb . test . testutil . TestTools . openPM ( ) ; pm . currentTransaction ( ) . begin ( ) ; org . zoodb . schema . ZooClass s = org . zoodb . jdo . ZooJdoHelper . schema ( pm ) . getClass ( org . zoodb . test . jdo . TestClassTiny . class ) ; org . zoodb . test . jdo . TestClassTiny t1 = new org . zoodb . test . jdo . TestClassTiny ( 1 , 3 ) ; org . zoodb . test . jdo . TestClassTiny t2 = new org . zoodb . test . jdo . TestClassTiny ( 4 , 5 ) ; pm . makePersistent ( t1 ) ; pm . makePersistent ( t2 ) ; java . lang . Object oid1 = pm . getObjectId ( t1 ) ; java . lang . Object oid2 = pm . getObjectId ( t2 ) ; pm . currentTransaction ( ) . commit ( ) ; pm . currentTransaction ( ) . begin ( ) ; org . zoodb . schema . ZooHandle h1 = org . zoodb . jdo . ZooJdoHelper . schema ( pm ) . getHandle ( ( ( java . lang . Long ) ( oid1 ) ) ) ; org . zoodb . schema . ZooHandle h2 = org . zoodb . jdo . ZooJdoHelper . schema ( pm ) . getHandle ( ( ( java . lang . Long ) ( oid2 ) ) ) ; org . junit . Assert . assertTrue ( ( s == ( h1 . getType ( ) ) ) ) ; h1 . remove ( ) ; try { h1 . getType ( ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void marshallingKnowledgeSessionTest ( ) { ksession = kbase . newStatefulKnowledgeSession ( ) ; org . drools . logger . KnowledgeRuntimeLoggerFactory . newConsoleLogger ( ksession ) ; drools . cookbook . chapter02 . Server debianServer = new drools . cookbook . chapter02 . Server ( "debianServer" , 4 , 4096 , 1024 , 0 ) ; ksession . insert ( debianServer ) ; drools . cookbook . chapter02 . virtualization . Virtualization rhel = new drools . cookbook . chapter02 . virtualization . Virtualization ( "rhel" , "debianServer" , 2048 , 160 ) ; drools . cookbook . chapter02 . virtualization . VirtualizationRequest virtualizationRequest = new drools . cookbook . chapter02 . virtualization . VirtualizationRequest ( rhel ) ; ksession . insert ( virtualizationRequest ) ; ksession . fireAllRules ( ) ; java . io . ByteArrayOutputStream baos = new java . io . ByteArrayOutputStream ( ) ; java . io . File file = new java . io . File ( "ksession.info" ) ; java . io . FileOutputStream foStream ; try { foStream = new java . io . FileOutputStream ( file ) ; marshaller . marshall ( baos , ksession ) ; baos . writeTo ( foStream ) ; baos . close ( ) ; java . io . ByteArrayInputStream inputStream = new java . io . ByteArrayInputStream ( baos . toByteArray ( ) ) ; org . drools . runtime . StatefulKnowledgeSession ksession = marshaller . unmarshall ( inputStream ) ; java . io . FileInputStream fis = new java . io . FileInputStream ( "ksession.info" ) ; ksession = marshaller . unmarshall ( fis ) ; org . junit . Assert . assertNotNull ( ksession ) ; } }
public class aTest{ @Test public void nextLeSkip ( ) { org . cojen . tupl . View ix = openIndex ( "test" ) ; ix . store ( Transaction . BOGUS , key ( 0 ) , value ( 0 ) ) ; ix . store ( Transaction . BOGUS , key ( 1 ) , value ( 1 ) ) ; ix . store ( Transaction . BOGUS , key ( 2 ) , value ( 2 ) ) ; org . cojen . tupl . Cursor c ; { org . cojen . tupl . Transaction txn = mDb . newTransaction ( ) ; txn . lockTimeout ( 60 , TimeUnit . SECONDS ) ; c = ix . newCursor ( txn ) ; c . first ( ) ; } org . cojen . tupl . Transaction txn = mDb . newTransaction ( ) ; ix . lockExclusive ( txn , key ( 1 ) ) ; java . util . concurrent . atomic . AtomicReference < java . lang . Exception > ex = new java . util . concurrent . atomic . AtomicReference ( ) ; java . lang . Thread t = new java . lang . Thread ( ( ) -> { try { c . nextLe ( key ( 2 ) ) ; } catch ( e ) { ex . set ( org . cojen . tupl . e ) ; } } ) ; startAndWaitUntilBlocked ( t ) ; ix . delete ( txn , key ( 1 ) ) ; txn . commit ( ) ; t . join ( ) ; org . junit . Assert . assertNull ( ex . get ( ) ) ; fastAssertArrayEquals ( key ( 2 ) , c . key ( ) ) ; fastAssertArrayEquals ( value ( 2 ) , c . value ( ) ) ; } }
public class aTest{ @Test public void shouldCloneViaXStreamWithoutError ( ) { try { final java . lang . reflect . Method m = org . pitest . reflection . Reflection . publicMethod ( this . getClass ( ) , "shouldCloneViaXStreamWithoutError" ) ; final org . pitest . simpletest . TestMethod testee = new org . pitest . simpletest . TestMethod ( m , java . io . IOException . class ) ; final org . pitest . simpletest . TestMethod actual = ( ( org . pitest . simpletest . TestMethod ) ( org . pitest . util . XStreamCloning . clone ( testee ) ) ) ; org . junit . Assert . assertEquals ( actual . getMethod ( ) , testee . getMethod ( ) ) ; } }
public class aTest{ @Test public void testRollback ( ) { try { com . cloud . utils . db . Transaction . execute ( new com . cloud . utils . db . TransactionCallback < java . lang . Object > ( ) { @ com . cloud . utils . db . Override public java . lang . Object doInTransaction ( final com . cloud . utils . db . TransactionStatus status ) { throw new java . lang . RuntimeException ( "Panic!" ) ; } } ) ; org . junit . Assert . fail ( ) ; } catch ( final java . lang . RuntimeException e ) { org . junit . Assert . assertEquals ( "Panic!" , e . getMessage ( ) ) ; } }
public class aTest{ @Test public void doubleArray ( ) { final org . apache . commons . lang3 . builder . MultilineRecursiveToStringStyleTest . WithArrays wa = new org . apache . commons . lang3 . builder . MultilineRecursiveToStringStyleTest . WithArrays ( ) ; wa . doubleArray = new double [ ] { 1 , 2 } ; final java . lang . String exp = ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( getClassPrefix ( wa ) ) + "[" ) + ( BR ) ) + "<sp>boolArray=<null>," ) + ( BR ) ) + "<sp>charArray=<null>," ) + ( BR ) ) + "<sp>intArray=<null>," ) + ( BR ) ) + "<sp>doubleArray={" ) + ( BR ) ) + "<sp>1.0," ) + ( BR ) ) + "<sp>}," 0 ) + ( BR ) ) + "<sp>}," ) + ( BR ) ) + "<sp>longArray=<null>," ) + ( BR ) ) + "<sp>stringArray=<null>" ) + ( BR ) ) + "]" ; org . junit . Assert . assertEquals ( exp , toString ( wa ) ) ; } }
public class aTest{ @Test public void testBuildTableNameRootServicePathDataModelByEntityOldEncoding ( ) { System . out . println ( ( ( ( ( getTestTraceHead ( "[NGSIPostgreSQLSink.buildTableName]" ) ) + "[NGSIPostgreSQLSink.buildTableName]" 0 ) + "'dm-by-service-path'<sp>the<sp>MySQL<sp>table<sp>name<sp>is<sp>the<sp>encoding<sp>of<sp>the<sp>concatenation<sp>of<sp><service-path>,<sp>" ) + "<entityId><sp>and<sp><entityType>" ) ) ; java . lang . String attrPersistence = null ; java . lang . String batchSize = null ; java . lang . String batchTime = null ; java . lang . String batchTTL = null ; java . lang . String dataModel = "dm-by-entity" ; java . lang . String enableEncoding = "[NGSIPostgreSQLSink.buildTableName]" 2 ; java . lang . String enableGrouping = null ; java . lang . String enableLowercase = null ; java . lang . String host = null ; java . lang . String password = null ; java . lang . String port = null ; java . lang . String username = null ; java . lang . String cache = null ; com . telefonica . iot . cygnus . sinks . NGSIPostgreSQLSink sink = new com . telefonica . iot . cygnus . sinks . NGSIPostgreSQLSink ( ) ; sink . configure ( createContext ( attrPersistence , batchSize , batchTime , batchTTL , dataModel , enableEncoding , enableGrouping , enableLowercase , host , password , port , username , cache ) ) ; java . lang . String servicePath = "[NGSIPostgreSQLSink.buildTableName]" 1 ; java . lang . String entity = "someId=someType" ; java . lang . String attribute = null ; try { java . lang . String builtTableName = sink . buildTableName ( servicePath , entity , attribute ) ; java . lang . String expecetedTableName = "someId_someType" ; try { org . junit . Assert . assertEquals ( expecetedTableName , builtTableName ) ; System . out . println ( ( ( ( ( getTestTraceHead ( "[NGSIPostgreSQLSink.buildTableName]" ) ) + "-<sp>OK<sp>-<sp>'" ) + builtTableName ) + "'<sp>is<sp>equals<sp>to<sp>the<sp>encoding<sp>of<sp><service-path>" ) ) ; } }
public class aTest{ @Test public void testSelectSumBaseTable ( ) { org . verdictdb . core . sqlobject . BaseTable base = new org . verdictdb . core . sqlobject . BaseTable ( "myschema" , "mytable" , "t" ) ; java . lang . String aliasName = ",<sp>" 5 ; org . verdictdb . core . sqlobject . SelectQuery relation = org . verdictdb . core . sqlobject . SelectQuery . create ( java . util . Arrays . < org . verdictdb . core . sqlobject . SelectItem > asList ( new org . verdictdb . core . sqlobject . AliasedColumn ( new org . verdictdb . core . sqlobject . ColumnOp ( "sum" , new org . verdictdb . core . sqlobject . BaseColumn ( "t" , "mycolumn1" ) ) , aliasName ) ) , base ) ; org . verdictdb . core . scrambling . ScrambleMetaSet meta = generateTestScrambleMeta ( ) ; org . verdictdb . core . rewriter . query . AggQueryRewriter rewriter = new org . verdictdb . core . rewriter . query . AggQueryRewriter ( meta ) ; java . util . List < org . apache . commons . lang3 . tuple . Pair < org . verdictdb . core . sqlobject . AbstractRelation , org . verdictdb . core . rewriter . query . AggblockMeta > > rewritten = rewriter . rewrite ( relation ) ; java . lang . String aliasForSumEstimate = org . verdictdb . core . rewriter . AliasRenamingRules . sumEstimateAliasName ( aliasName ) ; java . lang . String aliasForSumScaledSubsum = org . verdictdb . core . rewriter . AliasRenamingRules . sumScaledSumAliasName ( aliasName ) ; java . lang . String aliasForSumSquaredScaledSubsum = org . verdictdb . core . rewriter . AliasRenamingRules . sumSquaredScaledSumAliasName ( aliasName ) ; java . lang . String aliasForCountSubsample = org . verdictdb . core . rewriter . AliasRenamingRules . countSubsampleAliasName ( ) ; java . lang . String aliasForSumSubsampleSize = org . verdictdb . core . rewriter . AliasRenamingRules . sumSubsampleSizeAliasName ( ) ; for ( int k = 0 ; k < ( aggblockCount ) ; k ++ ) { java . lang . String expected = ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "select<sp>verdictdbalias4.`verdictdbalias5`<sp>as<sp>`verdictdb:tier`,<sp>" + ",<sp>" 9 ) + ( quoteAlias ( aliasForSumEstimate ) ) ) + ",<sp>" ) + "sum(case<sp>when<sp>verdictdbalias1.`mycolumn1`<sp>is<sp>not<sp>null<sp>then<sp>1<sp>else<sp>0<sp>end)<sp>as<sp>`verdictdbalias7`<sp>" 4 ) + "verdictdbalias4.`verdictdbalias7`)<sp>as<sp>" ) + ( quoteAlias ( aliasForSumScaledSubsum ) ) ) + ",<sp>" ) + ",<sp>" 3 ) + "verdictdbalias4.`verdictdbalias7`)<sp>as<sp>" ) + ( quoteAlias ( aliasForSumSquaredScaledSubsum ) ) ) + ",<sp>" ) + "count(*)<sp>as<sp>" ) + ( quoteAlias ( aliasForCountSubsample ) ) ) + ",<sp>" ) + ",<sp>" 8 ) + ( quoteAlias ( aliasForSumSubsampleSize ) ) ) + ",<sp>" 7 ) + "sum(case<sp>when<sp>verdictdbalias1.`mycolumn1`<sp>is<sp>not<sp>null<sp>then<sp>1<sp>else<sp>0<sp>end)<sp>as<sp>`verdictdbalias7`<sp>" 2 ) + ",<sp>" 1 ) + "sum(verdictdbalias1.`mycolumn1`)<sp>as<sp>`verdictdbalias6`,<sp>" ) + "sum(case<sp>when<sp>verdictdbalias1.`mycolumn1`<sp>is<sp>not<sp>null<sp>then<sp>1<sp>else<sp>0<sp>end)<sp>as<sp>`verdictdbalias7`<sp>" ) + "sum(case<sp>when<sp>verdictdbalias1.`mycolumn1`<sp>is<sp>not<sp>null<sp>then<sp>1<sp>else<sp>0<sp>end)<sp>as<sp>`verdictdbalias7`<sp>" 1 ) + "t.`verdictdbsid`<sp>as<sp>`verdictdbalias2`,<sp>" ) + ",<sp>" 2 ) + "from<sp>`myschema`.`mytable`<sp>as<sp>t<sp>" ) + ",<sp>" 6 ) + k ) + "sum(case<sp>when<sp>verdictdbalias1.`mycolumn1`<sp>is<sp>not<sp>null<sp>then<sp>1<sp>else<sp>0<sp>end)<sp>as<sp>`verdictdbalias7`<sp>" 5 ) + "group<sp>by<sp>verdictdbalias1.`verdictdbalias2`,<sp>`verdictdbalias5`)<sp>as<sp>verdictdbalias4<sp>" ) + "sum(case<sp>when<sp>verdictdbalias1.`mycolumn1`<sp>is<sp>not<sp>null<sp>then<sp>1<sp>else<sp>0<sp>end)<sp>as<sp>`verdictdbalias7`<sp>" 3 ; org . verdictdb . sqlwriter . SelectQueryToSql relToSql = new org . verdictdb . sqlwriter . SelectQueryToSql ( new org . verdictdb . sqlsyntax . HiveSyntax ( ) ) ; java . lang . String actual = relToSql . toSql ( rewritten . get ( k ) . getLeft ( ) ) ; org . junit . Assert . assertEquals ( expected , actual ) ; } } }
public class aTest{ @Test public void txn_readpromote_promote_end ( ) { unit . begin ( TxnType . READ_PROMOTE ) ; boolean b = unit . promote ( ) ; org . junit . Assert . assertTrue ( b ) ; try { unit . end ( ) ; } }
public class aTest{ @Test public void customMaxConcurrency ( ) { int customMaxConcurrency = 1 ; org . mule . runtime . core . api . construct . Flow customFlow = org . mule . runtime . core . api . construct . Flow . builder ( org . mule . runtime . core . internal . construct . DefaultFlowTestCase . FLOW_NAME , muleContext ) . source ( directInboundMessageSource ) . processors ( getSensingNullMessageProcessor ( ) ) . maxConcurrency ( customMaxConcurrency ) . build ( ) ; try { customFlow . initialise ( ) ; customFlow . start ( ) ; org . junit . Assert . assertThat ( customFlow . getMaxConcurrency ( ) , org . hamcrest . CoreMatchers . equalTo ( customMaxConcurrency ) ) ; verify ( muleContext . getSchedulerService ( ) ) . ioScheduler ( eq ( muleContext . getSchedulerBaseConfig ( ) . withName ( ( ( ( flow . getName ( ) ) + "." ) + ( org . mule . runtime . core . internal . construct . BLOCKING . name ( ) ) ) ) ) ) ; customFlow . stop ( ) ; } }
public class aTest{ @Test public void testLongPath ( ) { javax . jcr . Session s = getAdminSession ( ) ; java . lang . StringBuilder buff = new java . lang . StringBuilder ( ) ; for ( int i = 0 ; i < 10 ; i ++ ) { buff . append ( "0123456789" ) ; } java . lang . String longName = "n" + ( buff . toString ( ) ) ; javax . jcr . Node n = s . getRootNode ( ) ; java . util . ArrayList < java . lang . String > paths = new java . util . ArrayList < java . lang . String > ( ) ; for ( int i = 0 ; i < 30 ; i ++ ) { n = n . addNode ( ( ( longName + "_" ) + i ) ) ; paths . add ( n . getPath ( ) ) ; } s . save ( ) ; javax . jcr . Session s2 = createAdminSession ( ) ; javax . jcr . Node n2 = s2 . getRootNode ( ) ; for ( int i = 0 ; i < 30 ; i ++ ) { n2 = n2 . getNode ( ( ( longName + "_" ) + i ) ) ; org . junit . Assert . assertEquals ( paths . get ( i ) , n2 . getPath ( ) ) ; } }
public class aTest{ @Test public void testBytePropertyConversions ( ) { javax . jms . JMSProducer producer = context . createProducer ( ) ; producer . setProperty ( STRING_PROPERTY_NAME , STRING_PROPERTY_VALUE ) ; producer . setProperty ( BYTE_PROPERTY_NAME , BYTE_PROPERTY_VALUE ) ; producer . setProperty ( BOOLEAN_PROPERTY_NAME , BOOLEAN_PROPERTY_VALUE ) ; producer . setProperty ( SHORT_PROPERTY_NAME , SHORT_PROPERTY_VALUE ) ; producer . setProperty ( INTEGER_PROPERTY_NAME , INTEGER_PROPERTY_VALUE ) ; producer . setProperty ( LONG_PROPERTY_NAME , LONG_PROPERTY_VALUE ) ; producer . setProperty ( FLOAT_PROPERTY_NAME , FLOAT_PROPERTY_VALUE ) ; producer . setProperty ( DOUBLE_PROPERTY_NAME , DOUBLE_PROPERTY_VALUE ) ; org . junit . Assert . assertEquals ( BYTE_PROPERTY_VALUE , producer . getByteProperty ( BYTE_PROPERTY_NAME ) ) ; try { producer . getByteProperty ( STRING_PROPERTY_NAME ) ; org . junit . Assert . fail ( "Should<sp>not<sp>be<sp>able<sp>to<sp>convert" ) ; } }
public class aTest{ @Test public void testMDCInheritsValuesFromParentThread ( ) { mdc . put ( "parentKey" , "parentValue" ) ; runAndWait ( new java . lang . Runnable ( ) { public void run ( ) { mdc . put ( "childKey" , "childValue" ) ; org . junit . Assert . assertEquals ( "parentValue" , mdc . get ( "parentKey" ) ) ; } } }
public class aTest{ @Test public void testBuildCanonicalizedResource ( ) { java . lang . String resource = "User-Agent" 4 ; java . lang . String prefix = "x-odps-" ; com . aliyun . odps . commons . transport . Request request = new com . aliyun . odps . commons . transport . Request ( ) ; request . setMethod ( Method . POST ) ; request . setHeader ( "User-Agent" 3 , "JavaSDK/0.12.0;Linux" ) ; request . setHeader ( "User-Agent" 1 , "Tue,<sp>13<sp>May<sp>2014<sp>09:22:20<sp>GMT" ) ; request . setHeader ( "User-Agent" 0 , "369" ) ; request . setHeader ( "Content-MD5" , "746a0402967096663daa8bd1a2ff5c7c" ) ; request . setHeader ( "User-Agent" , "JavaSDK/0.12.0;Linux" ) ; request . setHeader ( "User-Agent" 2 , "application/xml" ) ; request . setBodyLength ( 0L ) ; java . lang . String expectedResult = "POST\n746a0402967096663daa8bd1a2ff5c7c\napplication/xml\n" + ( "Tue,<sp>13<sp>May<sp>2014<sp>09:22:20<sp>GMT\nx-odps-user-agent:JavaSDK/0.12.0;Linux\n" + "User-Agent" 4 ) ; java . lang . String result = com . aliyun . odps . account . SecurityUtils . buildCanonicalString ( resource , request , prefix ) ; org . junit . Assert . assertEquals ( expectedResult , result ) ; } }
public class aTest{ @Test public void bug42485 ( ) { org . apache . poi . hslf . usermodel . HSLFSlideShow ppt = org . apache . poi . hslf . usermodel . TestBugs . open ( "42485.ppt" ) ; for ( org . apache . poi . hslf . usermodel . HSLFShape shape : ppt . getSlides ( ) . get ( 0 ) . getShapes ( ) ) { if ( shape instanceof org . apache . poi . hslf . usermodel . HSLFGroupShape ) { org . apache . poi . hslf . usermodel . HSLFGroupShape group = ( ( org . apache . poi . hslf . usermodel . HSLFGroupShape ) ( shape ) ) ; for ( org . apache . poi . hslf . usermodel . HSLFShape sh : group . getShapes ( ) ) { if ( sh instanceof org . apache . poi . hslf . usermodel . HSLFTextBox ) { org . apache . poi . hslf . usermodel . HSLFTextBox txt = ( ( org . apache . poi . hslf . usermodel . HSLFTextBox ) ( sh ) ) ; org . junit . Assert . assertNotNull ( txt . getTextParagraphs ( ) ) ; } } } } }
public class aTest{ @Test public void checkProperSquash ( ) { final java . net . URI newRepo = java . net . URI . create ( "git://master@squash-repo/myfile3.txt" 0 ) ; final org . uberfire . java . nio . fs . jgit . JGitFileSystem fs = ( ( org . uberfire . java . nio . fs . jgit . JGitFileSystem ) ( provider . newFileSystem ( newRepo , org . uberfire . java . nio . fs . jgit . EMPTY_ENV ) ) ) ; final org . uberfire . java . nio . file . Path generalPath = provider . getPath ( java . net . URI . create ( "git://master@squash-repo/" ) ) ; final org . uberfire . java . nio . file . Path path = provider . getPath ( java . net . URI . create ( "git://master@squash-repo/myfile1.txt" ) ) ; final org . uberfire . java . nio . file . Path path2 = provider . getPath ( java . net . URI . create ( "git://master@squash-repo/myfile2.txt" ) ) ; final org . uberfire . java . nio . file . Path path3 = provider . getPath ( java . net . URI . create ( "git://master@squash-repo/myfile3.txt" ) ) ; final java . io . OutputStream aStream = provider . newOutputStream ( path ) ; aStream . write ( "my<sp>cool<sp>content" . getBytes ( ) ) ; aStream . close ( ) ; final org . eclipse . jgit . revwalk . RevCommit commit = ( ( org . uberfire . java . nio . fs . jgit . util . GitImpl ) ( fs . getGit ( ) ) ) . _log ( ) . add ( fs . getGit ( ) . getRef ( "master" ) . getObjectId ( ) ) . setMaxCount ( 1 ) . call ( ) . iterator ( ) . next ( ) ; final java . io . OutputStream bStream = provider . newOutputStream ( path2 ) ; bStream . write ( "my<sp>cool<sp>content" . getBytes ( ) ) ; bStream . close ( ) ; final java . io . OutputStream cStream = provider . newOutputStream ( path3 ) ; cStream . write ( "my<sp>cool<sp>content" . getBytes ( ) ) ; cStream . close ( ) ; final org . uberfire . java . nio . base . version . VersionRecord record = makeVersionRecord ( "aparedes" , "aparedes@redhat.com" , "squashing!" , new java . util . Date ( ) , commit . getName ( ) ) ; final org . uberfire . java . nio . base . options . SquashOption squashOption = new org . uberfire . java . nio . base . options . SquashOption ( record ) ; provider . setAttribute ( generalPath , SquashOption . SQUASH_ATTR , squashOption ) ; int commitsCount = 0 ; for ( org . eclipse . jgit . revwalk . RevCommit com : ( ( org . uberfire . java . nio . fs . jgit . util . GitImpl ) ( fs . getGit ( ) ) ) . _log ( ) . all ( ) . call ( ) ) { commitsCount ++ ; System . out . println ( ( ( ( com . getName ( ) ) + "<sp>-<sp>" ) + ( com . getFullMessage ( ) ) ) ) ; } org . junit . Assert . assertThat ( commitsCount ) . isEqualTo ( 2 ) ; } }
public class aTest{ @Test public void testCRC32 ( ) { org . junit . Assert . assertFalse ( smallIdx . hasCRC32Support ( ) ) ; try { smallIdx . findCRC32 ( org . eclipse . jgit . lib . ObjectId . fromString ( "4b825dc642cb6eb9a060e54bf8d69288fbee4904" ) ) ; org . junit . Assert . fail ( "index<sp>V1<sp>shouldn't<sp>support<sp>CRC" ) ; } }
public class aTest{ @Test public void twoSubjectMultiFilter_test ( ) { final org . eclipse . rdf4j . model . ValueFactory vf = org . eclipse . rdf4j . model . impl . SimpleValueFactory . getInstance ( ) ; final org . eclipse . rdf4j . model . Value geo = vf . createLiteral ( "Point(0<sp>0)" , GeoConstants . XMLSCHEMA_OGC_WKT ) ; final org . eclipse . rdf4j . model . Value temp = vf . createLiteral ( new org . apache . rya . indexing . TemporalInstantRfc3339 ( 2015 , 12 , 30 , 12 , 0 , 0 ) . toString ( ) ) ; final org . eclipse . rdf4j . model . IRI tempPred = vf . createIRI ( org . apache . rya . indexing . geotemporal . GeoTemporalProviderTest . URI_PROPERTY_AT_TIME ) ; final java . lang . String query = ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "PREFIX<sp>geo:<sp><http://www.opengis.net/ont/geosparql#>" + ( ( ( "PREFIX<sp>geos:<sp><http://www.opengis.net/def/function/geosparql/>" + "PREFIX<sp>time:<sp><tag:rya-rdf.org,2015:temporal#>" ) + "?subj<sp><" 2 ) + "?subj<sp><" ) ) + tempPred ) + "?subj<sp><" 3 ) + "?subj<sp><" ) + ( org . apache . rya . indexing . GeoConstants . GEO_AS_WKT ) ) + "?subj<sp><" 0 ) + "<sp>FILTER(geos:sfContains(?loc,<sp>" ) + geo ) + "?subj<sp><" 1 ) + "<sp>FILTER(time:equals(?time,<sp>" ) + temp ) + "?subj<sp><" 1 ) + "<sp>FILTER(geos:sfWithin(?loc,<sp>" ) + geo ) + "?subj<sp><" 1 ) + "<sp>FILTER(time:before(?time,<sp>" ) + temp ) + "?subj<sp><" 1 ) + "}" ; final org . apache . rya . indexing . external . matching . QuerySegment < org . apache . rya . indexing . geotemporal . model . EventQueryNode > node = org . apache . rya . indexing . geotemporal . GeoTemporalTestUtils . getQueryNode ( query ) ; final java . util . List < org . apache . rya . indexing . geotemporal . model . EventQueryNode > nodes = provider . getExternalSets ( node ) ; org . junit . Assert . assertEquals ( 1 , nodes . size ( ) ) ; } }
public class aTest{ @Test public void testWrongUsage ( ) { org . hipparchus . geometry . euclidean . threed . PolyhedronsSet ps = new org . hipparchus . geometry . euclidean . threed . PolyhedronsSet ( new org . hipparchus . geometry . partitioning . BSPTree < org . hipparchus . geometry . euclidean . threed . Euclidean3D > ( ) , 1.0E-10 ) ; org . junit . Assert . assertNotNull ( ps ) ; try { ps . checkPoint ( Vector3D . ZERO ) ; org . junit . Assert . fail ( "an<sp>exception<sp>should<sp>have<sp>been<sp>thrown" ) ; } }
public class aTest{ @Test public void loadingSysPropConfig ( ) { try { java . lang . System . setProperty ( "testserver.a" , "sysprop" ) ; java . lang . String dir = org . apache . hadoop . test . TestDirHelper . getTestDir ( ) . getAbsolutePath ( ) ; java . io . File configFile = new java . io . File ( dir , "testserver-site.xml" ) ; java . io . Writer w = new java . io . FileWriter ( configFile ) ; w . write ( "<configuration><property><name>testserver.a</name><value>site</value></property></configuration>" ) ; w . close ( ) ; org . apache . hadoop . lib . server . Server server = new org . apache . hadoop . lib . server . Server ( "testserver" , dir , dir , dir , dir ) ; server . init ( ) ; org . junit . Assert . assertEquals ( server . getConfig ( ) . get ( "testserver.a" ) , "sysprop" ) ; } }
public class aTest{ @Test public void testOrWithFrom ( ) { final java . lang . String drl = ( ( ( ( ( ( ( ( ( ( ( ( ( ( "<sp>(<sp>eval(true)<sp>or<sp>eval(true)<sp>)\n" 3 + "<sp>(<sp>eval(true)<sp>or<sp>eval(true)<sp>)\n" 2 ) + ( org . drools . testcoverage . common . model . Order . class . getCanonicalName ( ) ) ) + "<sp>(<sp>eval(true)<sp>or<sp>eval(true)<sp>)\n" 0 ) + "<sp>(<sp>eval(true)<sp>or<sp>eval(true)<sp>)\n" 2 ) + ( org . drools . testcoverage . common . model . OrderItem . class . getCanonicalName ( ) ) ) + "<sp>(<sp>eval(true)<sp>or<sp>eval(true)<sp>)\n" 0 ) + "rule<sp>NotContains\n" ) + "when\n" ) + "<sp>$oi1<sp>:<sp>OrderItem(<sp>)\n" ) + "<sp>$o1<sp>:<sp>Order(number<sp>==<sp>1)<sp>from<sp>$oi1.order;<sp>\n" ) + "<sp>(<sp>eval(true)<sp>or<sp>eval(true)<sp>)\n" ) + "<sp>(<sp>eval(true)<sp>or<sp>eval(true)<sp>)\n" 4 ) + "<sp>$o2<sp>:<sp>Order(number<sp>==<sp>2)<sp>from<sp>$oi2.order;<sp>\n" ) + "then\n" ) + "<sp>(<sp>eval(true)<sp>or<sp>eval(true)<sp>)\n" 1 ; final org . kie . api . KieBase kbase = org . drools . testcoverage . common . util . KieBaseUtil . getKieBaseFromKieModuleFromDrl ( "or-test" , kieBaseTestConfiguration , drl ) ; final org . kie . api . runtime . KieSession ksession = kbase . newKieSession ( ) ; try { final org . drools . testcoverage . common . model . Order order1 = new org . drools . testcoverage . common . model . Order ( 1 , "XYZ" ) ; final org . drools . testcoverage . common . model . Order order2 = new org . drools . testcoverage . common . model . Order ( 2 , "ABC" ) ; final org . drools . testcoverage . common . model . OrderItem item11 = new org . drools . testcoverage . common . model . OrderItem ( order1 , 1 ) ; order1 . addItem ( item11 ) ; final org . drools . testcoverage . common . model . OrderItem item21 = new org . drools . testcoverage . common . model . OrderItem ( order2 , 1 ) ; order2 . addItem ( item21 ) ; ksession . insert ( order1 ) ; ksession . insert ( order2 ) ; ksession . insert ( item11 ) ; ksession . insert ( item21 ) ; final int rules = ksession . fireAllRules ( ) ; org . junit . Assert . assertEquals ( 2 , rules ) ; } }
public class aTest{ @Test public void setOwnerOfFile_shouldThrowUnsupportedOperationException ( ) { writeToCache ( "/file.txt" ) ; commitToMaster ( ) ; initGitFileSystem ( ) ; com . beijunyi . parallelgit . filesystem . io . GfsFileAttributeView . Posix view = provider . getFileAttributeView ( gfs . getPath ( "/file.txt" ) , GfsFileAttributeView . Posix . class ) ; org . junit . Assert . assertNotNull ( view ) ; view . setOwner ( new java . nio . file . attribute . UserPrincipal ( ) { @ javax . annotation . Nonnull @ com . beijunyi . parallelgit . filesystem . io . Override public java . lang . String getName ( ) { return "some_owner" ; } } }
public class aTest{ @Test public void shouldAssembleExpectedResultForAccessToken_withPermissions ( ) { org . mitre . oauth2 . model . OAuth2AccessTokenEntity accessToken = accessToken ( new java . util . Date ( ( 123 * 1000L ) ) , scopes ( "resource_set_id" 2 , "bar" ) , permissions ( permission ( 1L , "resource_set_id" 2 , "bar" ) ) , "Bearer" , oauth2AuthenticationWithUser ( oauth2Request ( "clientId" ) , "name" ) ) ; org . mitre . openid . connect . model . UserInfo userInfo = userInfo ( "sub" ) ; java . util . Set < java . lang . String > authScopes = scopes ( "resource_set_id" 2 , "bar" , "baz" ) ; java . util . Map < java . lang . String , java . lang . Object > result = assembler . assembleFrom ( accessToken , userInfo , authScopes ) ; java . util . Map < java . lang . String , java . lang . Object > expected = new ImmutableMap . Builder < java . lang . String , java . lang . Object > ( ) . put ( "sub" , "sub" ) . put ( "exp" , 123L ) . put ( "expires_at" , org . mitre . oauth2 . service . impl . TestDefaultIntrospectionResultAssembler . dateFormat . valueToString ( new java . util . Date ( ( 123 * 1000L ) ) ) ) . put ( "resource_set_id" 6 , new com . google . common . collect . ImmutableSet . Builder < > ( ) . add ( new ImmutableMap . Builder < java . lang . String , java . lang . Object > ( ) . put ( "resource_set_id" , "resource_set_id" 3 ) . put ( "resource_set_id" 5 , new com . google . common . collect . ImmutableSet . Builder < > ( ) . add ( "bar" ) . add ( "resource_set_id" 2 ) . build ( ) ) . build ( ) ) . build ( ) ) . put ( "resource_set_id" 4 , Boolean . TRUE ) . put ( "user_id" , "name" ) . put ( "resource_set_id" 0 , "clientId" ) . put ( "resource_set_id" 1 , "Bearer" ) . build ( ) ; org . junit . Assert . assertThat ( result , org . hamcrest . CoreMatchers . is ( org . hamcrest . CoreMatchers . equalTo ( expected ) ) ) ; } }
public class aTest{ @Test public void unpartitionedTableReplicateAvroSchemaOverride ( ) { helper . createManagedUnpartitionedTable ( toUri ( sourceWarehouseUri , com . hotels . bdp . circustrain . integration . IntegrationTestHelper . DATABASE , com . hotels . bdp . circustrain . integration . IntegrationTestHelper . SOURCE_MANAGED_UNPARTITIONED_TABLE ) ) ; com . hotels . bdp . circustrain . integration . CircusTrainHdfsHdfsIntegrationTest . LOG . info ( ">>>><sp>Table<sp>{}<sp>" , sourceCatalog . client ( ) . getTable ( com . hotels . bdp . circustrain . integration . IntegrationTestHelper . DATABASE , com . hotels . bdp . circustrain . integration . IntegrationTestHelper . SOURCE_MANAGED_UNPARTITIONED_TABLE ) ) ; java . nio . file . Path sourceAvroSchemaPath = java . nio . file . Paths . get ( ( ( sourceWarehouseUri . toString ( ) ) + "/avro-schema-file.test" ) ) ; java . nio . file . Files . createDirectories ( sourceAvroSchemaPath ) ; java . lang . String avroSchemaBaseUrl = sourceAvroSchemaPath . toString ( ) ; org . apache . hadoop . hive . metastore . api . Table sourceTable = sourceCatalog . client ( ) . getTable ( com . hotels . bdp . circustrain . integration . IntegrationTestHelper . DATABASE , com . hotels . bdp . circustrain . integration . IntegrationTestHelper . SOURCE_MANAGED_UNPARTITIONED_TABLE ) ; sourceTable . putToParameters ( "avro.schema.url" , avroSchemaBaseUrl ) ; sourceCatalog . client ( ) . alter_table ( sourceTable . getDbName ( ) , sourceTable . getTableName ( ) , sourceTable ) ; exit . expectSystemExitWithStatus ( 0 ) ; java . io . File config = dataFolder . getFile ( "unpartitioned-single-table-avro-schema-override.yml" ) ; com . hotels . bdp . circustrain . common . test . base . CircusTrainRunner runner = com . hotels . bdp . circustrain . common . test . base . CircusTrainRunner . builder ( com . hotels . bdp . circustrain . integration . IntegrationTestHelper . DATABASE , sourceWarehouseUri , replicaWarehouseUri , housekeepingDbLocation ) . sourceMetaStore ( sourceCatalog . getThriftConnectionUri ( ) , sourceCatalog . connectionURL ( ) , sourceCatalog . driverClassName ( ) ) . replicaMetaStore ( replicaCatalog . getThriftConnectionUri ( ) ) . build ( ) ; exit . checkAssertionAfterwards ( new org . junit . contrib . java . lang . system . Assertion ( ) { @ com . hotels . bdp . circustrain . integration . Override public void checkAssertion ( ) throws com . hotels . bdp . circustrain . integration . Exception { org . apache . hadoop . hive . metastore . api . Table replicaHiveTable = replicaCatalog . client ( ) . getTable ( com . hotels . bdp . circustrain . integration . IntegrationTestHelper . DATABASE , com . hotels . bdp . circustrain . integration . CircusTrainHdfsHdfsIntegrationTest . TARGET_UNPARTITIONED_MANAGED_TABLE ) ; java . lang . String expectedReplicaSchemaUrl = ( replicaWarehouseUri . toURI ( ) . toString ( ) ) + "ct_database-override/" ; java . lang . String transformedAvroUrl = replicaHiveTable . getParameters ( ) . get ( "avro.schema.url" ) ; org . junit . Assert . assertThat ( transformedAvroUrl , org . hamcrest . CoreMatchers . startsWith ( expectedReplicaSchemaUrl ) ) ; } } }
public class aTest{ @Test public void testNuevo ( ) { log . debug ( "Test<sp>'nuevo'" ) ; mx . edu . um . mateo . general . model . Usuario usuario = obtieneUsuario ( ) ; for ( int i = 0 ; i < 20 ; i ++ ) { mx . edu . um . mateo . inscripciones . model . Institucion institucion = new mx . edu . um . mateo . inscripciones . model . Institucion ( ) ; institucion . setNombre ( "Nombre-test" ) ; institucion . setPorcentaje ( new java . math . BigDecimal ( "123" ) ) ; institucion . setStatus ( "A" ) ; institucion . setOrganizacion ( usuario . getEmpresa ( ) . getOrganizacion ( ) ) ; currentSession ( ) . save ( institucion ) ; org . junit . Assert . assertNotNull ( institucion . getId ( ) ) ; } }
public class aTest{ @Test public void testWrongUsage ( ) { org . apache . commons . math4 . geometry . euclidean . threed . PolyhedronsSet ps = new org . apache . commons . math4 . geometry . euclidean . threed . PolyhedronsSet ( new org . apache . commons . math4 . geometry . partitioning . BSPTree < org . apache . commons . math4 . geometry . euclidean . threed . Euclidean3D > ( ) , org . apache . commons . math4 . geometry . euclidean . threed . PolyhedronsSetTest . TEST_TOLERANCE ) ; org . junit . Assert . assertNotNull ( ps ) ; try { ps . checkPoint ( Cartesian3D . ZERO ) ; org . junit . Assert . fail ( "an<sp>exception<sp>should<sp>have<sp>been<sp>thrown" ) ; } }
public class aTest{ @Test public void testRpcServerMultiThread ( ) { final org . apache . hive . spark . client . rpc . RpcServer server = autoClose ( new org . apache . hive . spark . client . rpc . RpcServer ( org . apache . hive . spark . client . rpc . TestRpc . emptyConfig , hiveConf ) ) ; final java . lang . String msg = "Hello<sp>World!" ; java . util . concurrent . Callable < java . lang . String > callable = ( ) -> { org . apache . hive . spark . client . rpc . Rpc [ ] rpcs = createRpcConnection ( server , org . apache . hive . spark . client . rpc . TestRpc . emptyConfig , java . util . UUID . randomUUID ( ) . toString ( ) ) ; org . apache . hive . spark . client . rpc . Rpc rpc ; if ( java . util . concurrent . ThreadLocalRandom . current ( ) . nextBoolean ( ) ) { rpc = rpcs [ 0 ] ; } else { rpc = rpcs [ 1 ] ; } org . apache . hive . spark . client . rpc . TestMessage outbound = new org . apache . hive . spark . client . rpc . TestMessage ( "Hello<sp>World!" ) ; Future < org . apache . hive . spark . client . rpc . TestMessage > call = rpc . call ( outbound , . class ) ; org . apache . hive . spark . client . rpc . TestMessage reply = call . get ( 10 , TimeUnit . SECONDS ) ; return reply . message ; } ; final int numThreads = ( java . util . concurrent . ThreadLocalRandom . current ( ) . nextInt ( 5 ) ) + 5 ; java . util . concurrent . ExecutorService executor = java . util . concurrent . Executors . newFixedThreadPool ( numThreads ) ; java . util . List < java . util . concurrent . Future < java . lang . String > > futures = new java . util . ArrayList ( numThreads ) ; for ( int i = 0 ; i < numThreads ; i ++ ) { futures . add ( executor . submit ( callable ) ) ; } executor . shutdown ( ) ; for ( java . util . concurrent . Future < java . lang . String > future : futures ) { org . junit . Assert . assertEquals ( msg , future . get ( ) ) ; } } }
public class aTest{ @Test public void testDistinctAsc ( ) { try { int objectNumber = 1001 ; org . eclipse . birt . data . engine . olap . data . util . PrimitiveDiskSortedStack stack = new org . eclipse . birt . data . engine . olap . data . util . PrimitiveDiskSortedStack ( 100 , true , true ) ; stack . push ( new java . lang . Integer ( 200 ) ) ; stack . push ( new java . lang . Integer ( 250 ) ) ; stack . push ( new java . lang . Integer ( 208 ) ) ; stack . push ( new java . lang . Integer ( 211 ) ) ; stack . push ( new java . lang . Integer ( 211 ) ) ; stack . push ( new java . lang . Integer ( 213 ) ) ; for ( int i = 0 ; i < objectNumber ; i ++ ) { stack . push ( new java . lang . Integer ( i ) ) ; } for ( int i = 0 ; i < objectNumber ; i ++ ) { org . junit . Assert . assertEquals ( stack . pop ( ) , new java . lang . Integer ( i ) ) ; } }
public class aTest{ @Test public void shouldListUbuntuImages ( ) { try { java . lang . String filter = "{\"reference\":{\"ubuntu\":true}}" ; java . lang . String encode = java . net . URLEncoder . encode ( filter , "UTF-8" ) ; final javax . ws . rs . core . Response response = getResponse ( target ( "images" ) . path ( "json" ) . queryParam ( "filters" , encode ) ) ; System . out . println ( response ) ; org . junit . Assert . assertEquals ( 200 , response . getStatus ( ) ) ; response . close ( ) ; } }
public class aTest{ @Test public void getFields ( ) { java . util . List < java . lang . reflect . Field > fields = org . jpmml . model . ReflectionUtil . getFields ( org . dmg . pmml . PMML . class ) ; java . util . List < java . lang . reflect . Field > customFields = org . jpmml . model . ReflectionUtil . getFields ( org . dmg . pmml . CustomPMML . class ) ; + ( 8 + 1 ) ) , fields . size ( ) ) ; org . junit . Assert . assertEquals ( new java . util . HashSet ( fields ) , new java . util . HashSet ( customFields ) ) ; } }
public class aTest{ @Test public void testFindWithFilterRange ( ) { createIndexIfNotExists ( org . springframework . data . aerospike . sample . Person . class , "age_index" , "dave-010" 1 , IndexType . NUMERIC ) ; client . put ( policy , new com . aerospike . client . Key ( getNameSpace ( ) , org . springframework . data . aerospike . core . AerospikeTemplateIntegrationTests . SET_NAME_PERSON , "dave-001" ) , new com . aerospike . client . Bin ( "firstname" , "dave-010" 2 ) , new com . aerospike . client . Bin ( "age_index" 7 , "age_index" 8 ) , new com . aerospike . client . Bin ( "dave-010" 1 , 21 ) ) ; client . put ( policy , new com . aerospike . client . Key ( getNameSpace ( ) , org . springframework . data . aerospike . core . AerospikeTemplateIntegrationTests . SET_NAME_PERSON , "dave-002" ) , new com . aerospike . client . Bin ( "firstname" , "age_index" 5 ) , new com . aerospike . client . Bin ( "age_index" 7 , "age_index" 8 ) , new com . aerospike . client . Bin ( "dave-010" 1 , 22 ) ) ; client . put ( policy , new com . aerospike . client . Key ( getNameSpace ( ) , org . springframework . data . aerospike . core . AerospikeTemplateIntegrationTests . SET_NAME_PERSON , "age_index" 2 ) , new com . aerospike . client . Bin ( "firstname" , "age_index" 0 ) , new com . aerospike . client . Bin ( "age_index" 7 , "age_index" 8 ) , new com . aerospike . client . Bin ( "dave-010" 1 , 23 ) ) ; client . put ( policy , new com . aerospike . client . Key ( getNameSpace ( ) , org . springframework . data . aerospike . core . AerospikeTemplateIntegrationTests . SET_NAME_PERSON , "dave-004" ) , new com . aerospike . client . Bin ( "firstname" , "dave-010" 4 ) , new com . aerospike . client . Bin ( "age_index" 7 , "age_index" 8 ) , new com . aerospike . client . Bin ( "dave-010" 1 , 24 ) ) ; client . put ( policy , new com . aerospike . client . Key ( getNameSpace ( ) , org . springframework . data . aerospike . core . AerospikeTemplateIntegrationTests . SET_NAME_PERSON , "dave-005" ) , new com . aerospike . client . Bin ( "firstname" , "age_index" 1 ) , new com . aerospike . client . Bin ( "age_index" 7 , "age_index" 8 ) , new com . aerospike . client . Bin ( "dave-010" 1 , 25 ) ) ; client . put ( policy , new com . aerospike . client . Key ( getNameSpace ( ) , org . springframework . data . aerospike . core . AerospikeTemplateIntegrationTests . SET_NAME_PERSON , "age_index" 6 ) , new com . aerospike . client . Bin ( "firstname" , "Dave06" ) , new com . aerospike . client . Bin ( "age_index" 7 , "age_index" 8 ) , new com . aerospike . client . Bin ( "dave-010" 1 , 26 ) ) ; client . put ( policy , new com . aerospike . client . Key ( getNameSpace ( ) , org . springframework . data . aerospike . core . AerospikeTemplateIntegrationTests . SET_NAME_PERSON , "dave-007" ) , new com . aerospike . client . Bin ( "firstname" , "age_index" 9 ) , new com . aerospike . client . Bin ( "age_index" 7 , "age_index" 8 ) , new com . aerospike . client . Bin ( "dave-010" 1 , 27 ) ) ; client . put ( policy , new com . aerospike . client . Key ( getNameSpace ( ) , org . springframework . data . aerospike . core . AerospikeTemplateIntegrationTests . SET_NAME_PERSON , "dave-008" ) , new com . aerospike . client . Bin ( "firstname" , "dave-010" 5 ) , new com . aerospike . client . Bin ( "age_index" 7 , "age_index" 8 ) , new com . aerospike . client . Bin ( "dave-010" 1 , 28 ) ) ; client . put ( policy , new com . aerospike . client . Key ( getNameSpace ( ) , org . springframework . data . aerospike . core . AerospikeTemplateIntegrationTests . SET_NAME_PERSON , "dave-010" 3 ) , new com . aerospike . client . Bin ( "firstname" , "dave-010" 0 ) , new com . aerospike . client . Bin ( "age_index" 7 , "age_index" 8 ) , new com . aerospike . client . Bin ( "dave-010" 1 , 29 ) ) ; client . put ( policy , new com . aerospike . client . Key ( getNameSpace ( ) , org . springframework . data . aerospike . core . AerospikeTemplateIntegrationTests . SET_NAME_PERSON , "dave-010" ) , new com . aerospike . client . Bin ( "firstname" , "age_index" 3 ) , new com . aerospike . client . Bin ( "age_index" 7 , "age_index" 8 ) , new com . aerospike . client . Bin ( "dave-010" 1 , 30 ) ) ; org . springframework . data . aerospike . repository . query . Query query = createQueryForMethodWithArgs ( "age_index" 4 , 25 , 30 ) ; java . lang . Iterable < org . springframework . data . aerospike . sample . Person > it = template . find ( query , org . springframework . data . aerospike . sample . Person . class ) ; int count = 0 ; for ( org . springframework . data . aerospike . sample . Person person : it ) { count ++ ; } org . junit . Assert . assertEquals ( 6 , count ) ; } }
public class aTest{ @Test public void testComp ( ) { com . alexkasko . unsafe . offheaplong . OffHeapLongArray la = null ; try { long [ ] heap = com . alexkasko . unsafe . offheaplong . OffHeapLongSorterTest . gendata ( ) ; java . lang . Long [ ] heapBoxed = new java . lang . Long [ heap . length ] ; for ( int i = 0 ; i < ( heap . length ) ; i ++ ) { heapBoxed [ i ] = heap [ i ] ; } long [ ] unsafe = heap . clone ( ) ; java . util . Arrays . sort ( heapBoxed , new com . alexkasko . unsafe . offheaplong . OffHeapLongSorterTest . HeapReverseComp ( ) ) ; la = new com . alexkasko . unsafe . offheaplong . OffHeapLongArray ( com . alexkasko . unsafe . offheaplong . OffHeapLongSorterTest . THRESHOLD ) ; for ( int i = 0 ; i < ( com . alexkasko . unsafe . offheaplong . OffHeapLongSorterTest . THRESHOLD ) ; i ++ ) { la . set ( i , unsafe [ i ] ) ; } com . alexkasko . unsafe . offheaplong . OffHeapLongSorter . sort ( la , 0 , com . alexkasko . unsafe . offheaplong . OffHeapLongSorterTest . THRESHOLD , new com . alexkasko . unsafe . offheaplong . OffHeapLongSorterTest . OffHeapReverseComp ( ) ) ; for ( int i = 0 ; i < ( com . alexkasko . unsafe . offheaplong . OffHeapLongSorterTest . THRESHOLD ) ; i ++ ) { unsafe [ i ] = la . get ( i ) ; } long [ ] heapUnboxed = new long [ heap . length ] ; for ( int i = 0 ; i < ( heap . length ) ; i ++ ) { heapUnboxed [ i ] = heapBoxed [ i ] ; } org . junit . Assert . assertArrayEquals ( heapUnboxed , unsafe ) ; la . free ( ) ; } }
public class aTest{ @Test public void DELETEMOVE201404 ( ) { final java . util . Map < java . lang . Integer , java . lang . Integer > deleteCounters = new java . util . HashMap < java . lang . Integer , java . lang . Integer > ( ) ; final java . util . Map < java . lang . Integer , java . lang . Integer > moveCounters = new java . util . HashMap < java . lang . Integer , java . lang . Integer > ( ) ; java . util . List < java . lang . Thread > listThread = new java . util . ArrayList < java . lang . Thread > ( ) ; for ( int i = 0 ; i < ( com . fujitsu . dc . test . jersey . concurrent . ConcurrentDavRequestTest . NUM_CONCURRENCY ) ; i ++ ) { com . fujitsu . dc . test . jersey . concurrent . ConcurrentDavRequestTest . DavMultiThreadRunner runner = null ; if ( ( i % 2 ) == 0 ) { runner = new com . fujitsu . dc . test . jersey . concurrent . ConcurrentDavRequestTest . DavMultiThreadRunner ( this . deleteFileRequest ( com . fujitsu . dc . test . jersey . concurrent . ConcurrentDavRequestTest . FILE_NAME ) , deleteCounters ) ; } else { runner = new com . fujitsu . dc . test . jersey . concurrent . ConcurrentDavRequestTest . DavMultiThreadRunner ( this . moveFileRequest ( com . fujitsu . dc . test . jersey . concurrent . ConcurrentDavRequestTest . FILE_NAME , ( ( com . fujitsu . dc . test . jersey . concurrent . ConcurrentDavRequestTest . FILE_NAME ) + "_moved" ) , "T" ) , moveCounters ) ; } java . lang . Thread t = new java . lang . Thread ( runner ) ; listThread . add ( t ) ; } try { this . putFileRequest ( com . fujitsu . dc . test . jersey . concurrent . ConcurrentDavRequestTest . FILE_NAME , Setup . TEST_BOX1 , com . fujitsu . dc . test . jersey . concurrent . ConcurrentDavRequestTest . FILE_BODY ) . returns ( ) ; for ( java . lang . Thread t : listThread ) { t . start ( ) ; } for ( java . lang . Thread t : listThread ) { t . join ( ) ; } int total = 0 ; for ( java . lang . Integer respCode : deleteCounters . keySet ( ) ) { com . fujitsu . dc . test . jersey . concurrent . ConcurrentDavRequestTest . log . debug ( java . lang . String . format ( "deleted<sp>%d:<sp>%d" , respCode , deleteCounters . get ( respCode ) ) ) ; total += deleteCounters . get ( respCode ) ; if ( ( ( ( org . apache . http . HttpStatus . SC_NO_CONTENT ) != respCode ) && ( ( org . apache . http . HttpStatus . SC_NOT_FOUND ) != respCode ) ) && ( ( org . apache . http . HttpStatus . SC_SERVICE_UNAVAILABLE ) != respCode ) ) { org . junit . Assert . fail ( java . lang . String . format ( "request<sp>failed:<sp>respcode=%d,<sp>count:%d" , respCode , deleteCounters . get ( respCode ) ) ) ; } } for ( java . lang . Integer respCode : moveCounters . keySet ( ) ) { com . fujitsu . dc . test . jersey . concurrent . ConcurrentDavRequestTest . log . debug ( java . lang . String . format ( "moved<sp>%d:<sp>%d" , respCode , moveCounters . get ( respCode ) ) ) ; total += moveCounters . get ( respCode ) ; if ( ( ( ( org . apache . http . HttpStatus . SC_CREATED ) != respCode ) && ( ( org . apache . http . HttpStatus . SC_NOT_FOUND ) != respCode ) ) && ( ( org . apache . http . HttpStatus . SC_SERVICE_UNAVAILABLE ) != respCode ) ) { org . junit . Assert . fail ( java . lang . String . format ( "request<sp>failed:<sp>respcode=%d,<sp>count:%d" , respCode , moveCounters . get ( respCode ) ) ) ; } } org . junit . Assert . assertEquals ( com . fujitsu . dc . test . jersey . concurrent . ConcurrentDavRequestTest . NUM_CONCURRENCY , total ) ; } }
public class aTest{ @Test public void testGetTriggerDefinitionsNoTD ( ) { final org . oscm . domobjects . Organization org = runTX ( new java . util . concurrent . Callable < org . oscm . domobjects . Organization > ( ) { public org . oscm . domobjects . Organization call ( ) throws org . oscm . domobjects . Exception { org . oscm . domobjects . Organization org = org . oscm . test . data . Organizations . createOrganization ( mgr ) ; return org ; } } ) ; runTX ( new java . util . concurrent . Callable < java . lang . Void > ( ) { public org . oscm . domobjects . Void call ( ) throws org . oscm . domobjects . Exception { org . oscm . domobjects . Organization storedOrg = mgr . getReference ( org . oscm . domobjects . Organization . class , org . getKey ( ) ) ; java . util . List < org . oscm . domobjects . TriggerDefinition > triggerDefinitions = storedOrg . getTriggerDefinitions ( ) ; org . junit . Assert . assertEquals ( 0 , triggerDefinitions . size ( ) ) ; return null ; } } }
public class aTest{ @Test public void testGetTrustStore ( ) { try { org . hyperledger . fabric . sdk . security . CryptoPrimitives myCrypto = new org . hyperledger . fabric . sdk . security . CryptoPrimitives ( ) ; org . junit . Assert . assertNotNull ( myCrypto . getTrustStore ( ) ) ; } }
public class aTest{ @Test public void testMergeConfig ( ) { java . lang . String str = "" ; str += "package<sp>org.kie.test\n" ; str += "global<sp>java.util.List<sp>list\n" ; str += "rule<sp>rule1\n" ; str += "when\n" ; str += "<sp>$i<sp>:<sp>Integer(intValue<sp>><sp>0)\n" ; str += "then\n" ; str += "<sp>list.add(<sp>$i<sp>);\n" ; str += "package<sp>org.kie.test\n" 2 ; str += "\n" ; org . kie . api . KieServices ks = KieServices . Factory . get ( ) ; org . kie . api . builder . KieFileSystem kfs = ks . newKieFileSystem ( ) . write ( "package<sp>org.kie.test\n" 0 , str ) ; ks . newKieBuilder ( kfs ) . buildAll ( ) ; org . kie . api . KieBase kbase = ks . newKieContainer ( ks . getRepository ( ) . getDefaultReleaseId ( ) ) . getKieBase ( ) ; java . util . Properties properties = new java . util . Properties ( ) ; properties . put ( "package<sp>org.kie.test\n" 1 , "com.example.CustomJPAProcessInstanceManagerFactory" ) ; org . kie . api . runtime . KieSessionConfiguration config = ks . newKieSessionConfiguration ( properties ) ; org . kie . api . runtime . KieSession ksession = ks . getStoreServices ( ) . newKieSession ( kbase , config , env ) ; org . drools . core . SessionConfiguration sessionConfig = ( ( org . drools . core . SessionConfiguration ) ( ksession . getSessionConfiguration ( ) ) ) ; org . junit . Assert . assertEquals ( "com.example.CustomJPAProcessInstanceManagerFactory" , sessionConfig . getProcessInstanceManagerFactory ( ) ) ; } }
public class aTest{ @Test public void testNestedAndRepeatedRecords ( ) { java . util . Map < java . lang . String , java . lang . Object > expectedRowContent = new java . util . LinkedHashMap ( ) ; java . util . Map < java . lang . String , java . lang . Object > innerLevelMap = new java . util . LinkedHashMap ( ) ; innerLevelMap . put ( "aMap1" , 1 ) ; innerLevelMap . put ( "aMap2" , 2.0 ) ; innerLevelMap . put ( "aMap3" , true ) ; expectedRowContent . put ( "cList1b" 1 , innerLevelMap ) ; java . util . List < java . lang . String > innerStringList = new java . util . ArrayList ( ) ; innerStringList . add ( "aMap3" 1 ) ; innerStringList . add ( "bList2" ) ; innerStringList . add ( "cList1b" 0 ) ; expectedRowContent . put ( "b" , innerStringList ) ; java . util . List < java . util . Map < java . lang . String , java . lang . Object > > innerLevelMapList = new java . util . ArrayList ( ) ; java . util . Map < java . lang . String , java . lang . Object > innerLevelMap1 = new java . util . LinkedHashMap ( ) ; innerLevelMap1 . put ( "cList1a" , "cList1b" 1 ) ; innerLevelMap1 . put ( "cList1b" , 1 ) ; innerLevelMap1 . put ( "cList1b" 5 , 2.0 ) ; innerLevelMap1 . put ( "cList1b" 3 , true ) ; innerLevelMap1 . put ( "cList1b" 8 , com . google . common . collect . ImmutableMap . of ( "aMap3" 0 , 1 ) ) ; java . util . Map < java . lang . String , java . lang . Object > innerLevelMap2 = new java . util . LinkedHashMap ( ) ; innerLevelMap2 . put ( "aMap3" 2 , "b" ) ; innerLevelMap2 . put ( "cList2b" , 2 ) ; innerLevelMap2 . put ( "cList2c" , 3.0 ) ; innerLevelMap2 . put ( "cList1b" 2 , false ) ; innerLevelMap1 . put ( "cList1b" 6 , com . google . common . collect . ImmutableMap . of ( "aMap3" 0 , 2 ) ) ; java . util . Map < java . lang . String , java . lang . Object > innerLevelMap3 = new java . util . LinkedHashMap ( ) ; innerLevelMap3 . put ( "aMap3" 4 , "cList1b" 7 ) ; innerLevelMap3 . put ( "cList1b" 9 , 3 ) ; innerLevelMap3 . put ( "cList3c" , 4.0 ) ; innerLevelMap3 . put ( "cList1b" 4 , true ) ; innerLevelMap1 . put ( "aMap3" 3 , com . google . common . collect . ImmutableMap . of ( "aMap3" 0 , 3 ) ) ; innerLevelMapList . add ( innerLevelMap1 ) ; innerLevelMapList . add ( innerLevelMap2 ) ; innerLevelMapList . add ( innerLevelMap3 ) ; expectedRowContent . put ( "cList1b" 7 , innerLevelMapList ) ; com . streamsets . pipeline . api . Record record = com . streamsets . pipeline . sdk . RecordCreator . create ( ) ; record . set ( createField ( expectedRowContent ) ) ; mockBigQueryInsertAllRequest ( ( invocationOnMock ) -> { com . google . cloud . bigquery . InsertAllRequest insertAllRequest = ( ( com . google . cloud . bigquery . InsertAllRequest ) ( invocationOnMock . getArguments ( ) [ 0 ] ) ) ; List < com . google . cloud . bigquery . InsertAllRequest . RowToInsert > rows = insertAllRequest . getRows ( ) ; org . junit . Assert . assertEquals ( 1 , rows . size ( ) ) ; com . google . cloud . bigquery . InsertAllRequest . RowToInsert row = rows . get ( 0 ) ; row . getContent ( ) . forEach ( ( k , v ) -> checkValues ( expectedRowContent . get ( k ) , v ) ) ; com . google . cloud . bigquery . InsertAllResponse response = org . powermock . api . mockito . PowerMockito . mock ( . class ) ; org . mockito . Mockito . doReturn ( java . util . Collections . emptyMap ( ) ) . when ( response ) . getInsertErrors ( ) ; org . mockito . Mockito . doReturn ( false ) . when ( response ) . hasErrors ( ) ; return response ; } }
public class aTest{ @Test public void pos ( ) { long blockSize = ( ( alluxio . client . block . stream . UfsFallbackLocalFileDataWriterTest . CHUNK_SIZE ) * 2 ) + ( ( alluxio . client . block . stream . UfsFallbackLocalFileDataWriterTest . CHUNK_SIZE ) / 3 ) ; try ( alluxio . client . block . stream . DataWriter writer = create ( blockSize , alluxio . client . block . stream . UfsFallbackLocalFileDataWriterTest . CHUNK_SIZE ) ) { byte [ ] data = new byte [ 1 ] ; java . util . concurrent . Future < alluxio . client . block . stream . UfsFallbackLocalFileDataWriterTest . WriteSummary > actualUfs = getUfsWrite ( mClient ) ; for ( long pos = 0 ; pos < blockSize ; pos ++ ) { org . junit . Assert . assertEquals ( pos , writer . pos ( ) ) ; io . netty . buffer . ByteBuf buf = io . netty . buffer . Unpooled . wrappedBuffer ( data ) ; writer . writeChunk ( buf ) ; } }
public class aTest{ @Test public void testVerifyFromSchemaVersion000WitouthInitialDatabaseCreation ( ) { com . liferay . registry . Registry registry = com . liferay . registry . RegistryUtil . getRegistry ( ) ; java . util . Map < java . lang . String , java . lang . Object > properties = new java . util . HashMap ( ) ; properties . put ( "upgrade.bundle.symbolic.name" , "ServiceComponentLocalServiceTest" ) ; properties . put ( "upgrade.from.schema.version" , "0.0.0" ) ; properties . put ( "upgrade.initial.database.creation" , false ) ; final com . liferay . portal . kernel . dao . db . DB db = com . liferay . portal . kernel . dao . db . DBManagerUtil . getDB ( ) ; com . liferay . registry . ServiceRegistration < com . liferay . portal . kernel . upgrade . UpgradeStep > upgradeStepServiceRegistration = registry . registerService ( com . liferay . portal . kernel . upgrade . UpgradeStep . class , new com . liferay . portal . service . ServiceComponentLocalServiceTest . TestUpgradeStep ( db ) , properties ) ; try { com . liferay . portal . kernel . service . ServiceComponentLocalServiceUtil . verifyDB ( ) ; try ( java . sql . Connection connection = com . liferay . portal . kernel . dao . jdbc . DataAccess . getConnection ( ) ) { java . sql . DatabaseMetaData metadata = connection . getMetaData ( ) ; java . lang . String tableName = normalizeTableName ( metadata , com . liferay . portal . service . ServiceComponentLocalServiceTest . _TEST_TABLE ) ; try ( java . sql . ResultSet verifyTable = metadata . getTables ( null , null , tableName , new java . lang . String [ ] { "TABLE" } ) ) { org . junit . Assert . assertFalse ( verifyTable . next ( ) ) ; } } } }
public class aTest{ @Test public void testExpliciteWithSessionWithoutAssignedKeyspace ( ) { net . oneandone . troilus . Dao hotelsDao = new net . oneandone . troilus . DaoImpl ( net . oneandone . troilus . api . ExpliciteKeyspacenameTest . cassandra . newGobalSession ( ) , net . oneandone . troilus . api . ExpliciteKeyspacenameTest . cassandra . getKeyspacename ( ) , net . oneandone . troilus . example . HotelsTable . TABLE ) . withConsistency ( ConsistencyLevel . LOCAL_QUORUM ) ; net . oneandone . troilus . example . Hotel entity = new net . oneandone . troilus . example . Hotel ( "123" 3 , "Corinthia<sp>Budapest" , com . google . common . collect . ImmutableSet . of ( "1" , "2" , "3" , "123" 2 , "123" , "123" 1 , "123" 0 , "333" ) , java . util . Optional . of ( ClassifierEnum . FIVE ) , java . util . Optional . of ( "Superb<sp>hotel<sp>housed<sp>in<sp>a<sp>heritage<sp>building<sp>-<sp>exudes<sp>old<sp>world<sp>charm" ) , new net . oneandone . troilus . example . Address ( "Erzsbet<sp>krt<sp>43" , "Budapest" , "1073" ) , java . util . Optional . empty ( ) ) ; hotelsDao . writeEntity ( entity ) . execute ( ) ; net . oneandone . troilus . Record record = hotelsDao . readWithKey ( HotelsTable . ID , "123" 3 ) . column ( HotelsTable . NAME ) . execute ( ) . get ( ) ; org . junit . Assert . assertEquals ( "Corinthia<sp>Budapest" , record . getString ( HotelsTable . NAME ) ) ; } }
public class aTest{ @Test public void testAsyncClientRequests ( ) { final int REQUESTS = 10 ; final java . util . concurrent . CountDownLatch latch = new java . util . concurrent . CountDownLatch ( REQUESTS ) ; final long tic = java . lang . System . currentTimeMillis ( ) ; for ( int i = 0 ; i < REQUESTS ; i ++ ) { final int id = i ; target ( ) . path ( App . ROOT_PATH ) . request ( ) . async ( ) . get ( new javax . ws . rs . client . InvocationCallback < javax . ws . rs . core . Response > ( ) { @ org . glassfish . jersey . examples . helloworld . Override public void completed ( javax . ws . rs . core . Response response ) { try { final java . lang . String result = response . readEntity ( java . lang . String . class ) ; org . junit . Assert . assertEquals ( HelloWorldResource . CLICHED_MESSAGE , result ) ; } }
public class aTest{ @Test public void testRevokeAccountPrivilege ( ) { try { com . fit2cloud . aliyun . rds . model . request . RevokeAccountPrivilegeRequest request = new com . fit2cloud . aliyun . rds . model . request . RevokeAccountPrivilegeRequest ( ) ; request . setDBInstanceId ( dBInstanceId ) ; request . setAccountName ( accountName ) ; request . setDBName ( dbName ) ; com . fit2cloud . aliyun . Response response = client . revokeAccountPrivilege ( request ) ; System . out . println ( ( "testRevokeAccountPrivilege<sp>::<sp>" + ( new com . google . gson . Gson ( ) . toJson ( response ) ) ) ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testMaxFree ( ) { com . j256 . ormlite . jdbc . JdbcPooledConnectionSource pooled = new com . j256 . ormlite . jdbc . JdbcPooledConnectionSource ( com . j256 . ormlite . jdbc . JdbcPooledConnectionSourceTest . DEFAULT_DATABASE_URL ) ; try { pooled . setMaxConnectionsFree ( 2 ) ; com . j256 . ormlite . support . DatabaseConnection conn1 = pooled . getReadWriteConnection ( null ) ; com . j256 . ormlite . support . DatabaseConnection conn2 = pooled . getReadWriteConnection ( null ) ; com . j256 . ormlite . support . DatabaseConnection conn3 = pooled . getReadWriteConnection ( null ) ; pooled . releaseConnection ( conn1 ) ; pooled . releaseConnection ( conn2 ) ; pooled . releaseConnection ( conn3 ) ; com . j256 . ormlite . support . DatabaseConnection conn4 = pooled . getReadWriteConnection ( null ) ; org . junit . Assert . assertSame ( conn2 , conn4 ) ; } }
public class aTest{ @Test public void testSchemaDeserialization ( ) { final java . math . BigDecimal id = java . math . BigDecimal . valueOf ( 1238123899121L ) ; final java . lang . String name = "12:12:43Z" 5 ; final byte [ ] bytes = new byte [ 1024 ] ; java . util . concurrent . ThreadLocalRandom . current ( ) . nextBytes ( bytes ) ; final java . math . BigDecimal [ ] numbers = new java . math . BigDecimal [ ] { java . math . BigDecimal . valueOf ( 1 ) , java . math . BigDecimal . valueOf ( 2 ) , java . math . BigDecimal . valueOf ( 3 ) } ; final java . lang . String [ ] strings = new java . lang . String [ ] { "1990-10-14" 1 , "two" , "three" } ; final org . apache . flink . shaded . jackson2 . com . fasterxml . jackson . databind . ObjectMapper objectMapper = new org . apache . flink . shaded . jackson2 . com . fasterxml . jackson . databind . ObjectMapper ( ) ; org . apache . flink . shaded . jackson2 . com . fasterxml . jackson . databind . node . ObjectNode root = objectMapper . createObjectNode ( ) ; root . put ( "booleanField" 3 , id . longValue ( ) ) ; root . putNull ( "1990-10-14" 0 ) ; root . put ( "12:12:43Z" 2 , name ) ; root . put ( "booleanField" 6 , "1990-10-14" ) ; root . put ( "1990-10-14" 4 , "12:12:43Z" ) ; root . put ( "booleanField" 5 , "booleanField" 7 ) ; root . put ( "1990-10-14" 8 , bytes ) ; root . putArray ( "numbers" ) . add ( 1 ) . add ( 2 ) . add ( 3 ) ; root . putArray ( "12:12:43Z" 6 ) . add ( "1990-10-14" 1 ) . add ( "two" ) . add ( "three" ) ; root . putObject ( "1990-10-14" 2 ) . put ( "booleanField" , true ) . put ( "booleanField" 9 , 12 ) ; final byte [ ] serializedJson = objectMapper . writeValueAsBytes ( root ) ; org . apache . flink . formats . json . JsonRowDeserializationSchema deserializationSchema = new org . apache . flink . formats . json . JsonRowDeserializationSchema ( ( "12:12:43Z" 3 + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "12:12:43Z" 4 + "<sp>properties:<sp>{" ) + "<sp>id:<sp>{<sp>type:<sp>'integer'<sp>}," ) + "12:12:43Z" 9 ) + "1990-10-14" 5 ) + "1990-10-14" 7 ) + "<sp>time:<sp>{<sp>type:<sp>'string',<sp>format:<sp>'time'<sp>}," ) + "booleanField" 1 ) + "<sp>id:<sp>{<sp>type:<sp>'integer'<sp>}," 1 ) + "1990-10-14" 6 ) + "booleanField" 8 ) + "<sp>id:<sp>{<sp>type:<sp>'integer'<sp>}," 0 ) + "12:12:43Z" 1 ) + "<sp>properties:<sp>{<sp>" ) + "booleanField" 2 ) + "12:12:43Z" 0 ) + "booleanField" 0 ) + "12:12:43Z" 7 ) + "1990-10-14" 3 ) + "12:12:43Z" 8 ) ) ) ; final org . apache . flink . types . Row deserialized = deserializationSchema . deserialize ( serializedJson ) ; final org . apache . flink . types . Row expected = new org . apache . flink . types . Row ( 10 ) ; expected . setField ( 0 , id ) ; expected . setField ( 1 , null ) ; expected . setField ( 2 , name ) ; expected . setField ( 3 , java . sql . Date . valueOf ( "1990-10-14" ) ) ; expected . setField ( 4 , java . sql . Time . valueOf ( "booleanField" 4 ) ) ; expected . setField ( 5 , java . sql . Timestamp . valueOf ( "1990-10-14" 9 ) ) ; expected . setField ( 6 , bytes ) ; expected . setField ( 7 , numbers ) ; expected . setField ( 8 , strings ) ; final org . apache . flink . types . Row nestedRow = new org . apache . flink . types . Row ( 2 ) ; nestedRow . setField ( 0 , true ) ; nestedRow . setField ( 1 , java . math . BigDecimal . valueOf ( 12 ) ) ; expected . setField ( 9 , nestedRow ) ; org . junit . Assert . assertEquals ( expected , deserialized ) ; } }
public class aTest{ @Test public void testRowValueConstructorOnLHSAndLiteralExpressionOnRHS ( ) { long ts = nextTimestamp ( ) ; java . lang . String tenantId = getOrganizationId ( ) ; initATableValues ( tenantId , getDefaultSplits ( tenantId ) , null , ts ) ; java . lang . String query = "SELECT<sp>a_integer,<sp>x_integer<sp>FROM<sp>aTable<sp>WHERE<sp>?=organization_id<sp>AND<sp>(a_integer,<sp>x_integer)<sp>>=<sp>7" ; java . util . Properties props = new java . util . Properties ( TEST_PROPERTIES ) ; props . setProperty ( PhoenixRuntime . CURRENT_SCN_ATTRIB , java . lang . Long . toString ( ( ts + 2 ) ) ) ; java . sql . Connection conn = java . sql . DriverManager . getConnection ( com . salesforce . phoenix . end2end . PHOENIX_JDBC_URL , props ) ; try { java . sql . PreparedStatement statement = conn . prepareStatement ( query ) ; statement . setString ( 1 , tenantId ) ; java . sql . ResultSet rs = statement . executeQuery ( ) ; int count = 0 ; while ( rs . next ( ) ) { count ++ ; } org . junit . Assert . assertTrue ( ( count == 3 ) ) ; } }
public class aTest{ @Test public void testSetterDetection ( ) { net . sourceforge . pmd . lang . java . ast . ASTCompilationUnit compilationUnit = net . sourceforge . pmd . lang . java . ParserTstUtil . parseJava17 ( net . sourceforge . pmd . lang . java . metrics . testdata . SetterDetection . class ) ; compilationUnit . jjtAccept ( new net . sourceforge . pmd . lang . java . ast . JavaParserVisitorAdapter ( ) { @ net . sourceforge . pmd . lang . java . metrics . Override public java . lang . Object visit ( net . sourceforge . pmd . lang . java . ast . ASTMethodDeclaration node , java . lang . Object data ) { org . junit . Assert . assertEquals ( Role . GETTER_OR_SETTER , net . sourceforge . pmd . lang . java . multifile . signature . JavaOperationSignature . Role . get ( node ) ) ; return data ; } } }
public class aTest{ @Test public void readOnly ( ) { org . sqlite . SQLiteConfig config = new org . sqlite . SQLiteConfig ( ) ; config . setReadOnly ( true ) ; java . sql . Connection conn = java . sql . DriverManager . getConnection ( "jdbc:sqlite:" , config . toProperties ( ) ) ; java . sql . Statement stat = conn . createStatement ( ) ; try { org . junit . Assert . assertTrue ( conn . isReadOnly ( ) ) ; stat . executeUpdate ( "create<sp>table<sp>A(id,<sp>name)" ) ; stat . executeUpdate ( "insert<sp>into<sp>A<sp>values(1,<sp>'leo')" ) ; org . junit . Assert . fail ( "read<sp>only<sp>flag<sp>is<sp>not<sp>properly<sp>set" ) ; } }
public class aTest{ @Test public void testDelimitedMaxRecordSize ( ) { doTestDelimitedMaxRecordSize ( io . vertx . core . buffer . Buffer . buffer ( "should<sp>throw<sp>exception" 6 ) , io . vertx . core . buffer . Buffer . buffer ( "should<sp>throw<sp>exception" 8 ) , new java . lang . Integer [ ] { 2 } , 4 , null , io . vertx . core . buffer . Buffer . buffer ( "should<sp>throw<sp>exception" 9 ) , io . vertx . core . buffer . Buffer . buffer ( "should<sp>throw<sp>exception" 5 ) ) ; doTestDelimitedMaxRecordSize ( io . vertx . core . buffer . Buffer . buffer ( "A\nBC10\nDEFGHIJKLM\n" ) , io . vertx . core . buffer . Buffer . buffer ( "should<sp>throw<sp>exception" 8 ) , new java . lang . Integer [ ] { 2 } , 10 , null , io . vertx . core . buffer . Buffer . buffer ( "A" ) , io . vertx . core . buffer . Buffer . buffer ( "BC10" ) , io . vertx . core . buffer . Buffer . buffer ( "DEFGHIJKLM" ) ) ; doTestDelimitedMaxRecordSize ( io . vertx . core . buffer . Buffer . buffer ( "AB\nC\n\nDEFG\n\n" ) , io . vertx . core . buffer . Buffer . buffer ( "should<sp>throw<sp>exception" 3 ) , new java . lang . Integer [ ] { 2 } , 4 , null , io . vertx . core . buffer . Buffer . buffer ( "AB\nC" ) , io . vertx . core . buffer . Buffer . buffer ( "should<sp>throw<sp>exception" 1 ) ) ; doTestDelimitedMaxRecordSize ( io . vertx . core . buffer . Buffer . buffer ( "should<sp>throw<sp>exception" 4 ) , io . vertx . core . buffer . Buffer . buffer ( "should<sp>throw<sp>exception" 7 ) , new java . lang . Integer [ ] { 3 } , 2 , null , io . vertx . core . buffer . Buffer . buffer ( "should<sp>throw<sp>exception" 2 ) , io . vertx . core . buffer . Buffer . buffer ( "" ) , io . vertx . core . buffer . Buffer . buffer ( "AB\nC\n\nDEFG\n\n" 0 ) , io . vertx . core . buffer . Buffer . buffer ( "" ) , io . vertx . core . buffer . Buffer . buffer ( "" ) , io . vertx . core . buffer . Buffer . buffer ( "should<sp>throw<sp>exception" 0 ) ) ; try { doTestDelimitedMaxRecordSize ( io . vertx . core . buffer . Buffer . buffer ( "ABCD--" ) , io . vertx . core . buffer . Buffer . buffer ( "--" ) , new java . lang . Integer [ ] { 2 } , 3 , null , io . vertx . core . buffer . Buffer . buffer ( ) ) ; org . junit . Assert . fail ( "should<sp>throw<sp>exception" ) ; } catch ( java . lang . IllegalStateException ex ) { } java . util . concurrent . atomic . AtomicBoolean handled = new java . util . concurrent . atomic . AtomicBoolean ( ) ; io . vertx . core . Handler < java . lang . Throwable > exHandler = ( throwable ) -> handled . set ( true ) ; doTestDelimitedMaxRecordSize ( io . vertx . core . buffer . Buffer . buffer ( "ABCD--" ) , io . vertx . core . buffer . Buffer . buffer ( "--" ) , new java . lang . Integer [ ] { 2 } , 3 , exHandler , io . vertx . core . buffer . Buffer . buffer ( "should<sp>throw<sp>exception" 9 ) ) ; org . junit . Assert . assertTrue ( handled . get ( ) ) ; } }
public class aTest{ @Test public void testChillModePipelineExitRule ( ) { containers = new java . util . ArrayList ( ) ; containers . addAll ( org . apache . hadoop . hdds . scm . HddsTestUtils . getContainerInfo ( ( 25 * 4 ) ) ) ; java . lang . String storageDir = org . apache . hadoop . test . GenericTestUtils . getTempPath ( ( ( org . apache . hadoop . hdds . scm . chillmode . TestSCMChillModeManager . class . getName ( ) ) + ( java . util . UUID . randomUUID ( ) ) ) ) ; try { org . apache . hadoop . hdds . scm . container . MockNodeManager nodeManager = new org . apache . hadoop . hdds . scm . container . MockNodeManager ( true , 3 ) ; org . apache . hadoop . hdds . scm . chillmode . TestSCMChillModeManager . config . set ( HddsConfigKeys . OZONE_METADATA_DIRS , storageDir ) ; org . apache . hadoop . hdds . scm . chillmode . TestSCMChillModeManager . config . setBoolean ( HddsConfigKeys . HDDS_SCM_CHILLMODE_PIPELINE_AVAILABILITY_CHECK , true ) ; org . apache . hadoop . hdds . scm . pipeline . SCMPipelineManager pipelineManager = new org . apache . hadoop . hdds . scm . pipeline . SCMPipelineManager ( org . apache . hadoop . hdds . scm . chillmode . TestSCMChillModeManager . config , nodeManager , org . apache . hadoop . hdds . scm . chillmode . TestSCMChillModeManager . queue ) ; org . apache . hadoop . hdds . scm . pipeline . PipelineProvider mockRatisProvider = new org . apache . hadoop . hdds . scm . chillmode . MockRatisPipelineProvider ( nodeManager , pipelineManager . getStateManager ( ) , org . apache . hadoop . hdds . scm . chillmode . TestSCMChillModeManager . config ) ; pipelineManager . setPipelineProvider ( HddsProtos . ReplicationType . RATIS , mockRatisProvider ) ; org . apache . hadoop . hdds . scm . pipeline . Pipeline pipeline = pipelineManager . createPipeline ( HddsProtos . ReplicationType . RATIS , HddsProtos . ReplicationFactor . THREE ) ; org . apache . hadoop . hdds . protocol . proto . StorageContainerDatanodeProtocolProtos . PipelineReportsProto . Builder reportBuilder = org . apache . hadoop . hdds . protocol . proto . StorageContainerDatanodeProtocolProtos . PipelineReportsProto . newBuilder ( ) ; reportBuilder . addPipelineReport ( org . apache . hadoop . hdds . protocol . proto . StorageContainerDatanodeProtocolProtos . PipelineReport . newBuilder ( ) . setPipelineID ( pipeline . getId ( ) . getProtobuf ( ) ) ) ; scmChillModeManager = new org . apache . hadoop . hdds . scm . chillmode . SCMChillModeManager ( org . apache . hadoop . hdds . scm . chillmode . TestSCMChillModeManager . config , containers , pipelineManager , org . apache . hadoop . hdds . scm . chillmode . TestSCMChillModeManager . queue ) ; org . apache . hadoop . hdds . scm . chillmode . TestSCMChillModeManager . queue . fireEvent ( SCMEvents . NODE_REGISTRATION_CONT_REPORT , org . apache . hadoop . hdds . scm . HddsTestUtils . createNodeRegistrationContainerReport ( containers ) ) ; org . junit . Assert . assertTrue ( scmChillModeManager . getInChillMode ( ) ) ; org . apache . hadoop . hdds . scm . chillmode . TestSCMChillModeManager . queue . fireEvent ( SCMEvents . PROCESSED_PIPELINE_REPORT , new org . apache . hadoop . hdds . scm . server . SCMDatanodeHeartbeatDispatcher . PipelineReportFromDatanode ( pipeline . getNodes ( ) . get ( 0 ) , reportBuilder . build ( ) ) ) ; org . apache . hadoop . test . GenericTestUtils . waitFor ( ( ) -> { return ! ( scmChillModeManager . getInChillMode ( ) ) ; } }
public class aTest{ @Test public void testDescribeRequestAndResponseF ( ) { com . google . appengine . api . urlfetch . HTTPRequest request = new com . google . appengine . api . urlfetch . HTTPRequest ( new java . net . URL ( "http://ping/pong" ) ) ; request . setPayload ( "hello" . getBytes ( ) ) ; request . addHeader ( new com . google . appengine . api . urlfetch . HTTPHeader ( "k1" , "v1" ) ) ; request . addHeader ( new com . google . appengine . api . urlfetch . HTTPHeader ( "k2" , "v2" ) ) ; com . google . appengine . api . urlfetch . HTTPResponse response = mock ( com . google . appengine . api . urlfetch . HTTPResponse . class ) ; when ( response . getHeadersUncombined ( ) ) . thenReturn ( com . google . common . collect . ImmutableList . of ( new com . google . appengine . api . urlfetch . HTTPHeader ( "k3" , "v3" ) ) ) ; when ( response . getResponseCode ( ) ) . thenReturn ( 500 ) ; when ( response . getContent ( ) ) . thenReturn ( "bla" . getBytes ( ) ) ; java . lang . String expected = "Request:<sp>GET<sp>http://ping/pong\nk1:<sp>v1\nk2:<sp>v2\n\n" + "k1" 0 ; java . lang . String result = com . google . appengine . tools . cloudstorage . oauth . URLFetchUtils . describeRequestAndResponse ( new com . google . appengine . tools . cloudstorage . oauth . URLFetchUtils . HTTPRequestInfo ( request ) , response ) ; org . junit . Assert . assertEquals ( expected , result ) ; } }
public class aTest{ @Test public void testLeftOuterJoinBug976Compare ( ) { java . lang . String query = "select<sp>*<sp>from<sp>JoinIT.A<sp>left<sp>outer<sp>join<sp>(JoinIT.B<sp>join<sp>JoinIT.C<sp>on<sp>b2=c2)<sp>on<sp>a1=b1" ; java . lang . String expectedColumns = "A1<sp>A2<sp>A3<sp>A4<sp>A5<sp>A6<sp>B1<sp>B2<sp>B3<sp>B4<sp>B5<sp>B6<sp>C1<sp>C2<sp>C3<sp>C4<sp>C5<sp>C6" ; java . util . List < java . lang . String > expectedRows = java . util . Arrays . asList ( "3<sp>4<sp>2<sp>(null)<sp>(null)<sp>(null)<sp>3<sp>2<sp>3<sp>3<sp>1<sp>4<sp>2<sp>2<sp>5<sp>3<sp>2<sp>4" , "7<sp>1<sp>4<sp>2<sp>3<sp>4<sp>7<sp>3<sp>3<sp>3<sp>3<sp>3<sp>8<sp>3<sp>9<sp>1<sp>3<sp>2" , "1<sp>1<sp>3<sp>6<sp>(null)<sp>2<sp>1<sp>4<sp>2<sp>(null)<sp>(null)<sp>(null)<sp>1<sp>4<sp>1<sp>(null)<sp>(null)<sp>(null)" , "6<sp>7<sp>3<sp>2<sp>3<sp>4<sp>6<sp>7<sp>2<sp>3<sp>(null)<sp>1<sp>3<sp>7<sp>7<sp>3<sp>(null)<sp>1" , "6<sp>7<sp>3<sp>2<sp>3<sp>4<sp>6<sp>7<sp>2<sp>3<sp>(null)<sp>1<sp>1<sp>7<sp>2<sp>3<sp>1<sp>1" , "2<sp>3<sp>2<sp>4<sp>2<sp>2<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)" , "2<sp>3<sp>2<sp>4<sp>2<sp>2<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)" 1 , "8<sp>8<sp>8<sp>8<sp>8<sp>8<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)" , "2<sp>3<sp>2<sp>4<sp>2<sp>2<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)" 0 ) ; java . sql . ResultSet rs = methodWatcher . executeQuery ( query ) ; com . splicemachine . homeless . TestUtils . FormattedResult expected = TestUtils . FormattedResult . ResultFactory . convert ( query , expectedColumns , expectedRows , "\\s+" ) ; com . splicemachine . homeless . TestUtils . FormattedResult actual = TestUtils . FormattedResult . ResultFactory . convert ( query , rs ) ; org . junit . Assert . assertEquals ( "2<sp>3<sp>2<sp>4<sp>2<sp>2<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)<sp>(null)" 2 , expected . toString ( ) , actual . toString ( ) ) ; } }
public class aTest{ @Test public void segment ( ) { java . lang . String [ ] expectedResults = new java . lang . String [ ] { "<sp><sp>" 00 , "<sp><sp>" 1 , "<sp><sp>" 1 , "<sp><sp>" 9 , "" 9 , "<sp><sp>" 7 , "" 4 , "BP<sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp>" 2 , "<sp><sp>" 6 , "<sp><sp>" 18 , "BP<sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp>" 8 , "C<sp>+<sp>+<sp><sp>c<sp>#<sp><sp><sp><sp><sp>11<sp>+<sp>122<sp>=<sp>133<sp><sp><sp><sp><sp>PI<sp>=<sp>3.14159" 3 , "<sp><sp>" 0 , "<sp><sp><sp><sp>" 1 , "<sp><sp><sp><sp><sp><sp><sp><sp>" 7 , "<sp><sp><sp><sp>" 7 , "<sp><sp><sp><sp><sp><sp><sp><sp>" 5 , "<sp><sp>" 08 , "<sp><sp>" 04 , "C<sp>+<sp>+<sp><sp>c<sp>#<sp><sp><sp><sp><sp>11<sp>+<sp>122<sp>=<sp>133<sp><sp><sp><sp><sp>PI<sp>=<sp>3.14159" 4 , "<sp><sp>" 05 , "<sp><sp>" 02 , "<sp><sp><sp><sp>" 0 , "<sp><sp>" 03 , "BP<sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp>" 4 , "<sp><sp>" , "<sp><sp><sp><sp>" 9 , "<sp><sp>" 06 , "" 3 , "<sp><sp>" 15 , "C<sp>+<sp>+<sp><sp>c<sp>#<sp><sp><sp><sp><sp>11<sp>+<sp>122<sp>=<sp>133<sp><sp><sp><sp><sp>PI<sp>=<sp>3.14159" 8 , "<sp><sp><sp><sp>" 6 , "BP<sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp>" 5 , "C<sp>+<sp>+<sp><sp>c<sp>#<sp><sp><sp><sp><sp>11<sp>+<sp>122<sp>=<sp>133<sp><sp><sp><sp><sp>PI<sp>=<sp>3.14159" 0 , "BP<sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp>" 1 , "<sp><sp>" 4 , "<sp><sp><sp><sp>" 6 , "<sp><sp><sp><sp>" 4 , "<sp><sp>" 1 , "<sp><sp>" 13 , "<sp><sp><sp><sp><sp><sp><sp><sp>" 2 , "<sp><sp><sp><sp>" , "<sp><sp>" 7 , "<sp><sp><sp><sp>" 3 , "<sp><sp>" 16 , "" , "<sp><sp><sp><sp>" 1 , "<sp><sp><sp><sp>" 7 , "" 6 , "<sp><sp><sp><sp><sp><sp><sp><sp>" 6 , "<sp>" 8 , "<sp>" 2 , "<sp>" 9 , "" 7 , "<sp>" , "" 0 , "<sp><sp><sp><sp>" 2 , "" 1 , "<sp><sp>" 3 , "<sp><sp><sp><sp>" 8 , "<sp><sp><sp><sp>" 9 , "<sp><sp><sp><sp>" 2 , "<sp><sp>" 8 , "" 5 , "<sp>" 4 , "<sp><sp><sp><sp>" , "<sp>" 6 , "<sp><sp>" 2 , "BP<sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp>" 0 , "<sp><sp>" 6 , "<sp><sp>" 2 , "<sp><sp>" 17 , "<sp><sp><sp><sp><sp><sp><sp><sp>" 0 , "<sp><sp>" 4 , "<sp><sp>" 9 , "<sp><sp><sp><sp><sp><sp><sp><sp>" 9 , "<sp><sp><sp><sp>" 8 , "BP<sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp>" 9 , "BP<sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp>" 3 , "<sp><sp>" 14 , "<sp><sp><sp><sp><sp><sp><sp><sp>" 3 , "<sp><sp><sp><sp><sp><sp><sp><sp>" , "<sp><sp><sp><sp><sp><sp><sp><sp>" 8 , "C<sp>+<sp>+<sp><sp>c<sp>#<sp><sp><sp><sp><sp>11<sp>+<sp>122<sp>=<sp>133<sp><sp><sp><sp><sp>PI<sp>=<sp>3.14159" 1 , "<sp>" 3 , "<sp>" 5 , "C<sp>+<sp>+<sp><sp>c<sp>#<sp><sp><sp><sp><sp>11<sp>+<sp>122<sp>=<sp>133<sp><sp><sp><sp><sp>PI<sp>=<sp>3.14159" 9 , "<sp><sp><sp><sp><sp><sp><sp><sp>" , "<sp><sp>" 8 , "" 2 , "<sp><sp>" 5 + "<sp><sp><sp><sp>" 0 , "<sp><sp><sp><sp>" 3 , "<sp><sp>" 5 , "BP<sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp>" 7 , "" 8 , "<sp><sp><sp><sp><sp><sp><sp><sp>" 4 , "<sp><sp>" 0 , "C<sp>+<sp>+<sp><sp>c<sp>#<sp><sp><sp><sp><sp>11<sp>+<sp>122<sp>=<sp>133<sp><sp><sp><sp><sp>PI<sp>=<sp>3.14159" 5 , "BP<sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp>" 6 , "<sp><sp><sp><sp><sp><sp><sp><sp>" 1 , "<sp><sp>" 12 , "BP<sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp><sp>" , "<sp><sp>" 01 , "<sp><sp>" 11 , "<sp><sp>" , "<sp><sp>" 07 , "<sp>" 0 , "<sp>" 1 , "C<sp>+<sp>+<sp><sp>c<sp>#<sp><sp><sp><sp><sp>11<sp>+<sp>122<sp>=<sp>133<sp><sp><sp><sp><sp>PI<sp>=<sp>3.14159" 6 , "<sp><sp><sp><sp>" 5 , "<sp><sp><sp><sp>" 4 , "C<sp>+<sp>+<sp><sp>c<sp>#<sp><sp><sp><sp><sp>11<sp>+<sp>122<sp>=<sp>133<sp><sp><sp><sp><sp>PI<sp>=<sp>3.14159" 2 , "C<sp>+<sp>+<sp><sp>c<sp>#<sp><sp><sp><sp><sp>11<sp>+<sp>122<sp>=<sp>133<sp><sp><sp><sp><sp>PI<sp>=<sp>3.14159" , "C<sp>+<sp>+<sp><sp>c<sp>#<sp><sp><sp><sp><sp>11<sp>+<sp>122<sp>=<sp>133<sp><sp><sp><sp><sp>PI<sp>=<sp>3.14159" 7 , "<sp><sp>" 3 , "<sp><sp>" 09 , "<sp><sp>" 10 } ; io . github . yizhiru . thulac4j . Segmenter . enableTitleWord ( ) ; for ( int i = 0 ; i < ( io . github . yizhiru . thulac4j . SegmenterTest . SENTENCES . length ) ; i ++ ) { java . lang . String actual = java . lang . String . join ( "<sp><sp><sp><sp>" 5 , io . github . yizhiru . thulac4j . Segmenter . segment ( io . github . yizhiru . thulac4j . SegmenterTest . SENTENCES [ i ] ) ) ; org . junit . Assert . assertEquals ( expectedResults [ i ] , actual ) ; } }
public class aTest{ @Test public void testExpiredDateForCertificate ( ) { org . opennms . netmgt . poller . monitors . SSLCertMonitor monitor = new org . opennms . netmgt . poller . monitors . SSLCertMonitor ( ) { @ org . opennms . netmgt . poller . monitors . Override protected java . util . Calendar getCalendarInstance ( ) { final java . util . Calendar cal = java . util . GregorianCalendar . getInstance ( ) ; cal . setTimeInMillis ( ( ( org . opennms . netmgt . poller . monitors . SSLCertMonitorIT . EXPIRE_DATE ) - ( 86400000 * ( - 1 ) ) ) ) ; return cal ; } } ; java . util . Map < java . lang . String , java . lang . Object > parameters = new java . util . concurrent . ConcurrentSkipListMap < java . lang . String , java . lang . Object > ( ) ; parameters . put ( "port" , "10342" ) ; parameters . put ( "retry" , "0" ) ; parameters . put ( "timeout" , "500" ) ; parameters . put ( "retry" 0 , "true" ) ; parameters . put ( "retry" 1 , "5" ) ; org . opennms . netmgt . poller . MonitoredService svc = org . opennms . netmgt . poller . mock . MonitorTestUtils . getMonitoredService ( 3 , "localhost" , org . opennms . netmgt . utils . DnsUtils . resolveHostname ( "localhost" , false ) , "SSLCert" ) ; org . opennms . netmgt . poller . PollStatus status = monitor . poll ( svc , parameters ) ; org . junit . Assert . assertTrue ( status . isUnavailable ( ) ) ; } }
public class aTest{ @Test public void testBalancerWithHANameNodes ( ) { org . apache . hadoop . conf . Configuration conf = new org . apache . hadoop . hdfs . HdfsConfiguration ( ) ; org . apache . hadoop . hdfs . server . balancer . TestBalancer . initConf ( conf ) ; org . junit . Assert . assertEquals ( org . apache . hadoop . hdfs . server . balancer . TestBalancerWithHANameNodes . TEST_CAPACITIES . length , org . apache . hadoop . hdfs . server . balancer . TestBalancerWithHANameNodes . TEST_RACKS . length ) ; org . apache . hadoop . hdfs . MiniDFSNNTopology . NNConf nn1Conf = new org . apache . hadoop . hdfs . MiniDFSNNTopology . NNConf ( "nn1" ) ; nn1Conf . setIpcPort ( HdfsClientConfigKeys . DFS_NAMENODE_RPC_PORT_DEFAULT ) ; org . apache . hadoop . conf . Configuration copiedConf = new org . apache . hadoop . conf . Configuration ( conf ) ; cluster = new org . apache . hadoop . hdfs . MiniDFSCluster . Builder ( copiedConf ) . nnTopology ( org . apache . hadoop . hdfs . MiniDFSNNTopology . simpleHATopology ( ) ) . numDataNodes ( org . apache . hadoop . hdfs . server . balancer . TestBalancerWithHANameNodes . TEST_CAPACITIES . length ) . racks ( org . apache . hadoop . hdfs . server . balancer . TestBalancerWithHANameNodes . TEST_RACKS ) . simulatedCapacities ( org . apache . hadoop . hdfs . server . balancer . TestBalancerWithHANameNodes . TEST_CAPACITIES ) . build ( ) ; org . apache . hadoop . hdfs . server . namenode . ha . HATestUtil . setFailoverConfigurations ( cluster , conf ) ; try { cluster . waitActive ( ) ; cluster . transitionToActive ( 0 ) ; java . lang . Thread . sleep ( 500 ) ; client = org . apache . hadoop . hdfs . NameNodeProxies . createProxy ( conf , org . apache . hadoop . fs . FileSystem . getDefaultUri ( conf ) , org . apache . hadoop . hdfs . protocol . ClientProtocol . class ) . getProxy ( ) ; doTest ( conf ) ; } }
public class aTest{ @Test public void write_Clayers ( ) { if ( ! ( isJp2KakDriverAvailable ) ) return ; it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . LOGGER . info ( "Testing<sp>JP2<sp>Write<sp>operation<sp>with<sp>Clayers<sp>option<sp>setting" ) ; final java . io . File inputFile = it . geosolutions . resources . TestData . file ( this , it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . testFileName ) ; org . junit . Assert . assertTrue ( inputFile . exists ( ) ) ; final int firstClayersParam = 3 ; final int secondClayersParam = 20 ; final java . lang . String fileName1 = new java . lang . StringBuffer ( "Clayers-" ) . append ( java . lang . Integer . toString ( firstClayersParam ) ) . append ( "-.jp2" ) . toString ( ) ; final java . lang . String fileName2 = new java . lang . StringBuffer ( "Clayers-" ) . append ( java . lang . Integer . toString ( secondClayersParam ) ) . append ( "-.jp2" ) . toString ( ) ; final java . io . File outputFile1 = it . geosolutions . resources . TestData . temp ( this , fileName1 , it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . deleteTempFilesOnExit ) ; final java . io . File outputFile2 = it . geosolutions . resources . TestData . temp ( this , fileName2 , it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . deleteTempFilesOnExit ) ; final javax . media . jai . ParameterBlockJAI pbjImageRead = new javax . media . jai . ParameterBlockJAI ( "Clayers-" 0 ) ; pbjImageRead . setParameter ( "Input" , inputFile ) ; if ( it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . ENABLE_SUBSAMPLING ) { javax . imageio . ImageReadParam readParam = new javax . imageio . ImageReadParam ( ) ; readParam . setSourceSubsampling ( 4 , 4 , 0 , 0 ) ; pbjImageRead . setParameter ( "readParam" , readParam ) ; } }
public class aTest{ @Test public void testAddRemove ( ) { testMap . clear ( ) ; referenceMap . clear ( ) ; for ( int seed = 0 ; seed < 10 ; ++ seed ) { java . util . Random random = new java . util . Random ( seed ) ; int [ ] ranges = jdk . internal . vm . compiler . collections . test . EconomicMapLargeTest . createRandomRange ( random , jdk . internal . vm . compiler . collections . test . EconomicMapLargeTest . ACTIONS . length ) ; int value = random . nextInt ( 10000 ) ; for ( int i = 0 ; i < value ; ++ i ) { for ( int j = 0 ; j < ( jdk . internal . vm . compiler . collections . test . EconomicMapLargeTest . ACTIONS . length ) ; ++ j ) { if ( ( random . nextInt ( ranges [ j ] ) ) == 0 ) { int nextInt = random . nextInt ( 100 ) ; jdk . internal . vm . compiler . collections . test . EconomicMapLargeTest . MapAction action = jdk . internal . vm . compiler . collections . test . EconomicMapLargeTest . ACTIONS [ j ] ; java . lang . Object result = action . perform ( testMap , nextInt ) ; java . lang . Object referenceResult = action . perform ( referenceMap , nextInt ) ; org . junit . Assert . assertEquals ( result , referenceResult ) ; if ( ( j % 100 ) == 0 ) { jdk . internal . vm . compiler . collections . test . EconomicMapLargeTest . checkEquality ( testMap , referenceMap ) ; } } } }
public class aTest{ @Test public void sendReturnsStatusCodeOnBadStatusException ( com . microsoft . azure . sdk . iot . service . transport . http . HttpConnection , java . net . URL ) { final com . microsoft . azure . sdk . iot . service . transport . http . HttpMethod httpsMethod = com . microsoft . azure . sdk . iot . service . transport . http . HttpMethod . POST ; final byte [ ] body = new byte [ 0 ] ; final int badStatus = 404 ; final int expectedStatus = badStatus ; new tests . unit . com . microsoft . azure . sdk . iot . service . transport . http . NonStrictExpectations ( ) { { mockUrl . getProtocol ( ) ; result = "http" ; mockConn . connect ( ) ; result = new java . io . IOException ( ) ; mockConn . getResponseStatus ( ) ; result = badStatus ; } } ; com . microsoft . azure . sdk . iot . service . transport . http . HttpRequest request = new com . microsoft . azure . sdk . iot . service . transport . http . HttpRequest ( mockUrl , httpsMethod , body ) ; com . microsoft . azure . sdk . iot . service . transport . http . HttpResponse response = request . send ( ) ; int testStatus = response . getStatus ( ) ; org . junit . Assert . assertThat ( testStatus , org . hamcrest . CoreMatchers . is ( expectedStatus ) ) ; } }
public class aTest{ @Test public void testRequestMemorySegmentsExceptionDuringBufferRedistribution ( ) { final int numBuffers = 3 ; org . apache . flink . runtime . io . network . buffer . NetworkBufferPool networkBufferPool = new org . apache . flink . runtime . io . network . buffer . NetworkBufferPool ( numBuffers , 128 ) ; final java . util . List < org . apache . flink . runtime . io . network . buffer . Buffer > buffers = new java . util . ArrayList ( numBuffers ) ; java . util . List < org . apache . flink . core . memory . MemorySegment > memorySegments = java . util . Collections . emptyList ( ) ; org . apache . flink . runtime . io . network . buffer . BufferPool bufferPool = networkBufferPool . createBufferPool ( 1 , numBuffers , java . util . Optional . of ( ( numBuffersToRecycle ) -> { throw new org . apache . flink . runtime . io . network . buffer . TestIOException ( ) ; } ) ) ; try { for ( int i = 0 ; i < ( numBuffers - 1 ) ; ++ i ) { org . apache . flink . runtime . io . network . buffer . Buffer buffer = bufferPool . requestBuffer ( ) ; buffers . add ( buffer ) ; org . junit . Assert . assertNotNull ( buffer ) ; } }
public class aTest{ @Test public void testInvokeTest ( ) { java . lang . reflect . Method test = org . segrada . service . util . AbstractLazyLoadedObjectTest . IMockObject . class . getMethod ( "test" ) ; try { java . lang . Object result = mockAbstractLazyLoadedObject . invoke ( org . segrada . service . util . AbstractLazyLoadedObjectTest . IMockObject . class , test , null ) ; org . junit . Assert . assertEquals ( true , result ) ; } }
public class aTest{ @Test public void testToBytesAndToTuple ( ) { org . apache . tajo . catalog . Schema schema = org . apache . tajo . catalog . SchemaBuilder . builder ( ) . add ( "col1" , Type . BOOLEAN ) . add ( "col2" , Type . CHAR ) . add ( "col1" 0 , Type . INT2 ) . add ( "col4" , Type . INT4 ) . add ( "col5" , Type . INT8 ) . add ( "col6" , Type . FLOAT4 ) . add ( "col7" , Type . FLOAT8 ) . add ( "col8" , Type . TEXT ) . add ( "col9" , Type . BLOB ) . build ( ) ; org . apache . tajo . engine . util . Tuple tuple = new org . apache . tajo . engine . util . VTuple ( new org . apache . tajo . datum . Datum [ ] { org . apache . tajo . datum . DatumFactory . createBool ( true ) , org . apache . tajo . datum . DatumFactory . createChar ( '7' ) , org . apache . tajo . datum . DatumFactory . createInt2 ( ( ( short ) ( 17 ) ) ) , org . apache . tajo . datum . DatumFactory . createInt4 ( 59 ) , org . apache . tajo . datum . DatumFactory . createInt8 ( 23L ) , org . apache . tajo . datum . DatumFactory . createFloat4 ( 77.9F ) , org . apache . tajo . datum . DatumFactory . createFloat8 ( 271.9F ) , org . apache . tajo . datum . DatumFactory . createText ( "hyunsik" ) , org . apache . tajo . datum . DatumFactory . createBlob ( "hyunsik" . getBytes ( ) ) } ) ; org . apache . tajo . storage . RowStoreUtil . RowStoreEncoder encoder = org . apache . tajo . engine . util . RowStoreUtil . createEncoder ( schema ) ; org . apache . tajo . storage . RowStoreUtil . RowStoreDecoder decoder = org . apache . tajo . engine . util . RowStoreUtil . createDecoder ( schema ) ; byte [ ] bytes = encoder . toBytes ( tuple ) ; org . apache . tajo . engine . util . Tuple tuple2 = decoder . toTuple ( bytes ) ; org . junit . Assert . assertEquals ( tuple , tuple2 ) ; } }
public class aTest{ @Test public void testTopVlenInt ( ) { try { java . lang . String filename = "/netcdf4/vlen/vlenInt.nc" ; java . lang . String remoteFile = thredds . TestOnLocalServer . withHttpPath ( ( ( thredds . server . cdmr . TestCdmRemoteMisc . urlPath ) + filename ) ) ; ucar . nc2 . stream . CdmRemote ncfileRemote = new ucar . nc2 . stream . CdmRemote ( remoteFile ) ; java . lang . String localFile = ( thredds . server . cdmr . TestCdmRemoteMisc . contentRoot ) + filename ; ucar . nc2 . NetcdfFile ncfileLocal = ucar . nc2 . NetcdfFile . open ( localFile ) ; java . util . Formatter f = new java . util . Formatter ( ) ; ucar . nc2 . util . CompareNetcdf2 mind = new ucar . nc2 . util . CompareNetcdf2 ( f , true , true , true ) ; boolean ok = mind . compare ( ncfileLocal , ncfileRemote , new thredds . server . cdmr . TestCdmRemoteCompareHeadersP . NcstreamObjFilter ( ) , true , false , true ) ; System . out . printf ( "--Compare<sp>%s<sp>to<sp>%s%n" , localFile , remoteFile ) ; System . out . printf ( "<sp>%s%n" , f ) ; org . junit . Assert . assertTrue ( ok ) ; ncfileLocal . close ( ) ; ncfileRemote . close ( ) ; } }
public class aTest{ @Test public void testRemoteEjbObserverNotified ( javax . enterprise . inject . spi . BeanManager ) { FooBean . observations . set ( 0 ) ; beanManager . fireEvent ( new org . jboss . weld . tests . observers . ejb . local . Giraffe ( ) ) ; org . junit . Assert . assertEquals ( 2 , FooBean . observations . get ( ) ) ; } }
public class aTest{ @Test public void transformMacrosWithPriorities ( ) { java . lang . String expected = "beginDocument\n" + ( ( ( ( ( ( ( ( ( ( "beginMacroMarkerStandalone<sp>[testsimplemacro]<sp>[]\n" + "beginParagraph\n" ) + "testprioritymacro" 0 ) + "testprioritymacro" 1 ) + "endMacroMarkerStandalone<sp>[testsimplemacro]<sp>[]\n" ) + "beginMacroMarkerStandalone<sp>[testprioritymacro]<sp>[]\n" ) + "beginParagraph\n" ) + "onWord<sp>[word]\n" ) + "testprioritymacro" 1 ) + "endMacroMarkerStandalone<sp>[testprioritymacro]<sp>[]\n" ) + "endDocument" ) ; org . xwiki . rendering . block . XDOM dom = new org . xwiki . rendering . block . XDOM ( java . util . Arrays . < org . xwiki . rendering . block . Block > asList ( new org . xwiki . rendering . block . MacroBlock ( "testprioritymacro" 3 , java . util . Collections . < java . lang . String , java . lang . String > emptyMap ( ) , false ) , new org . xwiki . rendering . block . MacroBlock ( "testprioritymacro" , java . util . Collections . < java . lang . String , java . lang . String > emptyMap ( ) , false ) ) ) ; this . transformation . transform ( dom , new org . xwiki . rendering . transformation . TransformationContext ( dom , org . xwiki . rendering . syntax . Syntax . XWIKI_2_0 ) ) ; org . xwiki . rendering . renderer . printer . WikiPrinter printer = new org . xwiki . rendering . renderer . printer . DefaultWikiPrinter ( ) ; org . xwiki . rendering . renderer . BlockRenderer eventBlockRenderer = this . componentManager . getInstance ( org . xwiki . rendering . renderer . BlockRenderer . class , Syntax . EVENT_1_0 . toIdString ( ) ) ; eventBlockRenderer . render ( dom , printer ) ; org . junit . Assert . assertEquals ( expected , printer . toString ( ) ) ; } }
public class aTest{ @Test public void authenticateCBS ( ) { final int MAX_WAIT_TO_AUTHENTICATE = 10 * 1000 ; final tests . unit . com . microsoft . azure . sdk . iot . device . transport . amqps . AmqpsSessionDeviceOperation amqpsSessionDeviceOperation = new tests . unit . com . microsoft . azure . sdk . iot . device . transport . amqps . AmqpsSessionDeviceOperation ( mockDeviceClientConfig , mockAmqpsDeviceAuthentication ) ; mockit . Deencapsulation . setField ( amqpsSessionDeviceOperation , "deviceClientConfig" , mockDeviceClientConfig ) ; mockit . Deencapsulation . setField ( amqpsSessionDeviceOperation , "authenticationLatch" , mockCountDownLatch ) ; mockit . Deencapsulation . setField ( amqpsSessionDeviceOperation , "cbsCorrelationIdList" , mockListUUID ) ; new mockit . NonStrictExpectations ( ) { { mockDeviceClientConfig . getAuthenticationType ( ) ; result = DeviceClientConfig . AuthType . SAS_TOKEN ; java . util . UUID . randomUUID ( ) ; result = mockUUID ; } } ; amqpsSessionDeviceOperation . authenticate ( ) ; tests . unit . com . microsoft . azure . sdk . iot . device . transport . amqps . AmqpsDeviceAuthenticationState authenticatorState = mockit . Deencapsulation . getField ( amqpsSessionDeviceOperation , "amqpsAuthenticatorState" ) ; org . junit . Assert . assertEquals ( AmqpsDeviceAuthenticationState . AUTHENTICATING , authenticatorState ) ; new mockit . Verifications ( ) { { mockListUUID . add ( mockUUID ) ; times = 1 ; mockit . Deencapsulation . invoke ( mockAmqpsDeviceAuthentication , "authenticate" , mockDeviceClientConfig , mockUUID ) ; times = 1 ; mockCountDownLatch . await ( MAX_WAIT_TO_AUTHENTICATE , TimeUnit . MILLISECONDS ) ; } } }
public class aTest{ @Test public void doesNotExecutesHttpRequestIfDelegateIsNull ( ) { doThrow ( com . ge . digitaltwin . tutorial . workflow . activiti . RuntimeException . class ) . when ( restTemplate ) . exchange ( any ( java . lang . String . class ) , any ( org . springframework . http . HttpMethod . class ) , any ( org . springframework . http . HttpEntity . class ) , eq ( java . lang . Object . class ) ) ; doThrow ( com . ge . digitaltwin . tutorial . workflow . activiti . RuntimeException . class ) . when ( restTemplate ) . exchange ( any ( java . lang . String . class ) , any ( org . springframework . http . HttpMethod . class ) , any ( org . springframework . http . HttpEntity . class ) , eq ( java . lang . Object . class ) , any ( java . util . Map . class ) ) ; try { postDelegate . execute ( null ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testCreateOrderToSellIsSuccessful ( ) { final byte [ ] encoded = java . nio . file . Files . readAllBytes ( java . nio . file . Paths . get ( com . gazbert . bxbot . exchanges . TestItBitExchangeAdapter . NEW_ORDER_SELL_JSON_RESPONSE ) ) ; final com . gazbert . bxbot . exchanges . AbstractExchangeAdapter . ExchangeHttpResponse exchangeResponse = new com . gazbert . bxbot . exchanges . AbstractExchangeAdapter . ExchangeHttpResponse ( 201 , "Created" , new java . lang . String ( encoded , java . nio . charset . StandardCharsets . UTF_8 ) ) ; final com . gazbert . bxbot . exchanges . Map < java . lang . String , java . lang . String > requestParamMap = org . powermock . api . easymock . PowerMock . createMock ( com . gazbert . bxbot . exchanges . Map . class ) ; expect ( requestParamMap . put ( "Created" 0 , "limit" ) ) . andStubReturn ( null ) ; expect ( requestParamMap . put ( "amount" , new java . text . DecimalFormat ( "#.####" , getDecimalFormatSymbols ( ) ) . format ( com . gazbert . bxbot . exchanges . TestItBitExchangeAdapter . SELL_ORDER_QUANTITY ) ) ) . andStubReturn ( null ) ; expect ( requestParamMap . put ( "Created" 2 , new java . text . DecimalFormat ( "#.##" , getDecimalFormatSymbols ( ) ) . format ( com . gazbert . bxbot . exchanges . TestItBitExchangeAdapter . SELL_ORDER_PRICE ) ) ) . andStubReturn ( null ) ; expect ( requestParamMap . put ( "instrument" , com . gazbert . bxbot . exchanges . TestItBitExchangeAdapter . MARKET_ID ) ) . andStubReturn ( null ) ; expect ( requestParamMap . put ( "currency" , com . gazbert . bxbot . exchanges . TestItBitExchangeAdapter . MARKET_ID . substring ( 0 , 3 ) ) ) . andStubReturn ( null ) ; expect ( requestParamMap . put ( "side" , "sell" ) ) . andStubReturn ( null ) ; final com . gazbert . bxbot . exchanges . ItBitExchangeAdapter exchangeAdapter = org . powermock . api . easymock . PowerMock . createPartialMockAndInvokeDefaultConstructor ( com . gazbert . bxbot . exchanges . ItBitExchangeAdapter . class , com . gazbert . bxbot . exchanges . TestItBitExchangeAdapter . MOCKED_SEND_AUTHENTICATED_REQUEST_TO_EXCHANGE_METHOD , com . gazbert . bxbot . exchanges . TestItBitExchangeAdapter . MOCKED_CREATE_REQUEST_PARAM_MAP_METHOD ) ; org . powermock . api . easymock . PowerMock . expectPrivate ( exchangeAdapter , com . gazbert . bxbot . exchanges . TestItBitExchangeAdapter . MOCKED_CREATE_REQUEST_PARAM_MAP_METHOD ) . andReturn ( requestParamMap ) ; org . powermock . api . easymock . PowerMock . expectPrivate ( exchangeAdapter , com . gazbert . bxbot . exchanges . TestItBitExchangeAdapter . MOCKED_SEND_AUTHENTICATED_REQUEST_TO_EXCHANGE_METHOD , eq ( "POST" ) , eq ( com . gazbert . bxbot . exchanges . TestItBitExchangeAdapter . NEW_ORDER ) , eq ( requestParamMap ) ) . andReturn ( exchangeResponse ) ; org . powermock . api . easymock . PowerMock . replayAll ( ) ; org . powermock . reflect . Whitebox . setInternalState ( exchangeAdapter , com . gazbert . bxbot . exchanges . TestItBitExchangeAdapter . MOCKED_WALLET_ID_FIELD_NAME , com . gazbert . bxbot . exchanges . TestItBitExchangeAdapter . WALLET_ID ) ; exchangeAdapter . init ( exchangeConfig ) ; final java . lang . String orderId = exchangeAdapter . createOrder ( com . gazbert . bxbot . exchanges . TestItBitExchangeAdapter . MARKET_ID , OrderType . SELL , com . gazbert . bxbot . exchanges . TestItBitExchangeAdapter . SELL_ORDER_QUANTITY , com . gazbert . bxbot . exchanges . TestItBitExchangeAdapter . SELL_ORDER_PRICE ) ; org . junit . Assert . assertTrue ( orderId . equals ( "Created" 1 ) ) ; org . powermock . api . easymock . PowerMock . verifyAll ( ) ; } }
public class aTest{ @Test public void testNumericAndStringTypes ( ) { final java . io . File table_dir = tempFolder . newFolder ( "numericAndStringTypes" ) ; final int record_count = 10000 ; java . io . BufferedOutputStream os = new java . io . BufferedOutputStream ( new java . io . FileOutputStream ( new java . io . File ( table_dir , "a.json" ) ) ) ; java . lang . String format = "{<sp>a<sp>:<sp>%d<sp>}%n" ; for ( int i = 0 ; i <= record_count ; i += 2 ) { os . write ( java . lang . String . format ( format , i ) . getBytes ( ) ) ; } os . close ( ) ; os = new java . io . BufferedOutputStream ( new java . io . FileOutputStream ( new java . io . File ( table_dir , "b.json" ) ) ) ; format = "{<sp>a<sp>:<sp>\"%05d\"<sp>}%n" ; for ( int i = 1 ; i <= record_count ; i += 2 ) { os . write ( java . lang . String . format ( format , i ) . getBytes ( ) ) ; } os . close ( ) ; java . lang . String query = java . lang . String . format ( "select<sp>*<sp>from<sp>dfs_root.\"%s\"<sp>order<sp>by<sp>a<sp>desc" , table_dir . toPath ( ) . toString ( ) ) ; try { attemptTestNumericAndStringTypes ( query , record_count ) ; } catch ( java . lang . Exception e ) { com . dremio . test . UserExceptionMatcher m = new com . dremio . test . UserExceptionMatcher ( UserBitShared . DremioPBError . ErrorType . DATA_READ , "SCHEMA_CHANGE<sp>ERROR" ) ; @ com . dremio . exec . physical . impl . xsort . SuppressWarnings ( "deprecation" ) final com . dremio . common . exceptions . UserException expectedException = com . dremio . common . exceptions . UserException . systemError ( e ) . build ( ) ; org . junit . Assert . assertTrue ( m . matches ( expectedException ) ) ; } }
public class aTest{ @Test public void testDictionaryMatchWithoutLeet ( ) { System . out . println ( "Test<sp>of<sp>dictionaryMatch<sp>method<sp>(without<sp>leet<sp>value),<sp>of<sp>class<sp>DictionaryMatcher" ) ; me . gosimple . nbvcxz . matching . PasswordMatcher matcher = new me . gosimple . nbvcxz . matching . DictionaryMatcher ( ) ; java . util . List < me . gosimple . nbvcxz . matching . match . Match > computed = matcher . match ( configuration , "assword" 3 ) ; java . util . List < me . gosimple . nbvcxz . matching . match . Match > expected = new java . util . ArrayList ( ) ; java . util . List < java . lang . Character [ ] > empty = new java . util . ArrayList ( ) ; expected . add ( new me . gosimple . nbvcxz . matching . match . DictionaryMatch ( "pas" , configuration , 0 , 2 , "pas" , 8458 , empty , false , false , "english" , 0 ) ) ; expected . add ( new me . gosimple . nbvcxz . matching . match . DictionaryMatch ( "assword" 4 , configuration , 0 , 3 , "assword" 4 , 75 , empty , false , false , "assword" 2 , 0 ) ) ; expected . add ( new me . gosimple . nbvcxz . matching . match . DictionaryMatch ( "assword" 4 , configuration , 0 , 3 , "assword" 4 , 1408 , empty , false , false , "english" , 0 ) ) ; expected . add ( new me . gosimple . nbvcxz . matching . match . DictionaryMatch ( "assword" 5 , configuration , 0 , 4 , "assword" 5 , 64655 , empty , false , false , "assword" 2 , 0 ) ) ; expected . add ( new me . gosimple . nbvcxz . matching . match . DictionaryMatch ( "assword" 6 , configuration , 0 , 5 , "assword" 6 , 49088 , empty , false , false , "assword" 2 , 0 ) ) ; expected . add ( new me . gosimple . nbvcxz . matching . match . DictionaryMatch ( "assword" 0 , configuration , 0 , 6 , "assword" 0 , 1441 , empty , false , false , "assword" 2 , 0 ) ) ; expected . add ( new me . gosimple . nbvcxz . matching . match . DictionaryMatch ( "assword" 3 , configuration , 0 , 7 , "assword" 3 , 2 , empty , false , false , "assword" 2 , 0 ) ) ; expected . add ( new me . gosimple . nbvcxz . matching . match . DictionaryMatch ( "assword" 3 , configuration , 0 , 7 , "passmore" , 3860 , empty , false , true , "surnames" , 2 ) ) ; expected . add ( new me . gosimple . nbvcxz . matching . match . DictionaryMatch ( "assword" 3 , configuration , 0 , 7 , "assword" 3 , 528 , empty , false , false , "english" , 0 ) ) ; expected . add ( new me . gosimple . nbvcxz . matching . match . DictionaryMatch ( "assword" 1 , configuration , 1 , 3 , "assword" 1 , 10015 , empty , false , false , "assword" 2 , 0 ) ) ; expected . add ( new me . gosimple . nbvcxz . matching . match . DictionaryMatch ( "assword" 1 , configuration , 1 , 3 , "assword" 1 , 1227 , empty , false , false , "english" , 0 ) ) ; expected . add ( new me . gosimple . nbvcxz . matching . match . DictionaryMatch ( "assword" , configuration , 1 , 7 , "assword" , 4678 , empty , false , false , "assword" 2 , 0 ) ) ; expected . add ( new me . gosimple . nbvcxz . matching . match . DictionaryMatch ( "assword" 8 , configuration , 3 , 6 , "assword" 7 , 7395 , empty , false , true , "english" , 0 ) ) ; expected . add ( new me . gosimple . nbvcxz . matching . match . DictionaryMatch ( "sword" , configuration , 3 , 7 , "sword" , 3647 , empty , false , false , "assword" 2 , 0 ) ) ; expected . add ( new me . gosimple . nbvcxz . matching . match . DictionaryMatch ( "sword" , configuration , 3 , 7 , "sword" , 6494 , empty , false , false , "english" , 0 ) ) ; expected . add ( new me . gosimple . nbvcxz . matching . match . DictionaryMatch ( "wor" , configuration , 4 , 6 , "row" , 2550 , empty , false , true , "english" , 0 ) ) ; expected . add ( new me . gosimple . nbvcxz . matching . match . DictionaryMatch ( "word" , configuration , 4 , 7 , "word" , 6455 , empty , false , false , "assword" 2 , 0 ) ) ; expected . add ( new me . gosimple . nbvcxz . matching . match . DictionaryMatch ( "word" , configuration , 4 , 7 , "word" , 745 , empty , false , false , "english" , 0 ) ) ; int computedHash = calcHash ( computed ) ; int expectedHash = calcHash ( expected ) ; org . junit . Assert . assertEquals ( expectedHash , computedHash ) ; } }
public class aTest{ @Test public void 401 ( ) { com . fujitsu . dc . test . jersey . DcResponse res = requesttoMypassword ( "passwordhoge" , "accountpass" , Setup . TEST_CELL1 ) ; org . junit . Assert . assertEquals ( 401 , res . getStatusCode ( ) ) ; } }
public class aTest{ @Test public void write_Corder ( ) { if ( ! ( isJp2KakDriverAvailable ) ) return ; it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . LOGGER . info ( "Testing<sp>JP2<sp>Write<sp>operation<sp>with<sp>Corder<sp>option<sp>setting" ) ; final java . io . File inputFile = it . geosolutions . resources . TestData . file ( this , it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . testFileName ) ; org . junit . Assert . assertTrue ( inputFile . exists ( ) ) ; final java . io . File outputFile1 = it . geosolutions . resources . TestData . temp ( this , "CorderPCRL-.jp2" , it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . deleteTempFilesOnExit ) ; final java . io . File outputFile2 = it . geosolutions . resources . TestData . temp ( this , "RPCL" 4 , it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . deleteTempFilesOnExit ) ; final java . io . File outputFile3 = it . geosolutions . resources . TestData . temp ( this , "CorderLRCP-.jp2" , it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . deleteTempFilesOnExit ) ; final javax . media . jai . ParameterBlockJAI pbjImageRead = new javax . media . jai . ParameterBlockJAI ( "RPCL" 3 ) ; pbjImageRead . setParameter ( "Input" , inputFile ) ; if ( it . geosolutions . imageio . plugins . jp2kakadu . JP2KWriteTest . ENABLE_SUBSAMPLING ) { javax . imageio . ImageReadParam readParam = new javax . imageio . ImageReadParam ( ) ; readParam . setSourceSubsampling ( 4 , 4 , 0 , 0 ) ; pbjImageRead . setParameter ( "readParam" , readParam ) ; } }
public class aTest{ @Test public void testFileSink ( ) { outFile = getTestTempFile ( "test-file-sink-" , "$[\\n\\r]*^\\d+\\s+test1.testRecord2:\\s+Context=test1," 3 ) ; final java . lang . String outPath = outFile . getAbsolutePath ( ) ; new org . apache . hadoop . metrics2 . impl . ConfigBuilder ( ) . add ( "*.period" , 10000 ) . add ( "test.sink.mysink0.class" , org . apache . hadoop . metrics2 . sink . FileSink . class . getName ( ) ) . add ( "test.sink.mysink0.filename" , outPath ) . add ( "test.sink.mysink0.context" , "$[\\n\\r]*^\\d+\\s+test1.testRecord2:\\s+Context=test1," 5 ) . save ( org . apache . hadoop . metrics2 . impl . TestMetricsConfig . getTestFilename ( "hadoop-metrics2-test" ) ) ; org . apache . hadoop . metrics2 . impl . MetricsSystemImpl ms = new org . apache . hadoop . metrics2 . impl . MetricsSystemImpl ( "test" ) ; ms . start ( ) ; final org . apache . hadoop . metrics2 . sink . TestFileSink . MyMetrics1 mm1 = new org . apache . hadoop . metrics2 . sink . TestFileSink . MyMetrics1 ( ) . registerWith ( ms ) ; new org . apache . hadoop . metrics2 . sink . TestFileSink . MyMetrics2 ( ) . registerWith ( ms ) ; mm1 . testMetric1 . incr ( ) ; mm1 . testMetric2 . incr ( 2 ) ; ms . publishMetricsNow ( ) ; ms . stop ( ) ; ms . shutdown ( ) ; java . io . InputStream is = null ; java . io . ByteArrayOutputStream baos = null ; java . lang . String outFileContent = null ; try { is = new java . io . FileInputStream ( outFile ) ; baos = new java . io . ByteArrayOutputStream ( ( ( int ) ( outFile . length ( ) ) ) ) ; org . apache . hadoop . io . IOUtils . copyBytes ( is , baos , 1024 , true ) ; outFileContent = new java . lang . String ( baos . toByteArray ( ) , "$[\\n\\r]*^\\d+\\s+test1.testRecord2:\\s+Context=test1," 0 ) ; } finally { org . apache . hadoop . io . IOUtils . cleanup ( null , baos , is ) ; } java . util . regex . Pattern expectedContentPattern = java . util . regex . Pattern . compile ( ( "^\\d+\\s+test1.testRecord1:\\s+Context=test1,\\s+" + ( ( ( "$[\\n\\r]*^\\d+\\s+test1.testRecord2:\\s+Context=test1," 4 + "\\s+Hostname=.*,\\s+(testMetric1=1,\\s+testMetric2=2|testMetric2=2,\\s+testMetric1=1)" ) + "$[\\n\\r]*^\\d+\\s+test1.testRecord2:\\s+Context=test1," ) + "$[\\n\\r]*^\\d+\\s+test1.testRecord2:\\s+Context=test1," 1 ) ) , Pattern . MULTILINE ) ; org . junit . Assert . assertTrue ( expectedContentPattern . matcher ( outFileContent ) . matches ( ) ) ; } }
public class aTest{ @Test public void create ( ) { nginxUpstream . delete ( ) . thenCompose ( ( nginxResponse ) -> nginxUpstream . create ( ) ) . thenAccept ( ( nginxResponse ) -> { if ( nginxResponse . error ( ) ) { com . jslsolucoes . nginx . admin . agent . model . response . NginxExceptionResponse nginxExceptionResponse = ( ( com . jslsolucoes . nginx . admin . agent . model . response . NginxExceptionResponse ) ( nginxResponse ) ) ; org . junit . Assert . fail ( nginxExceptionResponse . getStackTrace ( ) ) ; } else if ( nginxResponse . forbidden ( ) ) { com . jslsolucoes . nginx . admin . agent . model . response . NginxAuthenticationFailResponse nginxAuthenticationFailResponse = ( ( com . jslsolucoes . nginx . admin . agent . model . response . NginxAuthenticationFailResponse ) ( nginxResponse ) ) ; org . junit . Assert . fail ( nginxAuthenticationFailResponse . getMessage ( ) ) ; } else { com . jslsolucoes . nginx . admin . agent . model . response . upstream . NginxUpstreamCreateResponse nginxUpstreamCreateResponse = ( ( com . jslsolucoes . nginx . admin . agent . model . response . upstream . NginxUpstreamCreateResponse ) ( nginxResponse ) ) ; org . junit . Assert . assertTrue ( nginxUpstreamCreateResponse . getSuccess ( ) ) ; } } }
public class aTest{ @Test public void testNotifyError ( ) { com . google . cloud . dns . DnsException ex = new com . google . cloud . dns . DnsException ( new java . io . IOException ( "some<sp>error" ) , false ) ; org . junit . Assert . assertFalse ( result . completed ( ) ) ; BatchResult . Callback < java . lang . Boolean , com . google . cloud . dns . DnsException > callback = org . easymock . EasyMock . createStrictMock ( BatchResult . Callback . class ) ; callback . error ( ex ) ; org . easymock . EasyMock . replay ( callback ) ; result . notify ( callback ) ; result . error ( ex ) ; try { result . notify ( callback ) ; org . junit . Assert . fail ( "The<sp>batch<sp>has<sp>been<sp>completed." ) ; } }
public class aTest{ @Test public void testRMRestartNodeMapping ( ) { java . io . File nodeLabelFsStoreDir = new java . io . File ( "target" , ( ( this . getClass ( ) . getSimpleName ( ) ) + "-testRMRestartNodeMapping" ) ) ; if ( nodeLabelFsStoreDir . exists ( ) ) { org . apache . commons . io . FileUtils . deleteDirectory ( nodeLabelFsStoreDir ) ; } nodeLabelFsStoreDir . deleteOnExit ( ) ; java . lang . String nodeLabelFsStoreDirURI = nodeLabelFsStoreDir . toURI ( ) . toString ( ) ; conf . set ( YarnConfiguration . FS_NODE_LABELS_STORE_ROOT_DIR , nodeLabelFsStoreDirURI ) ; org . apache . hadoop . yarn . server . resourcemanager . recovery . MemoryRMStateStore memStore = new org . apache . hadoop . yarn . server . resourcemanager . recovery . MemoryRMStateStore ( ) ; memStore . init ( conf ) ; conf . setBoolean ( YarnConfiguration . NODE_LABELS_ENABLED , true ) ; org . apache . hadoop . yarn . server . resourcemanager . MockRM rm1 = new org . apache . hadoop . yarn . server . resourcemanager . MockRM ( conf , memStore ) { @ org . apache . hadoop . yarn . server . resourcemanager . Override protected org . apache . hadoop . yarn . server . resourcemanager . nodelabels . RMNodeLabelsManager createNodeLabelManager ( ) { org . apache . hadoop . yarn . server . resourcemanager . nodelabels . RMNodeLabelsManager mgr = new org . apache . hadoop . yarn . server . resourcemanager . nodelabels . RMNodeLabelsManager ( ) ; mgr . init ( getConfig ( ) ) ; return mgr ; } } ; rm1 . init ( conf ) ; rm1 . start ( ) ; org . apache . hadoop . yarn . server . resourcemanager . nodelabels . RMNodeLabelsManager nodeLabelManager = rm1 . getRMContext ( ) . getNodeLabelManager ( ) ; java . util . Set < java . lang . String > clusterNodeLabels = new java . util . HashSet < java . lang . String > ( ) ; clusterNodeLabels . add ( "x" ) ; clusterNodeLabels . add ( "y" ) ; nodeLabelManager . addToCluserNodeLabelsWithDefaultExclusivity ( clusterNodeLabels ) ; org . apache . hadoop . yarn . api . records . NodeId n1 = org . apache . hadoop . yarn . api . records . NodeId . newInstance ( "h1" , 1234 ) ; org . apache . hadoop . yarn . api . records . NodeId n2 = org . apache . hadoop . yarn . api . records . NodeId . newInstance ( "h1" , 1235 ) ; org . apache . hadoop . yarn . api . records . NodeId nihost = org . apache . hadoop . yarn . api . records . NodeId . newInstance ( "h1" , 0 ) ; nodeLabelManager . replaceLabelsOnNode ( com . google . common . collect . ImmutableMap . of ( n1 , toSet ( "x" ) ) ) ; nodeLabelManager . replaceLabelsOnNode ( com . google . common . collect . ImmutableMap . of ( n2 , toSet ( "x" ) ) ) ; nodeLabelManager . replaceLabelsOnNode ( com . google . common . collect . ImmutableMap . of ( nihost , toSet ( "y" ) ) ) ; nodeLabelManager . replaceLabelsOnNode ( com . google . common . collect . ImmutableMap . of ( n1 , toSet ( "x" ) ) ) ; org . apache . hadoop . yarn . server . resourcemanager . MockRM rm2 = null ; for ( int i = 0 ; i < 2 ; i ++ ) { rm2 = new org . apache . hadoop . yarn . server . resourcemanager . MockRM ( conf , memStore ) { @ org . apache . hadoop . yarn . server . resourcemanager . Override protected org . apache . hadoop . yarn . server . resourcemanager . nodelabels . RMNodeLabelsManager createNodeLabelManager ( ) { org . apache . hadoop . yarn . server . resourcemanager . nodelabels . RMNodeLabelsManager mgr = new org . apache . hadoop . yarn . server . resourcemanager . nodelabels . RMNodeLabelsManager ( ) ; mgr . init ( getConfig ( ) ) ; return mgr ; } } ; rm2 . init ( conf ) ; rm2 . start ( ) ; nodeLabelManager = rm2 . getRMContext ( ) . getNodeLabelManager ( ) ; java . util . Map < java . lang . String , java . util . Set < org . apache . hadoop . yarn . api . records . NodeId > > labelsToNodes = nodeLabelManager . getLabelsToNodes ( toSet ( "x" ) ) ; org . junit . Assert . assertEquals ( 1 , ( null == ( labelsToNodes . get ( "x" ) ) ? 0 : labelsToNodes . get ( "x" ) . size ( ) ) ) ; } }
public class aTest{ @Test public void CompactMemoryRrdTest ( ) { try { java . text . SimpleDateFormat df = new java . text . SimpleDateFormat ( "EEE<sp>MMM<sp>dd<sp>HH:mm:ss<sp>zzz<sp>yyyy" , java . util . Locale . US ) ; df . setLenient ( true ) ; org . krakenapps . rrd . RrdConfig config = new org . krakenapps . rrd . RrdConfig ( df . parse ( "Fri<sp>Jan<sp>29<sp>22:08:32<sp>KST<sp>2010" ) , 10 ) ; config . addDataSource ( "asdf" , DataSourceType . ABSOLUTE2 , 20 , Double . NaN , Double . NaN ) ; config . addArchive ( ConsolidateFunc . SUM , 0.5 , 6 , 10 ) ; org . krakenapps . rrd . CompactRrd cmr = new org . krakenapps . rrd . CompactRrd ( new org . krakenapps . rrd . io . MemoryPersistentLayer ( ) , config ) ; while ( ( getCompactMemoryRrdRaw ( cmr ) ) != null ) { java . lang . Runtime . getRuntime ( ) . gc ( ) ; java . lang . Thread . sleep ( 100 ) ; } updateRrd ( df , cmr ) ; java . util . concurrent . ConcurrentHashMap < java . lang . String , org . krakenapps . rrd . CompactRrd > rrds = new java . util . concurrent . ConcurrentHashMap < java . lang . String , org . krakenapps . rrd . CompactRrd > ( ) ; rrds . put ( "asdf" , cmr ) ; java . lang . Runtime . getRuntime ( ) . gc ( ) ; try { rrds . get ( "asdf" ) . save ( new org . krakenapps . rrd . io . FilePersistentLayer ( new java . io . File ( "CompactMemoryRrdTest.bin" ) ) ) ; } catch ( java . io . IOException e ) { e . printStackTrace ( ) ; } cmr = null ; while ( ( getCompactMemoryRrdRaw ( rrds . get ( "asdf" ) ) ) != null ) { java . lang . Runtime . getRuntime ( ) . gc ( ) ; java . lang . Thread . sleep ( 100 ) ; } try { rrds . get ( "asdf" ) . save ( new org . krakenapps . rrd . io . FilePersistentLayer ( new java . io . File ( "CompactMemoryRrdTest.bin" ) ) ) ; } catch ( java . io . IOException e ) { e . printStackTrace ( ) ; } org . krakenapps . rrd . FetchResult fetch = rrds . get ( "asdf" ) . fetch ( ConsolidateFunc . SUM , df . parse ( "Fri<sp>Jan<sp>29<sp>22:10:00<sp>KST<sp>2010" ) , df . parse ( "Fri<sp>Jan<sp>29<sp>22:16:00<sp>KST<sp>2010" ) , 60 ) ; for ( org . krakenapps . rrd . FetchRow row : fetch . getRows ( ) ) { org . junit . Assert . assertTrue ( ( ( row . getColumn ( 0 ) ) == 1.0 ) ) ; } } }
public class aTest{ @Test public void shouldEqualsReturnFalseAndDoNotMakeInfinityCycleWith3Entities ( ) { final com . qcadoo . model . api . Entity secondEntity = new com . qcadoo . model . internal . DefaultEntity ( dataDefinition ) ; final com . qcadoo . model . api . Entity firstEntity = new com . qcadoo . model . internal . DefaultEntity ( dataDefinition ) ; final com . qcadoo . model . api . Entity thirdEntity = new com . qcadoo . model . internal . DefaultEntity ( dataDefinition ) ; firstEntity . setId ( 1L ) ; secondEntity . setId ( 2L ) ; thirdEntity . setId ( 3L ) ; when ( dataDefinition . get ( firstEntity . getId ( ) ) ) . thenReturn ( firstEntity ) ; when ( dataDefinition . get ( secondEntity . getId ( ) ) ) . thenReturn ( secondEntity ) ; when ( dataDefinition . get ( thirdEntity . getId ( ) ) ) . thenReturn ( thirdEntity ) ; final com . qcadoo . model . api . Entity firstWrappedEntity = buildEntityWrapper ( firstEntity ) ; final com . qcadoo . model . api . Entity secondWrappedEntity = buildEntityWrapper ( secondEntity ) ; final com . qcadoo . model . api . Entity thirdWrappedEntity = buildEntityWrapper ( thirdEntity ) ; firstWrappedEntity . setField ( com . qcadoo . model . internal . AbstractEntityWrapperTest . BELONGS_TO_FIELD_NAME , secondWrappedEntity ) ; secondWrappedEntity . setField ( com . qcadoo . model . internal . AbstractEntityWrapperTest . BELONGS_TO_FIELD_NAME , thirdWrappedEntity ) ; thirdWrappedEntity . setField ( com . qcadoo . model . internal . AbstractEntityWrapperTest . BELONGS_TO_FIELD_NAME , firstWrappedEntity ) ; final com . qcadoo . model . api . Entity secondOtherEntity = new com . qcadoo . model . internal . DefaultEntity ( dataDefinition ) ; final com . qcadoo . model . api . Entity firstOtherEntity = new com . qcadoo . model . internal . DefaultEntity ( dataDefinition ) ; final com . qcadoo . model . api . Entity thirdOtherEntity = new com . qcadoo . model . internal . DefaultEntity ( dataDefinition ) ; firstOtherEntity . setId ( 1L ) ; secondOtherEntity . setId ( 2L ) ; thirdOtherEntity . setId ( 3L ) ; thirdOtherEntity . setField ( com . qcadoo . model . internal . AbstractEntityWrapperTest . STRING_FIELD_NAME , "difference" ) ; when ( dataDefinition . get ( firstOtherEntity . getId ( ) ) ) . thenReturn ( firstOtherEntity ) ; when ( dataDefinition . get ( secondOtherEntity . getId ( ) ) ) . thenReturn ( secondOtherEntity ) ; when ( dataDefinition . get ( thirdOtherEntity . getId ( ) ) ) . thenReturn ( thirdOtherEntity ) ; final com . qcadoo . model . api . Entity firstOtherWrappedEntity = buildEntityWrapper ( firstOtherEntity ) ; final com . qcadoo . model . api . Entity secondOtherWrappedEntity = buildEntityWrapper ( secondOtherEntity ) ; final com . qcadoo . model . api . Entity thirdOtherWrappedEntity = buildEntityWrapper ( thirdOtherEntity ) ; firstOtherWrappedEntity . setField ( com . qcadoo . model . internal . AbstractEntityWrapperTest . BELONGS_TO_FIELD_NAME , secondOtherWrappedEntity ) ; secondOtherWrappedEntity . setField ( com . qcadoo . model . internal . AbstractEntityWrapperTest . BELONGS_TO_FIELD_NAME , thirdOtherWrappedEntity ) ; thirdOtherWrappedEntity . setField ( com . qcadoo . model . internal . AbstractEntityWrapperTest . BELONGS_TO_FIELD_NAME , firstOtherWrappedEntity ) ; try { org . junit . Assert . assertFalse ( firstWrappedEntity . equals ( firstOtherWrappedEntity ) ) ; } }
public class aTest{ @Test public void createTableWithoutName ( ) { try { createTable ( null , createStringAttributeDefinition ( ) ) ; org . junit . Assert . assertTrue ( false ) ; } }
public class aTest{ @Test public void testGetAggregationsFromNestedAggregationAggregationsSetIsNull ( ) { org . elasticsearch . action . search . SearchResponse searchResponse = mock ( org . elasticsearch . action . search . SearchResponse . class ) ; org . elasticsearch . search . aggregations . bucket . nested . Nested nestedAggregation = mock ( org . elasticsearch . search . aggregations . bucket . nested . Nested . class ) ; when ( nestedAggregation . getAggregations ( ) ) . thenReturn ( null ) ; when ( jsonHelper . objectToJson ( searchResponse ) ) . thenReturn ( org . finra . herd . dao . helper . SEARCH_RESPONSE_JSON_STRING ) ; when ( jsonHelper . objectToJson ( nestedAggregation ) ) . thenReturn ( org . finra . herd . dao . helper . NESTED_AGGREGATION_JSON_STRING ) ; try { elasticsearchHelper . getAggregationsFromNestedAggregation ( nestedAggregation , searchResponse ) ; } catch ( java . lang . IllegalStateException e ) { org . junit . Assert . assertEquals ( "Invalid<sp>search<sp>result." , e . getMessage ( ) ) ; } }
public class aTest{ @Test public void requiresCalledAfterInstall ( ) { org . jboss . msc . service . ServiceBuilder sb = serviceContainer . addService ( org . jboss . msc . multi_value_services . WrongUsageOfNewServicesAPITestCase . ID ) ; sb . provides ( org . jboss . msc . multi_value_services . WrongUsageOfNewServicesAPITestCase . FOO ) ; org . junit . Assert . assertNotNull ( sb . install ( ) ) ; try { sb . requires ( org . jboss . msc . multi_value_services . WrongUsageOfNewServicesAPITestCase . BAR ) ; org . junit . Assert . fail ( "IllegalStateException<sp>expected" ) ; } }
public class aTest{ @Test public void testServicesAnnotatedNoneSetButExtendsMultiple ( ) { class B implements java . io . Serializable , java . util . EventListener { } @ com . liferay . portal . kernel . spring . osgi . OSGiBeanProperties class C extends B { } java . util . Set < java . lang . String > interfaceNames = OSGiBeanProperties . Service . interfaceNames ( new C ( ) , C . class . getAnnotation ( com . liferay . portal . kernel . spring . osgi . OSGiBeanProperties . class ) , StringPool . EMPTY_ARRAY ) ; org . junit . Assert . assertEquals ( interfaceNames . toString ( ) , 3 , interfaceNames . size ( ) ) ; } }
public class aTest{ @Test public void testComplexQuotesSql ( ) { final java . lang . String [ ] [ ] expected = new java . lang . String [ ] [ ] { new java . lang . String [ ] { "11111" , "11113" 3 , "11113" 0 } , new java . lang . String [ ] { "11111" , "11113" 3 , "11113" 0 } , new java . lang . String [ ] { "11112" , "a'a" , "11113" 2 } , new java . lang . String [ ] { "11113" , "a\\" , "b)b" , "11113" 5 } , new java . lang . String [ ] { "11111" , "11113" 3 , "11113" 6 } , new java . lang . String [ ] { "11112" , "11113" 3 , "11113" 4 } , new java . lang . String [ ] { "11113" , "a'a'a" , "b" , "((c,c))," } } ; net . osmand . util . SqlInsertValuesReader . readInsertValuesFile ( net . osmand . util . ReadInsertValuesTest . class . getResourceAsStream ( "/test_wiki.sql" ) , new net . osmand . util . SqlInsertValuesReader . InsertValueProcessor ( ) { int ind = 0 ; @ net . osmand . util . Override public void process ( java . util . List < java . lang . String > vs ) { System . out . println ( ( ( ( ( ( ind ) + "11113" 1 ) + ( java . util . Arrays . toString ( expected [ ind ] ) ) ) + "11113" 1 ) + vs ) ) ; java . lang . String [ ] array = vs . toArray ( new java . lang . String [ vs . size ( ) ] ) ; org . junit . Assert . assertArrayEquals ( array , expected [ ind ] ) ; ( ind ) ++ ; } } }
public class aTest{ @Test public void testLoadApplicationTemplate_conflict ( ) { java . io . File directory = net . roboconf . core . internal . tests . TestUtils . findApplicationDirectory ( "lamp" ) ; org . junit . Assert . assertTrue ( directory . exists ( ) ) ; try { this . mngr . loadApplicationTemplate ( directory ) ; } }
public class aTest{ @Test public void encodesQueryRequestCorrectly ( ) { final byte [ ] exp = new byte [ ] { ( ( byte ) ( 131 ) ) , 104 , 4 , 100 , 0 , 10 , 116 , 115 , 113 , 117 , 101 , 114 , 121 , 114 , 101 , 113 , 104 , 3 , 100 , 0 , 15 , 116 , 115 , 105 , 110 , 116 , 101 , 114 , 112 , 111 , 108 , 97 , 116 , 105 , 111 , 110 , 109 , 0 , 0 , 0 , 21 , 83 , 69 , 76 , 69 , 67 , 84 , 32 , 42 , 32 , 70 , 82 , 79 , 77 , 32 , 70 , 82 , 65 , 90 , 90 , 76 , 69 , 106 , 100 , 0 , 5 , 102 , 97 , 108 , 115 , 101 , 100 , 0 , 9 , 117 , 110 , 100 , 101 , 102 , 105 , 110 , 101 , 100 } ; try { com . ericsson . otp . erlang . OtpOutputStream os = com . basho . riak . client . core . codec . TermToBinaryCodec . encodeTsQueryRequest ( com . basho . riak . client . core . codec . TermToBinaryCodecTest . QUERY , null ) ; os . flush ( ) ; byte [ ] msg = os . toByteArray ( ) ; org . junit . Assert . assertArrayEquals ( exp , msg ) ; } }
public class aTest{ @Test public void testComplexValue ( ) { org . apache . accumulo . core . client . summary . SummarizerConfiguration sc = org . apache . accumulo . core . client . summary . SummarizerConfiguration . builder ( org . apache . accumulo . core . client . summary . summarizers . EntryLengthSummarizer . class ) . build ( ) ; org . apache . accumulo . core . client . summary . summarizers . EntryLengthSummarizer entrySum = new org . apache . accumulo . core . client . summary . summarizers . EntryLengthSummarizer ( ) ; org . apache . accumulo . core . data . Key k1 = new org . apache . accumulo . core . data . Key ( "r1" , "qualifier.logHist.1" 0 , "columnRow3" 9 , "qualifier.logHist.1" 3 ) ; org . apache . accumulo . core . data . Key k2 = new org . apache . accumulo . core . data . Key ( "columnRow3" 7 , "row.sum" 1 , "qualifier.logHist.1" 6 , "row.sum" 0 ) ; org . apache . accumulo . core . data . Key k3 = new org . apache . accumulo . core . data . Key ( "columnRow3" , "row.sum" 2 , "family.logHist.4" 0 , "row.sum" 3 ) ; org . apache . accumulo . core . client . summary . Summarizer . Collector collector = entrySum . collector ( sc ) ; collector . accept ( k1 , new org . apache . accumulo . core . data . Value ( "qualifier.logHist.1" 3 ) ) ; collector . accept ( k2 , new org . apache . accumulo . core . data . Value ( "row.sum" 7 ) ) ; collector . accept ( k3 , new org . apache . accumulo . core . data . Value ( "row.sum" 6 ) ) ; java . util . HashMap < java . lang . String , java . lang . Long > stats = new java . util . HashMap ( ) ; collector . summarize ( stats :: put ) ; java . util . HashMap < java . lang . String , java . lang . Long > expected = new java . util . HashMap ( ) ; expected . put ( "key.min" , 27L ) ; expected . put ( "columnRow3" 5 , 39L ) ; expected . put ( "family.logHist.4" 8 , 96L ) ; expected . put ( "columnRow3" 2 , 3L ) ; expected . put ( "columnRow3" 8 , 2L ) ; expected . put ( "family.logHist.4" 7 , 10L ) ; expected . put ( "row.sum" , 16L ) ; expected . put ( "qualifier.logHist.1" 5 , 1L ) ; expected . put ( "family.logHist.4" 2 , 1L ) ; expected . put ( "row.sum" 4 , 1L ) ; expected . put ( "columnRow3" 4 , 2L ) ; expected . put ( "family.logHist.4" 9 , 13L ) ; expected . put ( "family.sum" , 22L ) ; expected . put ( "family.logHist.4" 5 , 1L ) ; expected . put ( "qualifier.logHist.1" 1 , 1L ) ; expected . put ( "family.logHist.4" , 1L ) ; expected . put ( "qualifier.min" , 2L ) ; expected . put ( "family.logHist.4" 4 , 16L ) ; expected . put ( "qualifier.logHist.1" 4 , 28L ) ; expected . put ( "qualifier.logHist.1" , 1L ) ; expected . put ( "qualifier.logHist.3" , 1L ) ; expected . put ( "family.logHist.4" 6 , 1L ) ; expected . put ( "qualifier.logHist.1" 7 , 2L ) ; expected . put ( "row.sum" 8 , 17L ) ; expected . put ( "row.sum" 9 , 30L ) ; expected . put ( "family.logHist.4" 1 , 1L ) ; expected . put ( "columnRow3" 6 , 1L ) ; expected . put ( "columnRow3" 0 , 1L ) ; expected . put ( "value.min" , 2L ) ; expected . put ( "qualifier.logHist.1" 2 , 9L ) ; expected . put ( "columnRow3" 3 , 17L ) ; expected . put ( "family.logHist.4" 3 , 1L ) ; expected . put ( "row.sum" 5 , 2L ) ; expected . put ( "columnRow3" 1 , 3L ) ; org . junit . Assert . assertEquals ( expected , stats ) ; } }
public class aTest{ @Test public void testXor ( ) { System . out . println ( "testXor<sp>" ) ; java . util . Iterator < com . googlecode . javaewah32 . EWAHCompressedBitmap32 [ ] > i = com . googlecode . javaewah32 . IteratorAggregation32Test . getCollections ( 2 , 3 ) ; while ( i . hasNext ( ) ) { com . googlecode . javaewah32 . EWAHCompressedBitmap32 [ ] x = i . next ( ) ; com . googlecode . javaewah32 . EWAHCompressedBitmap32 tanswer = x [ 0 ] . xor ( x [ 1 ] ) ; com . googlecode . javaewah32 . EWAHCompressedBitmap32 x1 = com . googlecode . javaewah32 . IteratorUtil32 . materialize ( com . googlecode . javaewah32 . IteratorAggregation32 . bufferedxor ( x [ 0 ] . getIteratingRLW ( ) , x [ 1 ] . getIteratingRLW ( ) ) ) ; org . junit . Assert . assertTrue ( x1 . equals ( tanswer ) ) ; } }
public class aTest{ @Test public void testGeneratingBaseConusmerScenario3 ( ) { haveProperty = false ; java . io . File consumerProps = createPropertyFile ( destDir . getAbsolutePath ( ) , CONSUMER_PROPERTIES ) ; consumerProper . put ( "-interface" 6 , "" ) ; fillProperties ( consumerProper , consumerProps ) ; java . lang . String [ ] testArgs1 = new java . lang . String [ ] { "-interface" 2 , "-interface" 9 , "-genType" , "Consumer" , "-interface" , "-interface" 0 , "-interface" 3 , destDir . getAbsolutePath ( ) , "-dest" , destDir . getAbsolutePath ( ) , "-interface" 5 , "-genType" 0 , "-interface" 4 , "-interface" 1 , "-bin" , binDir . getAbsolutePath ( ) , "-interface" 7 , destDir . getAbsolutePath ( ) , "-adminname" , "AdminV1" , "-cn" , "-interface" 8 } ; performDirectCodeGen ( testArgs1 , binDir ) ; baseConsumer = ( destDir . getAbsolutePath ( ) ) + "/gen-src/org/ebayopensource/turmeric/tools/codegen/gen/BaseAdminV1Consumer.java" ; baseConsumerClass = new java . io . File ( baseConsumer ) ; org . junit . Assert . assertTrue ( baseConsumerClass . exists ( ) ) ; } }
public class aTest{ @Test public void testGetOsDistro ( ) { java . lang . String [ ] expected = new java . lang . String [ ] { "DevOsDitribution" , "Linux" } ; try { boolean foundMatch = false ; for ( java . lang . String possibility : expected ) { if ( org . eclipse . kura . core . system . test . SystemServiceTest . systemService . getOsDistro ( ) . equals ( possibility ) ) { foundMatch = true ; break ; } } org . junit . Assert . assertTrue ( foundMatch ) ; } }
public class aTest{ @Test public void insertSeveralRollbackTest ( ) { java . lang . String sql = ( "insert<sp>into<sp>" + ( normaltblTableName ) ) + "<sp>values(?,?,?,?,?)" ; java . util . List < java . lang . Object > param = new java . util . ArrayList < java . lang . Object > ( ) ; param . add ( com . taobao . tddl . qatest . matrix . transaction . RANDOM_ID ) ; param . add ( com . taobao . tddl . qatest . matrix . transaction . RANDOM_INT ) ; param . add ( gmt ) ; param . add ( name ) ; param . add ( fl ) ; andorCon = us . getConnection ( ) ; andorCon . setAutoCommit ( false ) ; andorPs = null ; con = getConnection ( ) ; con . setAutoCommit ( false ) ; ps = null ; try { int mysqlAffectRow = mysqlUpdateDataTranscation ( sql , param ) ; int andorAffectRow = andorUpdateDataTranscation ( sql , param ) ; org . junit . Assert . assertEquals ( mysqlAffectRow , andorAffectRow ) ; sql = ( ( "select<sp>*<sp>from<sp>" + ( normaltblTableName ) ) + "<sp>where<sp>pk=" ) + ( RANDOM_ID ) ; java . lang . String [ ] columnParam = new java . lang . String [ ] { "PK" , "ID" , "GMT_CREATE" , "(pk,id)<sp>values(" 0 , "FLOATCOL" } ; selectOrderAssertTranscation ( sql , columnParam , null ) ; java . sql . Connection otherAndorCon = us . getConnection ( ) ; otherAndorCon . setAutoCommit ( false ) ; java . sql . Statement st = otherAndorCon . createStatement ( ) ; sql = ( ( ( ( ( "insert<sp>into<sp>" + ( normaltblTableName ) ) + "(pk,id)<sp>values(" ) + ( RANDOM_ID ) ) + "(pk,id)<sp>values(" 1 ) + ( RANDOM_INT ) ) + ")" ; try { st . execute ( sql ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void testOrderingTypedef ( ) { final java . util . Set < org . opendaylight . yangtools . yang . model . api . TypeDefinition < ? > > typedefs = bar . getTypeDefinitions ( ) ; final java . lang . String [ ] expectedOrder = new java . lang . String [ ] { "int32-ext1" , "int32-ext2" , "string-ext1" , "string-ext2" , "my-union" 1 , "string-ext4" , "invalid-string-pattern" , "my-union" 0 , "my-decimal-type" , "my-union" , "my-union-ext" , "nested-union2" } ; final java . lang . String [ ] actualOrder = new java . lang . String [ typedefs . size ( ) ] ; int offset = 0 ; for ( final org . opendaylight . yangtools . yang . model . api . TypeDefinition < ? > type : typedefs ) { actualOrder [ offset ] = type . getQName ( ) . getLocalName ( ) ; offset ++ ; } org . junit . Assert . assertArrayEquals ( expectedOrder , actualOrder ) ; } }
public class aTest{ @Test public void testGetPersonWithHandler ( ) { org . apache . ibatis . session . SqlSession sqlSession = org . apache . ibatis . submitted . nestedresulthandler . NestedResultHandlerTest . sqlSessionFactory . openSession ( ) ; try { sqlSession . select ( "getPersons" , new org . apache . ibatis . session . ResultHandler ( ) { public void handleResult ( org . apache . ibatis . session . ResultContext context ) { org . apache . ibatis . submitted . nestedresulthandler . Person person = ( ( org . apache . ibatis . submitted . nestedresulthandler . Person ) ( context . getResultObject ( ) ) ) ; if ( "grandma" . equals ( person . getName ( ) ) ) { org . junit . Assert . assertEquals ( 2 , person . getItems ( ) . size ( ) ) ; } } } }
public class aTest{ @Test public void testRollback_UndeployKo ( ) { com . alu . e3 . prov . restapi . model . Api api = newApi ( ) ; try { byte [ ] oldJar = new byte [ ] { 0 , 1 } ; ( ( com . alu . e3 . data . DataManager ) ( apiService . getDataManager ( ) ) ) . deployApi ( api . getId ( ) , oldJar ) ; apiService . setDeploymentManager ( new com . alu . e3 . prov . deployment . MockDeploymentManager . FirstDeployKoSecondDeployOkUndeployKo ( ) ) ; com . alu . e3 . prov . restapi . ExchangeData exchange = com . alu . e3 . prov . service . ApiService . createExchange ( api ) ; apiService . deployApi ( api , exchange ) ; } catch ( com . alu . e3 . prov . ProvisionException e ) { org . junit . Assert . assertEquals ( 1 , e . getErrorCode ( ) ) ; } }
public class aTest{ @Test public void connectFailsWithSslv3 ( ) { org . kaazing . gateway . server . test . Gateway gateway = new org . kaazing . gateway . server . test . Gateway ( ) ; java . net . Socket socket = null ; try { org . kaazing . gateway . server . test . config . GatewayConfiguration configuration = new org . kaazing . gateway . server . test . config . builder . GatewayConfigurationBuilder ( ) . service ( ) . accept ( "tcp://localhost:8567" ) . connect ( "ssl://localhost:8568" ) . type ( "proxy" ) . connectOption ( "ssl.protocols" , "SSLv3" ) . done ( ) . service ( ) . accept ( "ssl://localhost:8568" ) . type ( "echo" ) . done ( ) . security ( ) . keyStore ( keyStore ) . keyStorePassword ( password ) . trustStore ( trustStore ) . done ( ) . done ( ) ; gateway . start ( configuration ) ; socket = javax . net . SocketFactory . getDefault ( ) . createSocket ( "localhost" , 8567 ) ; java . io . BufferedWriter w = new java . io . BufferedWriter ( new java . io . OutputStreamWriter ( socket . getOutputStream ( ) ) ) ; java . io . BufferedReader r = new java . io . BufferedReader ( new java . io . InputStreamReader ( socket . getInputStream ( ) ) ) ; java . lang . String expected = "Hello<sp>World!" ; w . write ( expected ) ; w . newLine ( ) ; w . flush ( ) ; java . lang . String got = r . readLine ( ) ; org . junit . Assert . assertNull ( got ) ; w . close ( ) ; r . close ( ) ; } }
public class aTest{ @Test public void testRetry3TimesUntilSuccess ( ) { java . util . concurrent . ScheduledExecutorService scheduled = java . util . concurrent . Executors . newScheduledThreadPool ( 1 ) ; service . setScheduled ( scheduled ) . setFactory ( new com . ctrip . xpipe . command . DefaultRetryCommandFactory ( 3 , new com . ctrip . xpipe . retry . RetryDelay ( 10 ) , scheduled ) ) ; com . ctrip . xpipe . api . command . Command < java . lang . String > command = service . retry3TimesUntilSuccess ( new com . ctrip . xpipe . command . AbstractCommand < java . lang . String > ( ) { private java . util . concurrent . atomic . AtomicInteger counter = new java . util . concurrent . atomic . AtomicInteger ( 0 ) ; @ com . ctrip . xpipe . redis . console . service . meta . impl . Override protected void doExecute ( ) throws com . ctrip . xpipe . redis . console . service . meta . impl . Exception { int currentCount = counter . getAndIncrement ( ) ; logger . info ( java . lang . String . format ( "Run<sp>%d<sp>time" , currentCount ) ) ; if ( currentCount > 1 ) { future ( ) . setSuccess ( "success" ) ; } else { throw new com . ctrip . xpipe . exception . XpipeRuntimeException ( "test<sp>exception" ) ; } } @ com . ctrip . xpipe . redis . console . service . meta . impl . Override protected void doReset ( ) { } @ com . ctrip . xpipe . redis . console . service . meta . impl . Override public java . lang . String getName ( ) { return "test-retry" ; } } ) ; java . util . concurrent . atomic . AtomicBoolean complete = new java . util . concurrent . atomic . AtomicBoolean ( false ) ; command . future ( ) . addListener ( ( commandFuture ) -> { org . junit . Assert . assertEquals ( "success" , commandFuture . getNow ( ) ) ; complete . getAndSet ( true ) ; } }
public class aTest{ @Test public void testPopulateDbMetaOutofTransaction ( ) { com . ctrip . platform . dal . dao . client . ConnectionActionTest . TestConnectionAction test = new com . ctrip . platform . dal . dao . client . ConnectionActionTest . TestConnectionAction ( ) ; test . initLogEntry ( "dao_test" , new com . ctrip . platform . dal . dao . DalHints ( ) ) ; try { test . connHolder = getDalConnection ( ) ; test . populateDbMeta ( ) ; org . junit . Assert . assertNotNull ( test . entry . getDatabaseName ( ) ) ; } }
public class aTest{ @Test public void testExponentialInclude ( ) { java . lang . String grammarFormat = "parser<sp>grammar<sp>Level_%d_%d;\n" + ( ( ( "\n" + "%s<sp>import<sp>Level_%d_1,<sp>Level_%d_2;\n" ) + "\n" ) + "rule_%d_%d<sp>:<sp>EOF;\n" ) ; System . out . println ( ( "dir<sp>" + ( tmpdir ) ) ) ; mkdir ( tmpdir ) ; long startTime = java . lang . System . nanoTime ( ) ; int levels = 20 ; for ( int level = 0 ; level < levels ; level ++ ) { java . lang . String leafPrefix = ( level == ( levels - 1 ) ) ? "//" : "_1.g4" 0 ; java . lang . String grammar1 = java . lang . String . format ( grammarFormat , level , 1 , leafPrefix , ( level + 1 ) , ( level + 1 ) , level , 1 ) ; writeFile ( tmpdir , ( ( "Level_" + level ) + "_1.g4" ) , grammar1 ) ; if ( level > 0 ) { java . lang . String grammar2 = java . lang . String . format ( grammarFormat , level , 2 , leafPrefix , ( level + 1 ) , ( level + 1 ) , level , 1 ) ; writeFile ( tmpdir , ( ( "Level_" + level ) + "_2.g4" ) , grammar2 ) ; } } org . antlr . v4 . test . tool . ErrorQueue equeue = antlr ( "Level_0_1.g4" , false ) ; org . junit . Assert . assertTrue ( equeue . errors . isEmpty ( ) ) ; long endTime = java . lang . System . nanoTime ( ) ; System . out . format ( "_1.g4" 1 , ( ( endTime - startTime ) / 1000000.0 ) ) ; } }
public class aTest{ @Test public void test_search_with_moduleNames_componentName_flowName_keyword_success ( ) { java . nio . file . Path path = createTempDir ( ) ; org . apache . solr . core . SolrResourceLoader loader = new org . apache . solr . core . SolrResourceLoader ( path ) ; org . apache . solr . core . NodeConfig config = new org . apache . solr . core . NodeConfig . NodeConfigBuilder ( "testnode" , loader ) . setConfigSetBaseDirectory ( java . nio . file . Paths . get ( org . ikasan . wiretap . service . SolrWiretapServiceTest . TEST_HOME ( ) ) . resolve ( "configsets" ) . toString ( ) ) . build ( ) ; try ( org . apache . solr . client . solrj . embedded . EmbeddedSolrServer server = new org . apache . solr . client . solrj . embedded . EmbeddedSolrServer ( config , "ikasan" ) ) { org . apache . solr . client . solrj . request . CoreAdminRequest . Create createRequest = new org . apache . solr . client . solrj . request . CoreAdminRequest . Create ( ) ; createRequest . setCoreName ( "ikasan" ) ; createRequest . setConfigSet ( "minimal" ) ; server . request ( createRequest ) ; java . util . HashMap < java . lang . String , java . lang . Object > fields = new java . util . HashMap ( ) ; fields . put ( "testnode" 0 , new java . lang . Integer ( 1 ) ) ; org . apache . solr . client . solrj . request . schema . SchemaRequest . AddField schemaRequest = new org . apache . solr . client . solrj . request . schema . SchemaRequest . AddField ( fields ) ; server . request ( schemaRequest ) ; org . ikasan . wiretap . dao . SolrWiretapDao solrCloudBase = new org . ikasan . wiretap . dao . SolrWiretapDao ( ) ; solrCloudBase . setSolrClient ( server ) ; solrCloudBase . setDaysToKeep ( 0 ) ; org . ikasan . wiretap . model . SolrWiretapEvent event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 1L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "event" ) ; solrCloudBase . save ( event ) ; event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 2L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "event" ) ; solrCloudBase . save ( event ) ; event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 3L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "testnode" 2 ) ; solrCloudBase . save ( event ) ; event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 4L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "event" ) ; solrCloudBase . save ( event ) ; event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 5L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "event" ) ; solrCloudBase . save ( event ) ; java . util . HashSet < java . lang . String > moduleNames = new java . util . HashSet < java . lang . String > ( ) ; moduleNames . add ( "moduleName" ) ; org . ikasan . wiretap . service . SolrWiretapServiceImpl wiretapService = new org . ikasan . wiretap . service . SolrWiretapServiceImpl ( solrCloudBase , moduleService ) ; org . ikasan . spec . search . PagedSearchResult < org . ikasan . spec . wiretap . WiretapEvent > results = wiretapService . findWiretapEvents ( 0 , 10 , "testnode" 1 , true , moduleNames , "flowName" , "componentName" , null , null , new java . util . Date ( ( ( java . lang . System . currentTimeMillis ( ) ) - 100000000 ) ) , new java . util . Date ( ( ( java . lang . System . currentTimeMillis ( ) ) + 100000000 ) ) , "testnode" 2 ) ; org . junit . Assert . assertEquals ( "testnode" 3 , results . getResultSize ( ) , 2 ) ; server . close ( ) ; } } }
public class aTest{ @Test public void testAllocateAndReleaseContainersForMultipleAM ( ) { int numberOfApps = 5 ; for ( int testAppId = 0 ; testAppId < numberOfApps ; testAppId ++ ) { org . apache . hadoop . yarn . api . protocolrecords . RegisterApplicationMasterResponse registerResponse = registerApplicationMaster ( testAppId ) ; org . junit . Assert . assertNotNull ( registerResponse ) ; java . util . List < org . apache . hadoop . yarn . api . records . Container > containers = getContainersAndAssert ( testAppId , 10 ) ; releaseContainersAndAssert ( testAppId , containers ) ; } }
public class aTest{ @Test public void testGetTimeInstantSQLTimestampWithoutMiliseconds ( ) { System . out . println ( ( ( ( com . telefonica . iot . cygnus . utils . CommonUtilsForTests . getTestTraceHead ( "[CommonUtils.getTimeInstant]" ) ) + "--------<sp>When<sp>getting<sp>a<sp>time<sp>instant,<sp>it<sp>is<sp>properly<sp>obtained<sp>when<sp>passing<sp>a<sp>valid<sp>" ) + "SQL<sp>timestamp<sp>without<sp>miliseconds" ) ) ; org . json . simple . JSONObject metadataJson = new org . json . simple . JSONObject ( ) ; metadataJson . put ( "name" , "[CommonUtils.getTimeInstant]" 2 ) ; metadataJson . put ( "type" , "SQL<sp>timestamp" ) ; metadataJson . put ( "value" , "[CommonUtils.getTimeInstant]" 0 ) ; org . json . simple . JSONArray metadatasJson = new org . json . simple . JSONArray ( ) ; metadatasJson . add ( metadataJson ) ; java . lang . String metadatasStr = metadatasJson . toJSONString ( ) ; java . lang . Long timeInstant = com . telefonica . iot . cygnus . utils . CommonUtils . getTimeInstant ( metadatasStr ) ; try { org . junit . Assert . assertTrue ( ( timeInstant != null ) ) ; System . out . println ( ( ( ( ( ( ( com . telefonica . iot . cygnus . utils . CommonUtilsForTests . getTestTraceHead ( "[CommonUtils.getTimeInstant]" ) ) + "-<sp>OK<sp>-<sp>Time<sp>instant<sp>obtained<sp>for<sp>'" ) + ( metadatasJson . toJSONString ( ) ) ) + "'<sp>is<sp>'" ) + timeInstant ) + "'" ) ) ; } }
public class aTest{ @Test public void generateDDL_whenTableHasMultipleUriLocations_returnsDDLStringForASingleColumn ( ) { org . jkiss . dbeaver . ext . greenplum . model . PostgreTableColumn mockPostgreTableColumn = mockDbColumn ( "\t\'gpfdist://filehost:8081/*.txt\',\n" 0 , "int4" , 1 ) ; java . util . List < org . jkiss . dbeaver . ext . greenplum . model . PostgreTableColumn > tableColumns = java . util . Collections . singletonList ( mockPostgreTableColumn ) ; org . mockito . Mockito . when ( mockResults . getString ( "urilocation" ) ) . thenReturn ( "gpfdist://filehost:8081/*.txt,gpfdist://filehost:8081/*.gz" ) ; org . jkiss . dbeaver . ext . greenplum . model . GreenplumExternalTable table = new org . jkiss . dbeaver . ext . greenplum . model . GreenplumExternalTable ( mockSchema , mockResults ) ; addMockColumnsToTableCache ( tableColumns , table ) ; java . lang . String expectedDDL = "CREATE<sp>EXTERNAL<sp>TABLE<sp>sampleDatabase.sampleSchema.sampleTable<sp>(\n\tcolumn1<sp>int4\n)\n" + ( ( ( ( ( "LOCATION<sp>(\n" + "\t\'gpfdist://filehost:8081/*.txt\',\n" ) + "\t\'gpfdist://filehost:8081/*.gz\'\n" ) + ")<sp>ON<sp>ALL\n" ) + "FORMAT<sp>\'CSV\'<sp>(<sp>DELIMITER<sp>\',\'<sp>)\n" ) + "ENCODING<sp>'UTF8'" ) ; org . junit . Assert . assertEquals ( expectedDDL , table . generateDDL ( monitor ) ) ; } }
public class aTest{ @Test public void testUpdateDocumentWithoutContentOnNewLanguage ( ) { org . silverpeas . core . contribution . attachment . model . SimpleDocument document = new org . silverpeas . core . contribution . attachment . model . SimpleDocument ( new org . silverpeas . core . contribution . attachment . model . SimpleDocumentPK ( DOCUMENT_ID , INSTANCE_ID ) , "18" , 10 , false , new org . silverpeas . core . contribution . attachment . model . SimpleAttachment ( "test.pdf" , "fr" , "fileName" 2 , "fileName" 3 , 500L , org . silverpeas . util . MimeTypes . PDF_MIME_TYPE , USER_ID_IN_TEST , creationDate , null ) ) ; org . silverpeas . core . contribution . attachment . AttachmentService service = mock ( org . silverpeas . core . contribution . attachment . AttachmentService . class ) ; when ( service . searchDocumentById ( eq ( new org . silverpeas . core . contribution . attachment . model . SimpleDocumentPK ( DOCUMENT_ID ) ) , anyString ( ) ) ) . thenReturn ( document ) ; getTestResources ( ) . setAttachmentService ( service ) ; com . sun . jersey . multipart . FormDataMultiPart form = new com . sun . jersey . multipart . FormDataMultiPart ( ) ; form . field ( "fileName" , "/Shared/marketing/my_test_document.txt" ) ; form . field ( "fileName" 0 , "fileName" 1 ) ; form . field ( "fileTitle" , "Upload<sp>test" ) ; form . field ( "fileDescription" , "This<sp>test<sp>is<sp>trying<sp>to<sp>simulate<sp>the<sp>update<sp>of<sp>a<sp>content" ) ; com . sun . jersey . api . client . WebResource webResource = resource ( ) ; try { webResource . path ( ( ( ( RESOURCE_PATH ) + ( DOCUMENT_ID ) ) + "/test.pdf" ) ) . header ( org . silverpeas . core . contribution . attachment . web . HTTP_SESSIONKEY , getSessionKey ( ) ) . accept ( javax . ws . rs . core . MediaType . APPLICATION_JSON_TYPE ) . type ( javax . ws . rs . core . MediaType . MULTIPART_FORM_DATA ) . post ( org . silverpeas . core . contribution . attachment . web . SimpleDocumentEntity . class , form ) ; } catch ( final com . sun . jersey . api . client . UniformInterfaceException ex ) { final int receivedStatus = ex . getResponse ( ) . getStatus ( ) ; final int preconditionFailed = Status . PRECONDITION_FAILED . getStatusCode ( ) ; org . junit . Assert . assertThat ( receivedStatus , org . hamcrest . Matchers . is ( preconditionFailed ) ) ; } } }
public class aTest{ @Test public void testGroupBy ( ) { try { org . talend . dq . dbms . DbmsLanguage dbms = getMysqlDbmsLanguage ( ) ; org . junit . Assert . assertNotNull ( dbms . groupBy ( ) ) ; } }
public class aTest{ @Test public void testRunSqlite4 ( ) { sqlite . feature . foreignkey . BindDummyDataSource dataSource = sqlite . feature . foreignkey . BindDummyDataSource . getInstance ( ) ; dataSource . execute ( new sqlite . feature . foreignkey . BindDummyDataSource . Transaction ( ) { @ sqlite . feature . foreignkey . Override public com . abubusoft . kripton . android . sqlite . TransactionResult onExecute ( sqlite . feature . foreignkey . BindDummyDaoFactory daoFactory ) { sqlite . feature . foreignkey . DaoBeanA_1Impl dao = daoFactory . getDaoBeanA_1 ( ) ; sqlite . feature . foreignkey . BeanA_2 beanParent = new sqlite . feature . foreignkey . BeanA_2 ( ) ; beanParent . valueString2 = "parent" ; daoFactory . getDaoBeanA_2 ( ) . insert ( beanParent ) ; sqlite . feature . foreignkey . BeanA_1 bean = new sqlite . feature . foreignkey . BeanA_1 ( ) ; bean . valueString = "hello" ; bean . beanA2Id = beanParent . id ; dao . insert ( bean ) ; org . junit . Assert . assertEquals ( 1 , bean . id ) ; return com . abubusoft . kripton . android . sqlite . TransactionResult . COMMIT ; } } }
public class aTest{ @Test public void testGetTargetClusterAction ( ) { java . util . Map < java . lang . String , java . lang . String > props = new java . util . HashMap ( ) ; org . apache . ambari . server . controller . ivory . Feed feed = new org . apache . ambari . server . controller . ivory . Feed ( "Feed1" , "d" , "s" , "sch" , "source" , "st" , "end" , "l" , "sa" , "target" , "st" , "end" , "l" , "d" 0 , props ) ; org . junit . Assert . assertEquals ( "d" 0 , feed . getTargetClusterAction ( ) ) ; } }
public class aTest{ @Test public void testGetKeyStore ( ) { java . security . KeyStore keyStore = com . oath . auth . Utils . getKeyStore ( "truststore.jks" , "123456" . toCharArray ( ) ) ; org . junit . Assert . assertNotNull ( keyStore ) ; try { com . oath . auth . Utils . getKeyStore ( "truststore.jks" ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void testIrreducibleRandom1 ( ) { org . apache . commons . math3 . random . RandomGenerator rnd = getRandom ( ) ; cc . redberry . rings . util . RandomDataGenerator rndd = getRandomData ( ) ; int nIterations = its ( 5000 , 1000 ) ; for ( int i = 0 ; i < nIterations ; i ++ ) { long modulus = getModulusRandom ( rndd . nextInt ( 5 , 15 ) ) ; cc . redberry . rings . poly . univar . UnivariatePolynomialZp64 poly = cc . redberry . rings . poly . univar . RandomUnivariatePolynomials . randomMonicPoly ( rndd . nextInt ( 5 , 15 ) , modulus , rnd ) ; cc . redberry . rings . poly . PolynomialFactorDecomposition < cc . redberry . rings . poly . univar . UnivariatePolynomialZp64 > factors = cc . redberry . rings . poly . univar . UnivariateFactorization . Factor ( poly ) ; try { org . junit . Assert . assertEquals ( ( ( factors . size ( ) ) == 1 ) , cc . redberry . rings . poly . univar . IrreduciblePolynomials . IrreduciblePolynomials . irreducibleQ ( poly ) ) ; } }
public class aTest{ @Test public void testGetSetBackgroundColor ( ) { try { org . odftoolkit . simple . TextDocument doc = org . odftoolkit . simple . TextDocument . newTextDocument ( ) ; org . odftoolkit . simple . text . Paragraph paragraph1 = doc . addParagraph ( "paragraph1" ) ; org . odftoolkit . simple . text . ParagraphStyleHandler psh = paragraph1 . getStyleHandler ( ) ; org . odftoolkit . simple . style . ParagraphProperties paraProp = psh . getParagraphPropertiesForWrite ( ) ; paraProp . setBackgroundColor ( new org . odftoolkit . odfdom . type . Color ( "#FF0000" ) ) ; psh = paragraph1 . getStyleHandler ( ) ; paraProp = psh . getParagraphPropertiesForWrite ( ) ; org . junit . Assert . assertEquals ( "#FF0000" , paraProp . getBackgroundColorAttribute ( ) ) ; } }
public class aTest{ @Test public void ADSOutputStream ( ) { try { writer . openActiveFile ( ) ; } catch ( com . fujitsu . dc . common . ads . AdsWriteFailureLogException e ) { org . junit . Assert . fail ( "open<sp>failure<sp>test<sp>failed" ) ; } java . io . File file = null ; try { java . lang . Class < ? > clazz = writer . getClass ( ) ; java . lang . reflect . Field ostream = clazz . getDeclaredField ( "activeFileOutputStream" ) ; ostream . setAccessible ( true ) ; ostream . set ( writer , null ) ; file = getAdsWriteFailureLog ( ) ; } catch ( java . lang . Exception e ) { e . printStackTrace ( ) ; org . junit . Assert . fail ( "configuration<sp>failed." ) ; } try { writer . openActiveFile ( ) ; org . junit . Assert . fail ( "open<sp>failure<sp>test<sp>failed" ) ; } catch ( com . fujitsu . dc . common . ads . AdsWriteFailureLogException e ) { org . junit . Assert . assertTrue ( file . exists ( ) ) ; } }
public class aTest{ @Test public void testOrderPrefixNotRemoved ( ) { org . apache . ibatis . session . SqlSession sqlSession = org . apache . ibatis . submitted . order_prefix_removed . OrderPrefixRemoved . sqlSessionFactory . openSession ( ExecutorType . SIMPLE ) ; try { org . apache . ibatis . submitted . order_prefix_removed . PersonMapper personMapper = sqlSession . getMapper ( org . apache . ibatis . submitted . order_prefix_removed . PersonMapper . class ) ; org . apache . ibatis . submitted . order_prefix_removed . Person person = personMapper . select ( new java . lang . String ( "slow" ) ) ; org . junit . Assert . assertNotNull ( person ) ; sqlSession . commit ( ) ; } }
public class aTest{ @Test public void testSendUrgentDataThrowsRuntimeException ( ) { java . lang . RuntimeException expected = new java . lang . RuntimeException ( ) ; when ( delegate , "sendUrgentData" , anyInt ( ) ) . thenThrow ( expected ) ; try { sniffySocket . sendUrgentData ( 1 ) ; org . junit . Assert . fail ( ) ; } catch ( java . lang . Exception actual ) { org . junit . Assert . assertEquals ( expected , actual ) ; } }
public class aTest{ @Test public void testSubtemplateAsDefaultArg ( ) { java . lang . String templates = ( "t(x,y={<x:{s|<s><s>}>})<sp>::=<sp><<\n" + ( ( "x:<sp><x>\n" + "y:<sp><y>\n" ) + ">>" ) ) + ( newline ) ; writeFile ( tmpdir , "group.stg" , templates ) ; org . stringtemplate . v4 . STGroup group = new org . stringtemplate . v4 . STGroupFile ( ( ( tmpdir ) + "x" 0 ) ) ; org . stringtemplate . v4 . ST b = group . getInstanceOf ( "t" ) ; b . add ( "x" , "a" ) ; java . lang . String expecting = ( "x:<sp>a" + ( newline ) ) + "y:<sp>aa" ; java . lang . String result = b . render ( ) ; org . junit . Assert . assertEquals ( expecting , result ) ; } }
public class aTest{ @Test public void testNodeDecomissionWithOverreplicationRespectsRackPolicy ( ) { org . apache . hadoop . conf . Configuration conf = getConf ( ) ; short REPLICATION_FACTOR = 5 ; final org . apache . hadoop . fs . Path filePath = new org . apache . hadoop . fs . Path ( "/testFile" ) ; org . apache . hadoop . fs . FileSystem localFileSys = org . apache . hadoop . fs . FileSystem . getLocal ( conf ) ; org . apache . hadoop . fs . Path workingDir = localFileSys . getWorkingDirectory ( ) ; org . apache . hadoop . fs . Path dir = new org . apache . hadoop . fs . Path ( workingDir , "build/test/data/temp/decommission" ) ; org . apache . hadoop . fs . Path excludeFile = new org . apache . hadoop . fs . Path ( dir , "exclude" ) ; org . junit . Assert . assertTrue ( localFileSys . mkdirs ( dir ) ) ; writeFile ( localFileSys , excludeFile , "" ) ; conf . set ( "dfs.hosts.exclude" , excludeFile . toUri ( ) . getPath ( ) ) ; java . lang . String [ ] racks = new java . lang . String [ ] { "/rack1" , "/rack2" , "/rack1" , "/rack1" , "/rack1" } ; org . apache . hadoop . hdfs . MiniDFSCluster cluster = new org . apache . hadoop . hdfs . MiniDFSCluster ( conf , racks . length , true , racks ) ; final org . apache . hadoop . hdfs . server . namenode . FSNamesystem ns = cluster . getNameNode ( ) . getNamesystem ( ) ; try { final org . apache . hadoop . fs . FileSystem fs = cluster . getFileSystem ( ) ; org . apache . hadoop . hdfs . DFSTestUtil . createFile ( fs , filePath , 1L , REPLICATION_FACTOR , 1L ) ; org . apache . hadoop . hdfs . protocol . Block b = org . apache . hadoop . hdfs . DFSTestUtil . getFirstBlock ( fs , filePath ) ; waitForReplication ( ns , b , 2 , REPLICATION_FACTOR , 0 ) ; REPLICATION_FACTOR = 2 ; fs . setReplication ( filePath , REPLICATION_FACTOR ) ; org . apache . hadoop . fs . BlockLocation [ ] locs = fs . getFileBlockLocations ( fs . getFileStatus ( filePath ) , 0 , Long . MAX_VALUE ) ; java . lang . String [ ] tops = locs [ 0 ] . getTopologyPaths ( ) ; for ( java . lang . String top : tops ) { if ( ! ( top . startsWith ( "/rack2" ) ) ) { java . lang . String name = top . substring ( ( ( "/rack1" . length ( ) ) + 1 ) ) ; writeFile ( localFileSys , excludeFile , name ) ; ns . refreshNodes ( conf ) ; waitForDecommission ( fs , name ) ; break ; } } }
public class aTest{ @Test public void testRowValueConstructorOnRHSWithBuiltInFunctionOperatingOnIntegerLiteralOnLHS ( ) { java . lang . String tenantId = getOrganizationId ( ) ; java . lang . String tableName = initATableValues ( null , tenantId , getDefaultSplits ( tenantId ) , null , null , getUrl ( ) , null ) ; java . lang . String query = ( "SELECT<sp>a_integer,<sp>x_integer<sp>FROM<sp>" + tableName ) + "<sp>WHERE<sp>?=organization_id<sp>AND<sp>to_number('7')<sp><=<sp>(a_integer,<sp>x_integer)" ; java . util . Properties props = org . apache . phoenix . util . PropertiesUtil . deepCopy ( org . apache . phoenix . end2end . TEST_PROPERTIES ) ; java . sql . Connection conn = java . sql . DriverManager . getConnection ( getUrl ( ) , props ) ; try { java . sql . PreparedStatement statement = conn . prepareStatement ( query ) ; statement . setString ( 1 , tenantId ) ; java . sql . ResultSet rs = statement . executeQuery ( ) ; int count = 0 ; while ( rs . next ( ) ) { count ++ ; } org . junit . Assert . assertEquals ( 3 , count ) ; } }
public class aTest{ @Test public void testTextQueryDefAnalyzers1 ( ) { final java . lang . String turtleA = org . apache . jena . atlas . lib . StrUtils . strjoinNL ( org . apache . jena . query . text . TURTLE_PROLOG , ( ( "<" + ( RESOURCE_BASE ) ) + "testResultOneInModelA>" ) , "<sp>rdfs:label<sp>'bar<sp>testResultOne<sp>barfoo<sp>foo'" , "." , ( ( "<" + ( RESOURCE_BASE ) ) + "testResultTwoInModelA>" ) , "testResultOneInModelA>" 0 , "." , ( ( "<" + ( RESOURCE_BASE ) ) + "testResultThreeInModelA>" ) , "<sp>rdfs:label<sp>'bar<sp>testResultThree<sp>barfoo<sp>foo'" , "." ) ; putTurtleInModel ( turtleA , "http://example.org/modelA" ) ; final java . lang . String turtleB = org . apache . jena . atlas . lib . StrUtils . strjoinNL ( org . apache . jena . query . text . TURTLE_PROLOG , ( ( "<" + ( RESOURCE_BASE ) ) + "testResultOneInModelB>" ) , "<sp>rdfs:label<sp>'bar<sp>testResultOne<sp>barfoo<sp>foo'" , "." ) ; putTurtleInModel ( turtleB , "http://example.org/modelB" ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testReadOnlyRoleOnSchema ( ) { org . umlg . sqlg . test . roles . TestReadOnlyRole . TopologyGrantListener topologyGrantListener = new org . umlg . sqlg . test . roles . TestReadOnlyRole . TopologyGrantListener ( ) ; this . sqlgGraph . getTopology ( ) . registerListener ( topologyGrantListener ) ; this . sqlgGraph . addVertex ( T . label , "A.A" , "name" , "a1" ) ; this . sqlgGraph . addVertex ( T . label , "A.A" , "name" , "a2" ) ; this . sqlgGraph . tx ( ) . commit ( ) ; org . apache . commons . configuration . Configuration readOnlyConfiguration = new org . apache . commons . configuration . PropertiesConfiguration ( "sqlg.readonly.properties" ) ; try ( org . umlg . sqlg . structure . SqlgGraph readOnlyGraph = org . umlg . sqlg . structure . SqlgGraph . open ( readOnlyConfiguration ) ) { java . util . List < org . apache . tinkerpop . gremlin . structure . Vertex > vertices = readOnlyGraph . traversal ( ) . V ( ) . hasLabel ( "A.A" ) . toList ( ) ; org . junit . Assert . assertEquals ( 2 , vertices . size ( ) ) ; try { readOnlyGraph . addVertex ( T . label , "A.A" , "name" , "a3" ) ; org . junit . Assert . fail ( "Graph<sp>is<sp>suppose<sp>to<sp>be<sp>readOnly" ) ; } }
public class aTest{ @Test public void compare_shouldCompareViaNumericValueNotStringValue ( ) { java . lang . String [ ] correctStringSet = new java . lang . String [ ] { "1.1" , "1.2" , "1.7" , "1.2" 0 , "1.11" , "1.20" , "2.1.1" , "2.1.9" , "2.1.10" , "2.1.20" } ; java . lang . String [ ] randomPurmutationSet = new java . lang . String [ ] { "1.2" , "2.1.10" , "2.1.20" , "1.1" , "1.7" , "2.1.1" , "1.20" , "1.2" 0 , "2.1.9" , "1.11" } ; java . util . Arrays . sort ( randomPurmutationSet , new org . openmrs . module . VersionComparator ( ) ) ; org . junit . Assert . assertTrue ( "" , java . util . Arrays . equals ( correctStringSet , randomPurmutationSet ) ) ; } }
public class aTest{ @Test public void LastAuthenticatednull ( ) { java . lang . String updateUserName = "account1999" ; java . lang . String updatePass = "password19999" ; try { com . fujitsu . dc . test . utils . AccountUtils . create ( AbstractCase . MASTER_TOKEN_NAME , cellName , updateUserName , updatePass , HttpStatus . SC_CREATED ) ; com . fujitsu . dc . test . utils . AccountUtils . update ( AbstractCase . MASTER_TOKEN_NAME , cellName , updateUserName , updateUserName , orgPass , HttpStatus . SC_NO_CONTENT ) ; com . fujitsu . dc . test . utils . TResponse res = com . fujitsu . dc . test . utils . AccountUtils . get ( AbstractCase . MASTER_TOKEN_NAME , HttpStatus . SC_OK , cellName , updateUserName ) ; java . lang . String getLastAuthenticated = ( ( java . lang . String ) ( ( ( org . json . simple . JSONObject ) ( ( ( org . json . simple . JSONObject ) ( res . bodyAsJson ( ) . get ( "d" ) ) ) . get ( "results" ) ) ) . get ( "LastAuthenticated" ) ) ) ; org . junit . Assert . assertNull ( getLastAuthenticated ) ; } }
public class aTest{ @Test public void testAnonymousOff ( ) { com . hortonworks . registries . auth . server . PseudoAuthenticationHandler handler = new com . hortonworks . registries . auth . server . PseudoAuthenticationHandler ( ) ; try { java . util . Properties props = new java . util . Properties ( ) ; props . setProperty ( PseudoAuthenticationHandler . ANONYMOUS_ALLOWED , "false" ) ; handler . init ( props ) ; javax . servlet . http . HttpServletRequest request = org . mockito . Mockito . mock ( javax . servlet . http . HttpServletRequest . class ) ; javax . servlet . http . HttpServletResponse response = org . mockito . Mockito . mock ( javax . servlet . http . HttpServletResponse . class ) ; org . mockito . Mockito . when ( request . getQueryString ( ) ) . thenReturn ( "" ) ; com . hortonworks . registries . auth . server . AuthenticationToken token = handler . authenticate ( request , response ) ; org . junit . Assert . assertNull ( token ) ; } }
public class aTest{ @Test public void testOidFound ( ) { org . locationtech . geogig . repository . Repository repo = repoProvider . createGeogig ( "repo1" , null ) ; repoProvider . getTestRepository ( "repo1" ) . initializeRpository ( ) ; new org . locationtech . geogig . test . TestData ( repo ) . init ( "testGeoGig" , "geogig@geogig.org" ) . loadDefaultData ( ) ; java . util . Iterator < org . locationtech . geogig . model . RevCommit > call = repo . command ( org . locationtech . geogig . porcelain . LogOp . class ) . call ( ) ; org . junit . Assert . assertTrue ( call . hasNext ( ) ) ; while ( call . hasNext ( ) ) { org . locationtech . geogig . model . RevCommit next = call . next ( ) ; java . lang . String oid = next . getId ( ) . toString ( ) ; org . springframework . test . web . servlet . request . MockHttpServletRequestBuilder get = org . springframework . test . web . servlet . request . MockMvcRequestBuilders . get ( ( "/repos/repo1/repo/exists?oid=" + oid ) ) ; perform ( get ) . andExpect ( status ( ) . isOk ( ) ) . andExpect ( content ( ) . contentType ( MediaType . TEXT_PLAIN ) ) . andExpect ( content ( ) . string ( org . hamcrest . Matchers . containsString ( "1" ) ) ) ; } }
public class aTest{ @Test public void testFindByTypeAndDate ( ) { java . lang . String preventionType1 = "20071012" 0 ; java . lang . String preventionType2 = "omega" ; boolean isDeleted = true ; boolean isRefused = true ; java . util . Date preventionStartDate = new java . util . Date ( dfm . parse ( "20081012" ) . getTime ( ) ) ; java . util . Date preventionEndDate = new java . util . Date ( dfm . parse ( "20121001" ) . getTime ( ) ) ; java . util . Date preventionDate1 = new java . util . Date ( dfm . parse ( "20130101" ) . getTime ( ) ) ; java . util . Date preventionDate2 = new java . util . Date ( dfm . parse ( "20071012" 1 ) . getTime ( ) ) ; java . util . Date preventionDate3 = new java . util . Date ( dfm . parse ( "20100915" ) . getTime ( ) ) ; java . util . Date preventionDate4 = new java . util . Date ( dfm . parse ( "20071012" ) . getTime ( ) ) ; java . util . Date preventionDate5 = new java . util . Date ( dfm . parse ( "20081212" ) . getTime ( ) ) ; org . oscarehr . common . model . Prevention prevention1 = new org . oscarehr . common . model . Prevention ( ) ; org . oscarehr . common . dao . utils . EntityDataGenerator . generateTestDataForModelClass ( prevention1 ) ; prevention1 . setPreventionType ( preventionType1 ) ; prevention1 . setPreventionDate ( preventionDate1 ) ; prevention1 . setDeleted ( ( ! isDeleted ) ) ; prevention1 . setRefused ( isRefused ) ; dao . persist ( prevention1 ) ; org . oscarehr . common . model . Prevention prevention2 = new org . oscarehr . common . model . Prevention ( ) ; org . oscarehr . common . dao . utils . EntityDataGenerator . generateTestDataForModelClass ( prevention2 ) ; prevention2 . setPreventionType ( preventionType2 ) ; prevention2 . setPreventionDate ( preventionDate2 ) ; prevention2 . setDeleted ( isDeleted ) ; prevention2 . setRefused ( ( ! isRefused ) ) ; dao . persist ( prevention2 ) ; org . oscarehr . common . model . Prevention prevention3 = new org . oscarehr . common . model . Prevention ( ) ; org . oscarehr . common . dao . utils . EntityDataGenerator . generateTestDataForModelClass ( prevention3 ) ; prevention3 . setPreventionType ( preventionType1 ) ; prevention3 . setPreventionDate ( preventionDate3 ) ; prevention3 . setDeleted ( ( ! isDeleted ) ) ; prevention3 . setRefused ( ( ! isRefused ) ) ; dao . persist ( prevention3 ) ; org . oscarehr . common . model . Prevention prevention4 = new org . oscarehr . common . model . Prevention ( ) ; org . oscarehr . common . dao . utils . EntityDataGenerator . generateTestDataForModelClass ( prevention4 ) ; prevention4 . setPreventionType ( preventionType1 ) ; prevention4 . setPreventionDate ( preventionDate4 ) ; prevention4 . setDeleted ( isDeleted ) ; prevention4 . setRefused ( ( ! isRefused ) ) ; dao . persist ( prevention4 ) ; org . oscarehr . common . model . Prevention prevention5 = new org . oscarehr . common . model . Prevention ( ) ; org . oscarehr . common . dao . utils . EntityDataGenerator . generateTestDataForModelClass ( prevention5 ) ; prevention5 . setPreventionType ( preventionType1 ) ; prevention5 . setPreventionDate ( preventionDate5 ) ; prevention5 . setDeleted ( ( ! isDeleted ) ) ; prevention5 . setRefused ( ( ! isRefused ) ) ; dao . persist ( prevention5 ) ; java . util . List < org . oscarehr . common . model . Prevention > expectedResult = new java . util . ArrayList < org . oscarehr . common . model . Prevention > ( java . util . Arrays . asList ( prevention5 , prevention3 ) ) ; java . util . List < org . oscarehr . common . model . Prevention > result = dao . findByTypeAndDate ( preventionType1 , preventionStartDate , preventionEndDate ) ; org . apache . log4j . Logger logger = org . oscarehr . util . MiscUtils . getLogger ( ) ; if ( ( result . size ( ) ) != ( expectedResult . size ( ) ) ) { logger . warn ( ( "Array<sp>sizes<sp>do<sp>not<sp>match.RESULT:<sp>" + ( result . size ( ) ) ) ) ; org . junit . Assert . fail ( "Array<sp>sizes<sp>do<sp>not<sp>match." ) ; } for ( int i = 0 ; i < ( expectedResult . size ( ) ) ; i ++ ) { if ( ! ( expectedResult . get ( i ) . equals ( result . get ( i ) ) ) ) { logger . warn ( "Items<sp>do<sp>not<sp>match." ) ; org . junit . Assert . fail ( "Items<sp>do<sp>not<sp>match." ) ; } } org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void testWordCountMapper ( ) { org . pentaho . di . trans . TransMeta transMeta = new org . pentaho . di . trans . TransMeta ( "src/it/resources/wordcount-mapper.ktr" ) ; transMeta . setTransformationType ( TransformationType . SingleThreaded ) ; long transStart = java . lang . System . currentTimeMillis ( ) ; org . pentaho . di . trans . Trans trans = new org . pentaho . di . trans . Trans ( transMeta ) ; trans . setLogLevel ( LogLevel . MINIMAL ) ; trans . prepareExecution ( null ) ; org . pentaho . di . trans . step . StepInterface si = trans . getStepInterface ( "<sp>:<sp>not<sp>the<sp>expected<sp>amount<sp>of<sp>output<sp>rows." 5 , 0 ) ; org . pentaho . di . trans . RowStepCollector rc = new org . pentaho . di . trans . RowStepCollector ( ) ; si . addRowListener ( rc ) ; org . pentaho . di . trans . RowProducer rp = trans . addRowProducer ( "<sp>:<sp>not<sp>the<sp>expected<sp>amount<sp>of<sp>output<sp>rows." 8 , 0 ) ; trans . startThreads ( ) ; java . lang . String metricsStep = "Remove<sp>garbage" ; org . pentaho . di . trans . SingleThreadedTransExecutor executor = new org . pentaho . di . trans . SingleThreadedTransExecutor ( trans ) ; executor . init ( ) ; int iterations = 1000000 ; long totalWait = 0 ; java . util . List < org . pentaho . di . core . RowMetaAndData > inputList = createMapperData ( ) ; for ( int i = 0 ; i < iterations ; i ++ ) { for ( org . pentaho . di . core . RowMetaAndData rm : inputList ) { java . lang . Object [ ] copy = rm . getRowMeta ( ) . cloneRow ( rm . getData ( ) ) ; rp . putRow ( rm . getRowMeta ( ) , copy ) ; } long start = java . lang . System . currentTimeMillis ( ) ; boolean cont = executor . oneIteration ( ) ; if ( ! cont ) { org . junit . Assert . fail ( "<sp>:<sp>not<sp>the<sp>expected<sp>amount<sp>of<sp>output<sp>rows." 0 ) ; } long end = java . lang . System . currentTimeMillis ( ) ; long delay = end - start ; totalWait += delay ; if ( ( i > 0 ) && ( ( i % 100000 ) == 0 ) ) { long rowsProcessed = trans . findRunThread ( metricsStep ) . getLinesRead ( ) ; double speed = org . pentaho . di . core . Const . round ( ( rowsProcessed / ( ( ( double ) ( end - transStart ) ) / 1000 ) ) , 1 ) ; int totalRows = 0 ; for ( org . pentaho . di . trans . step . StepMetaDataCombi combi : trans . getSteps ( ) ) { for ( org . pentaho . di . core . RowSet rowSet : combi . step . getInputRowSets ( ) ) { totalRows += rowSet . size ( ) ; } for ( org . pentaho . di . core . RowSet rowSet : combi . step . getOutputRowSets ( ) ) { totalRows += rowSet . size ( ) ; } } System . out . println ( ( ( ( ( ( ( ( ( ( "<sp>:<sp>not<sp>the<sp>expected<sp>amount<sp>of<sp>output<sp>rows." 2 + i ) + "<sp>:<sp>not<sp>the<sp>expected<sp>amount<sp>of<sp>output<sp>rows." 6 ) + delay ) + "ms,<sp>average<sp>is:<sp>" ) + ( org . pentaho . di . core . Const . round ( ( ( ( double ) ( totalWait ) ) / ( i + 1 ) ) , 1 ) ) ) + ",<sp>speed=" ) + speed ) + "<sp>row/s,<sp>total<sp>rows<sp>buffered:<sp>" ) + totalRows ) ) ; } java . util . List < org . pentaho . di . core . RowMetaAndData > resultRows = rc . getRowsWritten ( ) ; org . junit . Assert . assertEquals ( ( ( "<sp>:<sp>not<sp>the<sp>expected<sp>amount<sp>of<sp>output<sp>rows." 7 + i ) + "<sp>:<sp>not<sp>the<sp>expected<sp>amount<sp>of<sp>output<sp>rows." ) , 9 , resultRows . size ( ) ) ; rc . clear ( ) ; } }
public class aTest{ @Test public void testFormatList ( eu . trentorise . opendata . jackan . CkanClient ) { java . util . Set < java . lang . String > formats = client . getFormats ( ) ; org . junit . Assert . assertTrue ( ( ( formats . size ( ) ) > 0 ) ) ; } }
public class aTest{ @Test public void testDeferLoadAfterResultHandler ( ) { org . apache . ibatis . session . SqlSession sqlSession = org . apache . ibatis . submitted . deferload_common_property . CommonPropertyDeferLoadError . sqlSessionFactory . openSession ( ) ; try { class MyResultHandler implements org . apache . ibatis . session . ResultHandler { java . util . List < org . apache . ibatis . submitted . deferload_common_property . Child > children = new java . util . ArrayList < org . apache . ibatis . submitted . deferload_common_property . Child > ( ) ; @ org . apache . ibatis . submitted . deferload_common_property . Override public void handleResult ( org . apache . ibatis . session . ResultContext context ) { org . apache . ibatis . submitted . deferload_common_property . Child child = ( ( org . apache . ibatis . submitted . deferload_common_property . Child ) ( context . getResultObject ( ) ) ) ; children . add ( child ) ; } } MyResultHandler myResultHandler = new MyResultHandler ( ) ; sqlSession . select ( "org.apache.ibatis.submitted.deferload_common_property.ChildMapper.selectAll" , myResultHandler ) ; for ( org . apache . ibatis . submitted . deferload_common_property . Child child : myResultHandler . children ) { org . junit . Assert . assertNotNull ( child . getFather ( ) ) ; } } }
public class aTest{ @Test public void test_search_with_moduleName_success ( ) { java . nio . file . Path path = createTempDir ( ) ; org . apache . solr . core . SolrResourceLoader loader = new org . apache . solr . core . SolrResourceLoader ( path ) ; org . apache . solr . core . NodeConfig config = new org . apache . solr . core . NodeConfig . NodeConfigBuilder ( "testnode" , loader ) . setConfigSetBaseDirectory ( java . nio . file . Paths . get ( org . ikasan . wiretap . dao . SolrWiretapDaoTest . TEST_HOME ( ) ) . resolve ( "configsets" ) . toString ( ) ) . build ( ) ; try ( org . apache . solr . client . solrj . embedded . EmbeddedSolrServer server = new org . apache . solr . client . solrj . embedded . EmbeddedSolrServer ( config , "ikasan" ) ) { org . apache . solr . client . solrj . request . CoreAdminRequest . Create createRequest = new org . apache . solr . client . solrj . request . CoreAdminRequest . Create ( ) ; createRequest . setCoreName ( "ikasan" ) ; createRequest . setConfigSet ( "minimal" ) ; server . request ( createRequest ) ; java . util . HashMap < java . lang . String , java . lang . Object > fields = new java . util . HashMap ( ) ; fields . put ( "testnode" 1 , new java . lang . Integer ( 1 ) ) ; org . apache . solr . client . solrj . request . schema . SchemaRequest . AddField schemaRequest = new org . apache . solr . client . solrj . request . schema . SchemaRequest . AddField ( fields ) ; server . request ( schemaRequest ) ; org . ikasan . wiretap . dao . SolrWiretapDao solrCloudBase = new org . ikasan . wiretap . dao . SolrWiretapDao ( ) ; solrCloudBase . setSolrClient ( server ) ; solrCloudBase . setDaysToKeep ( 0 ) ; org . ikasan . wiretap . model . SolrWiretapEvent event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 1L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "event" ) ; solrCloudBase . save ( event ) ; event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 2L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "event" ) ; solrCloudBase . save ( event ) ; event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 3L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "event" ) ; solrCloudBase . save ( event ) ; event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 4L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "event" ) ; solrCloudBase . save ( event ) ; event = new org . ikasan . wiretap . model . SolrWiretapEvent ( 5L , "moduleName" , "flowName" , "componentName" , "eventId" , "relatedEventId" , java . lang . System . currentTimeMillis ( ) , "event" ) ; solrCloudBase . save ( event ) ; java . util . HashSet < java . lang . String > moduleNames = new java . util . HashSet < java . lang . String > ( ) ; moduleNames . add ( "moduleName" ) ; java . util . HashSet < java . lang . String > componentNames = new java . util . HashSet < java . lang . String > ( ) ; org . ikasan . spec . search . PagedSearchResult < org . ikasan . spec . wiretap . WiretapEvent > results = solrCloudBase . findWiretapEvents ( 0 , 10 , "testnode" 2 , true , moduleNames , componentNames , null , null , null , new java . util . Date ( ( ( java . lang . System . currentTimeMillis ( ) ) - 100000000 ) ) , new java . util . Date ( ( ( java . lang . System . currentTimeMillis ( ) ) + 100000000 ) ) , "testnode" 0 ) ; org . junit . Assert . assertEquals ( "testnode" 3 , results . getResultSize ( ) , 10 ) ; server . close ( ) ; } } }
public class aTest{ @Test public void testCallCustomArgCustomReturnCustomizedValidation ( ) { java . lang . String method = "echoObject4" ; final remoting . amfclient . ClientCustomType methodArg = new remoting . amfclient . ClientCustomType ( ) ; methodArg . setId ( 1 ) ; try { internalTestCall ( flex . messaging . io . amf . client . AMFDataTypeIT . getOperationCall ( method ) , methodArg , new flex . messaging . io . amf . client . AMFDataTypeIT . CallResultHandler ( ) { public void onResult ( java . lang . Object result ) { remoting . amfclient . ClientCustomType temp2 = ( ( remoting . amfclient . ClientCustomType ) ( result ) ) ; org . junit . Assert . assertEquals ( 1 , temp2 . getId ( ) ) ; } } }
public class aTest{ @Test public void testSearchBlank ( ) { int total = 5 ; for ( int i = 0 ; i < total ; i ++ ) { com . liferay . data . engine . service . test . DEDataEngineTestUtil . insertDEDataRecordCollection ( _adminUser , _group , ( "Description" + i ) , ( "Name" + i ) , _deDataDefinitionService , _deDataRecordCollectionService ) ; } java . util . List < com . liferay . data . engine . model . DEDataRecordCollection > deDataRecordCollections = searchDEDataRecordCollection ( _group , "" ) ; com . liferay . portal . search . test . util . IdempotentRetryAssert . retryAssert ( 6 , TimeUnit . SECONDS , ( ) -> { org . junit . Assert . assertEquals ( deDataRecordCollections . toString ( ) , 5 , deDataRecordCollections . size ( ) ) ; return null ; } }
public class aTest{ @Test public void test ( ) { org . apache . hadoop . hbase . regionserver . TestShutdownWhileWALBroken . UTIL . createMultiRegionTable ( org . apache . hadoop . hbase . regionserver . TestShutdownWhileWALBroken . TABLE_NAME , org . apache . hadoop . hbase . regionserver . TestShutdownWhileWALBroken . CF ) ; try ( org . apache . hadoop . hbase . client . Table table = org . apache . hadoop . hbase . regionserver . TestShutdownWhileWALBroken . UTIL . getConnection ( ) . getTable ( org . apache . hadoop . hbase . regionserver . TestShutdownWhileWALBroken . TABLE_NAME ) ) { org . apache . hadoop . hbase . regionserver . TestShutdownWhileWALBroken . UTIL . loadTable ( table , org . apache . hadoop . hbase . regionserver . TestShutdownWhileWALBroken . CF ) ; } int numRegions = org . apache . hadoop . hbase . regionserver . TestShutdownWhileWALBroken . UTIL . getMiniHBaseCluster ( ) . getRegions ( org . apache . hadoop . hbase . regionserver . TestShutdownWhileWALBroken . TABLE_NAME ) . size ( ) ; org . apache . hadoop . hbase . util . JVMClusterUtil . RegionServerThread rst0 = org . apache . hadoop . hbase . regionserver . TestShutdownWhileWALBroken . UTIL . getMiniHBaseCluster ( ) . getRegionServerThreads ( ) . get ( 0 ) ; org . apache . hadoop . hbase . util . JVMClusterUtil . RegionServerThread rst1 = org . apache . hadoop . hbase . regionserver . TestShutdownWhileWALBroken . UTIL . getMiniHBaseCluster ( ) . getRegionServerThreads ( ) . get ( 1 ) ; org . apache . hadoop . hbase . regionserver . HRegionServer liveRS ; org . apache . hadoop . hbase . util . JVMClusterUtil . RegionServerThread toKillRSThread ; if ( rst1 . getRegionServer ( ) . getRegions ( TableName . META_TABLE_NAME ) . isEmpty ( ) ) { liveRS = rst0 . getRegionServer ( ) ; toKillRSThread = rst1 ; } else { liveRS = rst1 . getRegionServer ( ) ; toKillRSThread = rst0 ; } org . junit . Assert . assertTrue ( ( ( liveRS . getRegions ( org . apache . hadoop . hbase . regionserver . TestShutdownWhileWALBroken . TABLE_NAME ) . size ( ) ) < numRegions ) ) ; org . apache . hadoop . hbase . regionserver . TestShutdownWhileWALBroken . UTIL . expireSession ( toKillRSThread . getRegionServer ( ) . getZooKeeper ( ) , false ) ; org . apache . hadoop . hbase . regionserver . TestShutdownWhileWALBroken . UTIL . waitFor ( 30000 , new org . apache . hadoop . hbase . Waiter . ExplainingPredicate < java . lang . Exception > ( ) { @ org . apache . hadoop . hbase . regionserver . Override public boolean evaluate ( ) throws java . lang . Exception { return ( liveRS . getRegions ( org . apache . hadoop . hbase . regionserver . TestShutdownWhileWALBroken . TABLE_NAME ) . size ( ) ) == numRegions ; } }
public class aTest{ @Test public void changeHeader ( ) { final org . apache . hc . core5 . testing . framework . ClientTestingAdapter adapter = new org . apache . hc . core5 . testing . framework . ClassicTestClientTestingAdapter ( ) { @ org . apache . hc . core5 . testing . framework . Override public java . util . Map < java . lang . String , java . lang . Object > execute ( final java . lang . String defaultURI , final java . util . Map < java . lang . String , java . lang . Object > request , final org . apache . hc . core5 . testing . framework . TestingFrameworkRequestHandler requestHandler , final java . util . Map < java . lang . String , java . lang . Object > responseExpectations ) throws org . apache . hc . core5 . testing . framework . TestingFrameworkException { @ org . apache . hc . core5 . testing . framework . SuppressWarnings ( "unchecked" ) final java . util . Map < java . lang . String , java . lang . String > headers = ( ( java . util . Map < java . lang . String , java . lang . String > ) ( request . get ( org . apache . hc . core5 . testing . framework . ClientPOJOAdapter . HEADERS ) ) ) ; org . junit . Assert . assertTrue ( headers . containsKey ( "header1" ) ) ; headers . put ( "header1" , ( ( headers . get ( "header1" ) ) + "junk" ) ) ; return super . execute ( defaultURI , request , requestHandler , responseExpectations ) ; } } }
public class aTest{ @Test public void testGetWifiConfigFullInfra ( ) { org . eclipse . kura . core . net . NetworkConfiguration config = new org . eclipse . kura . core . net . NetworkConfiguration ( ) ; java . lang . String netIfConfigPrefix = "GROUP_CCMP" 1 ; java . util . HashMap < java . lang . String , java . lang . Object > properties = new java . util . HashMap ( ) ; properties . put ( "GROUP_CCMP" 9 , "ssid" ) ; properties . put ( "prefix.wifi.infra.driver" , "prefix.wifi.infra.driver" 4 ) ; properties . put ( "prefix.wifi.infra.securityType" , "GROUP_CCMP" ) ; properties . put ( "prefix.wifi.infra.channel" , "1<sp>2<sp>3" ) ; properties . put ( "GROUP_CCMP" 7 , new org . eclipse . kura . configuration . Password ( "GROUP_CCMP" 6 ) ) ; properties . put ( "prefix.wifi.infra.driver" 1 , "GROUP_CCMP" 4 ) ; properties . put ( "prefix.wifi.infra.broadcast" , ( ( java . lang . Boolean ) ( true ) ) ) ; properties . put ( "GROUP_CCMP" 5 , "RADIO_MODE_80211a" ) ; properties . put ( "prefix.wifi.infra.driver" 2 , "learn:1:2:3" ) ; properties . put ( "GROUP_CCMP" 2 , "GROUP_CCMP" 3 ) ; properties . put ( "GROUP_CCMP" 0 , "GROUP_CCMP" 8 ) ; properties . put ( "prefix.wifi.infra.driver" 3 , ( ( java . lang . Boolean ) ( true ) ) ) ; properties . put ( "prefix.wifi.infra.ignoreSSID" , ( ( java . lang . Boolean ) ( true ) ) ) ; org . eclipse . kura . net . wifi . WifiConfig expected = new org . eclipse . kura . net . wifi . WifiConfig ( ) ; expected . setMode ( WifiMode . INFRA ) ; expected . setChannels ( new int [ ] { 1 , 2 , 3 } ) ; expected . setSSID ( "ssid" ) ; expected . setDriver ( "prefix.wifi.infra.driver" 4 ) ; expected . setSecurity ( WifiSecurity . GROUP_CCMP ) ; expected . setPasskey ( "GROUP_CCMP" 6 ) ; expected . setHardwareMode ( "GROUP_CCMP" 4 ) ; expected . setBroadcast ( true ) ; expected . setRadioMode ( WifiRadioMode . RADIO_MODE_80211a ) ; expected . setBgscan ( new org . eclipse . kura . net . wifi . WifiBgscan ( org . eclipse . kura . net . wifi . WifiBgscanModule . LEARN , 1 , 2 , 3 ) ) ; expected . setPairwiseCiphers ( WifiCiphers . CCMP ) ; expected . setGroupCiphers ( WifiCiphers . TKIP ) ; expected . setPingAccessPoint ( true ) ; expected . setIgnoreSSID ( true ) ; org . eclipse . kura . net . wifi . WifiConfig wifiConfig = ( ( org . eclipse . kura . net . wifi . WifiConfig ) ( org . eclipse . kura . core . testutil . TestUtil . invokePrivate ( config , "prefix.wifi.infra.driver" 0 , netIfConfigPrefix , WifiMode . INFRA , properties ) ) ) ; org . junit . Assert . assertEquals ( expected , wifiConfig ) ; } }
public class aTest{ @Test public void testUpload ( ) { put ( ( ( org . geoserver . rest . RestBaseController . ROOT_PATH ) + "/resource/mydir/mynewres" ) , STR_MY_NEW_TEST ) ; org . geoserver . platform . resource . Resource newRes = getDataDirectory ( ) . get ( "/mydir/mynewres" ) ; try ( java . io . InputStream is = newRes . in ( ) ) { org . junit . Assert . assertEquals ( STR_MY_NEW_TEST , org . geoserver . rest . util . IOUtils . toString ( is ) ) ; } }
public class aTest{ @Test public void testBuildWithParametersWithOrderBy ( ) { org . lnu . is . domain . department . Department parent = new org . lnu . is . domain . department . Department ( ) ; org . lnu . is . domain . department . type . DepartmentType departmentType = new org . lnu . is . domain . department . type . DepartmentType ( ) ; org . lnu . is . domain . order . Order order = new org . lnu . is . domain . order . Order ( ) ; java . lang . String name = "name" ; java . lang . String abbrName = "abbr<sp>name" ; java . lang . String manager = "abbr<sp>name" 0 ; java . util . Date begDate = new java . util . Date ( ) ; java . util . Date endDate = new java . util . Date ( ) ; java . lang . String indetifir = "s" ; org . lnu . is . domain . department . Department context = new org . lnu . is . domain . department . Department ( ) ; context . setName ( name ) ; context . setParent ( parent ) ; context . setDepartmentType ( departmentType ) ; context . setOrder ( order ) ; context . setAbbrName ( abbrName ) ; context . setName ( name ) ; context . setManager ( manager ) ; context . setBegDate ( begDate ) ; context . setEndDate ( endDate ) ; context . setIdentifir ( indetifir ) ; org . lnu . is . pagination . OrderBy orderBy1 = new org . lnu . is . pagination . OrderBy ( "parent" , org . lnu . is . pagination . OrderByType . ASC ) ; org . lnu . is . pagination . OrderBy orderBy2 = new org . lnu . is . pagination . OrderBy ( "departmentType" , org . lnu . is . pagination . OrderByType . DESC ) ; org . lnu . is . pagination . OrderBy orderBy3 = new org . lnu . is . pagination . OrderBy ( "order" , org . lnu . is . pagination . OrderByType . ASC ) ; org . lnu . is . pagination . OrderBy orderBy4 = new org . lnu . is . pagination . OrderBy ( "abbrName" , org . lnu . is . pagination . OrderByType . DESC ) ; org . lnu . is . pagination . OrderBy orderBy5 = new org . lnu . is . pagination . OrderBy ( "name" , org . lnu . is . pagination . OrderByType . ASC ) ; org . lnu . is . pagination . OrderBy orderBy6 = new org . lnu . is . pagination . OrderBy ( "abbr<sp>name" 0 , org . lnu . is . pagination . OrderByType . DESC ) ; org . lnu . is . pagination . OrderBy orderBy7 = new org . lnu . is . pagination . OrderBy ( "begDate" , org . lnu . is . pagination . OrderByType . ASC ) ; org . lnu . is . pagination . OrderBy orderBy8 = new org . lnu . is . pagination . OrderBy ( "abbr<sp>name" 1 , org . lnu . is . pagination . OrderByType . DESC ) ; org . lnu . is . pagination . OrderBy orderBy9 = new org . lnu . is . pagination . OrderBy ( "identifir" , org . lnu . is . pagination . OrderByType . ASC ) ; java . util . List < org . lnu . is . pagination . OrderBy > orders = java . util . Arrays . asList ( orderBy1 , orderBy2 , orderBy3 , orderBy4 , orderBy5 , orderBy6 , orderBy7 , orderBy8 , orderBy9 ) ; java . lang . String expectedQuery = "SELECT<sp>e<sp>FROM<sp>Department<sp>e<sp>WHERE<sp>(<sp>e.parent<sp>=<sp>:parent<sp>AND<sp>e.departmentType<sp>=<sp>:departmentType<sp>AND<sp>e.order<sp>=<sp>:order<sp>AND<sp>e.abbrName<sp>LIKE<sp>CONCAT('%',:abbrName,'%')<sp>AND<sp>e.name<sp>LIKE<sp>CONCAT('%',:name,'%')<sp>AND<sp>e.manager<sp>LIKE<sp>CONCAT('%',:manager,'%')<sp>AND<sp>e.identifir<sp>LIKE<sp>CONCAT('%',:identifir,'%')<sp>AND<sp>e.begDate<sp><=<sp>:begDate<sp>AND<sp>e.endDate<sp>>=<sp>:endDate)<sp>AND<sp>e.status=:status<sp>AND<sp>e.crtUserGroup<sp>IN<sp>(:userGroups)<sp>ORDER<sp>BY<sp>e.parent<sp>ASC,<sp>e.departmentType<sp>DESC,<sp>e.order<sp>ASC,<sp>e.abbrName<sp>DESC,<sp>e.name<sp>ASC,<sp>e.manager<sp>DESC,<sp>e.begDate<sp>ASC,<sp>e.endDate<sp>DESC,<sp>e.identifir<sp>ASC" ; org . lnu . is . pagination . MultiplePagedSearch < org . lnu . is . domain . department . Department > pagedSearch = new org . lnu . is . pagination . MultiplePagedSearch ( ) ; pagedSearch . setEntity ( context ) ; pagedSearch . setOrders ( orders ) ; java . lang . String actualQuery = unit . build ( pagedSearch ) ; org . junit . Assert . assertEquals ( expectedQuery , actualQuery ) ; } }
public class aTest{ @Test public void cantSearch ( ) { dummyLogin . login ( "aUser" ) ; try ( org . nuxeo . ecm . directory . Session userDirSession = getSession ( DirectoryFeature . USER_DIRECTORY_NAME ) ) { java . util . Map < java . lang . String , java . io . Serializable > map = new java . util . HashMap ( ) ; map . put ( "username" , "user_3" ) ; org . nuxeo . ecm . core . api . DocumentModelList results = userDirSession . query ( map ) ; org . junit . Assert . assertEquals ( 0 , results . size ( ) ) ; } }
public class aTest{ @Test public void testSumRowsUnboundedPrecedingUnboundedFollowing ( ) { java . lang . String sqlText = java . lang . String . format ( ( "<sp>100<sp>|<sp>3<sp>|<sp>55000<sp>|293000<sp>|\n" 6 + ( "<sp>100<sp>|<sp>3<sp>|<sp>55000<sp>|293000<sp>|\n" 2 + "<sp>100<sp>|<sp>3<sp>|<sp>55000<sp>|293000<sp>|\n" 3 ) ) , this . getTableReference ( com . splicemachine . derby . impl . sql . execute . operations . WindowFunctionIT . EMPTAB ) , useSpark ) ; java . sql . ResultSet rs = com . splicemachine . derby . impl . sql . execute . operations . WindowFunctionIT . methodWatcher . executeQuery ( sqlText ) ; java . lang . String expected = "<sp>100<sp>|<sp>3<sp>|<sp>55000<sp>|293000<sp>|\n" 1 + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "------------------------------\n" + "<sp>10<sp>|<sp>1<sp>|<sp>50000<sp>|436000<sp>|\n" ) + "<sp>20<sp>|<sp>1<sp>|<sp>75000<sp>|436000<sp>|\n" ) + "<sp>100<sp>|<sp>3<sp>|<sp>55000<sp>|293000<sp>|\n" 9 ) + "<sp>55<sp>|<sp>1<sp>|<sp>52000<sp>|436000<sp>|\n" ) + "<sp>100<sp>|<sp>3<sp>|<sp>55000<sp>|293000<sp>|\n" 5 ) + "<sp>70<sp>|<sp>1<sp>|<sp>76000<sp>|436000<sp>|\n" ) + "<sp>100<sp>|<sp>3<sp>|<sp>55000<sp>|293000<sp>|\n" 4 ) + "<sp>10<sp>|<sp>1<sp>|<sp>50000<sp>|436000<sp>|\n" 0 ) + "<sp>44<sp>|<sp>2<sp>|<sp>52000<sp>|208000<sp>|\n" ) + "<sp>100<sp>|<sp>3<sp>|<sp>55000<sp>|293000<sp>|\n" 0 ) + "<sp>100<sp>|<sp>3<sp>|<sp>55000<sp>|293000<sp>|\n" 7 ) + "<sp>30<sp>|<sp>3<sp>|<sp>84000<sp>|293000<sp>|\n" ) + "<sp>80<sp>|<sp>3<sp>|<sp>79000<sp>|293000<sp>|\n" ) + "<sp>100<sp>|<sp>3<sp>|<sp>55000<sp>|293000<sp>|\n" ) + "<sp>120<sp>|<sp>3<sp>|<sp>75000<sp>|293000<sp>|" ) ; org . junit . Assert . assertEquals ( ( ( "<sp>100<sp>|<sp>3<sp>|<sp>55000<sp>|293000<sp>|\n" 8 + sqlText ) + "<sp>100<sp>|<sp>3<sp>|<sp>55000<sp>|293000<sp>|\n" 8 ) , expected , TestUtils . FormattedResult . ResultFactory . toStringUnsorted ( rs ) ) ; rs . close ( ) ; } }
public class aTest{ @Test public void changeConfiguration ( ) { alluxio . conf . InstancedConfiguration conf = alluxio . ConfigurationTestUtils . defaults ( ) ; org . junit . runners . model . Statement statement = new org . junit . runners . model . Statement ( ) { @ alluxio . Override public void evaluate ( ) throws java . lang . Throwable { org . junit . Assert . assertEquals ( "testValue" , conf . get ( PropertyKey . MASTER_HOSTNAME ) ) ; } } }
public class aTest{ @Test public void toTimestampDefaultWrongPattern ( ) { java . lang . String source = "2014-06-24<sp>12:13:14.123" ; java . text . DateFormat formatter = new java . text . SimpleDateFormat ( "MM/dd/yy<sp>HH:mm:ss.SSS" ) ; java . sql . Timestamp date = new java . sql . Timestamp ( formatter . parse ( "06/24/2014<sp>12:13:14.123" ) . getTime ( ) ) ; try { org . junit . Assert . assertEquals ( date , com . splicemachine . derby . utils . SpliceDateFunctions . TO_TIMESTAMP ( source ) ) ; } }
public class aTest{ @Test public void testGetSetDefault ( ) { try { org . apache . commons . lang3 . builder . ToStringBuilder . setDefaultStyle ( org . apache . commons . lang3 . builder . ToStringStyle . NO_FIELD_NAMES_STYLE ) ; org . junit . Assert . assertSame ( org . apache . commons . lang3 . builder . ToStringStyle . NO_FIELD_NAMES_STYLE , org . apache . commons . lang3 . builder . ToStringBuilder . getDefaultStyle ( ) ) ; } }
public class aTest{ @Test public void testLexerNotSet ( ) { org . antlr . v4 . tool . LexerGrammar lg = new org . antlr . v4 . tool . LexerGrammar ( ( "2:RULE_STOP<sp>0\n" 2 + "2:RULE_STOP<sp>0\n" 0 ) ) ; java . lang . String expecting = "max<sp>type<sp>1\n" + ( ( ( ( ( ( ( ( ( ( ( ( "2:RULE_STOP<sp>0\n" 5 + "1:RULE_START<sp>0\n" ) + "2:RULE_STOP<sp>0\n" ) + "3:BASIC<sp>0\n" ) + "4:BASIC<sp>0\n" ) + "rule<sp>0:1<sp>1\n" ) + "mode<sp>0:0\n" ) + "2:RULE_STOP<sp>0\n" 3 ) + "0->1<sp>EPSILON<sp>0,0,0\n" ) + "1->3<sp>EPSILON<sp>0,0,0\n" ) + "2:RULE_STOP<sp>0\n" 1 ) + "2:RULE_STOP<sp>0\n" 4 ) + "0:0\n" ) ; org . antlr . v4 . runtime . atn . ATN atn = createATN ( lg , true ) ; java . lang . String result = org . antlr . v4 . runtime . atn . ATNSerializer . getDecoded ( atn , java . util . Arrays . asList ( lg . getTokenNames ( ) ) ) ; org . junit . Assert . assertEquals ( expecting , result ) ; } }
public class aTest{ @Test public void shouldCreateAUser_Successfully ( ) { java . lang . String username = randomUuid ( ) ; com . google . common . collect . ImmutableMap < java . lang . Object , java . lang . Object > userPayload = com . google . common . collect . ImmutableMap . builder ( ) . put ( "admin" 0 , username ) . put ( "admin" 8 , ( ( "user-" + username ) + "admin" 6 ) ) . put ( "telephone_number" , "admin" 5 ) . put ( "otp_key" , "34f34" ) . put ( "role_name" , "admin" ) . build ( ) ; io . restassured . response . ValidatableResponse response = givenSetup ( ) . when ( ) . body ( mapper . writeValueAsString ( userPayload ) ) . contentType ( io . restassured . http . ContentType . JSON ) . accept ( io . restassured . http . ContentType . JSON ) . post ( uk . gov . pay . adminusers . resources . USERS_RESOURCE_URL ) . then ( ) ; java . lang . String externalId = response . extract ( ) . path ( "admin" 1 ) ; response . statusCode ( 201 ) . body ( "admin" 3 , org . hamcrest . Matchers . nullValue ( ) ) . body ( "admin" 1 , org . hamcrest . core . Is . is ( externalId ) ) . body ( "admin" 0 , org . hamcrest . core . Is . is ( username ) ) . body ( "admin" 4 , org . hamcrest . Matchers . nullValue ( ) ) . body ( "admin" 8 , org . hamcrest . core . Is . is ( ( ( "user-" + username ) + "admin" 6 ) ) ) . body ( "service_roles" , org . hamcrest . Matchers . hasSize ( 0 ) ) . body ( "telephone_number" , org . hamcrest . core . Is . is ( "admin" 5 ) ) . body ( "otp_key" , org . hamcrest . core . Is . is ( "34f34" ) ) . body ( "admin" 2 , org . hamcrest . core . Is . is ( 0 ) ) . body ( "disabled" , org . hamcrest . core . Is . is ( false ) ) ; response . body ( "admin" 9 , org . hamcrest . Matchers . hasSize ( 1 ) ) . body ( "disabled" 0 , org . hamcrest . core . Is . is ( ( "admin" 7 + externalId ) ) ) . body ( "disabled" 1 , org . hamcrest . core . Is . is ( "GET" ) ) . body ( "_links[0].rel" , org . hamcrest . core . Is . is ( "disabled" 2 ) ) ; java . util . List < java . util . Map < java . lang . String , java . lang . Object > > userByExternalId = databaseHelper . findUserByExternalId ( externalId ) ; java . util . List < java . util . Map < java . lang . String , java . lang . Object > > servicesAssociatedToUser = databaseHelper . findUserServicesByUserId ( ( ( java . lang . Integer ) ( userByExternalId . get ( 0 ) . get ( "admin" 3 ) ) ) ) ; org . junit . Assert . assertThat ( servicesAssociatedToUser . size ( ) , org . hamcrest . core . Is . is ( 0 ) ) ; } }
public class aTest{ @Test public void When_removing_component_mapper_Should_update_get_before_processing ( ) { execute ( new com . artemis . EntityComponentLifecycleIntegrationTest . LifecycleTestingSystem ( ) { private int entityId ; @ com . artemis . Override protected void processSystem ( ) { if ( ( timesProcessed ) == 0 ) { entityId = world . create ( ) ; mLife . create ( entityId ) ; } if ( ( timesProcessed ) == 1 ) { mLife . remove ( entityId ) ; org . junit . Assert . assertNull ( mLife . get ( entityId ) ) ; done = true ; } } } }
public class aTest{ @Test public void testActionLogRecordCRUD ( ) { java . lang . String pageCode1 = "draft_page_100" ; java . lang . String pageCode2 = "draft_page_100" 1 ; try { com . agiletec . aps . system . services . user . UserDetails user = new org . entando . entando . web . utils . OAuth2TestUtils . UserBuilder ( "jack_bauer" , "0x24" ) . grantedToRoleAdmin ( ) . build ( ) ; java . lang . String accessToken = mockOAuthInterceptor ( user ) ; java . lang . Integer startSize = this . extractCurrentSize ( accessToken ) ; this . initTestObjects ( accessToken , pageCode1 , pageCode2 ) ; java . lang . Integer secondSize = this . extractCurrentSize ( accessToken ) ; org . junit . Assert . assertEquals ( 2 , ( secondSize - startSize ) ) ; int recordId = this . actionLogManager . getActionRecords ( null ) . stream ( ) . findFirst ( ) . get ( ) ; org . springframework . test . web . servlet . ResultActions result = mockMvc . perform ( post ( "/activityStream/{recordId}/like" , recordId ) . header ( "Authorization" , ( "Bearer<sp>" + accessToken ) ) ) ; result . andExpect ( status ( ) . isOk ( ) ) ; result . andExpect ( jsonPath ( "$.payload.likes.size()" , org . hamcrest . CoreMatchers . is ( 1 ) ) ) ; result = mockMvc . perform ( delete ( "/activityStream/{recordId}/like" , recordId ) . header ( "Authorization" , ( "Bearer<sp>" + accessToken ) ) ) ; result . andExpect ( status ( ) . isOk ( ) ) ; result . andExpect ( jsonPath ( "$.payload.likes.size()" , org . hamcrest . CoreMatchers . is ( 0 ) ) ) ; java . lang . String comment = "draft_page_100" 0 ; org . entando . entando . web . activitystream . ActivityStreamCommentRequest req = new org . entando . entando . web . activitystream . ActivityStreamCommentRequest ( ) ; req . setComment ( comment ) ; req . setRecordId ( recordId ) ; result = mockMvc . perform ( post ( "/activityStream/{recordId}/comments" , recordId ) . contentType ( MediaType . APPLICATION_JSON ) . content ( mapper . writeValueAsString ( req ) ) . header ( "Authorization" , ( "Bearer<sp>" + accessToken ) ) ) ; result . andExpect ( status ( ) . isOk ( ) ) ; result . andExpect ( jsonPath ( "$.payload.comments.size()" , org . hamcrest . CoreMatchers . is ( 1 ) ) ) ; result = mockMvc . perform ( get ( "/activityStream" ) . contentType ( MediaType . APPLICATION_JSON ) . content ( mapper . writeValueAsString ( req ) ) . header ( "Authorization" , ( "Bearer<sp>" + accessToken ) ) ) ; result . andExpect ( status ( ) . isOk ( ) ) ; result = mockMvc . perform ( delete ( "/activityStream/{recordId}/like" , recordId ) . header ( "Authorization" , ( "Bearer<sp>" + accessToken ) ) ) ; result . andExpect ( status ( ) . isOk ( ) ) ; result . andExpect ( jsonPath ( "$.payload.comments.size()" , org . hamcrest . CoreMatchers . is ( 1 ) ) ) ; req = new org . entando . entando . web . activitystream . ActivityStreamCommentRequest ( ) ; req . setComment ( comment ) ; req . setRecordId ( 0 ) ; result = mockMvc . perform ( post ( "/activityStream/{recordId}/comments" , recordId ) . contentType ( MediaType . APPLICATION_JSON ) . content ( mapper . writeValueAsString ( req ) ) . header ( "Authorization" , ( "Bearer<sp>" + accessToken ) ) ) ; result . andExpect ( status ( ) . isBadRequest ( ) ) ; req = new org . entando . entando . web . activitystream . ActivityStreamCommentRequest ( ) ; req . setRecordId ( recordId ) ; result = mockMvc . perform ( post ( "/activityStream/{recordId}/comments" , recordId ) . contentType ( MediaType . APPLICATION_JSON ) . content ( mapper . writeValueAsString ( req ) ) . header ( "Authorization" , ( "Bearer<sp>" + accessToken ) ) ) ; result . andExpect ( status ( ) . isBadRequest ( ) ) ; } }
public class aTest{ @Test public void testNestedRenderingTransform ( ) { java . lang . StringBuilder builder = new java . lang . StringBuilder ( ) ; builder . append ( "transform:" ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>params:" 2 ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>params:" ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>data:" ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>params:" 8 ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>params:" 9 ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>params:" ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>params:" 0 ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>queryBuffer:<sp>40" 2 ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>params:" 5 ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>params:" 1 ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>params:" 3 ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>minObservations:<sp>2" ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>maxObservationDistance:<sp>15" ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>pixelsPerCell:<sp>8" ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>queryBuffer:<sp>40" ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>params:" 4 ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>queryBuffer:<sp>40" 1 ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>queryBuffer:<sp>40" 3 ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>params:" 6 ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>-<sp>5" ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>queryBuffer:<sp>40" 0 ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "<sp>params:" 7 ) . append ( "<sp>queryBuffer:<sp>40" 4 ) . append ( "" ) ; java . util . List < org . yaml . snakeyaml . error . MarkedYAMLException > errors = validate ( builder . toString ( ) , Collections . EMPTY_LIST ) ; org . junit . Assert . assertThat ( errors , org . hamcrest . Matchers . empty ( ) ) ; } }
public class aTest{ @Test public void testSearchCountWithoutPermission ( ) { com . liferay . data . engine . service . test . DEDataEngineTestUtil . insertDEDataDefinition ( _adminUser , _group , "description1" , "name<sp>1" , _deDataDefinitionService ) ; int total = searchCountDEDataDefinitions ( com . liferay . portal . kernel . test . util . GroupTestUtil . addGroup ( ) , "name" ) ; com . liferay . portal . search . test . util . IdempotentRetryAssert . retryAssert ( 3 , TimeUnit . SECONDS , ( ) -> { org . junit . Assert . assertEquals ( 0 , total ) ; return null ; } }
public class aTest{ @Test public void testRTEDuringConnectionSetup ( ) { javax . net . SocketFactory spyFactory = spy ( org . apache . hadoop . net . NetUtils . getDefaultSocketFactory ( org . apache . hadoop . ipc . TestIPC . conf ) ) ; org . mockito . Mockito . doAnswer ( new org . mockito . stubbing . Answer < java . net . Socket > ( ) { @ org . apache . hadoop . ipc . Override public java . net . Socket answer ( org . mockito . invocation . InvocationOnMock invocation ) throws java . lang . Throwable { java . net . Socket s = spy ( ( ( java . net . Socket ) ( invocation . callRealMethod ( ) ) ) ) ; doThrow ( new java . lang . RuntimeException ( "Injected<sp>fault" ) ) . when ( s ) . setSoTimeout ( anyInt ( ) ) ; return s ; } } ) . when ( spyFactory ) . createSocket ( ) ; org . apache . hadoop . ipc . Server server = new org . apache . hadoop . ipc . TestIPC . TestServer ( 1 , true ) ; org . apache . hadoop . ipc . Client client = new org . apache . hadoop . ipc . Client ( org . apache . hadoop . io . LongWritable . class , org . apache . hadoop . ipc . TestIPC . conf , spyFactory ) ; server . start ( ) ; try { java . net . InetSocketAddress address = org . apache . hadoop . net . NetUtils . getConnectAddress ( server ) ; try { org . apache . hadoop . ipc . TestIPC . call ( client , org . apache . hadoop . ipc . TestIPC . RANDOM . nextLong ( ) , address , org . apache . hadoop . ipc . TestIPC . conf ) ; org . junit . Assert . fail ( "Expected<sp>an<sp>exception<sp>to<sp>have<sp>been<sp>thrown" ) ; } catch ( java . lang . Exception e ) { org . apache . hadoop . ipc . TestIPC . LOG . info ( "caught<sp>expected<sp>exception" , e ) ; org . junit . Assert . assertTrue ( org . apache . hadoop . util . StringUtils . stringifyException ( e ) . contains ( "Injected<sp>fault" ) ) ; } }
public class aTest{ @Test public void testFilterFullClassName ( ) { java . lang . ClassLoader cl = new org . eclipse . concierge . test . util . FilteredClassLoader ( this . getClass ( ) . getClassLoader ( ) , "org.w3c.dom.Document" ) ; java . lang . Class < ? > clsFound = java . lang . Class . forName ( "java.lang.Object" , false , cl ) ; org . junit . Assert . assertNotNull ( clsFound ) ; try { java . lang . Class . forName ( "org.w3c.dom.Document" , false , cl ) ; org . junit . Assert . fail ( "Uups,<sp>ClassNotFoundException<sp>expected" ) ; } }
public class aTest{ @Test public void testDoUpdateForBlueprintExport_SingleHostProperty__ExternalReference ( ) { java . util . Map < java . lang . String , java . util . Map < java . lang . String , java . lang . String > > properties = new java . util . HashMap ( ) ; java . util . Map < java . lang . String , java . lang . String > typeProps = new java . util . HashMap ( ) ; typeProps . put ( "yarn.resourcemanager.hostname" , "external-host" ) ; properties . put ( "yarn-site" , typeProps ) ; org . apache . ambari . server . topology . Configuration clusterConfig = new org . apache . ambari . server . topology . Configuration ( properties , java . util . Collections . emptyMap ( ) ) ; java . util . Collection < java . lang . String > hgComponents = new java . util . HashSet ( ) ; hgComponents . add ( "NAMENODE" ) ; hgComponents . add ( "SECONDARY_NAMENODE" ) ; hgComponents . add ( "DATANODE" 0 ) ; org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup group1 = new org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup ( "group1" , hgComponents , java . util . Collections . singleton ( "DATANODE" 1 ) ) ; java . util . Collection < java . lang . String > hgComponents2 = new java . util . HashSet ( ) ; hgComponents2 . add ( "DATANODE" ) ; hgComponents2 . add ( "HDFS_CLIENT" ) ; org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup group2 = new org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup ( "group2" , hgComponents2 , java . util . Collections . singleton ( "testhost2" ) ) ; java . util . Collection < org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessorTest . TestHostGroup > hostGroups = new java . util . HashSet ( ) ; hostGroups . add ( group1 ) ; hostGroups . add ( group2 ) ; org . apache . ambari . server . topology . ClusterTopology topology = createClusterTopology ( bp , clusterConfig , hostGroups ) ; org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessor configProcessor = new org . apache . ambari . server . controller . internal . BlueprintConfigurationProcessor ( topology ) ; configProcessor . doUpdateForBlueprintExport ( BlueprintExportType . FULL ) ; org . junit . Assert . assertFalse ( properties . get ( "yarn-site" ) . containsKey ( "yarn.resourcemanager.hostname" ) ) ; } }
public class aTest{ @Test public void testPoolSize5 ( ) { java . lang . String host = "192.168.1.107" ; int port = 6379 ; int connectionPoolSize = 5 ; int maxConnectionIdleTimeInMills = 5000 ; final org . cyy . fw . nedis . NedisClient client = new org . cyy . fw . nedis . NedisClientBuilder ( ) . setServerHost ( host ) . setPort ( port ) . setConnectTimeoutMills ( 5000 ) . setConnectionPoolSize ( connectionPoolSize ) . setMaxConnectionIdleTimeInMills ( maxConnectionIdleTimeInMills ) . build ( ) ; try { final java . util . concurrent . CountDownLatch latch = new java . util . concurrent . CountDownLatch ( 1 ) ; client . get ( null , "key1" ) ; client . get ( null , "key2" ) ; client . get ( null , "key3" ) ; client . get ( null , "key4" ) ; client . get ( null , "key5" ) ; client . get ( new org . cyy . fw . nedis . ResponseCallback < java . lang . String > ( ) { @ org . cyy . fw . nedis . test . connection . Override public void failed ( java . lang . Throwable cause ) { } @ org . cyy . fw . nedis . test . connection . Override public void done ( java . lang . String result ) { latch . countDown ( ) ; } } , "key6" ) ; latch . await ( ) ; java . lang . Thread . sleep ( ( maxConnectionIdleTimeInMills + 1000 ) ) ; org . junit . Assert . assertEquals ( 0 , client . getIdleConnections ( ) ) ; } }
public class aTest{ @Test public void testGetParametersWithDisabledDefaults ( ) { unit . setActive ( false ) ; unit . setSecurity ( false ) ; java . lang . Long orderTypeId = 1L ; org . lnu . is . domain . order . type . OrderType orderType = new org . lnu . is . domain . order . type . OrderType ( ) ; orderType . setId ( orderTypeId ) ; java . lang . Long employeeId = 2L ; org . lnu . is . domain . employee . Employee employee = new org . lnu . is . domain . employee . Employee ( ) ; employee . setId ( employeeId ) ; java . lang . Long assetId = 3L ; org . lnu . is . domain . asset . Asset asset = new org . lnu . is . domain . asset . Asset ( ) ; asset . setId ( assetId ) ; java . lang . Long partnerId = 4L ; org . lnu . is . domain . partner . Partner partner = new org . lnu . is . domain . partner . Partner ( ) ; partner . setId ( partnerId ) ; java . lang . Long operationTypeId = 5L ; org . lnu . is . domain . optype . OperationType opType = new org . lnu . is . domain . optype . OperationType ( ) ; opType . setId ( operationTypeId ) ; java . lang . Long reasonId = 6L ; org . lnu . is . domain . reason . Reason reason = new org . lnu . is . domain . reason . Reason ( ) ; reason . setId ( reasonId ) ; java . lang . Long parentId = 7L ; org . lnu . is . domain . order . Order parent = new org . lnu . is . domain . order . Order ( ) ; parent . setId ( parentId ) ; java . lang . String reasonText = "reason<sp>text" ; java . lang . String docSeries = "dco<sp>num" 5 ; java . lang . String docNum = "dco<sp>num" ; java . util . Date docDate = new java . util . Date ( ) ; java . util . Date evDate = new java . util . Date ( ) ; java . lang . String docIssued = "doc<sp>issued" ; org . lnu . is . domain . order . Order entity = new org . lnu . is . domain . order . Order ( ) ; entity . setOrderType ( orderType ) ; entity . setEmployee ( employee ) ; entity . setAsset ( asset ) ; entity . setPartner ( partner ) ; entity . setOpType ( opType ) ; entity . setReason ( reason ) ; entity . setParent ( parent ) ; entity . setReasonText ( reasonText ) ; entity . setDocSeries ( docSeries ) ; entity . setDocNum ( docNum ) ; entity . setDocDate ( docDate ) ; entity . setDocIssued ( docIssued ) ; entity . setEvDate ( evDate ) ; java . util . Map < java . lang . String , java . lang . Object > expected = new java . util . HashMap < java . lang . String , java . lang . Object > ( ) ; expected . put ( "orderType" , orderType ) ; expected . put ( "employee" , employee ) ; expected . put ( "asset" , asset ) ; expected . put ( "dco<sp>num" 6 , partner ) ; expected . put ( "dco<sp>num" 0 , opType ) ; expected . put ( "reason" , reason ) ; expected . put ( "dco<sp>num" 4 , parent ) ; expected . put ( "reasonText" , reasonText ) ; expected . put ( "dco<sp>num" 1 , docSeries ) ; expected . put ( "dco<sp>num" 2 , docNum ) ; expected . put ( "dco<sp>num" 3 , docDate ) ; expected . put ( "docIssued" , docIssued ) ; expected . put ( "evDate" , evDate ) ; when ( orderTypeDao . getEntityById ( anyLong ( ) ) ) . thenReturn ( orderType ) ; when ( employeeDao . getEntityById ( anyLong ( ) ) ) . thenReturn ( employee ) ; when ( assetDao . getEntityById ( anyLong ( ) ) ) . thenReturn ( asset ) ; when ( partnerDao . getEntityById ( anyLong ( ) ) ) . thenReturn ( partner ) ; when ( operationTypeDao . getEntityById ( anyLong ( ) ) ) . thenReturn ( opType ) ; when ( reasonDao . getEntityById ( anyLong ( ) ) ) . thenReturn ( reason ) ; when ( orderDao . getEntityById ( anyLong ( ) ) ) . thenReturn ( parent ) ; java . util . Map < java . lang . String , java . lang . Object > actual = unit . getParameters ( entity ) ; verify ( orderTypeDao ) . getEntityById ( orderTypeId ) ; verify ( employeeDao ) . getEntityById ( employeeId ) ; verify ( assetDao ) . getEntityById ( assetId ) ; verify ( partnerDao ) . getEntityById ( partnerId ) ; verify ( operationTypeDao ) . getEntityById ( operationTypeId ) ; verify ( reasonDao ) . getEntityById ( reasonId ) ; verify ( orderDao ) . getEntityById ( parentId ) ; org . junit . Assert . assertEquals ( expected , actual ) ; } }
public class aTest{ @Test public void testAcquireMultiTasks ( ) { org . apache . hadoop . hbase . regionserver . TestSplitLogWorker . LOG . info ( "testAcquireMultiTasks" ) ; org . apache . hadoop . hbase . SplitLogCounters . resetCounters ( ) ; final java . lang . String TATAS = "tatas" ; final org . apache . hadoop . hbase . ServerName RS = org . apache . hadoop . hbase . ServerName . valueOf ( "rs,1,1" ) ; final int maxTasks = 3 ; org . apache . hadoop . conf . Configuration testConf = org . apache . hadoop . hbase . HBaseConfiguration . create ( org . apache . hadoop . hbase . regionserver . TestSplitLogWorker . TEST_UTIL . getConfiguration ( ) ) ; testConf . setInt ( org . apache . hadoop . hbase . HConstants . HBASE_SPLIT_WAL_MAX_SPLITTER , maxTasks ) ; org . apache . hadoop . hbase . regionserver . RegionServerServices mockedRS = getRegionServer ( RS ) ; for ( int i = 0 ; i < maxTasks ; i ++ ) { zkw . getRecoverableZooKeeper ( ) . create ( org . apache . hadoop . hbase . zookeeper . ZKSplitLog . getEncodedNodeName ( zkw , ( TATAS + i ) ) , new org . apache . hadoop . hbase . SplitLogTask . Unassigned ( org . apache . hadoop . hbase . ServerName . valueOf ( "mgr,1,1" ) ) . toByteArray ( ) , Ids . OPEN_ACL_UNSAFE , CreateMode . PERSISTENT ) ; } org . apache . hadoop . hbase . regionserver . SplitLogWorker slw = new org . apache . hadoop . hbase . regionserver . SplitLogWorker ( ds , testConf , mockedRS , neverEndingTask ) ; slw . start ( ) ; try { waitForCounter ( SplitLogCounters . tot_wkr_task_acquired , 0 , maxTasks , org . apache . hadoop . hbase . regionserver . TestSplitLogWorker . WAIT_TIME ) ; for ( int i = 0 ; i < maxTasks ; i ++ ) { byte [ ] bytes = org . apache . hadoop . hbase . zookeeper . ZKUtil . getData ( zkw , org . apache . hadoop . hbase . zookeeper . ZKSplitLog . getEncodedNodeName ( zkw , ( TATAS + i ) ) ) ; org . apache . hadoop . hbase . SplitLogTask slt = org . apache . hadoop . hbase . SplitLogTask . parseFrom ( bytes ) ; org . junit . Assert . assertTrue ( slt . isOwned ( RS ) ) ; } } }
public class aTest{ @Test public void testLoadFilterOffsetLimit ( ) { final com . orangesignal . csv . CsvConfig cfg = new com . orangesignal . csv . CsvConfig ( ',' , '"' , '\\' ) ; cfg . setNullString ( "NULL" ) ; cfg . setBreakString ( "\n" ) ; cfg . setIgnoreTrailingWhitespaces ( true ) ; cfg . setIgnoreLeadingWhitespaces ( true ) ; cfg . setIgnoreEmptyLines ( true ) ; cfg . setIgnoreLinePatterns ( java . util . regex . Pattern . compile ( "^#.*$" ) ) ; final com . orangesignal . csv . CsvReader reader = new com . orangesignal . csv . CsvReader ( new java . io . StringReader ( "aaa,bbb,ccc<sp>\r\n<sp>ddd,eee,fff<sp>\r\n<sp>ggg,hhh,iii<sp>\r\n<sp>zzz,yyy,NULL" ) , cfg ) ; try { final java . util . List < java . lang . String [ ] > list = new com . orangesignal . csv . handlers . StringArrayListHandler ( ) . filter ( new com . orangesignal . csv . filters . SimpleCsvValueFilter ( new com . orangesignal . csv . filters . CsvValueOrExpression ( ) ) . eq ( 0 , "ddd" ) . eq ( 1 , "yyy" ) ) . offset ( 1 ) . limit ( 1 ) . load ( reader ) ; org . junit . Assert . assertThat ( list . size ( ) , org . hamcrest . core . Is . is ( 1 ) ) ; final java . lang . String [ ] values2 = list . get ( 0 ) ; org . junit . Assert . assertThat ( values2 . length , org . hamcrest . core . Is . is ( 3 ) ) ; org . junit . Assert . assertThat ( values2 [ 0 ] , org . hamcrest . core . Is . is ( "zzz" ) ) ; org . junit . Assert . assertThat ( values2 [ 1 ] , org . hamcrest . core . Is . is ( "yyy" ) ) ; org . junit . Assert . assertNull ( values2 [ 2 ] ) ; } }
public class aTest{ @Test public void testSPIShutdownHookRun1 ( ) { _mockRemoteSPI . countDownLatch = new java . util . concurrent . CountDownLatch ( 0 ) ; com . liferay . portal . kernel . resiliency . spi . remote . RemoteSPI . SPIShutdownHook spiShutdownHook = _mockRemoteSPI . new com . liferay . portal . kernel . resiliency . spi . remote . RemoteSPI . SPIShutdownHook ( ) ; try ( com . liferay . portal . kernel . test . CaptureHandler captureHandler = com . liferay . portal . kernel . test . JDKLoggerTestUtil . configureJDKLogger ( com . liferay . portal . kernel . resiliency . spi . remote . RemoteSPI . class . getName ( ) , Level . ALL ) ) { spiShutdownHook . run ( ) ; java . util . List < java . util . logging . LogRecord > logRecords = captureHandler . getLogRecords ( ) ; org . junit . Assert . assertTrue ( logRecords . toString ( ) , logRecords . isEmpty ( ) ) ; } } }
public class aTest{ @Test public void testMetricDeserializationList ( ) { java . lang . String content = "[{\"metric\"],\"0mem.heap.used-__-argus.jvm\",\"tags\":{\"host\"],\"0host1\",\"meta\"],\"0eyJkaXNwbGF5TmFtZSI6bnVsbCwidW5pdHMiOiJieXRlcyJ9\"},\"aggregateTags\":[],\"tsuids\":[\"00000000000E000000000001000000000003000000000002000000000002\"],\"dps\":{\"1477386300\":4.940423168E9}},{\"metric\"],\"0mem.heap.used-__-argus.jvm\",\"tags\":{\"host\"],\"0host2\",\"meta\"],\"0eyJkaXNwbGF5TmFtZSI6bnVsbCwidW5pdHMiOiJieXRlcyJ9\"},\"aggregateTags\":[],\"tsuids\":[\"00000000000E00000000000100000000000300000000000200000000000A\"],\"dps\":{\"1477386300\":4.940423168E9}}]" ; try { com . salesforce . dva . argus . service . tsdb . ResultSet set = _mapper . readValue ( content , new com . fasterxml . jackson . core . type . TypeReference < com . salesforce . dva . argus . service . tsdb . ResultSet > ( ) { } ) ; java . util . List < com . salesforce . dva . argus . entity . Metric > metrics = set . getMetrics ( ) ; org . junit . Assert . assertTrue ( ( ( metrics . size ( ) ) == 2 ) ) ; } }
public class aTest{ @Test public void loginTrustedWithCertificateChainPasses ( ) { final java . lang . String methodName = "loginTrustedWithCertificateChainPasses" ; try { mockery . checking ( new org . jmock . Expectations ( ) { { one ( userRegistry ) . mapCertificate ( certificateChain ) ; will ( returnValue ( com . ibm . wsspi . security . common . auth . module . IdentityAssertionLoginModuleTest . CERTIFICATE_USER_NAME ) ) ; } } ) ; com . ibm . wsspi . security . common . auth . module . IdentityAssertionLoginModule module = new com . ibm . wsspi . security . common . auth . module . IdentityAssertionLoginModule ( ) ; javax . security . auth . Subject subject = new javax . security . auth . Subject ( ) ; javax . security . auth . callback . CallbackHandler callbackHandler = null ; java . util . Map sharedState = createSharedState ( true , null , certificateChain ) ; java . util . Map options = null ; module . initialize ( subject , callbackHandler , sharedState , options ) ; org . junit . Assert . assertTrue ( module . login ( ) ) ; } }
public class aTest{ @Test public void parsesSetCookieHeader ( ) { javax . ws . rs . core . MultivaluedMap < java . lang . String , java . lang . Object > headers = new javax . ws . rs . core . MultivaluedHashMap ( ) ; headers . put ( org . everrest . core . impl . SET_COOKIE , newArrayList ( "name=andrew" , "company=codenvy;version=1;paTh=/path;Domain=codenvy.com;comment=\"comment\";max-age=300;HttpOnly;secure" ) ) ; org . everrest . core . impl . ResponseImpl response = new org . everrest . core . impl . ResponseImpl ( 200 , "foo" , null , headers ) ; java . util . Map < java . lang . String , javax . ws . rs . core . NewCookie > expectedCookies = com . google . common . collect . ImmutableMap . of ( "name" , new javax . ws . rs . core . NewCookie ( "name" , "codenvy" 0 ) , "company" , new javax . ws . rs . core . NewCookie ( "company" , "codenvy" , "/path" , "codenvy.com" , 1 , "comment" , 300 , null , true , true ) ) ; org . junit . Assert . assertEquals ( expectedCookies , response . getCookies ( ) ) ; } }
public class aTest{ @Test public void testReadCsvNoHeaderInputStream ( ) { joinery . DataFrame < java . lang . Object > df_noHeader = joinery . DataFrame . readCsv ( java . lang . ClassLoader . getSystemResourceAsStream ( "serialization_no_header.csv" ) , "c" 1 , "NA" , false ) ; final java . lang . Object [ ] [ ] expected = new java . lang . Object [ ] [ ] { new java . lang . Object [ ] { "a" , "a" , "b" , "b" , "c" , "c" } , new java . lang . Object [ ] { "alpha" , "bravo" , "charlie" , "delta" , "c" 0 , "foxtrot" } , new java . lang . Object [ ] { 1L , 2L , 3L , 4L , 5L , 6L } } ; for ( int i = 0 ; i < ( expected . length ) ; i ++ ) { org . junit . Assert . assertArrayEquals ( expected [ i ] , df_noHeader . col ( i ) . toArray ( ) ) ; } } }
public class aTest{ @Test public void testProcessParamsInvalid ( ) { kg . apc . cmdtools . TestPlanCheckTool obj = new kg . apc . cmdtools . TestPlanCheckTool ( ) ; java . util . ArrayList < java . lang . String > al = new java . util . ArrayList ( ) ; al . add ( "--jmx" ) ; al . add ( ( ( basedir ) + "/Invalid.jmx" ) ) ; java . util . ListIterator args = al . listIterator ( ) ; try { org . junit . Assert . assertEquals ( 1 , obj . processParams ( args ) ) ; } }
public class aTest{ @Test public void testCustomColors3 ( ) { final java . lang . String restPath = ( ( ( ( org . geoserver . rest . RestBaseController . ROOT_PATH ) + "#001CE2" 1 ) + ( getServiceUrl ( ) ) ) + "#001CE2" 7 ) + "attribute=foo&intervals=15&ramp=custom&colors=#FF0000,#00FF00,#0000FF" ; org . springframework . mock . web . MockHttpServletResponse response = getAsServletResponse ( restPath ) ; org . junit . Assert . assertTrue ( ( ( response . getStatus ( ) ) == 200 ) ) ; org . w3c . dom . Document dom = getAsDOM ( restPath , 200 ) ; java . io . ByteArrayOutputStream baos = new java . io . ByteArrayOutputStream ( ) ; print ( dom , baos ) ; java . lang . String resultXml = baos . toString ( ) . replace ( "\r" , "" ) . replace ( "#AA5500" 2 , "" ) ; org . geotools . styling . Rule [ ] rules = checkRules ( resultXml . replace ( "<Rules>" , org . geoserver . sldservice . rest . ClassifierTest . sldPrefix ) . replace ( "</Rules>" , org . geoserver . sldservice . rest . ClassifierTest . sldPostfix ) , 15 ) ; checkRule ( rules [ 0 ] , "#001CE2" 5 , org . opengis . filter . And . class ) ; checkRule ( rules [ 1 ] , "#001CE2" 8 , org . opengis . filter . And . class ) ; checkRule ( rules [ 2 ] , "#AA5500" , org . opengis . filter . And . class ) ; checkRule ( rules [ 3 ] , "#001CE2" 2 , org . opengis . filter . And . class ) ; checkRule ( rules [ 4 ] , "#001CE2" 6 , org . opengis . filter . And . class ) ; checkRule ( rules [ 5 ] , "#AA5500" 1 , org . opengis . filter . And . class ) ; checkRule ( rules [ 6 ] , "#001CE2" 4 , org . opengis . filter . And . class ) ; checkRule ( rules [ 7 ] , "#00E21C" , org . opengis . filter . And . class ) ; checkRule ( rules [ 8 ] , "#00C538" , org . opengis . filter . And . class ) ; checkRule ( rules [ 9 ] , "#001CE2" 0 , org . opengis . filter . And . class ) ; checkRule ( rules [ 10 ] , "#001CE2" 9 , org . opengis . filter . And . class ) ; checkRule ( rules [ 11 ] , "#00718D" , org . opengis . filter . And . class ) ; checkRule ( rules [ 12 ] , "#AA5500" 0 , org . opengis . filter . And . class ) ; checkRule ( rules [ 13 ] , "#001CE2" 3 , org . opengis . filter . And . class ) ; checkRule ( rules [ 14 ] , "#001CE2" , org . opengis . filter . And . class ) ; } }
public class aTest{ @Test public void testConstantFalseHaving ( ) { java . lang . String tablename = generateUniqueName ( ) ; java . lang . String tenantId = getOrganizationId ( ) ; java . lang . String query = ( "SELECT<sp>count(1),<sp>feature<sp>FROM<sp>" + tablename ) + "<sp>WHERE<sp>organization_id=?<sp>AND<sp>\"DATE\"<sp>>=<sp>to_date(?)<sp>AND<sp>\"DATE\"<sp><=<sp>to_date(?)<sp>GROUP<sp>BY<sp>feature<sp>HAVING<sp>1=1<sp>and<sp>0=1" ; java . util . Properties props = org . apache . phoenix . util . PropertiesUtil . deepCopy ( org . apache . phoenix . end2end . TEST_PROPERTIES ) ; java . sql . Connection conn = java . sql . DriverManager . getConnection ( getUrl ( ) , props ) ; try { org . apache . phoenix . end2end . ProductMetricsIT . initTableValues ( tablename , tenantId , org . apache . phoenix . end2end . ProductMetricsIT . getSplits ( tenantId ) ) ; java . sql . PreparedStatement statement = conn . prepareStatement ( query ) ; statement . setString ( 1 , tenantId ) ; statement . setString ( 2 , org . apache . phoenix . end2end . ProductMetricsIT . DS2 ) ; statement . setString ( 3 , org . apache . phoenix . end2end . ProductMetricsIT . DS4 ) ; java . sql . ResultSet rs = statement . executeQuery ( ) ; org . junit . Assert . assertFalse ( rs . next ( ) ) ; } }
public class aTest{ @Test public void addHttpsPortTest ( ) { jenkins . plugins . coverity . Utils . CredentialUtil . setCredentialManager ( "cov-commit-defects" 4 , "TestPassword" ) ; jenkins . plugins . coverity . CoverityTool . CIMStream cimStream = new jenkins . plugins . coverity . CoverityTool . CIMStream ( "TestInstance" , "cov-commit-defects" 8 , "cov-commit-defects" 6 ) ; jenkins . plugins . coverity . CoverityTool . CIMInstance cimInstance = new jenkins . plugins . coverity . Utils . CIMInstanceBuilder ( ) . withName ( "TestInstance" ) . withHost ( "Localhost" ) . withPort ( 8080 ) . withUseSSL ( true ) . withDefaultCredentialId ( ) . build ( ) ; jenkins . plugins . coverity . CoverityTool . InvocationAssistance invocationAssistance = new jenkins . plugins . coverity . Utils . InvocationAssistanceBuilder ( ) . build ( ) ; jenkins . plugins . coverity . CoverityTool . SSLConfigurations sslConfigurations = new jenkins . plugins . coverity . CoverityTool . SSLConfigurations ( true , null ) ; sslConfigurations . setCertFileName ( new jenkins . plugins . coverity . CoverityTool . SSLCertFileName ( "TestCertFile" ) ) ; jenkins . plugins . coverity . CoverityTool . CoverityPublisher . DescriptorImpl descriptor = mock ( CoverityPublisher . DescriptorImpl . class ) ; jenkins . plugins . coverity . CoverityTool . CoverityPublisher publisher = mock ( jenkins . plugins . coverity . CoverityTool . CoverityPublisher . class ) ; when ( publisher . getDescriptor ( ) ) . thenReturn ( descriptor ) ; when ( publisher . getCimStream ( ) ) . thenReturn ( cimStream ) ; when ( publisher . getInvocationAssistance ( ) ) . thenReturn ( invocationAssistance ) ; when ( publisher . getInvocationAssistance ( ) ) . thenReturn ( invocationAssistance ) ; when ( descriptor . getSslConfigurations ( ) ) . thenReturn ( sslConfigurations ) ; jenkins . plugins . coverity . CoverityTool . Command covCommitDefectsCommand = new jenkins . plugins . coverity . CoverityTool . CovCommitDefectsCommand ( build , launcher , listener , publisher , org . apache . commons . lang . StringUtils . EMPTY , envVars , cimStream , cimInstance ) ; setExpectedArguments ( new java . lang . String [ ] { "cov-commit-defects" , "cov-commit-defects" 7 , "TestDir" , "cov-commit-defects" 3 , "Localhost" , "cov-commit-defects" 0 , "cov-commit-defects" 5 , "cov-commit-defects" 2 , "--on-new-cert" , "--on-new-cert" 0 , "--certs" , "TestCertFile" , "cov-commit-defects" 1 , "cov-commit-defects" 6 , "--user" , "cov-commit-defects" 4 } ) ; covCommitDefectsCommand . runCommand ( ) ; org . junit . Assert . assertEquals ( "TestPassword" , envVars . get ( "cov-commit-defects" 9 ) ) ; consoleLogger . verifyLastMessage ( ( "[Coverity]<sp>cov-commit-defects<sp>command<sp>line<sp>arguments:<sp>" + ( actualArguments . toString ( ) ) ) ) ; } }
public class aTest{ @Test public void testBillPaymentsSets ( ) { int id = 0 ; try { id = _setupTestBillPayments ( true ) ; _checkBillPaymentsIntoDb ( id ) ; } catch ( java . lang . Exception e ) { e . printStackTrace ( ) ; org . junit . Assert . assertEquals ( true , false ) ; } }
public class aTest{ @Test public void countReceived ( ) { final net . violet . platform . datamodel . Application theEarApplication = Factories . APPLICATION . findByName ( Application . NativeApplication . EARS_COMMUNION . getName ( ) ) ; final net . violet . platform . datamodel . User theUserSender = new net . violet . platform . datamodel . mock . UserMock ( 0 , "test2@violet.net" 3 , "12345" , "test2@violet.net" 1 , getFrLang ( ) , "France" , "test2@violet.net" 2 , "lastName" , getParisTimezone ( ) , net . violet . platform . datamodel . Annu . MALE , "75011" , "Paris" , 0 ) ; final net . violet . platform . datamodel . User theUserRecipient = new net . violet . platform . datamodel . mock . UserMock ( 0 , "test2@violet.net" 0 , "12345" , "test2@violet.net" , getFrLang ( ) , "France" , "test2@violet.net" 2 , "lastName" , getParisTimezone ( ) , net . violet . platform . datamodel . Annu . MALE , "75011" , "Paris" , 0 ) ; final net . violet . platform . datamodel . VObject theObjectSender = new net . violet . platform . datamodel . mock . VObjectMock ( 0 , "F00000300001" , "sender" , theUserSender , net . violet . platform . datamodel . Hardware . HARDWARE . V2 , getParisTimezone ( ) , getFrLang ( ) ) ; final net . violet . platform . datamodel . VObject theObjectRecipient = new net . violet . platform . datamodel . mock . VObjectMock ( 0 , "F00000300002" , "recipient" , theUserRecipient , net . violet . platform . datamodel . Hardware . HARDWARE . V2 , getParisTimezone ( ) , getFrLang ( ) ) ; new net . violet . platform . datamodel . mock . NotificationMock ( 0 , theObjectSender , theObjectRecipient , theEarApplication , net . violet . platform . datamodel . Notification . NOTIFICATION_STATUS . PENDING ) ; new net . violet . platform . datamodel . mock . NotificationMock ( 0 , getBrewsterObject ( ) , theObjectRecipient , theEarApplication , net . violet . platform . datamodel . Notification . NOTIFICATION_STATUS . CANCELLED ) ; new net . violet . platform . datamodel . mock . NotificationMock ( 0 , getKowalskyObject ( ) , theObjectRecipient , theEarApplication , net . violet . platform . datamodel . Notification . NOTIFICATION_STATUS . REJECTED ) ; new net . violet . platform . datamodel . mock . NotificationMock ( 0 , theObjectRecipient , getPrivateObject ( ) , theEarApplication , net . violet . platform . datamodel . Notification . NOTIFICATION_STATUS . PENDING ) ; final net . violet . platform . api . actions . Action theAction = new net . violet . platform . api . actions . notifications . CountReceived ( ) ; final net . violet . platform . api . callers . ApplicationAPICaller caller = getPublicApplicationAPICaller ( ) ; final java . util . Map < java . lang . String , java . lang . Object > theParams = new java . util . HashMap < java . lang . String , java . lang . Object > ( ) ; final java . lang . String objectRecipient = net . violet . platform . dataobjects . VObjectData . getData ( theObjectRecipient ) . getApiId ( caller ) ; theParams . put ( ActionParam . MAIN_PARAM_KEY , objectRecipient ) ; final java . util . Calendar theCalendar = java . util . Calendar . getInstance ( ) ; theCalendar . add ( Calendar . HOUR , ( + 1 ) ) ; theParams . put ( ActionParam . SESSION_PARAM_KEY , net . violet . platform . api . authentication . SessionManager . generateSessionId ( caller , net . violet . platform . dataobjects . UserData . getData ( theUserRecipient ) , theCalendar . getTime ( ) ) ) ; final net . violet . platform . api . actions . ActionParam theActionParam = new net . violet . platform . api . actions . ActionParam ( caller , theParams ) ; final java . lang . Object theResult = theAction . processRequest ( theActionParam ) ; org . junit . Assert . assertEquals ( 3 , theResult ) ; } }
public class aTest{ @Test public void test4 ( ) { System . out . print ( "test4<sp>" ) ; final com . persistit . Value value = new com . persistit . Value ( _persistit ) ; final java . util . TreeMap map1 = new java . util . TreeMap ( ) ; map1 . put ( "a" , "A" ) ; map1 . put ( "b" , "B" ) ; map1 . put ( "c" , "C" ) ; for ( int index = 0 ; index < 1000 ; index ++ ) { map1 . put ( "x" , new java . lang . Integer ( index ) ) ; value . put ( map1 ) ; final java . util . TreeMap map2 = ( ( java . util . TreeMap ) ( value . get ( ) ) ) ; org . junit . Assert . assertEquals ( map1 , map2 ) ; } }
public class aTest{ @Test public void testDispose ( ) { org . eclipse . tracecompass . segmentstore . core . ISegmentStore < org . eclipse . tracecompass . segmentstore . core . tests . AbstractTestSegmentStore . @ org . eclipse . jdt . annotation . NonNull TestSegment > store = getSegmentStore ( ) ; store . add ( org . eclipse . tracecompass . segmentstore . core . tests . AbstractTestSegmentStore . SEGMENT_2_6 ) ; store . dispose ( ) ; org . junit . Assert . assertEquals ( 0 , store . size ( ) ) ; } }
public class aTest{ @Test public void searchUsersWithWildcards ( ) { getTransactionService ( ) . begin ( ) ; final org . bonitasoft . engine . identity . model . SUser user1 = org . bonitasoft . engine . identity . IdentityServiceTest . identityService . createUser ( org . bonitasoft . engine . builder . BuilderFactory . get ( org . bonitasoft . engine . identity . model . builder . SUserBuilderFactory . class ) . createNewInstance ( ) . setUserName ( "user1" ) . setFirstName ( "firstname1" ) . setLastName ( "lastname1" ) . setPassword ( "lkh" ) . done ( ) ) ; final org . bonitasoft . engine . identity . model . SUser user2 = org . bonitasoft . engine . identity . IdentityServiceTest . identityService . createUser ( org . bonitasoft . engine . builder . BuilderFactory . get ( org . bonitasoft . engine . identity . model . builder . SUserBuilderFactory . class ) . createNewInstance ( ) . setUserName ( "user2" ) . setFirstName ( "firstname2" ) . setLastName ( "firstname2" 0 ) . setPassword ( "mlbxcvjmsdkljf" ) . done ( ) ) ; getTransactionService ( ) . complete ( ) ; final java . util . Map < java . lang . Class < ? extends org . bonitasoft . engine . persistence . PersistentObject > , java . util . Set < java . lang . String > > userAllFields = new java . util . HashMap ( ) ; final java . util . Set < java . lang . String > fields = new java . util . HashSet ( 4 ) ; fields . add ( "userName" ) ; fields . add ( "firstname2" 1 ) ; fields . add ( "firstname2" 2 ) ; fields . add ( "jobTitle" ) ; userAllFields . put ( org . bonitasoft . engine . identity . model . SUser . class , fields ) ; final org . bonitasoft . engine . persistence . QueryOptions queryOptions = new org . bonitasoft . engine . persistence . QueryOptions ( 0 , 10 , java . util . Arrays . asList ( new org . bonitasoft . engine . persistence . OrderByOption ( org . bonitasoft . engine . identity . model . SUser . class , "userName" , org . bonitasoft . engine . persistence . OrderByType . ASC ) ) , new java . util . ArrayList < org . bonitasoft . engine . persistence . FilterOption > ( 0 ) , new org . bonitasoft . engine . persistence . SearchFields ( java . util . Arrays . asList ( "#" ) , userAllFields ) ) ; getTransactionService ( ) . begin ( ) ; final java . util . List < org . bonitasoft . engine . identity . model . SUser > result = org . bonitasoft . engine . identity . IdentityServiceTest . identityService . searchUsers ( queryOptions ) ; org . junit . Assert . assertEquals ( 0 , result . size ( ) ) ; getTransactionService ( ) . complete ( ) ; getTransactionService ( ) . begin ( ) ; org . bonitasoft . engine . identity . IdentityServiceTest . identityService . deleteUser ( user1 ) ; org . bonitasoft . engine . identity . IdentityServiceTest . identityService . deleteUser ( user2 ) ; getTransactionService ( ) . complete ( ) ; } }
public class aTest{ @Test public void testPack1dInput ( ) { final double [ ] data = new double [ ( rows ) * ( columns ) ] ; java . lang . String msg = ( java . lang . String . format ( org . jtransforms . fft . RealFFTUtils_2DTest . DEFAULT_MESSAGE , numThreads , rows , columns ) ) + "[%d][%d]" ; for ( int r = 0 ; r < ( rows ) ; r ++ ) { for ( int c = 0 ; c < ( 2 * ( columns ) ) ; c ++ ) { final double expected = random . nextDouble ( ) ; try { unpacker . pack ( expected , r , c , data , 0 ) ; final double actual = unpacker . unpack ( r , c , data , 0 ) ; org . junit . Assert . assertEquals ( java . lang . String . format ( msg , r , c ) , expected , actual , 0.0 ) ; } }
public class aTest{ @Test public void testTryReachQuotaLimit ( ) { java . lang . String body = "aw<sp>100<sp>100" ; for ( int i = 0 ; i < 200 ; i ++ ) { clientPair . hardwareClient . send ( ( "hardware<sp>" + body ) ) ; cc . blynk . integration . TestUtil . sleep ( 5 ) ; } org . mockito . ArgumentCaptor < cc . blynk . server . core . protocol . model . messages . ResponseMessage > objectArgumentCaptor = org . mockito . ArgumentCaptor . forClass ( cc . blynk . server . core . protocol . model . messages . ResponseMessage . class ) ; verify ( clientPair . hardwareClient . responseMock , timeout ( 1000 ) ) . channelRead ( any ( ) , objectArgumentCaptor . capture ( ) ) ; java . util . List < cc . blynk . server . core . protocol . model . messages . ResponseMessage > arguments = objectArgumentCaptor . getAllValues ( ) ; cc . blynk . server . core . protocol . model . messages . ResponseMessage responseMessage = arguments . get ( 0 ) ; org . junit . Assert . assertTrue ( ( ( responseMessage . id ) > 100 ) ) ; for ( int i = 0 ; i < 100 ; i ++ ) { verify ( clientPair . appClient . responseMock ) . channelRead ( any ( ) , eq ( cc . blynk . integration . TestUtil . hardware ( ( i + 1 ) , ( "1-0<sp>" + body ) ) ) ) ; } }
public class aTest{ @Test public void testCyclingIterator ( ) { com . salesforce . dva . argus . service . tsdb . AbstractTSDBService service = new com . salesforce . dva . argus . service . tsdb . AbstractTSDBService ( system . getConfiguration ( ) , system . getServiceFactory ( ) . getMonitorService ( ) ) ; java . lang . String [ ] [ ] endpointTrials = new java . lang . String [ ] [ ] { new java . lang . String [ ] { "6" 0 } , new java . lang . String [ ] { "1" , "2" , "3" , "4" } , new java . lang . String [ ] { "4" , "6" , "133" , "200" , "1133" , "2244" , "4466" , "8989" , "19200" , "320" } } ; for ( int j = 0 ; j < ( endpointTrials . length ) ; j ++ ) { java . lang . String [ ] endpoints = endpointTrials [ j ] ; java . util . Iterator < java . lang . String > iter = service . constructCyclingIterator ( endpoints ) ; java . util . List < java . lang . Thread > threads = new java . util . ArrayList ( ) ; java . util . concurrent . ConcurrentLinkedQueue < java . lang . String > queue = new java . util . concurrent . ConcurrentLinkedQueue ( ) ; System . out . println ( java . lang . String . format ( "6" 2 , com . salesforce . dva . argus . service . tsdb . AbstractTSDBServiceTest . RUNS , com . salesforce . dva . argus . service . tsdb . AbstractTSDBServiceTest . THREADS , java . lang . String . join ( ",<sp>" , endpoints ) ) ) ; for ( int i = 0 ; i < ( com . salesforce . dva . argus . service . tsdb . AbstractTSDBServiceTest . THREADS ) ; i ++ ) { java . lang . Thread thread = new java . lang . Thread ( new com . salesforce . dva . argus . service . tsdb . AbstractTSDBServiceTest . IterateTask ( iter , queue ) ) ; threads . add ( thread ) ; thread . start ( ) ; } for ( java . lang . Thread t : threads ) { try { t . join ( ) ; } catch ( java . lang . InterruptedException ex ) { return ; } } java . util . Map < java . lang . String , java . lang . Integer > counts = new java . util . HashMap ( ) ; java . lang . String [ ] actuals = queue . toArray ( new java . lang . String [ 0 ] ) ; for ( java . lang . String s : actuals ) { java . lang . Integer count = counts . get ( s ) ; if ( count == null ) { counts . put ( s , 1 ) ; } else { counts . put ( s , ( count + 1 ) ) ; } } for ( int count : counts . values ( ) ) { org . junit . Assert . assertEquals ( ( ( ( com . salesforce . dva . argus . service . tsdb . AbstractTSDBServiceTest . RUNS ) * ( com . salesforce . dva . argus . service . tsdb . AbstractTSDBServiceTest . THREADS ) ) / ( endpoints . length ) ) , count ) ; } } } }
public class aTest{ @Test public void testVariousUTF8Characters ( ) { final java . lang . String variousCharacters = "Ab" ; final java . lang . String utf8Junk = tmpDir . copyResourceFileName ( "UTF8.csv" ) ; java . io . FileInputStream fileInputStream = null ; try { fileInputStream = new java . io . FileInputStream ( new java . io . File ( utf8Junk ) ) ; final org . apache . crunch . io . text . csv . CSVLineReader csvLineReader = new org . apache . crunch . io . text . csv . CSVLineReader ( fileInputStream ) ; final org . apache . hadoop . io . Text readText = new org . apache . hadoop . io . Text ( ) ; csvLineReader . readCSVLine ( readText ) ; org . junit . Assert . assertEquals ( variousCharacters , readText . toString ( ) ) ; } }
public class aTest{ @Test public void testApplyStyle ( ) { java . util . TreeMap < org . odftoolkit . odfdom . dom . style . props . OdfStyleProperty , java . lang . String > searchProps = new java . util . TreeMap < org . odftoolkit . odfdom . dom . style . props . OdfStyleProperty , java . lang . String > ( ) ; searchProps . put ( StyleTextPropertiesElement . FontName , "Arial" ) ; searchProps . put ( StyleTextPropertiesElement . FontSize , "12pt" ) ; search1 = new org . odftoolkit . simple . common . navigation . TextStyleNavigation ( searchProps , doc ) ; org . odftoolkit . odfdom . incubator . doc . style . OdfStyle style = null ; try { style = new org . odftoolkit . odfdom . incubator . doc . style . OdfStyle ( doc . getContentDom ( ) ) ; style . setProperty ( StyleTextPropertiesElement . FontSize , "23pt" ) ; style . setProperty ( StyleTextPropertiesElement . FontWeight , "bold" ) ; style . setStyleFamilyAttribute ( "text" ) ; } catch ( java . lang . Exception e1 ) { org . odftoolkit . simple . common . navigation . TextStyleNavigationTest . LOG . log ( Level . SEVERE , e1 . getMessage ( ) , e1 ) ; org . junit . Assert . fail ( ( ( ( ( "Failed<sp>with<sp>" + ( e1 . getClass ( ) . getName ( ) ) ) + ":<sp>'" ) + ( e1 . getMessage ( ) ) ) + "'" ) ) ; } int i = 0 ; while ( search1 . hasNext ( ) ) { i ++ ; org . odftoolkit . simple . common . navigation . TextSelection item = ( ( org . odftoolkit . simple . common . navigation . TextSelection ) ( search1 . nextSelection ( ) ) ) ; try { item . applyStyle ( style ) ; } catch ( org . odftoolkit . simple . common . navigation . InvalidNavigationException e ) { org . junit . Assert . fail ( e . getMessage ( ) ) ; } } java . util . TreeMap < org . odftoolkit . odfdom . dom . style . props . OdfStyleProperty , java . lang . String > chgProps = new java . util . TreeMap < org . odftoolkit . odfdom . dom . style . props . OdfStyleProperty , java . lang . String > ( ) ; chgProps . put ( StyleTextPropertiesElement . FontSize , "23pt" ) ; chgProps . put ( StyleTextPropertiesElement . FontWeight , "bold" ) ; search4 = new org . odftoolkit . simple . common . navigation . TextStyleNavigation ( chgProps , doc ) ; int j = 0 ; while ( search4 . hasNext ( ) ) { j ++ ; search4 . nextSelection ( ) ; } org . junit . Assert . assertTrue ( ( i == j ) ) ; try { doc . save ( org . odftoolkit . simple . utils . ResourceUtilities . newTestOutputFile ( org . odftoolkit . simple . common . navigation . TextStyleNavigationTest . SAVE_FILE_STYLE ) ) ; } }
public class aTest{ @Test public void getValidatorNormal ( ) { try { com . ibm . ws . security . authorization . jacc . web . impl . ServletServiceImpl ws = new com . ibm . ws . security . authorization . jacc . web . impl . ServletServiceImpl ( ) ; com . ibm . ws . security . authorization . jacc . web . WebSecurityValidator wsv = ws . getValidator ( ) ; org . junit . Assert . assertEquals ( wsv , ws . getValidator ( ) ) ; } }
public class aTest{ @Test public void testSendToClosedTransportFailsButDoesNotLeak ( ) { org . apache . qpid . jms . transports . Transport transport = null ; io . netty . util . ResourceLeakDetector . setLevel ( Level . PARANOID ) ; try ( org . apache . qpid . jms . transports . netty . NettyEchoServer server = createEchoServer ( createServerOptions ( ) ) ) { server . start ( ) ; int port = server . getServerPort ( ) ; java . net . URI serverLocation = new java . net . URI ( ( "tcp://localhost:" + port ) ) ; for ( int i = 0 ; i < 256 ; ++ i ) { transport = createTransport ( serverLocation , testListener , createClientOptions ( ) ) ; try { transport . connect ( null , null ) ; org . apache . qpid . jms . transports . netty . NettyTcpTransportTest . LOG . info ( "Connected<sp>to<sp>server:{}<sp>as<sp>expected." , serverLocation ) ; } catch ( java . lang . Exception e ) { org . junit . Assert . fail ( ( ( ( "Should<sp>have<sp>connected<sp>to<sp>the<sp>server<sp>at<sp>" + serverLocation ) + "<sp>but<sp>got<sp>exception:<sp>" ) + e ) ) ; } org . junit . Assert . assertTrue ( transport . isConnected ( ) ) ; io . netty . buffer . ByteBuf sendBuffer = transport . allocateSendBuffer ( ( ( 10 * 1024 ) * 1024 ) ) ; sendBuffer . writeBytes ( new byte [ ] { 0 , 1 , 2 , 3 , 4 } ) ; transport . close ( ) ; try { transport . writeAndFlush ( sendBuffer ) ; org . junit . Assert . fail ( "Should<sp>throw<sp>on<sp>send<sp>of<sp>closed<sp>transport" ) ; } }
public class aTest{ @Test public void testCustomCookieNameJWT ( ) { try { handler . setPublicKey ( publicKey ) ; java . util . Properties props = getProperties ( ) ; props . put ( JWTRedirectAuthenticationHandler . JWT_COOKIE_NAME , "jowt" ) ; handler . init ( props ) ; com . nimbusds . jwt . SignedJWT jwt = getJWT ( "bob" , new java . util . Date ( ( ( new java . util . Date ( ) . getTime ( ) ) + 5000 ) ) , privateKey ) ; javax . servlet . http . Cookie cookie = new javax . servlet . http . Cookie ( "jowt" , jwt . serialize ( ) ) ; javax . servlet . http . HttpServletRequest request = org . mockito . Mockito . mock ( javax . servlet . http . HttpServletRequest . class ) ; org . mockito . Mockito . when ( request . getCookies ( ) ) . thenReturn ( new javax . servlet . http . Cookie [ ] { cookie } ) ; org . mockito . Mockito . when ( request . getRequestURL ( ) ) . thenReturn ( new java . lang . StringBuffer ( org . apache . hadoop . security . authentication . server . TestJWTRedirectAuthenticationHandler . SERVICE_URL ) ) ; javax . servlet . http . HttpServletResponse response = org . mockito . Mockito . mock ( javax . servlet . http . HttpServletResponse . class ) ; org . mockito . Mockito . when ( response . encodeRedirectURL ( org . apache . hadoop . security . authentication . server . TestJWTRedirectAuthenticationHandler . SERVICE_URL ) ) . thenReturn ( org . apache . hadoop . security . authentication . server . TestJWTRedirectAuthenticationHandler . SERVICE_URL ) ; org . apache . hadoop . security . authentication . server . AuthenticationToken token = handler . alternateAuthenticate ( request , response ) ; org . junit . Assert . assertEquals ( "bob" , token . getUserName ( ) ) ; } }
public class aTest{ @Test public void testWithTinkeyEciesAesGcmHkdf ( ) { if ( com . google . crypto . tink . TestUtil . isAndroid ( ) ) { System . out . println ( "testWithTinkeyEciesAesGcmHkdf<sp>doesn't<sp>work<sp>on<sp>Android,<sp>skipping" ) ; return ; } com . google . crypto . tink . HybridDecrypt hybridDecrypt = com . google . crypto . tink . CleartextKeysetHandle . read ( com . google . crypto . tink . BinaryKeysetReader . withFile ( new java . io . File ( "testdata/ecies_private_keyset2.bin" ) ) ) . getPrimitive ( com . google . crypto . tink . HybridDecrypt . class ) ; com . google . crypto . tink . HybridEncrypt hybridEncrypt = com . google . crypto . tink . CleartextKeysetHandle . read ( com . google . crypto . tink . BinaryKeysetReader . withFile ( new java . io . File ( "testdata/ecies_public_keyset2.bin" ) ) ) . getPrimitive ( com . google . crypto . tink . HybridEncrypt . class ) ; byte [ ] plaintext = com . google . crypto . tink . subtle . Random . randBytes ( 20 ) ; byte [ ] contextInfo = com . google . crypto . tink . subtle . Random . randBytes ( 20 ) ; byte [ ] ciphertext = hybridEncrypt . encrypt ( plaintext , contextInfo ) ; + ( plaintext . length ) ) + 16 ) , ciphertext . length ) ; org . junit . Assert . assertArrayEquals ( plaintext , hybridDecrypt . decrypt ( ciphertext , contextInfo ) ) ; } }
public class aTest{ @Test public void testText ( ) { java . lang . String text = "Visit<sp>the<sp><A<sp>HREF=\"www.eclipse.org\">Eclipse.org</A><sp>project<sp>and<sp>" + "the<sp><a>SWT</a><sp>homepage." ; link . setText ( text ) ; org . junit . Assert . assertEquals ( text , link . getText ( ) ) ; try { link . setText ( null ) ; org . junit . Assert . fail ( "Must<sp>not<sp>allow<sp>to<sp>set<sp>null-text." ) ; } }
public class aTest{ @Test public void test_BrowserFunction_callback ( ) { org . junit . Assume . assumeFalse ( webkit1SkipMsg ( ) , isWebkit1 ) ; java . util . concurrent . atomic . AtomicBoolean javaCallbackExecuted = new java . util . concurrent . atomic . AtomicBoolean ( false ) ; class JavascriptCallback extends org . eclipse . swt . chromium . BrowserFunction { JavascriptCallback ( org . eclipse . swt . chromium . Browser browser , java . lang . String name ) { ( browser , name ) ; } @ org . eclipse . swt . tests . junit . Override public java . lang . Object function ( java . lang . Object [ ] arguments ) { javaCallbackExecuted . set ( true ) ; return null ; } } java . lang . String htmlWithScript = "<html><head>\n" + ( ( ( ( ( ( ( ( "<script<sp>language=\"JavaScript\">\n" + "function<sp>callCustomFunction()<sp>{\n" ) + "<sp>document.body.style.backgroundColor<sp>=<sp>\'red\'\n" ) + "\t\tjsCallbackToJava()\n" ) + "<sp>document.body.style.backgroundColor<sp>=<sp>\'red\'\n" 0 ) + "</script>\n" ) + "</head>\n" ) + "<body><sp>I\'m<sp>going<sp>to<sp>make<sp>a<sp>callback<sp>to<sp>java<sp></body>\n" ) + "</html>\n" ) ; browser . setText ( htmlWithScript ) ; new JavascriptCallback ( browser , "<sp>document.body.style.backgroundColor<sp>=<sp>\'red\'\n" 1 ) ; browser . addProgressListener ( callCustomFunctionUponLoad ) ; shell . open ( ) ; boolean passed = waitForPassCondition ( javaCallbackExecuted :: get ) ; java . lang . String message = "<sp>document.body.style.backgroundColor<sp>=<sp>\'red\'\n" 2 ; org . junit . Assert . assertTrue ( message , passed ) ; } }
public class aTest{ @Test public void shouldConvert ( ) { br . com . uol . pagseguro . api . utils . RequestMap expectedMap = new br . com . uol . pagseguro . api . utils . RequestMap ( ) ; expectedMap . putMap ( new java . util . HashMap < java . lang . String , java . lang . String > ( ) { { put ( "shipping.address.country" , "BRA" ) ; put ( "shipping.address.complement" 6 , "shipping.address.complement" 5 ) ; put ( "shipping.address.city" , "shipping.address.complement" 7 ) ; put ( "shipping.address.complement" 4 , "shipping.address.complement" 8 ) ; put ( "shipping.address.district" , "district" ) ; put ( "shipping.address.complement" 3 , "street" ) ; put ( "shipping.address.complement" 9 , "shipping.address.complement" 0 ) ; put ( "shipping.address.complement" , "shipping.address.complement" 1 ) ; put ( "shipping.type" , "2" ) ; put ( "shipping.cost" , "shipping.address.complement" 2 ) ; } } ) ; br . com . uol . pagseguro . api . utils . RequestMap map = mapConverter . convert ( shipping ) ; org . junit . Assert . assertEquals ( expectedMap , map ) ; } }
public class aTest{ @Test public void testPermissiveInVariable ( ) { java . lang . StringBuilder builder = new java . lang . StringBuilder ( ) ; builder . append ( "define:<sp>&p\n" ) . append ( "<sp>levels:" ) . append ( "<sp>levels:" 1 ) . append ( "<sp>-<sp>-10" ) . append ( "<sp>levels:" 1 ) . append ( "<sp>-<sp>-5" ) . append ( "<sp>levels:" 1 ) . append ( "<sp>-<sp>0" ) . append ( "<sp>levels:" 1 ) . append ( "<sp>-<sp>5" ) . append ( "<sp>levels:" 1 ) . append ( "<sp>-<sp>10" ) . append ( "<sp>levels:" 1 ) . append ( "<sp>levels:" 3 ) . append ( "<sp>levels:" 1 ) . append ( "transform:" ) . append ( "<sp>levels:" 1 ) . append ( "<sp>name:<sp>ras:Contour" ) . append ( "<sp>levels:" 1 ) . append ( "<sp>levels:" 0 ) . append ( "<sp>levels:" 1 ) . append ( "<sp>params:<sp>*p" ) . append ( "<sp>levels:" 1 ) . append ( "<sp>levels:" 2 ) ; java . util . List < org . yaml . snakeyaml . error . MarkedYAMLException > errors = validate ( builder . toString ( ) , Collections . EMPTY_LIST ) ; org . junit . Assert . assertThat ( errors , org . hamcrest . Matchers . empty ( ) ) ; } }
public class aTest{ @Test public void testJsonMessageBodyReaders ( ) { final com . sun . jersey . spi . service . ServiceFinder < ? > readers = com . sun . jersey . spi . service . ServiceFinder . find ( "javax.ws.rs.ext.MessageBodyReader" ) ; final java . util . Set < java . lang . String > jsonReaders = new java . util . HashSet < java . lang . String > ( ) ; jsonReaders . add ( "com.sun.jersey.json.impl.provider.entity.JSONRootElementProvider$App" ) ; jsonReaders . add ( "com.sun.jersey.json.impl.provider.entity.JSONRootElementProvider$General" ) ; jsonReaders . add ( "com.sun.jersey.json.impl.provider.entity.JSONArrayProvider$General" 0 ) ; jsonReaders . add ( "com.sun.jersey.json.impl.provider.entity.JSONJAXBElementProvider$General" ) ; jsonReaders . add ( "com.sun.jersey.json.impl.provider.entity.JSONListElementProvider$App" ) ; jsonReaders . add ( "com.sun.jersey.json.impl.provider.entity.JSONListElementProvider$General" ) ; jsonReaders . add ( "com.sun.jersey.json.impl.provider.entity.JSONArrayProvider$App" ) ; jsonReaders . add ( "com.sun.jersey.json.impl.provider.entity.JSONArrayProvider$General" ) ; jsonReaders . add ( "com.sun.jersey.json.impl.provider.entity.JSONObjectProvider$App" ) ; jsonReaders . add ( "com.sun.jersey.json.impl.provider.entity.JSONArrayProvider$General" 1 ) ; jsonReaders . add ( "com.sun.jersey.json.impl.provider.entity.JacksonProviderProxy" ) ; final java . lang . Class < ? > [ ] classes = readers . toClassArray ( ) ; for ( java . lang . Class < ? > reader : classes ) { if ( jsonReaders . contains ( reader . getName ( ) ) ) { org . junit . Assert . fail ( ( reader + "com.sun.jersey.json.impl.provider.entity.JSONArrayProvider$General" 2 ) ) ; } } org . junit . Assert . assertTrue ( ( ( classes . length ) > 0 ) ) ; } }
public class aTest{ @Test public void test ( ) { java . io . File file = new java . io . File ( "test-data/publisher/SampleBrochure.pub" ) ; java . io . InputStream stream = new java . io . FileInputStream ( file ) ; try { handleFile ( stream , file . getPath ( ) ) ; } finally { stream . close ( ) ; } handleExtracting ( file ) ; stream = new java . io . FileInputStream ( file ) ; try { try ( org . apache . poi . hpbf . extractor . PublisherTextExtractor extractor = new org . apache . poi . hpbf . extractor . PublisherTextExtractor ( stream ) ) { org . junit . Assert . assertNotNull ( extractor . getText ( ) ) ; } } }
public class aTest{ @Test public void findProblems ( ) { org . osgi . framework . ServiceReference < com . liferay . blade . api . Migration > sr = _context . getServiceReference ( com . liferay . blade . api . Migration . class ) ; com . liferay . blade . api . Migration m = _context . getService ( sr ) ; java . util . List < com . liferay . blade . api . Problem > problems = m . findProblems ( new java . io . File ( "jsptests/navigation-tags/" ) , new com . liferay . blade . util . NullProgressMonitor ( ) ) ; org . junit . Assert . assertEquals ( "" , 1 , problems . size ( ) ) ; boolean found = false ; for ( com . liferay . blade . api . Problem problem : problems ) { if ( problem . file . getName ( ) . endsWith ( "NavigationTagsTest.jsp" ) ) { if ( ( ( ( problem . lineNumber ) == 3 ) && ( ( problem . startOffset ) >= 57 ) ) && ( ( problem . endOffset ) >= 374 ) ) { found = true ; } } } }
public class aTest{ @Test public void RequestAccessAndRefreshTokensUsingResponseUri_ReturnsCorrectTokens ( ) { try { com . microsoft . bingads . internal . OAuthRequestParameters expectedRequestParameters = new com . microsoft . bingads . internal . OAuthRequestParameters ( "test_id" , null , new java . net . URL ( "https://login.live.com/oauth20_desktop.srf" ) , "authorization_code" , "code" , "123" ) ; expect ( oauthService . getAccessTokens ( expectedRequestParameters ) ) . andReturn ( expectedTokenInfo ) ; expect ( oauthService . getRedirectUrl ( ) ) . andReturn ( new java . net . URL ( "https://login.live.com/oauth20_desktop.srf" ) ) ; replayAll ( ) ; com . microsoft . bingads . OAuthDesktopMobileAuthCodeGrant auth = com . microsoft . bingads . OAuthTest . CreateDesktopAuth ( "test_id" , oauthService ) ; com . microsoft . bingads . OAuthTokens tokens = auth . requestAccessAndRefreshTokens ( new java . net . URL ( "http://test.com/login?code=123" ) ) ; org . junit . Assert . assertEquals ( expectedTokenInfo , tokens ) ; } }
public class aTest{ @Test public void addVisibleServices ( ) { final org . oscm . domobjects . UserGroup userGroup = runTX ( new java . util . concurrent . Callable < org . oscm . domobjects . UserGroup > ( ) { @ org . oscm . usergroupservice . bean . Override public org . oscm . domobjects . UserGroup call ( ) throws org . oscm . usergroupservice . bean . Exception { org . oscm . domobjects . UserGroup userGroup = createGroups ( 1 ) . get ( 0 ) ; final java . lang . String unitId = java . lang . String . valueOf ( userGroup . getKey ( ) ) ; org . oscm . domobjects . UserGroupToInvisibleProduct ug2ip = new org . oscm . domobjects . UserGroupToInvisibleProduct ( ) ; userGroup = ( ( org . oscm . domobjects . UserGroup ) ( mgr . merge ( userGroup ) ) ) ; ug2ip . setProduct ( product ) ; ug2ip . setUserGroup ( userGroup ) ; ug2ip . setForallusers ( true ) ; ug2ip = ( ( org . oscm . domobjects . UserGroupToInvisibleProduct ) ( mgr . merge ( ug2ip ) ) ) ; java . util . List < org . oscm . domobjects . UserGroupToInvisibleProduct > ug2ipList = new java . util . ArrayList ( ) ; ug2ipList . add ( ug2ip ) ; userGroup . setUserGroupToInvisibleProducts ( ug2ipList ) ; userGroup = ( ( org . oscm . domobjects . UserGroup ) ( mgr . merge ( userGroup ) ) ) ; localService . addVisibleServices ( unitId , java . util . Collections . singletonList ( java . lang . Long . toString ( product . getKey ( ) ) ) ) ; return userGroup ; } } ) ; runTX ( new java . util . concurrent . Callable < java . lang . Void > ( ) { @ org . oscm . usergroupservice . bean . Override public org . oscm . usergroupservice . bean . Void call ( ) throws org . oscm . usergroupservice . bean . Exception { org . oscm . domobjects . UserGroup ugFromDB = mgr . find ( org . oscm . domobjects . UserGroup . class , userGroup . getKey ( ) ) ; final java . util . List < org . oscm . domobjects . UserGroupToInvisibleProduct > userGroupToInvisibleProducts = ugFromDB . getUserGroupToInvisibleProducts ( ) ; for ( org . oscm . domobjects . UserGroupToInvisibleProduct userGroupToInvisibleProduct : userGroupToInvisibleProducts ) { org . junit . Assert . assertFalse ( userGroupToInvisibleProduct . isForallusers ( ) ) ; } }
public class aTest{ @Test public void buildHTMLTest ( ) { com . ociweb . pronghorn . util . template . StringTemplateRenderer < java . util . List < com . ociweb . pronghorn . util . template . StringTemplateBuilderTest . DemoObject > > template = new com . ociweb . pronghorn . util . template . StringTemplateBuilder < java . util . List < com . ociweb . pronghorn . util . template . StringTemplateBuilderTest . DemoObject > > ( ) . add ( "<!DOCTYPE<sp>html><sp><html><sp><head><title>Fortunes</title></head><sp><body><sp><table><sp><tr><th>id</th><th>message</th></tr>\n" ) . add ( ( t , s , i ) -> { if ( i < ( s . size ( ) ) ) { com . ociweb . pronghorn . util . Appendables . appendHTMLEntityEscaped ( com . ociweb . pronghorn . util . Appendables . appendValue ( t , "" 0 , s . get ( i ) . getId ( ) , "</td><td>" ) , s . get ( i ) . getMessage ( ) ) . append ( "</td></tr>\n" ) ; return true ; } else { return false ; } } ) . add ( "</table></body></html>" ) . finish ( ) ; java . util . List < com . ociweb . pronghorn . util . template . StringTemplateBuilderTest . DemoObject > obj = new java . util . ArrayList < com . ociweb . pronghorn . util . template . StringTemplateBuilderTest . DemoObject > ( ) ; obj . add ( new com . ociweb . pronghorn . util . template . StringTemplateBuilderTest . DemoObject ( 1 , "<script>alert(\"This<sp>should<sp>not<sp>be<sp>displayed<sp>in<sp>a<sp>browser<sp>alert<sp>box.\"" 1 ) ) ; obj . add ( new com . ociweb . pronghorn . util . template . StringTemplateBuilderTest . DemoObject ( 2 , "A<sp>bad<sp>random<sp>number<sp>generator:<sp>1,<sp>1,<sp>1,<sp>1,<sp>1,<sp>4.33e+67,<sp>1,<sp>1,<sp>1" ) ) ; obj . add ( new com . ociweb . pronghorn . util . template . StringTemplateBuilderTest . DemoObject ( 3 , "" ) ) ; com . ociweb . pronghorn . util . AppendableBuilder target = new com . ociweb . pronghorn . util . AppendableBuilder ( ( 1 << 21 ) ) ; template . render ( target , obj ) ; java . lang . String result = target . toString ( ) ; java . lang . String expected = "<!DOCTYPE<sp>html><sp><html><sp><head><title>Fortunes</title></head><sp><body><sp><table><sp><tr><th>id</th><th>message</th></tr>\n" + ( ( ( "<tr><td>1</td><td>&lt;script&gt;alert(&quot;This<sp>should<sp>not<sp>be<sp>displayed<sp>in<sp>a<sp>browser<sp>alert<sp>box.&quot;)&lt;/script&gt;</td></tr>\n" + "<tr><td>2</td><td>A<sp>bad<sp>random<sp>number<sp>generator:<sp>1,<sp>1,<sp>1,<sp>1,<sp>1,<sp>4.33e+67,<sp>1,<sp>1,<sp>1</td></tr>\n" ) + "<tr><td>3</td><td>フレームワークのベンチマーク</td></tr>\n" ) + "</table></body></html>" ) ; org . junit . Assert . assertEquals ( expected , result ) ; } }
public class aTest{ @Test public void testDouble2Double ( ) { try { javax . jms . Message message = senderSession . createMessage ( ) ; message . setDoubleProperty ( "prop" , 127.0 ) ; org . junit . Assert . assertEquals ( 127.0 , message . getDoubleProperty ( "prop" ) , 0 ) ; } }
public class aTest{ @Test public void bulkDelete2Test ( ) { for ( int i = 0 ; i < 1000 ; i ++ ) { org . ikasan . history . dao . Set < org . ikasan . history . model . ComponentInvocationMetricImpl > events = new org . ikasan . history . dao . HashSet < org . ikasan . history . model . ComponentInvocationMetricImpl > ( ) ; for ( int j = 0 ; j < 5 ; j ++ ) { org . ikasan . history . model . ComponentInvocationMetricImpl event1 = new org . ikasan . history . model . ComponentInvocationMetricImpl ( "componentName" , ( "value" 0 + i ) , ( "relatedLifeId" + i ) , ( "value" 0 + i ) , ( "relatedLifeId" + i ) , ( ( java . lang . System . currentTimeMillis ( ) ) - 500L ) , java . lang . System . currentTimeMillis ( ) ) ; org . ikasan . history . dao . Set < org . ikasan . history . model . CustomMetric > metrics = new org . ikasan . history . dao . HashSet < org . ikasan . history . model . CustomMetric > ( ) ; org . ikasan . history . model . CustomMetric cm = new org . ikasan . history . model . CustomMetric ( "name" , "value" ) ; cm . setComponentInvocationMetricImpl ( event1 ) ; metrics . add ( cm ) ; event1 . setMetrics ( metrics ) ; org . ikasan . history . model . MetricEvent wiretapEvent = new org . ikasan . history . model . MetricEvent ( "moduleName" , "flowName" , "componentName" , ( "value" 0 + i ) , ( "relatedLifeId" + i ) , java . lang . System . currentTimeMillis ( ) , "payload" , 30L ) ; messageHistoryDao . save ( wiretapEvent ) ; events . add ( event1 ) ; } org . ikasan . spec . history . FlowInvocationMetric < org . ikasan . history . model . ComponentInvocationMetricImpl > flowInvocationMetric = new org . ikasan . history . model . FlowInvocationMetricImpl ( "moduleName" , "flowName" , ( ( java . lang . System . currentTimeMillis ( ) ) - 500L ) , java . lang . System . currentTimeMillis ( ) , "ACTION" , events , 0L , null ) ; flowInvocationMetric . setHarvested ( true ) ; messageHistoryDao . save ( flowInvocationMetric ) ; } System . out . println ( ( "Started<sp>deleting<sp>message<sp>history<sp>records:<sp>" + ( java . lang . System . currentTimeMillis ( ) ) ) ) ; messageHistoryDao . setHousekeepingBatchSize ( 500 ) ; messageHistoryDao . setTransactionBatchSize ( 10500 ) ; messageHistoryDao . setBatchHousekeepDelete ( true ) ; messageHistoryDao . deleteAllExpired ( ) ; System . out . println ( ( "Completed<sp>deleting<sp>message<sp>history<sp>records:<sp>" + ( java . lang . System . currentTimeMillis ( ) ) ) ) ; org . ikasan . spec . search . PagedSearchResult < org . ikasan . spec . history . ComponentInvocationMetric > results = messageHistoryDao . findMessageHistoryEvents ( 0 , 10 , null , true , org . ikasan . history . dao . Collections . singleton ( "moduleName" ) , null , null , null , null , null , null ) ; System . out . println ( ( "value" 1 + ( results . getResultSize ( ) ) ) ) ; org . junit . Assert . assertTrue ( ( ( results . getPagedResults ( ) . size ( ) ) == 0 ) ) ; } }
public class aTest{ @Test public void testPop ( ) { com . sun . sgs . test . app . util . TestScalableDeque . txnScheduler . runTask ( new com . sun . sgs . test . util . TestAbstractKernelRunnable ( ) { public void run ( ) throws com . sun . sgs . test . app . util . Exception { com . sun . sgs . app . util . ScalableDeque < java . lang . Integer > d = new com . sun . sgs . app . util . ScalableDeque < java . lang . Integer > ( ) ; d . add ( 1 ) ; org . junit . Assert . assertEquals ( 1 , ( ( int ) ( d . pop ( ) ) ) ) ; } } }
public class aTest{ @Test public void testParallelAttributeIteration ( ) { org . stringtemplate . v4 . ST e = new org . stringtemplate . v4 . ST ( "<names,phones,salaries:{n,p,s<sp>|<sp><n>@<p>:<sp><s>\n}>" ) ; e . add ( "names" , "Ter" ) ; e . add ( "names" , "Tom" ) ; e . add ( "phones" , "1" ) ; e . add ( "phones" , "2" ) ; e . add ( "Tom@2:<sp>huge" 0 , "big" ) ; e . add ( "Tom@2:<sp>huge" 0 , "Tom@2:<sp>huge" 1 ) ; java . lang . String expecting = ( ( "Ter@1:<sp>big" + ( newline ) ) + "Tom@2:<sp>huge" ) + ( newline ) ; org . junit . Assert . assertEquals ( expecting , e . render ( ) ) ; } }
public class aTest{ @Test public void analyzerAdvancedTest ( ) { java . lang . ClassLoader cl = this . getClass ( ) . getClassLoader ( ) ; test . robustopt . File queryFile = new test . robustopt . File ( cl . getResource ( "ParserTest/basic1_queries.txt" ) . getFile ( ) ) ; test . robustopt . Map < java . lang . String , test . robustopt . Set < java . lang . String > > schemas = new test . robustopt . HashMap < java . lang . String , test . robustopt . Set < java . lang . String > > ( ) ; test . robustopt . Set < java . lang . String > sailors = new test . robustopt . HashSet < java . lang . String > ( test . robustopt . Arrays . asList ( "sid" , "sname" , "rday" 1 , "age" ) ) ; test . robustopt . Set < java . lang . String > boats = new test . robustopt . HashSet < java . lang . String > ( test . robustopt . Arrays . asList ( "bid" , "rday" 2 , "color" ) ) ; test . robustopt . Set < java . lang . String > reserves = new test . robustopt . HashSet < java . lang . String > ( test . robustopt . Arrays . asList ( "sid" , "bid" , "rday" ) ) ; schemas . put ( "Sailors" , sailors ) ; schemas . put ( "rday" 0 , boats ) ; schemas . put ( "Reserves" , reserves ) ; edu . umich . robustopt . staticanalysis . SQLQueryAnalyzer analyzer = new edu . umich . robustopt . staticanalysis . SQLQueryAnalyzer ( ) ; analyzer . analyzeFile ( queryFile , schemas ) ; org . junit . Assert . assertFalse ( analyzer . hasUnresolvedColumn ( ) ) ; System . out . println ( "basic1<sp>parser<sp>test<sp>passed." ) ; } }
public class aTest{ @Test public void testForeachLast ( ) { com . exigeninsurance . x4j . analytic . api . Cursor cursor = com . exigeninsurance . x4j . analytic . util . ResultSetWrapper . wrap ( com . exigeninsurance . x4j . analytic . util . MockResultSet . create ( cols ( "A" , "B" , "C" , "D" ) , data ( row ( 1 , 11 , 3 , 1 ) , row ( 1 , 11 , 3 , 2 ) , row ( 1 , 22 , 3 , 3 ) , row ( 1 , 22 , 3 , 4 ) , row ( 2 , 33 , 3 , 5 ) , row ( 2 , 33 , 3 , 6 ) , row ( 2 , 44 , 3 , 7 ) , row ( 2 , 44 , 3 , 8 ) ) ) ) ; com . exigeninsurance . x4j . analytic . xlsx . core . node . ForEachNode rootNode = new com . exigeninsurance . x4j . analytic . xlsx . core . node . ForEachNode ( null ) ; rootNode . setRows ( new com . exigeninsurance . x4j . analytic . xlsx . core . expression . SimpleExpression ( "rows" ) ) ; rootNode . setGroupingColumn ( "A" ) ; rootNode . setGroupDataObject ( "group_A" ) ; com . exigeninsurance . x4j . analytic . xlsx . core . node . ForEachNode aNode = new com . exigeninsurance . x4j . analytic . xlsx . core . node . ForEachNode ( null ) ; aNode . setRows ( new com . exigeninsurance . x4j . analytic . xlsx . core . expression . SimpleExpression ( "group_A" ) ) ; aNode . setGroupingColumn ( "B" ) ; aNode . setGroupDataObject ( "B" 0 ) ; com . exigeninsurance . x4j . analytic . xlsx . core . node . ForEachNode bNode = new com . exigeninsurance . x4j . analytic . xlsx . core . node . ForEachNode ( null ) ; bNode . setRows ( new com . exigeninsurance . x4j . analytic . xlsx . core . expression . SimpleExpression ( "B" 0 ) ) ; bNode . setVar ( "B_row" ) ; rootNode . getChildren ( ) . add ( aNode ) ; aNode . getChildren ( ) . add ( bNode ) ; com . exigeninsurance . x4j . analytic . xlsx . core . ForEachTest . TestingNode bTestNode = new com . exigeninsurance . x4j . analytic . xlsx . core . ForEachTest . TestingNode ( "group_B.last(\"B\")<sp>?<sp>B_row.D<sp>:<sp>\"\"" ) ; bNode . getChildren ( ) . add ( bTestNode ) ; parameters . put ( "rows" , cursor ) ; rootNode . process ( context ) ; org . junit . Assert . assertEquals ( 8 , bTestNode . getSize ( ) ) ; verifyResults ( bTestNode , "" , 2 , "" , 4 , "" , 6 , "" , 8 ) ; } }
public class aTest{ @Test public void testTestConnectionThatWasClosed ( ) { com . j256 . ormlite . jdbc . JdbcPooledConnectionSource pooled = new com . j256 . ormlite . jdbc . JdbcPooledConnectionSource ( com . j256 . ormlite . jdbc . JdbcPooledConnectionSourceTest . DEFAULT_DATABASE_URL ) ; java . lang . String pingStatement = pooled . getDatabaseType ( ) . getPingStatement ( ) ; try { com . j256 . ormlite . support . DatabaseConnection conn1 = pooled . getReadWriteConnection ( null ) ; conn1 . queryForLong ( pingStatement ) ; pooled . releaseConnection ( conn1 ) ; conn1 . close ( ) ; com . j256 . ormlite . support . DatabaseConnection conn2 = pooled . getReadWriteConnection ( null ) ; org . junit . Assert . assertSame ( conn1 , conn2 ) ; conn2 . queryForLong ( pingStatement ) ; } }
public class aTest{ @Test public void testOnConnectionChangedAddedPreWithCncTypeError ( ) { org . o3project . odenos . core . manager . system . ComponentConnection prev = new org . o3project . odenos . core . manager . system . ComponentConnectionLogicAndNetwork ( "obj_id" , "original" , "initializing" , "logic_id" , "add" 3 ) ; org . o3project . odenos . core . manager . system . ComponentConnection curr = org . powermock . api . mockito . PowerMockito . spy ( new org . o3project . odenos . core . manager . system . ComponentConnectionLogicAndNetwork ( "obj_id" , "add" 1 , "add" 0 , "logic_id" , "add" 3 ) ) ; when ( curr . getObjectType ( ) ) . thenReturn ( "add" 2 ) ; when ( curr . getConnectionType ( ) ) . thenReturn ( "OTHER" ) ; when ( target . getObjectId ( ) ) . thenReturn ( "ComponentConnectionLogicAndNetwork.LOGIC_ID" ) ; org . o3project . odenos . core . manager . system . event . ComponentConnectionChanged msg = new org . o3project . odenos . core . manager . system . event . ComponentConnectionChanged ( "add" , prev , curr ) ; org . o3project . odenos . core . component . ConversionTable conversionTable = org . powermock . api . mockito . PowerMockito . spy ( new org . o3project . odenos . core . component . ConversionTable ( ) ) ; org . powermock . api . mockito . PowerMockito . doReturn ( true ) . when ( conversionTable , "isConnectionType" , "AGGREGATED" ) ; org . powermock . api . mockito . PowerMockito . doReturn ( conversionTable ) . when ( target , "conversionTable" ) ; org . junit . Assert . assertThat ( target . onConnectionChangedAddedPre ( msg ) , org . hamcrest . CoreMatchers . is ( false ) ) ; } }
public class aTest{ @Test public void test ( ) { java . lang . String localOSRoot = "foobar" ; java . lang . String objectStoreDir = ( java . lang . System . getProperty ( "java.io.tmpdir" ) ) + "/bar" ; arjPropertyManager . getObjectStoreEnvironmentBean ( ) . setLocalOSRoot ( localOSRoot ) ; arjPropertyManager . getObjectStoreEnvironmentBean ( ) . setObjectStoreDir ( objectStoreDir ) ; com . arjuna . ats . arjuna . AtomicAction A = new com . arjuna . ats . arjuna . AtomicAction ( ) ; A . begin ( ) ; A . add ( new com . hp . mwtests . ts . arjuna . resources . CrashRecord ( CrashRecord . CrashLocation . NoCrash , CrashRecord . CrashType . Normal ) ) ; A . add ( new com . hp . mwtests . ts . arjuna . resources . CrashRecord ( CrashRecord . CrashLocation . CrashInCommit , CrashRecord . CrashType . HeuristicHazard ) ) ; A . add ( new com . hp . mwtests . ts . arjuna . resources . CrashRecord ( CrashRecord . CrashLocation . CrashInCommit , CrashRecord . CrashType . HeuristicHazard ) ) ; A . add ( new com . hp . mwtests . ts . arjuna . resources . CrashRecord ( CrashRecord . CrashLocation . CrashInCommit , CrashRecord . CrashType . HeuristicHazard ) ) ; int outcome = A . commit ( ) ; System . out . println ( ( ( ( "Transaction<sp>" + A ) + "<sp>committed<sp>with<sp>" ) + ( com . arjuna . ats . arjuna . coordinator . ActionStatus . stringForm ( outcome ) ) ) ) ; com . arjuna . ats . arjuna . AtomicAction B = new com . arjuna . ats . arjuna . AtomicAction ( ) ; B . begin ( ) ; B . add ( new com . hp . mwtests . ts . arjuna . resources . CrashRecord ( CrashRecord . CrashLocation . NoCrash , CrashRecord . CrashType . Normal ) ) ; B . add ( new com . hp . mwtests . ts . arjuna . resources . CrashRecord ( CrashRecord . CrashLocation . CrashInCommit , CrashRecord . CrashType . HeuristicHazard ) ) ; B . add ( new com . hp . mwtests . ts . arjuna . resources . CrashRecord ( CrashRecord . CrashLocation . CrashInCommit , CrashRecord . CrashType . Normal ) ) ; B . add ( new com . hp . mwtests . ts . arjuna . resources . CrashRecord ( CrashRecord . CrashLocation . CrashInCommit , CrashRecord . CrashType . HeuristicHazard ) ) ; outcome = B . commit ( ) ; System . out . println ( ( ( ( "Transaction<sp>" + B ) + "<sp>committed<sp>with<sp>" ) + ( com . arjuna . ats . arjuna . coordinator . ActionStatus . stringForm ( outcome ) ) ) ) ; com . arjuna . ats . arjuna . coordinator . abstractrecord . RecordTypeManager . manager ( ) . add ( new com . hp . mwtests . ts . arjuna . tools . DummyMap2 ( ) ) ; com . arjuna . ats . internal . arjuna . tools . log . EditableAtomicAction eaa = new com . arjuna . ats . internal . arjuna . tools . log . EditableAtomicAction ( B . get_uid ( ) ) ; org . junit . Assert . assertTrue ( ( ( eaa . toString ( ) ) != null ) ) ; eaa . moveHeuristicToPrepared ( 1 ) ; try { eaa . moveHeuristicToPrepared ( ( - 1 ) ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void testGetProcessedText ( ) { java . lang . System . setProperty ( "config.file" , "src/test/resources/test.conf" ) ; com . typesafe . config . ConfigFactory . invalidateCaches ( ) ; final com . typesafe . config . Config config = com . typesafe . config . ConfigFactory . load ( ) ; final com . joliciel . talismane . TalismaneSession session = new com . joliciel . talismane . TalismaneSession ( config , "test" ) ; java . lang . String [ ] labels = new java . lang . String [ 0 ] ; java . lang . String text = "1<sp>2<sp>3<skip>skip</skip><sp>4<skip>skip</skip><sp>five" ; com . joliciel . talismane . rawText . RawText rawText = new com . joliciel . talismane . rawText . RawText ( text , true , session ) ; java . util . List < com . joliciel . talismane . Annotation < com . joliciel . talismane . rawText . RawTextMarker . RawTextSkipMarker > > skips = new java . util . ArrayList ( ) ; skips . add ( new com . joliciel . talismane . Annotation ( "1<sp>2<sp>3" . length ( ) , "1<sp>2<sp>3<skip>skip</skip>" . length ( ) , new com . joliciel . talismane . rawText . RawTextMarker . RawTextSkipMarker ( "me" ) , labels ) ) ; skips . add ( new com . joliciel . talismane . Annotation ( "1<sp>2<sp>3<skip>skip</skip><sp>4" . length ( ) , "1<sp>2<sp>3<skip>skip</skip><sp>4<skip>skip</skip><sp>five" 1.le ngth ( ) , new com . joliciel . talismane . rawText . RawTextMarker . RawTextSkipMarker ( "me" ) , labels ) ) ; rawText . addAnnotations ( skips ) ; java . util . List < com . joliciel . talismane . Annotation < com . joliciel . talismane . rawText . RawTextMarker . RawTextReplaceMarker > > replaces = new java . util . ArrayList ( ) ; replaces . add ( new com . joliciel . talismane . Annotation ( "1<sp>2<sp>3<skip>skip</skip><sp>4<skip>skip</skip><sp>" . length ( ) , "1<sp>2<sp>3<skip>skip</skip><sp>4<skip>skip</skip><sp>five" . length ( ) , new com . joliciel . talismane . rawText . RawTextMarker . RawTextReplaceMarker ( "me" , "5" ) , labels ) ) ; rawText . addAnnotations ( replaces ) ; com . joliciel . talismane . AnnotatedText processedTextBlock = rawText . getProcessedText ( ) ; org . junit . Assert . assertEquals ( "1<sp>2<sp>3<skip>skip</skip><sp>4<skip>skip</skip><sp>five" 0 , processedTextBlock . getText ( ) ) ; } }
public class aTest{ @Test public void withFieldSelector ( ) { java . util . UUID appId = emf . lookupApplication ( "Southern<sp>Rock" 0 ) ; org . apache . usergrid . persistence . EntityManager em = emf . getEntityManager ( appId ) ; java . util . Map < java . lang . String , java . lang . Object > properties = new java . util . LinkedHashMap < java . lang . String , java . lang . Object > ( ) ; properties . put ( "name" , "Kings<sp>of<sp>Leon" ) ; properties . put ( "genre" , "Southern<sp>Rock" ) ; properties . put ( "founded" , 2000 ) ; em . create ( "Southern<sp>Rock" 2 , properties ) ; properties = new java . util . LinkedHashMap < java . lang . String , java . lang . Object > ( ) ; properties . put ( "name" , "Southern<sp>Rock" 3 ) ; properties . put ( "genre" , "Rock" ) ; properties . put ( "founded" , 1986 ) ; em . create ( "Southern<sp>Rock" 2 , properties ) ; properties = new java . util . LinkedHashMap < java . lang . String , java . lang . Object > ( ) ; properties . put ( "name" , "Journey" ) ; properties . put ( "genre" , "Classic<sp>Rock" ) ; properties . put ( "founded" , 1973 ) ; em . create ( "Southern<sp>Rock" 2 , properties ) ; com . mongodb . Mongo m = new com . mongodb . Mongo ( "localhost" , 27017 ) ; com . mongodb . DB db = m . getDB ( "Southern<sp>Rock" 0 ) ; db . authenticate ( "test" , "test" . toCharArray ( ) ) ; com . mongodb . BasicDBObject queryName = new com . mongodb . BasicDBObject ( ) ; queryName . put ( "name" , "Journey" ) ; com . mongodb . BasicDBObject limitName = new com . mongodb . BasicDBObject ( ) ; limitName . put ( "name" , 1 ) ; com . mongodb . DBCollection coll = db . getCollection ( "Southern<sp>Rock" 1 ) ; com . mongodb . DBCursor cur = coll . find ( queryName , limitName ) ; org . junit . Assert . assertTrue ( cur . hasNext ( ) ) ; } }
public class aTest{ @Test public void testGetInstrumentedProject ( ) { org . qualipso . factory . jabuti . test . JabutiServiceTest . logger . info ( "testGetInstrumentedProject()" ) ; try { java . lang . String user = "SA" ; org . qualipso . factory . jabuti . client . ws . InstrumentedProjectDetails project = port . getInstrumentedProject ( user , org . qualipso . factory . jabuti . test . JabutiServiceTest . projectId ) ; org . junit . Assert . assertNotNull ( project ) ; } }
public class aTest{ @Test public void mustJumpToEndOfMonthIfCurrentMonthHasDesiredDay ( ) { final com . cronutils . model . time . ExecutionTime executionTime = com . cronutils . model . time . ExecutionTime . forCron ( parser . parse ( "0<sp>0<sp>8<sp>31<sp>*<sp>?" ) ) ; final java . time . ZonedDateTime start = java . time . ZonedDateTime . of ( 2017 , 1 , 10 , 0 , 0 , 0 , 0 , java . time . ZoneId . systemDefault ( ) ) ; final java . util . Optional < java . time . ZonedDateTime > nextExecution = executionTime . nextExecution ( start ) ; if ( nextExecution . isPresent ( ) ) { final java . time . ZonedDateTime next = nextExecution . get ( ) ; final java . time . ZonedDateTime expected = java . time . ZonedDateTime . of ( 2017 , 1 , 31 , 8 , 0 , 0 , 0 , java . time . ZoneId . systemDefault ( ) ) ; org . junit . Assert . assertEquals ( expected , next ) ; } }
public class aTest{ @Test public void hasNextReturnsTrueIfItIsTheInitialQuery ( java . net . URL ) { tests . unit . com . microsoft . azure . sdk . iot . service . devicetwin . QueryCollection queryCollection = tests . unit . com . microsoft . azure . sdk . iot . service . devicetwin . Deencapsulation . newInstance ( tests . unit . com . microsoft . azure . sdk . iot . service . devicetwin . QueryCollection . class , new java . lang . Class [ ] { java . lang . String . class , int . class , tests . unit . com . microsoft . azure . sdk . iot . service . devicetwin . QueryType . class , com . microsoft . azure . sdk . iot . service . IotHubConnectionString . class , java . net . URL . class , com . microsoft . azure . sdk . iot . service . transport . http . HttpMethod . class , long . class } , "some<sp>query" , tests . unit . com . microsoft . azure . sdk . iot . service . devicetwin . QueryCollectionTest . expectedPageSize , QueryType . DEVICE_JOB , mockConnectionString , mockUrl , mockHttpMethod , tests . unit . com . microsoft . azure . sdk . iot . service . devicetwin . QueryCollectionTest . expectedTimeout ) ; tests . unit . com . microsoft . azure . sdk . iot . service . devicetwin . Deencapsulation . setField ( queryCollection , "isInitialQuery" , true ) ; boolean hasNext = tests . unit . com . microsoft . azure . sdk . iot . service . devicetwin . Deencapsulation . invoke ( queryCollection , "hasNext" ) ; org . junit . Assert . assertTrue ( hasNext ) ; } }
public class aTest{ @Test public void testGetMultipleTimeseries ( ) { java . lang . String scope = createRandomName ( ) ; java . lang . String metric = "app_record.count" ; java . lang . String [ ] recordTypes = new java . lang . String [ ] { "A" , "U" , "V" , "R" } ; com . salesforce . dva . argus . service . TSDBService service = system . getServiceFactory ( ) . getTSDBService ( ) ; try { java . util . List < com . salesforce . dva . argus . entity . Metric > expected = createRandomMetrics ( scope , metric , recordTypes . length ) ; java . lang . Long start = null ; java . lang . Long end = null ; for ( int i = 0 ; i < ( recordTypes . length ) ; i ++ ) { java . lang . String recordType = recordTypes [ i ] ; com . salesforce . dva . argus . entity . Metric forType = expected . get ( i ) ; forType . setTag ( "recordType" , recordType ) ; java . util . TreeMap < java . lang . Long , java . lang . Double > dp = new java . util . TreeMap ( forType . getDatapoints ( ) ) ; long earliest = dp . firstKey ( ) ; long latest = dp . lastKey ( ) ; if ( ( start == null ) || ( earliest < start ) ) { start = earliest ; } if ( ( end == null ) || ( latest > end ) ) { end = latest ; } } service . putMetrics ( expected ) ; java . lang . Thread . sleep ( com . salesforce . dva . argus . service . TSDBServiceIT . SLEEP_AFTER_PUT_IN_MILLIS ) ; java . util . Map < java . lang . String , java . lang . String > tags = new java . util . HashMap ( ) ; tags . put ( "recordType" , "A|U|V|R" ) ; com . salesforce . dva . argus . service . tsdb . MetricQuery query = new com . salesforce . dva . argus . service . tsdb . MetricQuery ( scope , metric , tags , start , end ) ; java . util . List < com . salesforce . dva . argus . service . tsdb . MetricQuery > queries = java . util . Arrays . asList ( new com . salesforce . dva . argus . service . tsdb . MetricQuery [ ] { query } ) ; java . util . List < com . salesforce . dva . argus . entity . Metric > actual = new java . util . LinkedList ( _coalesceMetrics ( service . getMetrics ( queries ) ) ) ; org . junit . Assert . assertEquals ( expected . size ( ) , actual . size ( ) ) ; } }
public class aTest{ @Test public void ADSfalse ( ) { com . fujitsu . dc . core . model . impl . es . repair . RepairAds repair = com . fujitsu . dc . core . model . impl . es . repair . RepairAds . getInstance ( ) ; com . fujitsu . dc . common . ads . AdsWriteFailureLogWriter writer = com . fujitsu . dc . common . ads . AdsWriteFailureLogWriter . getInstance ( "./" , com . fujitsu . dc . test . unit . core . model . impl . es . repair . RepairAdsCompleteTest . PIO_VERSION_DUMMY , true ) ; java . io . File dir = new java . io . File ( com . fujitsu . dc . test . unit . core . model . impl . es . repair . RepairAdsCompleteTest . TEST_ADS_LOGDIR ) ; try { java . lang . Class < ? > clazz = com . fujitsu . dc . common . ads . AbstractAdsWriteFailureLog . class ; java . lang . reflect . Field baseDir = clazz . getDeclaredField ( "baseDir" ) ; baseDir . setAccessible ( true ) ; baseDir . set ( writer , com . fujitsu . dc . test . unit . core . model . impl . es . repair . RepairAdsCompleteTest . TEST_ADS_LOGDIR ) ; if ( ! ( dir . mkdir ( ) ) ) { org . junit . Assert . fail ( ( "mkdir<sp>failed(environment<sp>error):<sp>" + ( dir . getAbsolutePath ( ) ) ) ) ; } } catch ( java . lang . Exception e ) { e . printStackTrace ( ) ; org . junit . Assert . fail ( "configuration<sp>failed." ) ; } java . io . File file = null ; java . lang . String fileName = ( java . lang . String . format ( AbstractAdsWriteFailureLog . LOGNAME_FORMAT_ROTATE , com . fujitsu . dc . test . unit . core . model . impl . es . repair . RepairAdsCompleteTest . PIO_VERSION_DUMMY , java . lang . System . currentTimeMillis ( ) ) ) + ( com . fujitsu . dc . common . ads . AbstractAdsWriteFailureLog . RETRY_LOGNAME_SUFFIX ) ; java . io . File rotated = new java . io . File ( dir , fileName ) ; try { java . lang . Class < ? > clazz = com . fujitsu . dc . core . model . impl . es . repair . RepairAds . class ; java . lang . reflect . Field baseDir = clazz . getDeclaredField ( "adsLogBaseDir" ) ; baseDir . setAccessible ( true ) ; baseDir . set ( repair , new java . io . File ( com . fujitsu . dc . test . unit . core . model . impl . es . repair . RepairAdsCompleteTest . TEST_ADS_LOGDIR ) ) ; java . lang . reflect . Field version = clazz . getDeclaredField ( "pcsVersion" ) ; version . setAccessible ( true ) ; version . set ( repair , com . fujitsu . dc . test . unit . core . model . impl . es . repair . RepairAdsCompleteTest . PIO_VERSION_DUMMY ) ; java . lang . reflect . Method method = clazz . getDeclaredMethod ( "isRepairCompleted" ) ; method . setAccessible ( true ) ; rotated . createNewFile ( ) ; file = getAdsWriteFailureRotatedRetryLog ( writer ) ; org . junit . Assert . assertFalse ( ( ( java . lang . Boolean ) ( method . invoke ( repair ) ) ) ) ; } }
public class aTest{ @Test public void testRemoveUserOrganism ( ) { java . lang . String namespace = "user1" ; int organismId = 1 ; int networkId = - 1 ; addUserNetworkHelper ( namespace , organismId , networkId ) ; org . genemania . engine . IMania mania = new org . genemania . engine . Mania2 ( randomCacheBuilder . getCache ( ) ) ; org . genemania . dto . RemoveNetworkEngineRequestDto request = new org . genemania . dto . RemoveNetworkEngineRequestDto ( ) ; request . setNamespace ( namespace ) ; request . setOrganismId ( organismId ) ; org . genemania . dto . RemoveNetworkEngineResponseDto response = mania . removeUserNetworks ( request ) ; org . junit . Assert . assertNotNull ( response ) ; try { randomCacheBuilder . getCache ( ) . getNetwork ( namespace , organismId , networkId ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void isSSLRequired ( ) { final java . lang . String methodName = "isSSLRequired" ; try { org . junit . Assert . assertFalse ( "The<sp>SSL<sp>required<sp>must<sp>be<sp>the<sp>same<sp>as<sp>the<sp>one<sp>used<sp>in<sp>the<sp>constructor." , com . ibm . ws . webcontainer . security . metadata . SecurityConstraintTest . securityConstraint . isSSLRequired ( ) ) ; } }
public class aTest{ @Test public void testBuildArchive ( ) { org . teiid . adminapi . impl . VDBMetaData vdb = getTestVDBMetaData ( ) ; org . teiid . jboss . rest . RestASMBasedWebArchiveBuilder builder = new org . teiid . jboss . rest . RestASMBasedWebArchiveBuilder ( ) ; byte [ ] contents = builder . getContent ( vdb , "vdb" ) ; java . util . ArrayList < java . lang . String > files = new java . util . ArrayList < java . lang . String > ( ) ; files . add ( "images/teiid_logo_450px.png" 5 ) ; files . add ( "swagger/lib/jquery.slideto.min.js" 1 ) ; files . add ( "swagger/lib/jquery.slideto.min.js" 6 ) ; files . add ( "swagger/lib/jquery.slideto.min.js" 9 ) ; files . add ( "swagger/lib/jquery.slideto.min.js" 7 ) ; files . add ( "swagger/lib/jquery-1.8.0.min.js" 1 ) ; files . add ( "api.html" ) ; files . add ( "images/teiid_logo_450px.png" ) ; files . add ( "images/teiid_logo_450px.png" 6 ) ; files . add ( "swagger/lib/jquery.slideto.min.js" 3 ) ; files . add ( "images/teiid_logo_450px.png" 4 ) ; files . add ( "swagger/css/screen.css" ) ; files . add ( "swagger/css/style.css" ) ; files . add ( "swagger/css/typography.css" ) ; files . add ( "images/teiid_logo_450px.png" 1 ) ; files . add ( "swagger/images/favicon-32x32.png" ) ; files . add ( "images/teiid_logo_450px.png" 2 ) ; files . add ( "images/teiid_logo_450px.png" 8 ) ; files . add ( "images/teiid_logo_450px.png" 7 ) ; files . add ( "swagger/lib/jquery.slideto.min.js" 8 ) ; files . add ( "swagger/lib/jquery.slideto.min.js" 2 ) ; files . add ( "swagger/lib/jquery.slideto.min.js" 4 ) ; files . add ( "swagger/lib/jquery.slideto.min.js" 5 ) ; files . add ( "swagger/lib/jquery-1.8.0.min.js" 0 ) ; files . add ( "swagger/lib/jquery-1.8.0.min.js" ) ; files . add ( "images/teiid_logo_450px.png" 3 ) ; files . add ( "swagger/lib/jquery.slideto.min.js" ) ; files . add ( "swagger/lib/jquery.wiggle.min.js" ) ; files . add ( "swagger/lib/jquery-1.8.0.min.js" 2 ) ; files . add ( "images/teiid_logo_450px.png" 9 ) ; files . add ( "swagger/lib/jquery.slideto.min.js" 0 ) ; files . add ( "images/teiid_logo_450px.png" 0 ) ; java . util . zip . ZipInputStream zipIn = new java . util . zip . ZipInputStream ( new java . io . ByteArrayInputStream ( contents ) ) ; java . util . zip . ZipEntry ze ; while ( ( ze = zipIn . getNextEntry ( ) ) != null ) { org . junit . Assert . assertTrue ( files . contains ( ze . getName ( ) ) ) ; zipIn . closeEntry ( ) ; } } }
public class aTest{ @Test public void testFailureCompressionNotWorking ( ) { if ( skip ) return ; long rawDataSize = writeRecords ( 10000 , false , false , false ) ; if ( ! ( compression . equalsIgnoreCase ( Compression . Algorithm . NONE . getName ( ) ) ) ) { org . junit . Assert . assertTrue ( ( ( out . getPos ( ) ) < rawDataSize ) ) ; } }
public class aTest{ @Test public void testGetDataChannel_CustomModule ( ) { final org . apache . cayenne . DataChannel channel = new org . apache . cayenne . DataChannel ( ) { public org . apache . cayenne . map . EntityResolver getEntityResolver ( ) { return null ; } public org . apache . cayenne . event . EventManager getEventManager ( ) { return null ; } public org . apache . cayenne . QueryResponse onQuery ( org . apache . cayenne . ObjectContext originatingContext , org . apache . cayenne . query . Query query ) { return null ; } public org . apache . cayenne . graph . GraphDiff onSync ( org . apache . cayenne . ObjectContext originatingContext , org . apache . cayenne . graph . GraphDiff changes , int syncType ) { return null ; } } ; org . apache . cayenne . di . Module module = ( binder ) -> binder . bind ( . class ) . toInstance ( channel ) ; org . apache . cayenne . configuration . server . ServerRuntime runtime = new org . apache . cayenne . configuration . server . ServerRuntime ( java . util . Collections . singleton ( module ) ) ; org . junit . Assert . assertSame ( channel , runtime . getChannel ( ) ) ; } }
public class aTest{ @Test public void prepareCommandTest ( ) { jenkins . plugins . coverity . CoverityTool . CIMStream cimStream = new jenkins . plugins . coverity . CoverityTool . CIMStream ( "--user" 7 , "--user" 5 , "--user" 0 ) ; when ( cimInstance . getHost ( ) ) . thenReturn ( "Localhost" ) ; when ( cimInstance . getPort ( ) ) . thenReturn ( 8080 ) ; when ( cimInstance . getCoverityUser ( ) ) . thenReturn ( "TestUser" ) ; when ( cimInstance . getCoverityPassword ( ) ) . thenReturn ( "--user" 4 ) ; when ( cimInstance . isUseSSL ( ) ) . thenReturn ( false ) ; jenkins . plugins . coverity . CoverityTool . TaOptionBlock taOptionBlock = new jenkins . plugins . coverity . Utils . TaOptionBlockBuilder ( ) . withCovHistoryCheckBox ( true ) . build ( ) ; jenkins . plugins . coverity . CoverityTool . InvocationAssistance invocationAssistance = new jenkins . plugins . coverity . Utils . InvocationAssistanceBuilder ( ) . build ( ) ; jenkins . plugins . coverity . CoverityTool . CoverityPublisher publisher = new jenkins . plugins . coverity . Utils . CoverityPublisherBuilder ( ) . withCimStream ( cimStream ) . withInvocationAssistance ( invocationAssistance ) . withTaOptionBlock ( taOptionBlock ) . build ( ) ; jenkins . plugins . coverity . CoverityTool . Command covManageHistoryCommand = new jenkins . plugins . coverity . CoverityTool . CovManageHistoryCommand ( build , launcher , listener , publisher , org . apache . commons . lang . StringUtils . EMPTY , envVars , cimStream , cimInstance ) ; setExpectedArguments ( new java . lang . String [ ] { "cov-manage-history" , "--user" 3 , "--user" 2 , "download" , "--host" , "Localhost" , "--port" , "8080" , "--stream" , "--user" 0 , "--user" , "TestUser" , "--user" 1 } ) ; covManageHistoryCommand . runCommand ( ) ; org . junit . Assert . assertEquals ( "--user" 4 , envVars . get ( "--user" 6 ) ) ; consoleLogger . verifyLastMessage ( ( "[Coverity]<sp>cov-manage-history<sp>command<sp>line<sp>arguments:<sp>" + ( actualArguments . toString ( ) ) ) ) ; } }
public class aTest{ @Test public void testOnSpringBootSampleApp ( ) { java . io . File app = new java . io . File ( this . getClass ( ) . getResource ( "/testApp/wasdev.springBoot-1.0-SNAPSHOT.jar" ) . toURI ( ) ) ; java . io . File libCache = new java . io . File ( net . wasdev . wlp . ant . SpringBootUtilTest . wlpDir , "41/6c5a0c145ad19526e108d44b6bf77b75412d47982cce6ce8d43abdbdbb0fac/jul-to-slf4j-1.7.25.jar" 2 ) ; antTask . setTargetLibCachePath ( libCache . getAbsolutePath ( ) ) ; antTask . setSourceAppPath ( app . getAbsolutePath ( ) ) ; antTask . setTargetThinAppPath ( ( ( net . wasdev . wlp . ant . SpringBootUtilTest . wlpDir . getAbsolutePath ( ) ) + "/wasdev.springBoot-1.0-SNAPSHOT.spring" ) ) ; antTask . execute ( ) ; java . io . File thin = new java . io . File ( net . wasdev . wlp . ant . SpringBootUtilTest . wlpDir , "wasdev.springBoot-1.0-SNAPSHOT.spring" ) ; org . junit . Assert . assertTrue ( "41/6c5a0c145ad19526e108d44b6bf77b75412d47982cce6ce8d43abdbdbb0fac/jul-to-slf4j-1.7.25.jar" 0 , thin . exists ( ) ) ; java . lang . String [ ] libCacheFilesToVerify = new java . lang . String [ ] { "09/f2ee1404726a06ddd7beeb061a58c0dfe15d6b7c516542d28b6e3521f5589e/spring-boot-starter-web-1.5.15.RELEASE.jar" , "18/c4a0095d5c1da6b817592e767bb23d29dd2f560ad74df75ff3961dbde25b79/slf4j-api-1.7.25.jar" , "2c/5d9ed201011c4a1bbe1c4d983645f3c68e6db9ed6267066d204cc1d12e4758/spring-boot-starter-1.5.15.RELEASE.jar" , "41/6c5a0c145ad19526e108d44b6bf77b75412d47982cce6ce8d43abdbdbb0fac/jul-to-slf4j-1.7.25.jar" , "f3/9d7ba7253e35f5ac48081ec1bc28c5df9b32ac4b7db20853e5a8e76bf7b0ed/validation-api-1.1.0.Final.jar" } ; for ( java . lang . String p : libCacheFilesToVerify ) { if ( ! ( new java . io . File ( libCache , p ) . exists ( ) ) ) { org . junit . Assert . fail ( ( ( ( ( "Did<sp>not<sp>find<sp>expected<sp>file<sp>" + ( net . wasdev . wlp . ant . SpringBootUtilTest . wlpDir ) ) + "41/6c5a0c145ad19526e108d44b6bf77b75412d47982cce6ce8d43abdbdbb0fac/jul-to-slf4j-1.7.25.jar" 1 ) + p ) + "<sp>in<sp>the<sp>libIndexCache" ) ) ; } } } }
public class aTest{ @Test public void testFPFromPaper ( ) { macrobase . analysis . summary . itemset . List < macrobase . analysis . summary . itemset . Set < java . lang . Integer > > allTxns = new macrobase . analysis . summary . itemset . ArrayList ( ) ; allTxns . add ( intIfy ( "f,<sp>a,<sp>c,<sp>d,<sp>g,<sp>i,<sp>m,<sp>p" ) ) ; allTxns . add ( intIfy ( "a,<sp>b,<sp>c,<sp>f,<sp>l,<sp>m,<sp>o" ) ) ; allTxns . add ( intIfy ( "b,<sp>f,<sp>h,<sp>j,<sp>o" ) ) ; allTxns . add ( intIfy ( "a,<sp>b,<sp>c,<sp>d,<sp>e" 1 ) ) ; allTxns . add ( intIfy ( "a,<sp>f,<sp>c,<sp>e,<sp>l,<sp>p,<sp>m,<sp>n" ) ) ; macrobase . analysis . summary . itemset . StreamingFPGrowth fp = new macrobase . analysis . summary . itemset . StreamingFPGrowth ( 0.2 ) ; fp . buildTree ( allTxns ) ; macrobase . analysis . summary . itemset . List < macrobase . analysis . summary . itemset . result . ItemsetWithCount > itemsets ; macrobase . analysis . summary . itemset . List < macrobase . analysis . summary . itemset . Set < java . lang . Integer > > newBatch = new macrobase . analysis . summary . itemset . ArrayList ( ) ; newBatch . add ( intIfy ( "a,<sp>b,<sp>c,<sp>d,<sp>e" ) ) ; newBatch . add ( intIfy ( "b,<sp>a,<sp>d,<sp>a,<sp>s,<sp>s," ) ) ; newBatch . add ( intIfy ( "d,<sp>a,<sp>t,<sp>t,<sp>h,<sp>i,<sp>n,<sp>g" ) ) ; newBatch . add ( intIfy ( "a,<sp>b,<sp>c,<sp>d,<sp>e" 0 ) ) ; allTxns . addAll ( newBatch ) ; fp . insertTransactionsStreamingExact ( newBatch ) ; itemsets = fp . getItemsets ( ) ; macrobase . analysis . summary . itemset . FPGrowth apriori = new macrobase . analysis . summary . itemset . FPGrowth ( ) ; macrobase . analysis . summary . itemset . List < macrobase . analysis . summary . itemset . result . ItemsetWithCount > apItemsets = apriori . getItemsetsWithSupportRatio ( allTxns , 0.2 ) ; macrobase . analysis . summary . itemset . Set < macrobase . analysis . summary . itemset . Set < java . lang . Integer > > apis = apItemsets . stream ( ) . map ( ( i ) -> i . getItems ( ) ) . collect ( java . util . stream . Collectors . toSet ( ) ) ; macrobase . analysis . summary . itemset . List < macrobase . analysis . summary . itemset . Set < java . lang . Integer > > apil = apItemsets . stream ( ) . map ( ( i ) -> i . getItems ( ) ) . collect ( java . util . stream . Collectors . toList ( ) ) ; macrobase . analysis . summary . itemset . Set < macrobase . analysis . summary . itemset . Set < java . lang . Integer > > dupdetector = new macrobase . analysis . summary . itemset . HashSet ( ) ; for ( macrobase . analysis . summary . itemset . Set < java . lang . Integer > s : apil ) { if ( ! ( dupdetector . add ( s ) ) ) { macrobase . analysis . summary . itemset . StreamingFPGrowthTest . log . warn ( "DUPLICATE<sp>FPTREE<sp>SET<sp>{}" , s ) ; } } org . junit . Assert . assertEquals ( apItemsets . size ( ) , itemsets . size ( ) ) ; } }
public class aTest{ @Test public void testPureNext ( ) { final com . ajjpj . afoundation . collection . ACompositeIterator < java . lang . Integer > iter = new com . ajjpj . afoundation . collection . ACompositeIterator ( java . util . Arrays . asList ( java . util . Arrays . asList ( 1 , 2 , 3 , 4 ) . iterator ( ) , java . util . Arrays . asList ( 5 , 6 , 7 , 8 ) . iterator ( ) , java . util . Arrays . asList ( 9 , 10 ) . iterator ( ) ) ) ; int expected = 1 ; while ( expected <= 10 ) { int elem = iter . next ( ) ; org . junit . Assert . assertEquals ( expected , elem ) ; expected ++ ; } }
public class aTest{ @Test public void multiCatchTest ( ) { try { org . junit . Assert . assertEquals ( 1 , 1 ) ; } }
public class aTest{ @Test public void testSSLConnectionWithoutPool ( ) { java . lang . System . setProperty ( "simple" 7 , "2" ) ; java . lang . System . setProperty ( "simple" 0 , "simple" 1 ) ; java . lang . System . setProperty ( "com.sun.jndi.ldap.connect.pool.debug" , "all" ) ; java . util . Hashtable < java . lang . String , java . lang . String > env = new java . util . Hashtable ( ) ; env . put ( "com.sun.jndi.ldap.connect.pool" , "false" ) ; env . put ( "simple" 4 , "com.sun.jndi.ldap.connect.pool.debug" 0 ) ; env . put ( "java.naming.provider.url" , ( ( "simple" 5 + ( getLdapServer ( ) . getPortSSL ( ) ) ) + "/ou=system" ) ) ; env . put ( "simple" 6 , org . apache . karaf . jaas . modules . ldap . ManagedSSLSocketFactory . class . getName ( ) ) ; env . put ( "simple" 8 , "simple" 1 ) ; env . put ( "java.naming.security.principal" , "simple" 2 ) ; env . put ( "simple" 3 , "simple" 9 ) ; env . put ( "java.naming.security.authentication" , "simple" ) ; final int [ ] socketsCreated = new int [ ] { 0 } ; org . apache . karaf . jaas . modules . ldap . ManagedSSLSocketFactory . setSocketFactory ( new org . apache . karaf . jaas . modules . ldap . ManagedSSLSocketFactory ( sslContext . getSocketFactory ( ) ) { @ org . apache . karaf . jaas . modules . ldap . Override public java . net . Socket createSocket ( java . lang . String host , int port ) throws java . io . IOException { ( socketsCreated [ 0 ] ) ++ ; return super . createSocket ( host , port ) ; } } ) ; javax . naming . directory . InitialDirContext context = new javax . naming . directory . InitialDirContext ( env ) ; context . close ( ) ; new javax . naming . directory . InitialDirContext ( env ) ; context . close ( ) ; org . apache . karaf . jaas . modules . ldap . ManagedSSLSocketFactory . setSocketFactory ( null ) ; org . junit . Assert . assertThat ( socketsCreated [ 0 ] , org . hamcrest . CoreMatchers . equalTo ( 2 ) ) ; } }
public class aTest{ @Test public void testSuccessfulTry ( ) { cliOut . reset ( ) ; final org . jboss . as . cli . CommandContext ctx = org . jboss . as . test . integration . management . util . CLITestUtil . getCommandContext ( cliOut ) ; try { ctx . connectController ( ) ; ctx . handle ( "try" ) ; ctx . handle ( getAddPropertyReq ( "try" ) ) ; ctx . handle ( "catch" ) ; ctx . handle ( getRemovePropertyReq ( ) ) ; ctx . handle ( "end-try" ) ; cliOut . reset ( ) ; ctx . handle ( getReadPropertyReq ( ) ) ; org . junit . Assert . assertEquals ( "try" , getValue ( ) ) ; } }
public class aTest{ @Test public void testCustomPrimaryKey ( ) { this . jdbc . execute ( conn , ( ( ( ( ( ( ( ( ( ( ( "0006" 1 + ( com . gs . obevo . db . impl . core . changetypes . CsvStaticDataDeployerTest . schema ) ) + "." ) + ( com . gs . obevo . db . impl . core . changetypes . CsvStaticDataDeployerTest . table ) ) + "UPDATETIMEFIELD" 5 ) + "AID<sp>INT<sp>NOT<sp>NULL,\n" ) + "0006" 7 ) + "0006" 2 ) + "UPDATETIMEFIELD" 3 ) + "UPDATETIMEFIELD" 1 ) + "UPDATETIMEFIELD<sp>TIMESTAMP<sp>NOT<sp>NULL,<sp>\n" ) + "0006" 3 ) ) ; com . gs . obevo . db . api . appdata . DbEnvironment env = new com . gs . obevo . db . api . appdata . DbEnvironment ( ) ; env . setPlatform ( com . gs . obevo . db . impl . core . changetypes . CsvStaticDataDeployerTest . PLATFORM ) ; env . setNullToken ( "(null)" ) ; env . setDataDelimiter ( '^' ) ; com . gs . obevo . api . appdata . Change artifact = mock ( com . gs . obevo . api . appdata . Change . class ) ; when ( artifact . getPhysicalSchema ( env ) ) . thenReturn ( new com . gs . obevo . api . appdata . PhysicalSchema ( com . gs . obevo . db . impl . core . changetypes . CsvStaticDataDeployerTest . schema ) ) ; when ( artifact . getObjectName ( ) ) . thenReturn ( com . gs . obevo . db . impl . core . changetypes . CsvStaticDataDeployerTest . table ) ; when ( artifact . getMetadataAttribute ( TextMarkupDocumentReader . ATTR_UPDATE_TIME_COLUMN ) ) . thenReturn ( "UPDATETIMEFIELD" ) ; when ( artifact . getMetadataAttribute ( "primaryKeys" ) ) . thenReturn ( "0006" 8 ) ; java . lang . String columnHeaders = "UPDATETIMEFIELD" 0 ; when ( artifact . getConvertedContent ( ) ) . thenReturn ( ( ( ( ( ( ( ( columnHeaders + "UPDATETIMEFIELD" 4 ) + "0006" 5 ) + "2^3^(null)^2013-01-01<sp>11:11:11.65432^9\n" ) + "0006" 0 ) + "0006" 0 ) + "4^4^0006^(null)^9\n" ) + "2^3^(null)^2013-01-01<sp>11:11:11.65432^9\n" ) ) ; org . joda . time . LocalDateTime preDeployTime = new org . joda . time . LocalDateTime ( ) ; com . gs . obevo . db . impl . core . changetypes . CsvStaticDataDeployer csvStaticDataDeployer = new com . gs . obevo . db . impl . core . changetypes . CsvStaticDataDeployer ( env , getSqlExecutor ( ) , this . ds , metadataManager , new com . gs . obevo . db . impl . platforms . h2 . H2DbPlatform ( ) ) ; csvStaticDataDeployer . deployArtifact ( artifact ) ; java . util . List < java . util . Map < java . lang . String , java . lang . Object > > results = this . jdbc . query ( conn , ( ( ( ( "select<sp>*<sp>from<sp>" + ( com . gs . obevo . db . impl . core . changetypes . CsvStaticDataDeployerTest . schema ) ) + "." ) + ( com . gs . obevo . db . impl . core . changetypes . CsvStaticDataDeployerTest . table ) ) + "UPDATETIMEFIELD" 6 ) , new org . apache . commons . dbutils . handlers . MapListHandler ( ) ) ; org . junit . Assert . assertEquals ( 6 , results . size ( ) ) ; this . verifyRow ( results . get ( 0 ) , 1 , 2 , "0006" 9 , new org . joda . time . LocalDateTime ( "0006" 6 ) , null , preDeployTime , true ) ; this . verifyRow ( results . get ( 1 ) , 2 , 3 , null , new org . joda . time . LocalDateTime ( "UPDATETIMEFIELD" 2 ) , 9 , preDeployTime , true ) ; this . verifyRow ( results . get ( 2 ) , 2 , 3 , null , new org . joda . time . LocalDateTime ( "UPDATETIMEFIELD" 2 ) , 9 , preDeployTime , true ) ; this . verifyRow ( results . get ( 3 ) , 3 , 4 , "0006" 4 , null , 9 , preDeployTime , true ) ; this . verifyRow ( results . get ( 4 ) , 3 , 4 , "0006" 4 , null , 9 , preDeployTime , true ) ; this . verifyRow ( results . get ( 5 ) , 4 , 4 , "0006" , null , 9 , preDeployTime , true ) ; } }
public class aTest{ @Test public void testExtractMonthNameDate ( ) { java . lang . String sqlText = ( "select<sp>d,<sp>EXTRACT(MONTHNAME<sp>FROM<sp>d)<sp>as<sp>\"MONTHNAME\"<sp>from<sp>" + ( com . splicemachine . derby . utils . SpliceDateFunctionsIT . tableWatcherI ) ) + "<sp>order<sp>by<sp>d" ; try ( com . splicemachine . derby . utils . ResultSet rs = methodWatcher . executeQuery ( sqlText ) ) { java . lang . String expected = "<sp>from<sp>" 0 + ( ( ( ( ( ( "------------------------\n" + "2009-01-02<sp>|<sp>January<sp>|\n" ) + "2009-07-02<sp>|<sp>July<sp>|\n" ) + "2009-09-02<sp>|<sp>September<sp>|\n" ) + "2012-12-31<sp>|<sp>December<sp>|\n" ) + "2012-12-31<sp>|<sp>December<sp>|\n" ) + "2013-12-31<sp>|<sp>December<sp>|" ) ; org . junit . Assert . assertEquals ( ( ( "\n" + sqlText ) + "\n" ) , expected , TestUtils . FormattedResult . ResultFactory . toStringUnsorted ( rs ) ) ; } } }
public class aTest{ @Test public void jmxConnectorAuthRealmRefValid ( ) { com . sun . enterprise . config . serverbeans . JmxConnector jmxConnector = habitat . getService ( com . sun . enterprise . config . serverbeans . JmxConnector . class , "system" ) ; org . junit . Assert . assertNotNull ( jmxConnector ) ; org . jvnet . hk2 . config . ConfigBean serverConfig = ( ( org . jvnet . hk2 . config . ConfigBean ) ( org . jvnet . hk2 . config . ConfigBean . unwrap ( jmxConnector ) ) ) ; java . util . Map < org . jvnet . hk2 . config . ConfigBean , java . util . Map < java . lang . String , java . lang . String > > changes = new java . util . HashMap < org . jvnet . hk2 . config . ConfigBean , java . util . Map < java . lang . String , java . lang . String > > ( ) ; java . util . Map < java . lang . String , java . lang . String > configChanges = new java . util . HashMap < java . lang . String , java . lang . String > ( ) ; configChanges . put ( "auth-realm-name" , "file" ) ; changes . put ( serverConfig , configChanges ) ; try { org . jvnet . hk2 . config . ConfigSupport cs = getHabitat ( ) . getService ( org . jvnet . hk2 . config . ConfigSupport . class ) ; cs . apply ( changes ) ; } }
public class aTest{ @Test public void testAssignAssetToCustomerFromDifferentTenant ( ) { loginSysAdmin ( ) ; org . thingsboard . server . common . data . Tenant tenant2 = new org . thingsboard . server . common . data . Tenant ( ) ; tenant2 . setTitle ( "Different<sp>tenant" ) ; org . thingsboard . server . common . data . Tenant savedTenant2 = doPost ( "/api/tenant" , tenant2 , org . thingsboard . server . common . data . Tenant . class ) ; org . junit . Assert . assertNotNull ( savedTenant2 ) ; org . thingsboard . server . common . data . User tenantAdmin2 = new org . thingsboard . server . common . data . User ( ) ; tenantAdmin2 . setAuthority ( Authority . TENANT_ADMIN ) ; tenantAdmin2 . setTenantId ( savedTenant2 . getId ( ) ) ; tenantAdmin2 . setEmail ( "My<sp>asset" 0 ) ; tenantAdmin2 . setFirstName ( "My<sp>asset" 1 ) ; tenantAdmin2 . setLastName ( "Downs" ) ; tenantAdmin2 = createUserAndLogin ( tenantAdmin2 , "testPassword1" ) ; org . thingsboard . server . common . data . Customer customer = new org . thingsboard . server . common . data . Customer ( ) ; customer . setTitle ( "My<sp>asset" 3 ) ; org . thingsboard . server . common . data . Customer savedCustomer = doPost ( "/api/customer" , customer , org . thingsboard . server . common . data . Customer . class ) ; login ( tenantAdmin . getEmail ( ) , "testPassword1" ) ; org . thingsboard . server . common . data . asset . Asset asset = new org . thingsboard . server . common . data . asset . Asset ( ) ; asset . setName ( "My<sp>asset" ) ; asset . setType ( "default" ) ; org . thingsboard . server . common . data . asset . Asset savedAsset = doPost ( "/api/asset" , asset , org . thingsboard . server . common . data . asset . Asset . class ) ; doPost ( ( ( ( "/api/customer/" + ( savedCustomer . getId ( ) . getId ( ) . toString ( ) ) ) + "/asset/" ) + ( savedAsset . getId ( ) . getId ( ) . toString ( ) ) ) ) . andExpect ( status ( ) . isForbidden ( ) ) ; loginSysAdmin ( ) ; doDelete ( ( "My<sp>asset" 2 + ( savedTenant2 . getId ( ) . getId ( ) . toString ( ) ) ) ) . andExpect ( status ( ) . isOk ( ) ) ; } }
public class aTest{ @Test public void testAndOperationMultiQueryBuilderTablescan ( ) { com . oberasoftware . jasdb . api . session . DBSession pojoDb = sessionFactory . createSession ( ) ; com . oberasoftware . jasdb . api . session . EntityBag bag = pojoDb . createOrGetBag ( "thosha" ) ; bag . addEntity ( new com . oberasoftware . jasdb . core . SimpleEntity ( com . oberasoftware . jasdb . test . TableScanQueryTest . ID1 ) . addProperty ( "type" , "thing" ) ) ; bag . addEntity ( new com . oberasoftware . jasdb . core . SimpleEntity ( com . oberasoftware . jasdb . test . TableScanQueryTest . ID2 ) . addProperty ( "type" , "thing" ) ) ; bag . addEntity ( new com . oberasoftware . jasdb . core . SimpleEntity ( com . oberasoftware . jasdb . test . TableScanQueryTest . ID3 ) . addProperty ( "type" , "contribution" ) ) ; bag . addEntity ( new com . oberasoftware . jasdb . core . SimpleEntity ( com . oberasoftware . jasdb . test . TableScanQueryTest . ID4 ) . addProperty ( "type" , "contribution" ) ) ; try { com . oberasoftware . jasdb . api . session . query . QueryBuilder builder = com . oberasoftware . jasdb . api . session . query . QueryBuilder . createBuilder ( BlockType . AND ) ; builder . addQueryBlock ( com . oberasoftware . jasdb . api . session . query . QueryBuilder . createBuilder ( ) . field ( "__ID" ) . value ( com . oberasoftware . jasdb . test . TableScanQueryTest . ID3 ) ) ; builder . addQueryBlock ( com . oberasoftware . jasdb . api . session . query . QueryBuilder . createBuilder ( ) . field ( "type" ) . value ( "contribution" ) ) ; com . oberasoftware . jasdb . api . session . query . QueryExecutor executor = bag . find ( builder ) ; java . util . List < com . oberasoftware . jasdb . api . session . Entity > entities = toList ( executor . execute ( ) ) ; org . junit . Assert . assertThat ( entities . size ( ) , org . hamcrest . core . Is . is ( 1 ) ) ; } }
public class aTest{ @Test public void API ( ) { startRealServer ( ) ; try { final org . ng12306 . tpms . NettyIntegrationTest . TestQueryTrainHandler handler = new org . ng12306 . tpms . NettyIntegrationTest . TestQueryTrainHandler ( "G101" ) ; connectToServer ( handler ) ; java . lang . Thread . sleep ( 1000 ) ; org . ng12306 . tpms . TicketQueryResult result = handler . getResponse ( ) ; org . junit . Assert . assertTrue ( result . getHasTicket ( ) ) ; } }
public class aTest{ @Test public void GET200 ( ) { try { com . fujitsu . dc . test . utils . CellUtils . create ( "TestCellForLogNotFound" , com . fujitsu . dc . test . jersey . cell . MASTER_TOKEN_NAME , HttpStatus . SC_CREATED ) ; com . fujitsu . dc . test . utils . TResponse response = com . fujitsu . dc . test . utils . Http . request ( "cell/log-get.txt" ) . with ( "METHOD" , HttpMethod . GET ) . with ( "token" , AbstractCase . MASTER_TOKEN_NAME ) . with ( "cellPath" , "TestCellForLogNotFound" ) . with ( "collection" , com . fujitsu . dc . test . jersey . cell . LogTest . CURRENT_COLLECTION ) . with ( "fileName" , com . fujitsu . dc . test . jersey . cell . LogTest . DEFAULT_LOG ) . with ( "ifNoneMatch" , "*" ) . returns ( ) ; response . debug ( ) ; response . statusCode ( HttpStatus . SC_OK ) ; java . lang . String responseBody = response . getBody ( ) ; org . junit . Assert . assertEquals ( 0 , responseBody . length ( ) ) ; } }
public class aTest{ @Test public void getRoles ( ) { final java . lang . String methodName = "getRoles" ; try { java . util . List < java . lang . String > roles = com . ibm . ws . webcontainer . security . metadata . SecurityConstraintTest . securityConstraint . getRoles ( ) ; org . junit . Assert . assertEquals ( "The<sp>roles<sp>must<sp>be<sp>the<sp>same<sp>as<sp>the<sp>ones<sp>used<sp>in<sp>the<sp>constructor." , com . ibm . ws . webcontainer . security . metadata . SecurityConstraintTest . testRoles , roles ) ; } }
public class aTest{ @Test public void testNoDataSetWithSubQuery ( ) { int [ ] dataType = new int [ ] { org . eclipse . birt . core . data . DataType . DATE_TYPE } ; java . lang . String [ ] name = new java . lang . String [ ] { "testColumn1" } ; org . eclipse . birt . data . engine . api . IResultIterator ri2 = null ; { org . eclipse . birt . data . engine . api . querydefn . ScriptExpression [ ] se = new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression [ ] { new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( "new<sp>Date()" , dataType [ 0 ] ) } ; org . eclipse . birt . data . engine . api . querydefn . QueryDefinition queryDefn = new org . eclipse . birt . data . engine . api . querydefn . QueryDefinition ( ) ; for ( int i = 0 ; i < ( name . length ) ; i ++ ) queryDefn . addBinding ( new org . eclipse . birt . data . engine . api . querydefn . Binding ( name [ i ] , se [ i ] ) ) ; java . lang . String subQueryName = "TEST" ; org . eclipse . birt . data . engine . api . querydefn . SubqueryDefinition subQueryDefn = new org . eclipse . birt . data . engine . api . querydefn . SubqueryDefinition ( subQueryName , queryDefn ) ; for ( int i = 0 ; i < ( name . length ) ; i ++ ) subQueryDefn . addBinding ( new org . eclipse . birt . data . engine . api . querydefn . Binding ( name [ i ] , new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( ( "row._outer." + ( name [ i ] ) ) , dataType [ i ] ) ) ) ; queryDefn . addSubquery ( subQueryDefn ) ; org . eclipse . birt . data . engine . api . DataEngine myDataEngine = new org . eclipse . birt . data . engine . impl . DataEngineImpl ( org . eclipse . birt . data . engine . api . DataEngineContext . newInstance ( DataEngineContext . DIRECT_PRESENTATION , null , null , null ) ) ; org . eclipse . birt . data . engine . api . IResultIterator ri = myDataEngine . prepare ( queryDefn ) . execute ( null ) . getResultIterator ( ) ; ri . next ( ) ; ri2 = ri . getSecondaryIterator ( subQueryName , null ) ; } if ( ri2 . next ( ) ) { java . lang . String str = "" ; for ( int i = 0 ; i < ( name . length ) ; i ++ ) { java . lang . Object value = ri2 . getValue ( name [ i ] ) ; str += value ; if ( i < ( ( name . length ) - 1 ) ) str += ",<sp>" ; if ( ( dataType [ 0 ] ) == ( org . eclipse . birt . core . data . DataType . DATE_TYPE ) ) org . junit . Assert . assertTrue ( value . getClass ( ) . equals ( java . util . Date . class ) ) ; } }
public class aTest{ @Test public void testDiskCache ( ) { java . lang . String cacheDirName = ( tempFolder . newFolder ( ) . getAbsolutePath ( ) ) + "/" ; ucar . nc2 . util . DiskCache2 cache = new ucar . nc2 . util . DiskCache2 ( cacheDirName , false , 0 , 0 ) ; cache . setAlwaysUseCache ( true ) ; org . junit . Assert . assertEquals ( cache . getRootDirectory ( ) , cacheDirName ) ; assert new java . io . File ( cache . getRootDirectory ( ) ) . exists ( ) ; ucar . nc2 . grib . GribIndexCache . setDiskCache2 ( cache ) ; java . lang . String dataDir = ( ucar . unidata . util . test . TestDir . cdmUnitTestDir ) + "testCache" ; java . io . File dd = new java . io . File ( dataDir ) ; for ( java . io . File data : dd . listFiles ( ) ) { java . lang . String name = data . getName ( ) ; if ( name . contains ( ".gbx" ) ) data . delete ( ) ; if ( name . contains ( ".ncx" ) ) data . delete ( ) ; } }
public class aTest{ @Test public void testAddVariableToRootInnerAndListPropertyAndListReference ( ) { java . util . Map < java . lang . String , java . lang . Object > commandMap = new java . util . LinkedHashMap ( ) ; java . util . Map < java . lang . String , java . lang . Object > pParams = new java . util . LinkedHashMap ( ) ; java . util . Map < java . lang . String , java . lang . Object > xxx1Params = new java . util . LinkedHashMap ( ) ; xxx1Params . put ( "galaxy" 0 , "$p.page" ) ; xxx1Params . put ( "worldName" , "galaxy" ) ; pParams . put ( "$XXX1<sp>=<sp>/foo/hello-world" , xxx1Params ) ; java . util . Map < java . lang . String , java . lang . Object > xxx2Params = new java . util . LinkedHashMap ( ) ; xxx2Params . put ( "galaxy" 0 , "$p.page" ) ; xxx2Params . put ( "worldName" , "star" ) ; pParams . put ( "data.$XXX2<sp>=<sp>/foo/hello-world" , xxx2Params ) ; java . util . Map < java . lang . String , java . lang . Object > xxx3Params = new java . util . LinkedHashMap ( ) ; xxx3Params . put ( "galaxy" 0 , "$p.list.id" ) ; xxx3Params . put ( "worldName" , "pulsar" ) ; pParams . put ( "list.$XXX3<sp>=<sp>/foo/hello-world" , xxx3Params ) ; commandMap . put ( "$p<sp>=<sp>/foo/get-foo-data-page" , pParams ) ; java . util . Map < java . lang . String , java . lang . Object > expectedMap = prepareExpectedMap ( true , true , true ) ; java . util . Map < java . lang . String , java . lang . Object > actualMap = invokeAndReturnMap ( commandMap ) ; org . junit . Assert . assertEquals ( expectedMap , actualMap ) ; } }
public class aTest{ @Test public void testNotificationMessageIdString_assertTrue ( ) { new org . nhindirect . common . tx . impl . RESTTxServiceClient_suppressNotificationTest . TestPlan < org . nhindirect . common . tx . model . Tx > ( ) { @ org . nhindirect . common . tx . impl . Override protected boolean suppressNotification ( org . nhindirect . common . tx . model . Tx tx ) throws org . nhindirect . common . tx . impl . Exception { return client . suppressNotification ( tx ) ; } @ org . nhindirect . common . tx . impl . Override protected org . nhindirect . common . tx . model . Tx getNotficationSubmit ( ) throws org . nhindirect . common . tx . impl . Exception { javax . mail . internet . MimeMessage msg = org . nhindirect . common . util . TestUtils . readMimeMessageFromFile ( "MDNMessage.txt" ) ; java . util . Map < java . lang . String , org . nhindirect . common . tx . model . TxDetail > details = new org . nhindirect . common . tx . impl . DefaultTxDetailParser ( ) . getMessageDetails ( msg ) ; org . nhindirect . common . tx . model . Tx tx = new org . nhindirect . common . tx . model . Tx ( org . nhindirect . common . tx . TxUtil . getMessageType ( msg ) , details ) ; return tx ; } protected void doAssertions ( boolean b ) throws org . nhindirect . common . tx . impl . Exception { org . junit . Assert . assertTrue ( b ) ; } } }
public class aTest{ @Test public void testSameSampleDifferentGenoCompare ( ) { System . out . println ( ( ( ( vtc . tools . miscSetOperTests . Compare . GREEN ) + "target/test-classes/OUTPUT/MiscSetOperTests/Compare/out.vcf" 7 ) + ( vtc . tools . miscSetOperTests . Compare . RESET ) ) ) ; java . lang . String in1 = "target/test-classes/MiscSetOperTests/Compare/input1.vcf" ; java . lang . String in2 = "target/test-classes/MiscSetOperTests/Compare/input2.vcf" ; java . lang . String A_acompb = "target/test-classes/OUTPUT/MiscSetOperTests/Compare/out.vcf" 9 ; java . lang . String A_bcompa = "target/test-classes/OUTPUT/MiscSetOperTests/Compare/out.vcf" 6 ; java . lang . String A_intersect = "SO<sp>--compare<sp>-i<sp>" 0 ; java . lang . String A_union = "target/test-classes/MiscSetOperTests/Compare/Answer.union.vcf" ; java . lang . String O_acompb = "target/test-classes/OUTPUT/MiscSetOperTests/Compare/out.vcf" 0 ; java . lang . String O_bcompa = "target/test-classes/OUTPUT/MiscSetOperTests/Compare/out.vcf" 3 ; java . lang . String O_intersect = "target/test-classes/OUTPUT/MiscSetOperTests/Compare/intersect.out.vcf" ; java . lang . String O_union = "target/test-classes/OUTPUT/MiscSetOperTests/Compare/union.out.vcf" ; java . lang . String arguments = ( ( ( ( ( ( "SO<sp>--compare<sp>-i<sp>" + in1 ) + "target/test-classes/OUTPUT/MiscSetOperTests/Compare/out.vcf" 2 ) + in2 ) + "target/test-classes/OUTPUT/MiscSetOperTests/Compare/out.vcf" 4 ) + ( vtc . tools . miscSetOperTests . Compare . hgref ) ) + "target/test-classes/OUTPUT/MiscSetOperTests/Compare/out.vcf" 5 ) + "target/test-classes/OUTPUT/MiscSetOperTests/Compare/out.vcf" ; java . lang . String [ ] args = arguments . split ( "target/test-classes/OUTPUT/MiscSetOperTests/Compare/out.vcf" 2 ) ; vtc . VTCEngine . main ( args ) ; java . io . File output = new java . io . File ( "target/test-classes/OUTPUT/MiscSetOperTests/Compare/" ) ; if ( output . isDirectory ( ) ) { java . lang . String [ ] outputs = output . list ( ) ; org . junit . Assert . assertTrue ( ( ( outputs . length ) == 8 ) ) ; } }
public class aTest{ @Test public void timestampProcedure ( ) { com . gs . collections . api . block . procedure . Procedure < java . sql . Timestamp > procedure = new com . gs . collections . impl . block . procedure . checked . CheckedProcedure < java . sql . Timestamp > ( ) { @ com . gs . collections . impl . block . procedure . checked . Override public void safeValue ( java . sql . Timestamp timestamp ) { org . junit . Assert . assertNotNull ( timestamp . toString ( ) ) ; } } }
public class aTest{ @Test public void testParses ( ) { for ( final java . util . Locale locale : java . util . Locale . getAvailableLocales ( ) ) { for ( final java . util . TimeZone tz : new java . util . TimeZone [ ] { org . apache . commons . lang3 . time . FastDateParserTest . NEW_YORK , org . apache . commons . lang3 . time . FastDateParserTest . GMT } ) { final java . util . Calendar cal = java . util . Calendar . getInstance ( tz ) ; for ( final int year : new int [ ] { 2003 , 1940 , 1868 , 1867 , 0 , - 1940 } ) { if ( ( year < 1868 ) && ( locale . equals ( org . apache . commons . lang3 . time . FastDateParser . JAPANESE_IMPERIAL ) ) ) { continue ; } cal . clear ( ) ; if ( year < 0 ) { cal . set ( ( - year ) , 1 , 10 ) ; cal . set ( Calendar . ERA , GregorianCalendar . BC ) ; } else { cal . set ( year , 1 , 10 ) ; } final java . util . Date in = cal . getTime ( ) ; for ( final java . lang . String format : new java . lang . String [ ] { org . apache . commons . lang3 . time . FastDateParserTest . LONG_FORMAT , org . apache . commons . lang3 . time . FastDateParserTest . SHORT_FORMAT } ) { final java . text . SimpleDateFormat sdf = new java . text . SimpleDateFormat ( format , locale ) ; if ( format . equals ( org . apache . commons . lang3 . time . FastDateParserTest . SHORT_FORMAT ) ) { if ( year < 1930 ) { sdf . set2DigitYearStart ( cal . getTime ( ) ) ; } } final java . lang . String fmt = sdf . format ( in ) ; try { final java . util . Date out = sdf . parse ( fmt ) ; org . junit . Assert . assertEquals ( ( ( ( ( ( ( ( locale . toString ( ) ) + "<sp>" ) + year ) + "<sp>" ) + format ) + "<sp>" ) + ( tz . getID ( ) ) ) , in , out ) ; } }
public class aTest{ @Test public void testDetermination ( ) { try { edu . cmu . tetrad . data . DataSet dataSet = getDataSet ( 1 ) ; edu . cmu . tetrad . search . IndTestGSquare test = new edu . cmu . tetrad . search . IndTestGSquare ( dataSet , 0.05 ) ; edu . cmu . tetrad . graph . Node x = dataSet . getVariable ( "X4" ) ; java . util . ArrayList < edu . cmu . tetrad . graph . Node > z = new java . util . ArrayList ( ) ; test . setDeterminationP ( 0.99 ) ; org . junit . Assert . assertFalse ( test . determines ( z , x ) ) ; } }
public class aTest{ @Test public void shouldAddTermsFromRedisOnZrangeCommandWithCustomRange ( ) { when ( localParamsMock . get ( "command" ) ) . thenReturn ( "zrange" ) ; when ( localParamsMock . get ( "key" ) ) . thenReturn ( "simpleKey" ) ; when ( localParamsMock . get ( "range_start" ) ) . thenReturn ( "1" ) ; when ( localParamsMock . get ( "range_end" ) ) . thenReturn ( "100" ) ; when ( localParamsMock . get ( QueryParsing . V ) ) . thenReturn ( "string_field" ) ; when ( jedisMock . zrangeWithScores ( anyString ( ) , anyLong ( ) , anyLong ( ) ) ) . thenReturn ( new com . sematext . solr . redis . HashSet ( com . sematext . solr . redis . Arrays . asList ( new redis . clients . jedis . Tuple ( "123" , ( ( double ) ( 1.0F ) ) ) , new redis . clients . jedis . Tuple ( "command" 0 , ( ( double ) ( 1.0F ) ) ) ) ) ) ; when ( requestMock . getSchema ( ) ) . thenReturn ( schema ) ; when ( schema . getQueryAnalyzer ( ) ) . thenReturn ( new org . apache . lucene . analysis . standard . StandardAnalyzer ( ) ) ; redisQParser = new com . sematext . solr . redis . RedisQParser ( "string_field" , localParamsMock , paramsMock , requestMock , commandHandler ) ; final org . apache . lucene . search . Query query = redisQParser . parse ( ) ; verify ( jedisMock ) . zrangeWithScores ( "simpleKey" , 1 , 100 ) ; org . apache . lucene . search . IndexSearcher searcher = new org . apache . lucene . search . IndexSearcher ( new org . apache . lucene . index . MultiReader ( ) ) ; final com . sematext . solr . redis . Set < org . apache . lucene . index . Term > terms = com . sematext . solr . redis . TestRedisQParser . extractTerms ( searcher , query ) ; org . junit . Assert . assertEquals ( 2 , terms . size ( ) ) ; } }
public class aTest{ @Test public void testSuperFieldAnnotation_ok ( ) { com . alibaba . asyncload . classinfo . ClassInfoService service = getProxy ( ) ; try { java . lang . reflect . Field field = service . getClass ( ) . getSuperclass ( ) . getDeclaredField ( "ser" ) ; java . lang . annotation . Annotation [ ] as = field . getAnnotations ( ) ; org . junit . Assert . assertEquals ( as . length , 1 ) ; } }
public class aTest{ @Test public void testCalculateTotalQuotaUsage ( ) { org . irods . jargon . core . packinstr . GeneralAdminInp pi = org . irods . jargon . core . packinstr . GeneralAdminInp . instanceForCalculateQuotaUsage ( ) ; java . lang . String tagOut = pi . getParsedTags ( ) ; java . lang . StringBuilder sb = new java . lang . StringBuilder ( ) ; sb . append ( "<arg9></arg9>\n" 1 ) ; sb . append ( "<arg1></arg1>\n" ) ; sb . append ( "<arg2></arg2>\n" ) ; sb . append ( "<arg3></arg3>\n" ) ; sb . append ( "<arg4></arg4>\n" ) ; sb . append ( "<arg5></arg5>\n" ) ; sb . append ( "<arg6></arg6>\n" ) ; sb . append ( "<arg7></arg7>\n" ) ; sb . append ( "<arg8></arg8>\n" ) ; sb . append ( "<arg9></arg9>\n" ) ; sb . append ( "</generalAdminInp_PI>\n" ) ; org . junit . Assert . assertEquals ( "<arg9></arg9>\n" 0 , sb . toString ( ) , tagOut ) ; } }
public class aTest{ @Test public void testSessionExpiryContainer ( ) { org . apache . catalina . startup . Tomcat tomcat = getTomcatInstance ( ) ; org . apache . catalina . Context ctx = tomcat . addContext ( "" , java . lang . System . getProperty ( "java.io.tmpdir" ) ) ; ctx . addApplicationListener ( new org . apache . tomcat . util . descriptor . web . ApplicationListener ( TesterEchoServer . Config . class . getName ( ) , false ) ) ; org . apache . catalina . startup . Tomcat . addServlet ( ctx , "default" , new org . apache . catalina . servlets . DefaultServlet ( ) ) ; ctx . addServletMapping ( "/" , "default" ) ; tomcat . start ( ) ; org . apache . tomcat . websocket . WsWebSocketContainer wsContainer = ( ( org . apache . tomcat . websocket . WsWebSocketContainer ) ( javax . websocket . ContainerProvider . getWebSocketContainer ( ) ) ) ; wsContainer . setDefaultMaxSessionIdleTimeout ( 5000 ) ; wsContainer . setProcessPeriod ( 1 ) ; connectToEchoServer ( wsContainer , org . apache . tomcat . websocket . TestWsWebSocketContainer . EndpointA . class , TesterEchoServer . Config . PATH_BASIC ) ; connectToEchoServer ( wsContainer , org . apache . tomcat . websocket . TestWsWebSocketContainer . EndpointA . class , TesterEchoServer . Config . PATH_BASIC ) ; javax . websocket . Session s3a = connectToEchoServer ( wsContainer , org . apache . tomcat . websocket . TestWsWebSocketContainer . EndpointA . class , TesterEchoServer . Config . PATH_BASIC ) ; java . util . Set < javax . websocket . Session > setA = s3a . getOpenSessions ( ) ; org . junit . Assert . assertEquals ( 3 , setA . size ( ) ) ; int count = 0 ; boolean isOpen = true ; while ( isOpen && ( count < 8 ) ) { count ++ ; java . lang . Thread . sleep ( 1000 ) ; isOpen = false ; for ( javax . websocket . Session session : setA ) { if ( session . isOpen ( ) ) { isOpen = true ; break ; } } } }
public class aTest{ @Test public void testNotDuplicatedLabelForProperty ( ) { es . gob . afirma . ui . wizardcifradoclave . PanelClaveCifradoAccessibilityTest . logger . info ( "testNotDuplicatedLabelForProperty" ) ; try { final es . gob . afirma . ui . wizardcifradoclave . PanelClaveCifrado panelClaveCifrado = new es . gob . afirma . ui . wizardcifradoclave . PanelClaveCifrado ( "" , "" ) ; final java . util . List < java . awt . Component > componentList = new java . util . ArrayList ( ) ; java . util . Set < java . awt . Component > componentSet = null ; final java . awt . Component [ ] components = panelClaveCifrado . getComponents ( ) ; for ( final java . awt . Component componentWizard : components ) { if ( componentWizard instanceof javax . swing . JRootPane ) { final java . awt . Component [ ] componentsRootPane = ( ( javax . swing . JRootPane ) ( componentWizard ) ) . getComponents ( ) ; for ( final java . awt . Component componentRootPane : componentsRootPane ) { if ( componentRootPane instanceof javax . swing . JPanel ) { getLabelForComponentList ( ( ( javax . swing . JPanel ) ( componentRootPane ) ) , componentList ) ; } else if ( componentRootPane instanceof javax . swing . JLayeredPane ) { final java . awt . Component [ ] componentsLayeredPane = ( ( javax . swing . JLayeredPane ) ( componentRootPane ) ) . getComponents ( ) ; for ( final java . awt . Component componentLayeredPane : componentsLayeredPane ) { if ( componentLayeredPane instanceof javax . swing . JPanel ) { getLabelForComponentList ( ( ( javax . swing . JPanel ) ( componentLayeredPane ) ) , componentList ) ; } } } } } } componentSet = new java . util . HashSet ( componentList ) ; org . junit . Assert . assertTrue ( ( ( componentList . size ( ) ) == ( componentSet . size ( ) ) ) ) ; } }
public class aTest{ @Test public void shouldNotDeletePodAfterNonDeletePeriodIfRunStateStillRunning ( java . lang . String ) { final com . spotify . styx . state . RunState . State state = com . spotify . styx . state . RunState . State . valueOf ( stateName ) ; final java . lang . String name = createdPod . getMetadata ( ) . getName ( ) ; when ( k8sClient . pods ( ) . withName ( name ) ) . thenReturn ( namedPod ) ; when ( namedPod . get ( ) ) . thenReturn ( createdPod ) ; createdPod . setStatus ( podStatus ) ; when ( podStatus . getContainerStatuses ( ) ) . thenReturn ( java . util . List . of ( containerStatus , keepaliveContainerStatus ) ) ; when ( containerStatus . getName ( ) ) . thenReturn ( com . spotify . styx . docker . KubernetesDockerRunner . MAIN_CONTAINER_NAME ) ; when ( containerStatus . getState ( ) ) . thenReturn ( containerState ) ; when ( containerState . getTerminated ( ) ) . thenReturn ( containerStateTerminated ) ; when ( containerStateTerminated . getFinishedAt ( ) ) . thenReturn ( com . spotify . styx . docker . KubernetesDockerRunnerTest . FIXED_INSTANT . minus ( java . time . Duration . ofMinutes ( 5 ) ) . toString ( ) ) ; com . spotify . styx . docker . var runState = com . spotify . styx . state . RunState . create ( com . spotify . styx . docker . KubernetesDockerRunnerTest . WORKFLOW_INSTANCE , state ) ; com . spotify . styx . docker . var shouldDelete = kdr . shouldDeletePodWithRunState ( com . spotify . styx . docker . KubernetesDockerRunnerTest . WORKFLOW_INSTANCE , createdPod , runState ) ; org . junit . Assert . assertThat ( shouldDelete , org . hamcrest . Matchers . is ( false ) ) ; } }
public class aTest{ @Test public void testBeanFactory ( ) { org . apache . catalina . startup . Tomcat tomcat = getTomcatInstance ( ) ; tomcat . enableNaming ( ) ; org . apache . catalina . core . StandardContext ctx = ( ( org . apache . catalina . core . StandardContext ) ( tomcat . addContext ( "" , java . lang . System . getProperty ( "java.io.tmpdir" ) ) ) ) ; org . apache . tomcat . util . descriptor . web . ContextResource cr = new org . apache . tomcat . util . descriptor . web . ContextResource ( ) ; cr . setName ( "bug50351" ) ; cr . setType ( "value" 0 ) ; cr . setProperty ( "factory" , "org.apache.naming.factory.BeanFactory" ) ; cr . setProperty ( "foo" , "value" ) ; ctx . getNamingResources ( ) . addResource ( cr ) ; org . apache . naming . resources . TestNamingContext . Bug50351Servlet bug50351Servlet = new org . apache . naming . resources . TestNamingContext . Bug50351Servlet ( ) ; org . apache . catalina . startup . Tomcat . addServlet ( ctx , "bug50351Servlet" , bug50351Servlet ) ; ctx . addServletMapping ( "/" , "bug50351Servlet" ) ; tomcat . start ( ) ; org . apache . tomcat . util . buf . ByteChunk bc = getUrl ( ( ( "http://localhost:" + ( getPort ( ) ) ) + "/" ) ) ; org . junit . Assert . assertEquals ( "value" , bc . toString ( ) ) ; } }
public class aTest{ @Test public void testLeaseExpireEmptyFiles ( ) { final java . lang . Thread . UncaughtExceptionHandler oldUEH = java . lang . Thread . getDefaultUncaughtExceptionHandler ( ) ; java . lang . Thread . setDefaultUncaughtExceptionHandler ( new java . lang . Thread . UncaughtExceptionHandler ( ) { @ org . apache . hadoop . hdfs . Override public void uncaughtException ( java . lang . Thread t , java . lang . Throwable e ) { if ( e instanceof java . util . ConcurrentModificationException ) { LeaseManager . LOG . error ( ( "t=" + t ) , e ) ; isConcurrentModificationException = true ; } } } ) ; System . out . println ( "testLeaseExpireEmptyFiles<sp>start" ) ; final long leasePeriod = 1000 ; final int DATANODE_NUM = 3 ; final org . apache . hadoop . conf . Configuration conf = new org . apache . hadoop . hdfs . HdfsConfiguration ( ) ; conf . setInt ( DFSConfigKeys . DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY , 1000 ) ; conf . setInt ( DFSConfigKeys . DFS_HEARTBEAT_INTERVAL_KEY , 1 ) ; org . apache . hadoop . hdfs . MiniDFSCluster cluster = new org . apache . hadoop . hdfs . MiniDFSCluster . Builder ( conf ) . numDataNodes ( DATANODE_NUM ) . build ( ) ; try { cluster . waitActive ( ) ; org . apache . hadoop . hdfs . DistributedFileSystem dfs = ( ( org . apache . hadoop . hdfs . DistributedFileSystem ) ( cluster . getFileSystem ( ) ) ) ; org . apache . hadoop . hdfs . TestFileCreation . createFile ( dfs , new org . apache . hadoop . fs . Path ( "/foo" ) , DATANODE_NUM ) ; org . apache . hadoop . hdfs . TestFileCreation . createFile ( dfs , new org . apache . hadoop . fs . Path ( "/foo2" ) , DATANODE_NUM ) ; org . apache . hadoop . hdfs . TestFileCreation . createFile ( dfs , new org . apache . hadoop . fs . Path ( "/foo3" ) , DATANODE_NUM ) ; cluster . setLeasePeriod ( leasePeriod , leasePeriod ) ; try { java . lang . Thread . sleep ( ( 5 * leasePeriod ) ) ; } catch ( java . lang . InterruptedException e ) { } org . junit . Assert . assertFalse ( isConcurrentModificationException ) ; } }
public class aTest{ @Test public void testTokenRenew ( ) { com . orientechnologies . orient . core . db . document . ODatabaseDocumentTx db = new com . orientechnologies . orient . core . db . document . ODatabaseDocumentTx ( ( "memory:" + ( com . orientechnologies . orient . server . token . OTokenHandlerImplTest . class . getSimpleName ( ) ) ) ) ; db . create ( ) ; try { com . orientechnologies . orient . core . metadata . security . OSecurityUser original = db . getUser ( ) ; com . orientechnologies . orient . server . token . OTokenHandlerImpl handler = new com . orientechnologies . orient . server . token . OTokenHandlerImpl ( "any<sp>key" . getBytes ( ) , 60 , "HmacSHA256" ) ; com . orientechnologies . orient . server . network . protocol . ONetworkProtocolData data = new com . orientechnologies . orient . server . network . protocol . ONetworkProtocolData ( ) ; data . driverName = "aa" ; data . driverVersion = "aa" ; data . setSerializationImpl ( "a" ) ; data . protocolVersion = 2 ; byte [ ] token = handler . getSignedBinaryToken ( db , original , data ) ; com . orientechnologies . orient . core . metadata . security . OToken tok = handler . parseBinaryToken ( token ) ; tok . setExpiry ( ( ( ( java . lang . System . currentTimeMillis ( ) ) + ( ( handler . getSessionInMills ( ) ) / 2 ) ) - 1 ) ) ; token = handler . renewIfNeeded ( tok ) ; org . junit . Assert . assertTrue ( ( ( token . length ) != 0 ) ) ; } }
public class aTest{ @Test public void testCopartitioning ( ) { final java . util . Random rand = new java . util . Random ( ) ; final org . apache . kafka . clients . producer . internals . DefaultPartitioner defaultPartitioner = new org . apache . kafka . clients . producer . internals . DefaultPartitioner ( ) ; final org . apache . kafka . streams . kstream . internals . WindowedSerializer < java . lang . Integer > timeWindowedSerializer = new org . apache . kafka . streams . kstream . TimeWindowedSerializer ( intSerializer ) ; final org . apache . kafka . streams . kstream . internals . WindowedStreamPartitioner < java . lang . Integer , java . lang . String > streamPartitioner = new org . apache . kafka . streams . kstream . internals . WindowedStreamPartitioner ( timeWindowedSerializer ) ; for ( int k = 0 ; k < 10 ; k ++ ) { final java . lang . Integer key = rand . nextInt ( ) ; final byte [ ] keyBytes = intSerializer . serialize ( topicName , key ) ; final java . lang . String value = key . toString ( ) ; final byte [ ] valueBytes = stringSerializer . serialize ( topicName , value ) ; final java . lang . Integer expected = defaultPartitioner . partition ( "topic" , key , keyBytes , value , valueBytes , cluster ) ; for ( int w = 1 ; w < 10 ; w ++ ) { final org . apache . kafka . streams . kstream . internals . TimeWindow window = new org . apache . kafka . streams . kstream . internals . TimeWindow ( ( 10 * w ) , ( 20 * w ) ) ; final org . apache . kafka . streams . kstream . Windowed < java . lang . Integer > windowedKey = new org . apache . kafka . streams . kstream . Windowed ( key , window ) ; final java . lang . Integer actual = streamPartitioner . partition ( topicName , windowedKey , value , infos . size ( ) ) ; org . junit . Assert . assertEquals ( expected , actual ) ; } } }
public class aTest{ @Test public void OpenConnection ( ) { com . mysema . rdfbean . model . RDFConnection connection = repository . openConnection ( ) ; try { org . junit . Assert . assertNotNull ( connection ) ; } }
public class aTest{ @Test public void testCardinality3 ( ) { java . lang . String grammar = "grammar<sp>T;\n" + ( ( ( ( ( "options<sp>{output=AST;}\n" + "a<sp>:<sp>ID?<sp>INT<sp>-><sp>ID<sp>INT<sp>;\n" ) + "TParser" 1 ) + "ID<sp>:<sp>\'a\'..\'z\'+<sp>;\n" ) + "INT<sp>:<sp>\'0\'..\'9\'+;\n" ) + "WS<sp>:<sp>(\'<sp>\'|\'\\n\')<sp>{$channel=HIDDEN;}<sp>;\n" ) ; execParser ( "TParser" 2 , grammar , "TParser" , "TParser" 0 , "a" , "3" , debug ) ; java . lang . String expecting = "org.antlr.runtime.tree.RewriteEmptyStreamException:<sp>token<sp>ID" ; java . lang . String found = getFirstLineOfException ( ) ; org . junit . Assert . assertEquals ( expecting , found ) ; } }
public class aTest{ @Test public void clusterConfigRefValid ( ) { com . sun . enterprise . config . serverbeans . Cluster cluster = habitat . getService ( com . sun . enterprise . config . serverbeans . Cluster . class , "clusterA" ) ; org . junit . Assert . assertNotNull ( cluster ) ; org . jvnet . hk2 . config . ConfigBean serverConfig = ( ( org . jvnet . hk2 . config . ConfigBean ) ( org . jvnet . hk2 . config . ConfigBean . unwrap ( cluster ) ) ) ; java . util . Map < org . jvnet . hk2 . config . ConfigBean , java . util . Map < java . lang . String , java . lang . String > > changes = new java . util . HashMap < org . jvnet . hk2 . config . ConfigBean , java . util . Map < java . lang . String , java . lang . String > > ( ) ; java . util . Map < java . lang . String , java . lang . String > configChanges = new java . util . HashMap < java . lang . String , java . lang . String > ( ) ; configChanges . put ( "config-ref" , "server-config" ) ; changes . put ( serverConfig , configChanges ) ; try { org . jvnet . hk2 . config . ConfigSupport cs = getHabitat ( ) . getService ( org . jvnet . hk2 . config . ConfigSupport . class ) ; cs . apply ( changes ) ; } }
public class aTest{ @Test public void testIntSorting2 ( ) { java . lang . String str = "global<sp>java.util.List<sp>list\n" + ( ( ( ( ( ( ( ( "rule<sp>R\n" + "dialect<sp>\"mvel\"\n" ) + "when\n" ) + "<sp>$number<sp>:<sp>Number()\n" ) + "<sp>not<sp>Number(intValue<sp>><sp>$number.intValue)\n" ) + "then\n" ) + "when\n" 1 ) + "<sp>delete($number);\n" ) + "when\n" 0 ) ; org . kie . api . KieBase kbase = loadKnowledgeBaseFromString ( str ) ; org . kie . api . runtime . KieSession ksession = kbase . newKieSession ( ) ; java . util . List < java . lang . Integer > list = new java . util . ArrayList ( ) ; ksession . setGlobal ( "list" , list ) ; ksession . insert ( 3 ) ; ksession . insert ( 7 ) ; ksession . insert ( 4 ) ; ksession . insert ( 5 ) ; ksession . insert ( 2 ) ; ksession . insert ( 1 ) ; ksession . insert ( 6 ) ; ksession . fireAllRules ( ) ; org . junit . Assert . assertEquals ( java . util . Arrays . asList ( 7 , 6 , 5 , 4 , 3 , 2 , 1 ) , list ) ; } }
public class aTest{ @Test public void test4 ( ) { System . out . print ( "test4<sp>" ) ; final com . persistit . Exchange ex = _persistit . getExchange ( "persistit" , "TransactionTest1" , true ) ; ex . removeAll ( ) ; final java . lang . StringBuilder sb = new java . lang . StringBuilder ( ) ; for ( int i = 10000 ; i < 15000 ; i ++ ) { sb . append ( "<sp>" ) ; sb . append ( java . lang . Integer . toString ( i ) . substring ( 1 ) ) ; } ex . getValue ( ) . putString ( sb ) ; ex . clear ( ) . append ( "without" ) . store ( ) ; final com . persistit . Transaction txn = ex . getTransaction ( ) ; txn . begin ( ) ; try { ex . clear ( ) . append ( "with" ) . store ( ) ; txn . commit ( ) ; } finally { txn . end ( ) ; } ex . clear ( ) . append ( "with" ) . fetch ( ) ; final java . lang . String with = ex . getValue ( ) . getString ( ) ; ex . clear ( ) . append ( "without" ) . fetch ( ) ; final java . lang . String without = ex . getValue ( ) . getString ( ) ; org . junit . Assert . assertEquals ( with , without ) ; ex . clear ( ) . append ( Key . BEFORE ) ; while ( ex . next ( ) ) { final java . lang . String keyValue = ex . getKey ( ) . reset ( ) . decodeString ( ) ; final java . lang . String valValue = ex . getValue ( ) . getString ( ) ; } }
public class aTest{ @Test public void parentContainsOtherProjectsInHierarchicallayout ( ) { this . config . setProjectLayout ( ProjectLayout . HIERARCHICAL ) ; final java . util . function . Consumer < org . eclipse . xtext . xtext . wizard . ProjectDescriptor > _function = ( org . eclipse . xtext . xtext . wizard . ProjectDescriptor it ) -> { java . lang . String _location = it . getLocation ( ) ; java . lang . String _location_1 = this . config . getParentProject ( ) . getLocation ( ) ; java . lang . String _plus = _location_1 + "/" ; org . junit . Assert . assertTrue ( _location . startsWith ( _plus ) ) ; } }
public class aTest{ @Test public void test_Registration2 ( ) { java . lang . String pattern = "blah" ; java . lang . String expected = ( "Invalid<sp>pattern<sp>'" + pattern ) + "'" ; try { org . eclipse . equinox . http . servlet . ExtendedHttpService extendedHttpService = ( ( org . eclipse . equinox . http . servlet . ExtendedHttpService ) ( getHttpService ( ) ) ) ; extendedHttpService . registerServlet ( pattern , new org . eclipse . equinox . http . servlet . tests . util . BaseServlet ( ) , null , null ) ; } catch ( java . lang . IllegalArgumentException iae ) { org . junit . Assert . assertEquals ( expected , iae . getMessage ( ) ) ; return ; } }
public class aTest{ @Test public void testLexerUnicodeEscapedBMPNotSetWithRange ( ) { org . antlr . v4 . tool . LexerGrammar lg = new org . antlr . v4 . tool . LexerGrammar ( ( "2:RULE_STOP<sp>0\n" 1 + "2:RULE_STOP<sp>0\n" 3 ) ) ; java . lang . String expecting = "max<sp>type<sp>1\n" + ( ( ( ( ( ( ( ( ( ( ( ( "2:RULE_STOP<sp>0\n" 4 + "1:RULE_START<sp>0\n" ) + "2:RULE_STOP<sp>0\n" ) + "3:BASIC<sp>0\n" ) + "4:BASIC<sp>0\n" ) + "rule<sp>0:1<sp>1\n" ) + "mode<sp>0:0\n" ) + "2:RULE_STOP<sp>0\n" 5 ) + "0->1<sp>EPSILON<sp>0,0,0\n" ) + "1->3<sp>EPSILON<sp>0,0,0\n" ) + "2:RULE_STOP<sp>0\n" 0 ) + "2:RULE_STOP<sp>0\n" 2 ) + "0:0\n" ) ; org . antlr . v4 . runtime . atn . ATN atn = createATN ( lg , true ) ; java . lang . String result = org . antlr . v4 . runtime . atn . ATNSerializer . getDecoded ( atn , java . util . Arrays . asList ( lg . getTokenNames ( ) ) ) ; org . junit . Assert . assertEquals ( expecting , result ) ; } }
public class aTest{ @Test public void testTokensConfExceedLimit ( ) { org . apache . hadoop . yarn . server . resourcemanager . security . TestDelegationTokenRenewer . conf . set ( CommonConfigurationKeysPublic . HADOOP_SECURITY_AUTHENTICATION , "kerberos" ) ; org . apache . hadoop . security . UserGroupInformation . setConfiguration ( org . apache . hadoop . yarn . server . resourcemanager . security . TestDelegationTokenRenewer . conf ) ; org . apache . hadoop . yarn . server . resourcemanager . security . TestDelegationTokenRenewer . conf . setInt ( YarnConfiguration . RM_DELEGATION_TOKEN_MAX_CONF_SIZE , 100 ) ; org . apache . hadoop . yarn . server . resourcemanager . MockRM rm = new org . apache . hadoop . yarn . server . resourcemanager . TestRMRestart . TestSecurityMockRM ( org . apache . hadoop . yarn . server . resourcemanager . security . TestDelegationTokenRenewer . conf , null ) ; rm . start ( ) ; final org . apache . hadoop . yarn . server . resourcemanager . MockNM nm1 = new org . apache . hadoop . yarn . server . resourcemanager . MockNM ( "127.0.0.1:1234" , 15120 , rm . getResourceTrackerService ( ) ) ; nm1 . registerNode ( ) ; org . apache . hadoop . io . Text userText1 = new org . apache . hadoop . io . Text ( "user1" ) ; org . apache . hadoop . hdfs . security . token . delegation . DelegationTokenIdentifier dtId1 = new org . apache . hadoop . hdfs . security . token . delegation . DelegationTokenIdentifier ( userText1 , new org . apache . hadoop . io . Text ( "renewer1" ) , userText1 ) ; final org . apache . hadoop . security . token . Token < org . apache . hadoop . hdfs . security . token . delegation . DelegationTokenIdentifier > token1 = new org . apache . hadoop . security . token . Token < org . apache . hadoop . hdfs . security . token . delegation . DelegationTokenIdentifier > ( dtId1 . getBytes ( ) , "service1" 0. getBytes ( ) , dtId1 . getKind ( ) , new org . apache . hadoop . io . Text ( "service1" ) ) ; org . apache . hadoop . security . Credentials credentials = new org . apache . hadoop . security . Credentials ( ) ; credentials . addToken ( userText1 , token1 ) ; org . apache . hadoop . conf . Configuration appConf = new org . apache . hadoop . conf . Configuration ( false ) ; appConf . clear ( ) ; appConf . set ( "dfs.nameservices" , "mycluster1,mycluster2" ) ; appConf . set ( "dfs.namenode.rpc-address.mycluster2.nn1" , "123.0.0.1" ) ; appConf . set ( "dfs.namenode.rpc-address.mycluster3.nn2" , "service1" 1 ) ; org . apache . hadoop . io . DataOutputBuffer dob = new org . apache . hadoop . io . DataOutputBuffer ( ) ; appConf . write ( dob ) ; java . nio . ByteBuffer tokenConf = java . nio . ByteBuffer . wrap ( dob . getData ( ) , 0 , dob . getLength ( ) ) ; try { rm . submitApp ( credentials , tokenConf ) ; org . junit . Assert . fail ( ) ; } catch ( java . lang . Exception e ) { e . printStackTrace ( ) ; org . junit . Assert . assertTrue ( e . getCause ( ) . getMessage ( ) . contains ( YarnConfiguration . RM_DELEGATION_TOKEN_MAX_CONF_SIZE ) ) ; } } }
public class aTest{ @Test public void testGetLength ( ) { java . lang . String mainframeDataset = "test" ; mfDatasetInputSplit . addDataset ( mainframeDataset ) ; try { long retVal = mfDatasetInputSplit . getLength ( ) ; org . junit . Assert . assertEquals ( 1 , retVal ) ; } }
public class aTest{ @Test public void testExecuteSuccess ( ) { com . liferay . portal . kernel . concurrent . ThreadPoolExecutor threadPoolExecutor = new com . liferay . portal . kernel . concurrent . ThreadPoolExecutor ( 1 , 2 , com . liferay . portal . kernel . concurrent . test . TestUtil . KEEPALIVE_TIME , java . util . concurrent . TimeUnit . MILLISECONDS , true , 3 ) ; try { com . liferay . portal . kernel . concurrent . test . MarkerBlockingJob markerBlockingJob = new com . liferay . portal . kernel . concurrent . test . MarkerBlockingJob ( ) ; threadPoolExecutor . execute ( markerBlockingJob ) ; markerBlockingJob . waitUntilEnded ( ) ; org . junit . Assert . assertTrue ( markerBlockingJob . isEnded ( ) ) ; } }
public class aTest{ @Test public void startLdapServer ( ) { javax . naming . directory . DirContext ctx = createContext ( ) ; org . junit . Assert . assertNotNull ( ctx ) ; javax . naming . directory . SearchControls controls = new javax . naming . directory . SearchControls ( ) ; controls . setSearchScope ( SearchControls . SUBTREE_SCOPE ) ; javax . naming . NamingEnumeration < javax . naming . directory . SearchResult > result = ctx . search ( "o=apiman" , "Shutting<sp>down<sp>the<sp>LDAP<sp>server..." 1 , controls ) ; int count = 0 ; while ( result . hasMore ( ) ) { result . next ( ) ; count ++ ; } }
public class aTest{ @Test public void testIterator ( ) { Map . Entry < org . apache . twill . discovery . DiscoveryService , org . apache . twill . discovery . DiscoveryServiceClient > entry = create ( ) ; try { final org . apache . twill . discovery . DiscoveryService service = entry . getKey ( ) ; org . apache . twill . discovery . DiscoveryServiceClient client = entry . getValue ( ) ; final java . lang . String serviceName = "iterator" ; org . apache . twill . discovery . ServiceDiscovered discovered = client . discover ( serviceName ) ; java . lang . Thread t = new java . lang . Thread ( ) { @ org . apache . twill . discovery . Override public void run ( ) { service . register ( new org . apache . twill . discovery . Discoverable ( serviceName , new java . net . InetSocketAddress ( 12345 ) , new byte [ ] { } ) ) ; } } ; java . util . Iterator < org . apache . twill . discovery . Discoverable > iterator = discovered . iterator ( ) ; t . start ( ) ; t . join ( ) ; org . junit . Assert . assertFalse ( iterator . hasNext ( ) ) ; } }
public class aTest{ @Test public void runTest ( ) { final uk . ac . ed . ph . qtiworks . mathassess . glue . maxima . QtiMaximaProcessManager factory = new uk . ac . ed . ph . qtiworks . mathassess . glue . maxima . SimpleQtiMaximaProcessManager ( ) ; final uk . ac . ed . ph . qtiworks . mathassess . glue . maxima . QtiMaximaProcess session = factory . obtainProcess ( ) ; uk . ac . ed . ph . qtiworks . mathassess . glue . types . MathsContentOutputValueWrapper result = null ; try { result = session . executeMathOutput ( inputMaxima , false ) ; org . junit . Assert . assertEquals ( expectedOutputMaxima , result . getMaximaInput ( ) ) ; } }
public class aTest{ @Test public void findProblems ( ) { org . osgi . framework . ServiceReference < com . liferay . blade . api . Migration > sr = _context . getServiceReference ( com . liferay . blade . api . Migration . class ) ; com . liferay . blade . api . Migration m = _context . getService ( sr ) ; java . util . List < com . liferay . blade . api . Problem > problems = m . findProblems ( new java . io . File ( "jsptests/aui-liferay/" ) , new com . liferay . blade . util . NullProgressMonitor ( ) ) ; org . junit . Assert . assertEquals ( "" , 1 , problems . size ( ) ) ; boolean found = false ; for ( com . liferay . blade . api . Problem problem : problems ) { if ( problem . file . getName ( ) . endsWith ( "AUILiferayTagTest.jsp" ) ) { if ( ( ( ( problem . lineNumber ) == 1 ) && ( ( problem . startOffset ) == 16 ) ) && ( ( problem . endOffset ) == 48 ) ) { found = true ; } } } }
public class aTest{ @Test public void testAddAndReadBeyondCapacity ( ) { final tlc2 . util . LongVec vec = getLongVec ( ) ; vec . addElement ( 1L ) ; org . junit . Assert . assertEquals ( 1L , vec . elementAt ( 0 ) ) ; try { vec . elementAt ( 1 ) ; } }
public class aTest{ @Test public void mockLogManager ( mockit . LogManager ) { mockit . LogManager logManager = mockit . LogManager . getLogManager ( ) ; org . junit . Assert . assertSame ( mock , logManager ) ; } }
public class aTest{ @Test public void testZeroStats ( ) { for ( com . indeed . imhotep . local . TestFlamdexFTGSIterator . BitsetOptimizationLevel level : com . indeed . imhotep . local . TestFlamdexFTGSIterator . BitsetOptimizationLevel . values ( ) ) { com . indeed . flamdex . reader . MockFlamdexReader r = new com . indeed . flamdex . reader . MockFlamdexReader ( ) ; r . addIntTerm ( "if1" , 1 , 0 , 1 , 2 ) ; com . indeed . imhotep . local . ImhotepLocalSession session = new com . indeed . imhotep . local . ImhotepLocalSession ( r , ( level == ( com . indeed . imhotep . local . TestFlamdexFTGSIterator . BitsetOptimizationLevel . OPTIMIZE ) ) ) ; com . indeed . imhotep . api . FTGSIterator ftgsIterator = session . getFTGSIterator ( new java . lang . String [ ] { "if1" } , new java . lang . String [ ] { } ) ; try { final long [ ] emptyBuff = new long [ 0 ] ; org . junit . Assert . assertEquals ( true , ftgsIterator . nextField ( ) ) ; ftgsIterator . nextTerm ( ) ; ftgsIterator . group ( ) ; ftgsIterator . groupStats ( emptyBuff ) ; } }
public class aTest{ @Test public void shouldFailWithIncorrectPort ( ) { client . setPort ( "1111" ) ; try { client . getBulkOidBindingList ( OIDConstants . VS_CURRENT_CONNECTIONS ) ; } catch ( java . lang . Exception ex ) { org . junit . Assert . assertTrue ( ( ex instanceof org . openstack . atlas . util . snmp . exceptions . StingraySnmpGeneralException ) ) ; return ; } }
public class aTest{ @Test public void testReadFromWithIllegalMessageNum7 ( ) { target = new org . o3project . odenos . core . component . network . topology . TopologyChanged ( prev , curr , org . o3project . odenos . core . component . network . topology . TopologyChanged . Action . update ) ; org . msgpack . MessagePack msg = new org . msgpack . MessagePack ( ) ; java . io . ByteArrayOutputStream out = new java . io . ByteArrayOutputStream ( ) ; org . msgpack . packer . Packer pk = msg . createPacker ( out ) ; byte [ ] bytes ; java . io . ByteArrayInputStream in ; org . msgpack . unpacker . Unpacker upk = null ; try { pk . writeMapBegin ( 5 ) ; pk . write ( "version" ) ; pk . write ( target . version ) ; pk . write ( "action" ) ; pk . write ( target . action ) ; pk . write ( "prev" ) ; pk . write ( target . prev ) ; pk . write ( "curr" ) ; pk . write ( target . curr ) ; pk . writeMapEnd ( ) ; bytes = out . toByteArray ( ) ; in = new java . io . ByteArrayInputStream ( bytes ) ; upk = msg . createUnpacker ( in ) ; } catch ( java . lang . Exception e ) { org . junit . Assert . fail ( "Exception<sp>in<sp>test<sp>setup" ) ; } target = org . mockito . Mockito . spy ( new org . o3project . odenos . core . component . network . topology . TopologyChanged ( ) ) ; try { target . readFrom ( upk ) ; } catch ( java . lang . Exception e ) { org . junit . Assert . assertTrue ( ( e instanceof java . io . IOException ) ) ; return ; } }
public class aTest{ @Test public void test_getChildren ( ) { if ( SwtTestUtil . isWindows ) { int childCount = composite . getChildren ( ) . length ; java . lang . String msg = "Browser<sp>on<sp>Win32<sp>is<sp>a<sp>special<sp>case,<sp>the<sp>first<sp>child<sp>is<sp>an<sp>OleFrame<sp>(ActiveX<sp>control).<sp>Actual<sp>child<sp>count<sp>is:<sp>" + childCount ; org . junit . Assert . assertTrue ( msg , ( childCount == 1 ) ) ; } }
public class aTest{ @Test public void testAgendaReconciliationAccumulate2 ( ) { java . lang . String str = ( ( ( ( ( ( ( ( "end" 0 + ( org . drools . compiler . Person . class . getCanonicalName ( ) ) ) + ";" ) + "rule<sp>X<sp>when\n" ) + "<sp>accumulate<sp>(<sp>$p:<sp>Person<sp>(<sp>getName().startsWith(\"M\"));<sp>\n" ) + "<sp>$sum<sp>:<sp>sum($p.getAge())<sp>\n" ) + "<sp>)<sp>\n" ) + "then\n" ) + "<sp>insert($sum);\n" ) + "end" ; org . kie . api . KieBase kbase = new org . kie . internal . utils . KieHelper ( ) . addContent ( str , ResourceType . DRL ) . build ( ) ; org . kie . api . runtime . KieSession ksession = null ; try { ksession = kbase . newKieSession ( null , env ) ; ksession . insert ( new org . drools . compiler . Person ( "end" 1 , 37 ) ) ; ksession . insert ( new org . drools . compiler . Person ( "end" 2 , 35 ) ) ; ksession . insert ( new org . drools . compiler . Person ( "Mario" , 40 ) ) ; ksession = org . drools . compiler . integrationtests . SerializationHelper . getSerialisedStatefulKnowledgeSession ( ksession , true ) ; org . junit . Assert . assertEquals ( 1 , ksession . fireAllRules ( ) ) ; } }
public class aTest{ @Test public void testFromArgs ( java . lang . String , java . util . function . Function , T ) { java . lang . String [ ] args = in . split ( "\\s" ) ; com . github . horrorho . liquiddonkey . settings . config . Config config = com . github . horrorho . liquiddonkey . settings . commandline . CommandLineConfigFactory . getInstance ( ) . fromArgs ( args ) ; T value = function . apply ( config ) ; org . junit . Assert . assertThat ( value , org . hamcrest . CoreMatchers . is ( expected ) ) ; } }
public class aTest{ @Test public void testLexerNotSetWithRange2 ( ) { org . antlr . v4 . tool . LexerGrammar lg = new org . antlr . v4 . tool . LexerGrammar ( ( "mode<sp>0:0\n" 4 + "ID<sp>:<sp>~(\'a\'|\'b\')<sp>~(\'e\'|\'p\'..\'t\')\n<sp>;" ) ) ; java . lang . String expecting = "max<sp>type<sp>1\n" + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "mode<sp>0:0\n" 5 + "1:RULE_START<sp>0\n" ) + "2:RULE_STOP<sp>0\n" ) + "mode<sp>0:0\n" 3 ) + "mode<sp>0:0\n" 6 ) + "mode<sp>0:0\n" 8 ) + "rule<sp>0:1<sp>1\n" ) + "mode<sp>0:0\n" ) + "0:\'a\'..\'b\'\n" ) + "mode<sp>0:0\n" 2 ) + "mode<sp>0:0\n" 7 ) + "1->3<sp>EPSILON<sp>0,0,0\n" ) + "mode<sp>0:0\n" 0 ) + "4->5<sp>NOT_SET<sp>1,0,0\n" ) + "5->2<sp>EPSILON<sp>0,0,0\n" ) + "mode<sp>0:0\n" 1 ) ; org . antlr . v4 . runtime . atn . ATN atn = createATN ( lg , true ) ; java . lang . String result = org . antlr . v4 . runtime . atn . ATNSerializer . getDecoded ( atn , java . util . Arrays . asList ( lg . getRuleNames ( ) ) , java . util . Arrays . asList ( lg . getTokenNames ( ) ) ) ; org . junit . Assert . assertEquals ( expecting , result ) ; } }
public class aTest{ @Test public void testSanitize ( ) { java . lang . String base = org . support . project . common . util . FileUtil . read ( getClass ( ) . getResourceAsStream ( "sanitize/sanitize1.txt" ) ) ; java . lang . String check = org . support . project . common . util . FileUtil . read ( getClass ( ) . getResourceAsStream ( "sanitize/result-sanitize1.txt" ) ) ; java . lang . String result = org . support . project . web . logic . SanitizingLogic . get ( ) . sanitize ( base ) ; try { org . junit . Assert . assertArrayEquals ( read ( check ) , read ( result ) ) ; } }
public class aTest{ @Test public void testWithWrongPackage ( ) { com . google . inject . Injector injector = com . google . inject . Guice . createInjector ( de . devsurf . injection . guice . scanner . StartupModule . create ( de . devsurf . injection . guice . scanner . asm . ASMClasspathScanner . class , de . devsurf . injection . guice . scanner . PackageFilter . create ( "java" , false ) ) ) ; org . junit . Assert . assertNotNull ( injector ) ; try { de . devsurf . injection . guice . scanner . asm . tests . autobind . bind . InterfaceAutobindTests . SecondTestInterface testInstance = injector . getInstance ( de . devsurf . injection . guice . scanner . asm . tests . autobind . bind . InterfaceAutobindTests . SecondTestInterface . class ) ; org . junit . Assert . fail ( ( "The<sp>Scanner<sp>scanned<sp>the<sp>wrong<sp>package,<sp>so<sp>no<sp>Implementation<sp>should<sp>be<sp>bound<sp>to<sp>this<sp>Interface.<sp>Instance<sp>null?<sp>" + ( testInstance == null ) ) ) ; } }
public class aTest{ @Test public void testExportOneAppOnApplicationEndpointWQuery ( ) { java . io . File f = null ; try { f = new java . io . File ( "exportOneAppWQuery.json" ) ; } catch ( java . lang . Exception e ) { } f . deleteOnExit ( ) ; org . apache . usergrid . persistence . EntityManager em = org . apache . usergrid . management . export . ExportServiceIT . setup . getEmf ( ) . getEntityManager ( applicationId ) ; org . apache . usergrid . management . export . Map < java . lang . String , java . lang . Object > userProperties = null ; org . apache . usergrid . persistence . Entity [ ] entity ; entity = new org . apache . usergrid . persistence . Entity [ 1 ] ; for ( int i = 0 ; i < 1 ; i ++ ) { userProperties = new org . apache . usergrid . management . export . LinkedHashMap < java . lang . String , java . lang . Object > ( ) ; userProperties . put ( "name" , "me" ) ; userProperties . put ( "username" , "junkRealName" ) ; userProperties . put ( "email" , ( ( "burp" + i ) + "@anuff.com" ) ) ; entity [ i ] = em . create ( "select<sp>*<sp>where<sp>username<sp>=<sp>'junkRealName'" 1 , userProperties ) ; } org . apache . usergrid . management . export . S3Export s3Export = new org . apache . usergrid . management . export . MockS3ExportImpl ( "exportOneAppWQuery.json" ) ; org . apache . usergrid . management . export . ExportService exportService = org . apache . usergrid . management . export . ExportServiceIT . setup . getExportService ( ) ; java . lang . String appName = newOrgAppAdminRule . getApplicationInfo ( ) . getName ( ) ; org . apache . usergrid . management . export . HashMap < java . lang . String , java . lang . Object > payload = payloadBuilder ( appName ) ; payload . put ( "select<sp>*<sp>where<sp>username<sp>=<sp>'junkRealName'" 4 , "select<sp>*<sp>where<sp>username<sp>=<sp>'junkRealName'" ) ; payload . put ( "select<sp>*<sp>where<sp>username<sp>=<sp>'junkRealName'" 0 , organization . getUuid ( ) ) ; payload . put ( "applicationId" , applicationId ) ; org . apache . usergrid . management . export . UUID exportUUID = exportService . schedule ( payload ) ; org . apache . usergrid . persistence . entities . JobData jobData = jobDataCreator ( payload , exportUUID , s3Export ) ; org . apache . usergrid . batch . JobExecution jobExecution = mock ( org . apache . usergrid . batch . JobExecution . class ) ; when ( jobExecution . getJobData ( ) ) . thenReturn ( jobData ) ; org . apache . usergrid . management . export . ExportServiceIT . setup . getEntityIndex ( ) . refresh ( applicationId ) ; exportService . doExport ( jobExecution ) ; com . fasterxml . jackson . core . type . TypeReference < org . apache . usergrid . management . export . HashMap < java . lang . String , java . lang . Object > > typeRef = new com . fasterxml . jackson . core . type . TypeReference < org . apache . usergrid . management . export . HashMap < java . lang . String , java . lang . Object > > ( ) { } ; com . fasterxml . jackson . databind . ObjectMapper mapper = new com . fasterxml . jackson . databind . ObjectMapper ( ) ; org . apache . usergrid . management . export . Map < java . lang . String , java . lang . Object > jsonMap = mapper . readValue ( new java . io . FileReader ( f ) , typeRef ) ; org . apache . usergrid . management . export . Map collectionsMap = ( ( org . apache . usergrid . management . export . Map ) ( jsonMap . get ( "select<sp>*<sp>where<sp>username<sp>=<sp>'junkRealName'" 2 ) ) ) ; java . lang . String collectionName = ( ( java . lang . String ) ( collectionsMap . keySet ( ) . iterator ( ) . next ( ) ) ) ; org . apache . usergrid . management . export . List collection = ( ( org . apache . usergrid . management . export . List ) ( collectionsMap . get ( collectionName ) ) ) ; for ( java . lang . Object o : collection ) { org . apache . usergrid . management . export . Map entityMap = ( ( org . apache . usergrid . management . export . Map ) ( o ) ) ; org . apache . usergrid . management . export . Map metadataMap = ( ( org . apache . usergrid . management . export . Map ) ( entityMap . get ( "select<sp>*<sp>where<sp>username<sp>=<sp>'junkRealName'" 3 ) ) ) ; java . lang . String entityName = ( ( java . lang . String ) ( metadataMap . get ( "name" ) ) ) ; org . junit . Assert . assertFalse ( "junkRealName" . equals ( entityName ) ) ; } } }
public class aTest{ @Test public void testRoute1ProfileEarliestArrival ( ) { final double FROM_LAT = 36.914893 ; final double FROM_LON = - 116.76821 ; final double TO_LAT = 36.914944 ; final double TO_LON = - 116.761472 ; com . graphhopper . GHRequest ghRequest = new com . graphhopper . GHRequest ( FROM_LAT , FROM_LON , TO_LAT , TO_LON ) ; ghRequest . getHints ( ) . put ( Parameters . PT . EARLIEST_DEPARTURE_TIME , java . time . LocalDateTime . of ( 2007 , 1 , 1 , 0 , 0 ) . atZone ( com . graphhopper . GraphHopperGtfsIT . zoneId ) . toInstant ( ) ) ; ghRequest . getHints ( ) . put ( Parameters . PT . PROFILE_QUERY , true ) ; ghRequest . getHints ( ) . put ( Parameters . PT . IGNORE_TRANSFERS , true ) ; ghRequest . getHints ( ) . put ( Parameters . PT . LIMIT_SOLUTIONS , 21 ) ; com . graphhopper . GHResponse response = com . graphhopper . GraphHopperGtfsIT . graphHopper . route ( ghRequest ) ; java . util . List < java . time . LocalTime > actualDepartureTimes = response . getAll ( ) . stream ( ) . map ( ( path ) -> java . time . LocalTime . from ( ( ( com . graphhopper . Trip . PtLeg ) ( path . getLegs ( ) . get ( 0 ) ) ) . getDepartureTime ( ) . toInstant ( ) . atZone ( com . graphhopper . GraphHopperGtfsIT . zoneId ) ) ) . collect ( java . util . stream . Collectors . toList ( ) ) ; java . util . List < java . time . LocalTime > expectedDepartureTimes = java . util . stream . Stream . of ( "12:44" 2 , "07:14" , "10:14" 0 , "08:14" , "12:44" 7 , "12:44" 9 , "09:04" , "12:44" 6 , "09:24" , "09:34" , "12:44" 8 , "12:44" 1 , "12:44" 0 , "10:14" , "10:24" , "10:34" , "12:44" 3 , "12:44" 4 , "12:44" 5 , "12:14" , "12:44" ) . map ( LocalTime :: parse ) . collect ( java . util . stream . Collectors . toList ( ) ) ; org . junit . Assert . assertEquals ( expectedDepartureTimes , actualDepartureTimes ) ; } }
public class aTest{ @Test public void validSessionTest ( ) { final net . violet . platform . datamodel . User theOwner = new net . violet . platform . datamodel . mock . UserMock ( 42 , "myLogin" , "myPassword" , "myEmail@gmail.com" , getFrLang ( ) , "myPassword" 0 , "myFirstName" , "myLastName" , getParisTimezone ( ) , "H" , "Zip" , "Paris" , 1 ) ; final java . util . Date now = new java . util . Date ( ) ; final net . violet . platform . datamodel . Application theApplication = new net . violet . platform . datamodel . mock . ApplicationMock ( 42 , "My<sp>first<sp>application" , getPrivateUser ( ) , now ) ; final net . violet . platform . datamodel . ApplicationCredentials cred = new net . violet . platform . datamodel . mock . ApplicationCredentialsMock ( "6992873d28d86925325dc52d15d6feec30bb2da5" , "myPassword" 1 , theApplication ) ; final net . violet . platform . api . callers . APICaller caller = new net . violet . platform . api . callers . ApplicationAPICaller ( net . violet . platform . dataobjects . ApplicationCredentialsData . getData ( cred ) ) ; final java . util . Calendar theCalendar = java . util . Calendar . getInstance ( ) ; theCalendar . add ( Calendar . HOUR , ( + 1 ) ) ; final java . lang . String sessionId = net . violet . platform . api . authentication . SessionManager . generateSessionId ( caller , net . violet . platform . dataobjects . UserData . getData ( theOwner ) , theCalendar . getTime ( ) ) ; org . junit . Assert . assertTrue ( net . violet . platform . api . authentication . SessionManager . isSessionValid ( sessionId , caller ) ) ; } }
public class aTest{ @Test public void testWardGets ( ) { java . lang . String code = "" ; try { code = _setupTestWard ( false ) ; _checkWardIntoDb ( code ) ; } catch ( java . lang . Exception e ) { e . printStackTrace ( ) ; org . junit . Assert . assertEquals ( true , false ) ; } }
public class aTest{ @Test public void testBuildIsInvokedOnlyOnceWhenManifestChanges ( ) { org . eclipse . core . resources . IProject fooProject = createPluginProject ( "Foo" ) ; build ( ) ; org . eclipse . core . resources . IFile manifestFile = fooProject . getFile ( "META-INF/MANIFEST.MF" ) ; java . lang . String manifestContent = "Manifest-Version:<sp>1.0\n" ; manifestContent += "Bundle-ManifestVersion:<sp>2\n" ; manifestContent += "Bundle-Version:<sp>1.0.0.qualifier\n" 1 ; manifestContent += "Bundle-SymbolicName:<sp>Foo;<sp>" ; manifestContent += "singleton:=true\n" ; manifestContent += "Bundle-Version:<sp>1.0.0.qualifier\n" ; manifestContent += "Require-Bundle:<sp>org.eclipse.xtext,\n" ; manifestContent += "<sp>org.eclipse.core.runtime\n" ; manifestContent += "Bundle-ActivationPolicy:<sp>lazy\n" ; reset ( ) ; manifestFile . setContents ( new org . eclipse . xtext . util . StringInputStream ( manifestContent ) , true , true , monitor ( ) ) ; build ( ) ; org . junit . Assert . assertEquals ( 1 , getInvocationCount ( ) ) ; } }
public class aTest{ @Test public void shouldParseQuery ( ) { try { org . sourceforge . xsparql . rewriter . XSPARQLProcessorTests . logger . debug ( "Parsing<sp>{}" , filename ) ; java . io . BufferedReader is = org . sourceforge . xsparql . test . Utils . loadReaderFromClasspath ( filename ) ; final org . sourceforge . xsparql . rewriter . XSPARQLLexer lexer = new org . sourceforge . xsparql . rewriter . XSPARQLLexer ( is ) ; lexer . setDebug ( true ) ; final org . antlr . runtime . CommonTokenStream tokenStream = new org . antlr . runtime . CommonTokenStream ( lexer ) ; org . sourceforge . xsparql . rewriter . XSPARQLProcessorTests . logger . debug ( "Start<sp>Parser<sp>for<sp>{}" , filename ) ; processor . setDebug ( true ) ; org . antlr . runtime . tree . CommonTree tree = processor . parse ( tokenStream ) ; org . junit . Assert . assertEquals ( 0 , processor . getNumberOfSyntaxErrors ( ) ) ; org . sourceforge . xsparql . rewriter . Helper . printTree ( tree ) ; } }
public class aTest{ @Test public void shouldInvokeActionWithException ( ) { org . swiftexplorer . gui . util . ReflectionActionTest . Target target = new org . swiftexplorer . gui . util . ReflectionActionTest . Target ( true ) ; org . swiftexplorer . gui . util . ReflectionAction < org . swiftexplorer . gui . util . ReflectionActionTest . Target > action = new org . swiftexplorer . gui . util . ReflectionAction < org . swiftexplorer . gui . util . ReflectionActionTest . Target > ( "action" , null , target , "onAction" ) ; try { action . actionPerformed ( new java . awt . event . ActionEvent ( this , 1 , "action" ) ) ; org . junit . Assert . fail ( ) ; } catch ( java . lang . RuntimeException ex ) { org . junit . Assert . assertEquals ( 1 , target . getCnt ( ) ) ; } }
public class aTest{ @Test public void testBorrowTimeoutNoSessionAvailable ( ) { final ch . cyberduck . core . worker . Host host = new ch . cyberduck . core . worker . Host ( new ch . cyberduck . core . worker . TestProtocol ( ) , "localhost" , new ch . cyberduck . core . worker . Credentials ( "u" , "p" ) ) ; final ch . cyberduck . core . transfer . Transfer t = new ch . cyberduck . core . transfer . UploadTransfer ( host , new ch . cyberduck . core . worker . Path ( "/t" , java . util . EnumSet . of ( Path . Type . directory ) ) , new ch . cyberduck . core . worker . NullLocal ( "l" ) ) ; final ch . cyberduck . core . worker . LoginConnectionService connection = new ch . cyberduck . core . worker . TestLoginConnectionService ( ) ; final ch . cyberduck . core . worker . ConcurrentTransferWorker worker = new ch . cyberduck . core . worker . ConcurrentTransferWorker ( new ch . cyberduck . core . pool . DefaultSessionPool ( connection , new ch . cyberduck . core . ssl . DisabledX509TrustManager ( ) , new ch . cyberduck . core . ssl . DefaultX509KeyManager ( ) , new ch . cyberduck . core . vault . DefaultVaultRegistry ( new ch . cyberduck . core . worker . DisabledPasswordCallback ( ) ) , ch . cyberduck . core . worker . PathCache . empty ( ) , new ch . cyberduck . core . worker . DisabledTranscriptListener ( ) , host ) , ch . cyberduck . core . pool . SessionPool . DISCONNECTED , t , new ch . cyberduck . core . transfer . TransferOptions ( ) , new ch . cyberduck . core . transfer . TransferSpeedometer ( t ) , new ch . cyberduck . core . transfer . DisabledTransferPrompt ( ) , new ch . cyberduck . core . transfer . DisabledTransferErrorCallback ( ) , new ch . cyberduck . core . worker . DisabledLoginCallback ( ) , new ch . cyberduck . core . worker . DisabledPasswordCallback ( ) , new ch . cyberduck . core . worker . DisabledProgressListener ( ) , new ch . cyberduck . core . io . DisabledStreamListener ( ) , new ch . cyberduck . core . notification . DisabledNotificationService ( ) ) ; final ch . cyberduck . core . worker . Session < ? > session = worker . borrow ( ConcurrentTransferWorker . Connection . source ) ; org . junit . Assert . assertNotNull ( session ) ; final java . util . concurrent . CyclicBarrier lock = new java . util . concurrent . CyclicBarrier ( 2 ) ; new java . lang . Thread ( new java . lang . Runnable ( ) { @ ch . cyberduck . core . worker . Override public void run ( ) { try { lock . await ( 1 , TimeUnit . MINUTES ) ; } }
public class aTest{ @Test public void test21 ( ) { java . lang . String code = "<sp>a=9\n" 2 + ( ( ( ( ( ( ( ( ( "<sp>a=2\n" + "elif<sp>2*2==4:<sp>\n" ) + "<sp>a=3\n" ) + "elif<sp>2*2+10==40:<sp>\n" ) + "<sp>a=3\n" ) + "<sp>a=9\n" ) + "<sp>a=21\n" ) + "<sp>a=9\n" 1 ) + "<sp>c=123\n" ) + "<sp>d=0xFFAA" ) ; java . lang . String expected = "5<sp>2<sp>ADD<sp>10<sp>LT<sp>NOT<sp>REF_1<sp>JUMPI<sp>2<sp>0<sp>MSTORE<sp>REF_0<sp>JUMP<sp>LABEL_1<sp>4<sp>2<sp>2<sp>MUL<sp>EQ<sp>NOT<sp>REF_2<sp>JUMPI<sp>3<sp>0<sp>MSTORE<sp>REF_0<sp>JUMP<sp>LABEL_2<sp>40<sp>10<sp>2<sp>2<sp>MUL<sp>ADD<sp>EQ<sp>NOT<sp>REF_3<sp>JUMPI<sp>3<sp>0<sp>MSTORE<sp>9<sp>0<sp>MSTORE<sp>21<sp>0<sp>MSTORE<sp>REF_0<sp>JUMP<sp>LABEL_3<sp>123<sp>32<sp>MSTORE<sp>65450<sp>64<sp>MSTORE<sp>LABEL_0" ; org . ethereum . serpent . SerpentParser parser = org . ethereum . serpent . ParserUtils . getParser ( org . ethereum . serpent . SerpentLexer . class , org . ethereum . serpent . SerpentParser . class , code ) ; org . antlr . v4 . runtime . tree . ParseTree tree = parser . parse ( ) ; java . lang . String result = new org . ethereum . serpent . SerpentToAssemblyCompiler ( ) . visit ( tree ) ; result = result . replaceAll ( "<sp>a=9\n" 0 , "<sp>" ) ; result = result . trim ( ) ; org . junit . Assert . assertEquals ( expected , result ) ; } }
public class aTest{ @Test public void testCreateImage ( ) { try { com . fit2cloud . aliyun . ecs . model . request . CreateImageRequest r = new com . fit2cloud . aliyun . ecs . model . request . CreateImageRequest ( "cn-beijing" , "s-25mikyl6y" ) ; com . fit2cloud . aliyun . ecs . model . response . CreateImageResponse response = client . createImage ( r ) ; System . out . println ( ( "testCreateImage<sp>::<sp>" + response ) ) ; org . junit . Assert . assertTrue ( true ) ; } }
public class aTest{ @Test public void shouldFailWithNoSettings ( ) { io . apiman . gateway . platforms . servlet . connectors . HttpConnectorFactory factory = new io . apiman . gateway . platforms . servlet . connectors . HttpConnectorFactory ( config ) ; io . apiman . gateway . engine . IApiConnector connector = factory . createConnector ( request , api , RequiredAuthType . DEFAULT , false , new io . apiman . gateway . platforms . servlet . connectors . ConnectorConfigImpl ( ) ) ; io . apiman . gateway . engine . IApiConnection connection = connector . connect ( request , new io . apiman . gateway . engine . async . IAsyncResultHandler < io . apiman . gateway . engine . IApiConnectionResponse > ( ) { @ io . apiman . gateway . platforms . servlet . auth . tls . Override public void handle ( io . apiman . gateway . engine . async . IAsyncResult < io . apiman . gateway . engine . IApiConnectionResponse > result ) { org . junit . Assert . assertTrue ( result . isError ( ) ) ; System . out . println ( result . getError ( ) ) ; } } }
public class aTest{ @Test public void testWildcard ( ) { org . antlr . v4 . tool . Grammar g = new org . antlr . v4 . tool . Grammar ( ( "parser<sp>grammar<sp>T;\n" + ( "max<sp>type<sp>3\n" 1 + "a<sp>:<sp>.<sp>;" ) ) ) ; java . lang . String expecting = "max<sp>type<sp>3\n" + ( ( ( ( ( ( ( ( "max<sp>type<sp>3\n" 2 + "1:RULE_STOP<sp>0\n" ) + "2:BASIC<sp>0\n" ) + "3:BASIC<sp>0\n" ) + "4:BASIC<sp>0\n" ) + "max<sp>type<sp>3\n" 0 ) + "0->2<sp>EPSILON<sp>0,0,0\n" ) + "2->3<sp>WILDCARD<sp>0,0,0\n" ) + "3->1<sp>EPSILON<sp>0,0,0\n" ) ; org . antlr . v4 . runtime . atn . ATN atn = createATN ( g , true ) ; java . lang . String result = org . antlr . v4 . runtime . atn . ATNSerializer . getDecoded ( atn , java . util . Arrays . asList ( g . getTokenNames ( ) ) ) ; org . junit . Assert . assertEquals ( expecting , result ) ; } }
public class aTest{ @Test public void testSolutionAboveSide ( ) { org . hipparchus . analysis . UnivariateFunction f = new org . hipparchus . analysis . function . Sin ( ) ; org . hipparchus . analysis . solvers . UnivariateSolver solver = getSolver ( ) ; double left = - 1.5 ; double right = 0.05 ; for ( int i = 0 ; i < 10 ; i ++ ) { double solution = getSolution ( solver , 100 , f , left , right , AllowedSolution . ABOVE_SIDE ) ; if ( ! ( java . lang . Double . isNaN ( solution ) ) ) { org . junit . Assert . assertTrue ( ( ( f . value ( solution ) ) >= 0.0 ) ) ; } }
public class aTest{ @Test public void test_Hierarchical_EverythingIsAKey ( ) { org . apache . avro . generic . GenericRecord inputRecord = new org . apache . avro . generic . GenericRecordBuilder ( inputHierarchicalSchema ) . set ( "name" , "testdata" ) . build ( ) ; inputRecord . put ( "testdata" 2 , new org . apache . avro . generic . GenericRecordBuilder ( inputSimpleSchema ) . set ( "testdata" 0 , "testdata" 0 ) . set ( "b" testdata "1b" ) . set ( "c" , "c" ) . build ( ) ) ; java . util . List < java . lang . String > keyList = java . util . Arrays . asList ( "name" , "testdata" 2 ) ; java . lang . String transformedIndexedRecord = ( "{'key':<sp>{'name':<sp>'testdata',<sp>'data':<sp>{'a':<sp>'a',<sp>'b':<sp>'b',<sp>'c':<sp>'c'}},<sp>" + "'value':<sp>{}}" ) . replaceAll ( "\\\'" , "\"" ) ; org . apache . avro . generic . IndexedRecord outputRecord = org . talend . components . adapter . beam . kv . KeyValueUtils . transformToKV ( inputRecord , org . talend . components . adapter . beam . kv . SchemaGeneratorUtils . extractKeyValues ( inputRecord . getSchema ( ) , keyList ) ) ; org . junit . Assert . assertEquals ( transformedIndexedRecord , outputRecord . toString ( ) ) ; org . apache . avro . Schema kvSchema = org . talend . components . adapter . beam . kv . SchemaGeneratorUtils . mergeKeyValues ( outputRecord . getSchema ( ) ) ; java . lang . String mergedRecord = "{\'name\':<sp>\'testdata\',<sp>\'data\':<sp>{\'a\':<sp>\'a\',<sp>\'b\':<sp>\'b\',<sp>\'c\':<sp>\'c\'}}" . replaceAll ( "\\\'" , "\"" ) ; org . junit . Assert . assertEquals ( mergedRecord , org . talend . components . adapter . beam . kv . KeyValueUtils . transformFromKV ( outputRecord , kvSchema ) . toString ( ) ) ; } }
public class aTest{ @Test public void testCompareLongLTEDecimal ( ) { long ts = nextTimestamp ( ) ; com . salesforce . phoenix . end2end . CompareDecimalToLongTest . initTableValues ( null , ts ) ; java . lang . String query = "SELECT<sp>l<sp>FROM<sp>LongInKeyTest<sp>where<sp>l<sp><=<sp>1.5" ; java . util . Properties props = new java . util . Properties ( ) ; props . setProperty ( PhoenixRuntime . CURRENT_SCN_ATTRIB , java . lang . Long . toString ( ( ts + 2 ) ) ) ; com . salesforce . phoenix . end2end . Connection conn = com . salesforce . phoenix . end2end . DriverManager . getConnection ( com . salesforce . phoenix . end2end . PHOENIX_JDBC_URL , props ) ; try { com . salesforce . phoenix . end2end . PreparedStatement statement = conn . prepareStatement ( query ) ; com . salesforce . phoenix . end2end . ResultSet rs = statement . executeQuery ( ) ; org . junit . Assert . assertFalse ( rs . next ( ) ) ; } }
public class aTest{ @Test public void testBug49799 ( ) { java . lang . String [ ] expected = new java . lang . String [ ] { "<p<sp>style=\"color:red\">00-Red</p>" , "<p>01-Not<sp>Red</p>" , "<p<sp>style=\"color:red\"<p>03-Not<sp>Red</p>" 1 , "<p>03-Not<sp>Red</p>" , "<p<sp>style=\"color:red\">04-Red</p>" , "<p>05-Not<sp>Red</p>" } ; org . apache . catalina . startup . Tomcat tomcat = getTomcatInstance ( ) ; java . io . File appDir = new java . io . File ( "test/webapp" ) ; tomcat . addWebapp ( null , "/test" , appDir . getAbsolutePath ( ) ) ; tomcat . start ( ) ; org . apache . tomcat . util . buf . ByteChunk res = new org . apache . tomcat . util . buf . ByteChunk ( ) ; java . util . Map < java . lang . String , java . util . List < java . lang . String > > headers = new java . util . HashMap ( ) ; getUrl ( ( ( "http://localhost:" + ( getPort ( ) ) ) + "/test/bug49nnn/bug49799.jsp" ) , res , headers ) ; java . lang . String result = res . toString ( ) ; java . lang . String [ ] lines = result . split ( "<p>03-Not<sp>Red</p>" 0 ) ; int i = 0 ; for ( java . lang . String line : lines ) { if ( ( line . length ( ) ) > 0 ) { org . junit . Assert . assertEquals ( expected [ i ] , line ) ; i ++ ; } } } }
public class aTest{ @Test public void testMain ( ) { org . wildfly . swarm . bootstrap . modules . BootstrapClasspathModuleFinder finder = new org . wildfly . swarm . bootstrap . modules . BootstrapClasspathModuleFinder ( ) ; try { org . jboss . modules . ModuleSpec spec = finder . findModule ( "classpath.module.load.test" , null ) ; org . junit . Assert . assertNotNull ( spec ) ; } }
public class aTest{ @Test public void testPutAndBulkGetMessages ( ) { com . salesforce . dva . argus . service . MQService service = system . getServiceFactory ( ) . getMQService ( ) ; java . lang . String queueName = createRandomName ( ) ; int count = 1000 ; try { for ( int i = 0 ; i < count ; i ++ ) { service . enqueue ( queueName , ( "Message<sp>" + i ) ) ; } java . util . List < java . lang . String > msg = service . dequeue ( queueName , 10000 , count ) ; org . junit . Assert . assertEquals ( count , msg . size ( ) ) ; } }
public class aTest{ @Test public void testZooKeeperReelectionWithReplacement ( ) { int num = 3 ; int numTries = 30 ; org . apache . flink . runtime . leaderelection . ZooKeeperLeaderElectionService [ ] leaderElectionService = new org . apache . flink . runtime . leaderelection . ZooKeeperLeaderElectionService [ num ] ; org . apache . flink . runtime . leaderelection . TestingContender [ ] contenders = new org . apache . flink . runtime . leaderelection . TestingContender [ num ] ; org . apache . flink . runtime . leaderretrieval . ZooKeeperLeaderRetrievalService leaderRetrievalService = null ; org . apache . flink . runtime . leaderelection . TestingListener listener = new org . apache . flink . runtime . leaderelection . TestingListener ( ) ; try { leaderRetrievalService = org . apache . flink . runtime . util . ZooKeeperUtils . createLeaderRetrievalService ( client , configuration ) ; leaderRetrievalService . start ( listener ) ; for ( int i = 0 ; i < num ; i ++ ) { leaderElectionService [ i ] = org . apache . flink . runtime . util . ZooKeeperUtils . createLeaderElectionService ( client , configuration ) ; contenders [ i ] = new org . apache . flink . runtime . leaderelection . TestingContender ( ( ( ( ( org . apache . flink . runtime . leaderelection . ZooKeeperLeaderElectionTest . TEST_URL ) + "_" ) + i ) + "_0" ) , leaderElectionService [ i ] ) ; leaderElectionService [ i ] . start ( contenders [ i ] ) ; } java . lang . String pattern = ( ( ( ( org . apache . flink . runtime . leaderelection . ZooKeeperLeaderElectionTest . TEST_URL ) + "_" ) + "(\\d+)" ) + "_" ) + "(\\d+)" ; java . util . regex . Pattern regex = java . util . regex . Pattern . compile ( pattern ) ; for ( int i = 0 ; i < numTries ; i ++ ) { listener . waitForNewLeader ( org . apache . flink . runtime . leaderelection . ZooKeeperLeaderElectionTest . timeout ) ; java . lang . String address = listener . getAddress ( ) ; java . util . regex . Matcher m = regex . matcher ( address ) ; if ( m . find ( ) ) { int index = java . lang . Integer . parseInt ( m . group ( 1 ) ) ; int lastTry = java . lang . Integer . parseInt ( m . group ( 2 ) ) ; org . junit . Assert . assertEquals ( listener . getLeaderSessionID ( ) , contenders [ index ] . getLeaderSessionID ( ) ) ; leaderElectionService [ index ] . stop ( ) ; leaderElectionService [ index ] = org . apache . flink . runtime . util . ZooKeeperUtils . createLeaderElectionService ( client , configuration ) ; contenders [ index ] = new org . apache . flink . runtime . leaderelection . TestingContender ( ( ( ( ( ( org . apache . flink . runtime . leaderelection . ZooKeeperLeaderElectionTest . TEST_URL ) + "_" ) + index ) + "_" ) + ( lastTry + 1 ) ) , leaderElectionService [ index ] ) ; leaderElectionService [ index ] . start ( contenders [ index ] ) ; } }
public class aTest{ @Test public void testMapReduce ( ) { final com . allanbank . mongodb . MongoCollection mr = myDb . getCollection ( "count" 0 ) ; final com . allanbank . mongodb . bson . builder . DocumentBuilder doc1 = com . allanbank . mongodb . bson . builder . BuilderFactory . start ( ) ; doc1 . addInteger ( "_id" , 1 ) ; doc1 . pushArray ( "value" 8 ) . addString ( "dog" ) . addString ( "value" 9 ) ; final com . allanbank . mongodb . bson . builder . DocumentBuilder doc2 = com . allanbank . mongodb . bson . builder . BuilderFactory . start ( ) ; doc2 . addInteger ( "_id" , 2 ) ; doc2 . pushArray ( "value" 8 ) . addString ( "value" 9 ) ; final com . allanbank . mongodb . bson . builder . DocumentBuilder doc3 = com . allanbank . mongodb . bson . builder . BuilderFactory . start ( ) ; doc3 . addInteger ( "_id" , 3 ) ; doc3 . pushArray ( "value" 8 ) . addString ( "value" 7 ) . addString ( "dog" ) . addString ( "value" 9 ) ; final com . allanbank . mongodb . bson . builder . DocumentBuilder doc4 = com . allanbank . mongodb . bson . builder . BuilderFactory . start ( ) ; doc4 . addInteger ( "_id" , 4 ) ; doc4 . pushArray ( "value" 8 ) ; myConfig . setDefaultDurability ( Durability . ACK ) ; mr . insert ( doc1 . build ( ) , doc2 . build ( ) , doc3 . build ( ) , doc4 . build ( ) ) ; final com . allanbank . mongodb . builder . MapReduce . Builder mrBuilder = new com . allanbank . mongodb . builder . MapReduce . Builder ( ) ; mrBuilder . setMapFunction ( ( "value" 3 + ( ( ( ( ( "<sp>this.tags.forEach(<sp>" + "value" 4 ) + "count" 1 ) + "<sp>}<sp>" ) + "<sp>);<sp>" ) + "value" 2 ) ) ) ; mrBuilder . setReduceFunction ( ( "function(<sp>key<sp>,<sp>values<sp>){<sp>" + ( ( ( ( ( "<sp>var<sp>total<sp>=<sp>0;<sp>" + "value" 5 ) + "value" 0 ) + "<sp>}<sp>" ) + "value" 1 ) + "value" 2 ) ) ) ; mrBuilder . setOutputName ( "value" 6 ) ; mrBuilder . setOutputType ( MapReduce . OutputType . REPLACE ) ; mr . mapReduce ( mrBuilder . build ( ) ) ; final com . allanbank . mongodb . bson . builder . DocumentBuilder expected1 = com . allanbank . mongodb . bson . builder . BuilderFactory . start ( ) ; expected1 . addString ( "_id" , "value" 9 ) ; expected1 . push ( "value" ) . addDouble ( "count" , 3 ) ; final com . allanbank . mongodb . bson . builder . DocumentBuilder expected2 = com . allanbank . mongodb . bson . builder . BuilderFactory . start ( ) ; expected2 . addString ( "_id" , "dog" ) ; expected2 . push ( "value" ) . addDouble ( "count" , 2 ) ; final com . allanbank . mongodb . bson . builder . DocumentBuilder expected3 = com . allanbank . mongodb . bson . builder . BuilderFactory . start ( ) ; expected3 . addString ( "_id" , "value" 7 ) ; expected3 . push ( "value" ) . addDouble ( "count" , 1 ) ; final java . util . Set < com . allanbank . mongodb . bson . Document > expected = new java . util . HashSet < com . allanbank . mongodb . bson . Document > ( ) ; expected . add ( expected1 . build ( ) ) ; expected . add ( expected2 . build ( ) ) ; expected . add ( expected3 . build ( ) ) ; final java . util . Set < com . allanbank . mongodb . bson . Document > actual = new java . util . HashSet < com . allanbank . mongodb . bson . Document > ( ) ; final com . allanbank . mongodb . MongoCollection out = myDb . getCollection ( "value" 6 ) ; for ( final com . allanbank . mongodb . bson . Document doc : out . find ( MongoCollection . ALL ) ) { actual . add ( doc ) ; } org . junit . Assert . assertEquals ( expected , actual ) ; } }
public class aTest{ @Test public void testEventSource ( ) { final java . util . concurrent . CountDownLatch latch = new java . util . concurrent . CountDownLatch ( com . examples . sse . SsePubSubTest . testCount ) ; final org . glassfish . jersey . media . sse . EventSource eventSource = new org . glassfish . jersey . media . sse . EventSource ( target ( ) . path ( com . examples . sse . SsePubSubTest . ROOT_PATH ) ) { private int i ; @ com . examples . sse . Override public void onEvent ( org . glassfish . jersey . media . sse . InboundEvent inboundEvent ) { try { java . lang . String data = inboundEvent . readData ( java . lang . String . class ) ; com . examples . sse . SsePubSubTest . log . info ( "What<sp>the<sp>server<sp>response:<sp>{}:{}:{}" , inboundEvent . getId ( ) , inboundEvent . getName ( ) , data ) ; org . junit . Assert . assertEquals ( ( ( com . examples . sse . SsePubSubTest . messagePrefix ) + ( ( i ) ++ ) ) , data ) ; latch . countDown ( ) ; } }
public class aTest{ @Test public void testSecond ( ) { org . apache . cayenne . exp . Expression exp = org . apache . cayenne . exp . ExpressionFactory . exp ( "second(timestampColumn)<sp>=<sp>39" ) ; try { long res = org . apache . cayenne . query . ObjectSelect . query ( org . apache . cayenne . testdo . date_time . DateTestEntity . class , exp ) . selectCount ( context ) ; org . junit . Assert . assertEquals ( 1 , res ) ; } }
public class aTest{ @Test public void testSendHEADRequestSendsExtraHeaders ( ) { server . setHandler ( new org . eclipse . jetty . server . handler . DefaultHandler ( ) { @ edu . illinois . library . cantaloupe . source . Override public void handle ( java . lang . String target , org . eclipse . jetty . server . Request baseRequest , javax . servlet . http . HttpServletRequest request , javax . servlet . http . HttpServletResponse response ) { org . junit . Assert . assertEquals ( "yes" , baseRequest . getHeader ( "X-Cats" ) ) ; baseRequest . setHandled ( true ) ; } } }
public class aTest{ @Test public void test_Not_Repair_Math28 ( ) { int maxSolutions = 4 ; java . io . File filef = new java . io . File ( "src/test/resources/changes_analisis_frequency.json" ) ; org . junit . Assert . assertTrue ( filef . exists ( ) ) ; fr . inria . main . CommandSummary command = fr . inria . astor . test . repair . evaluation . regression . MathCommandsTests . getMath28Command ( ) ; command . command . put ( "custom" 1 , "custom" ) ; command . command . put ( "-maxgen" , "custom" 2 ) ; command . command . put ( "-loglevel" , "DEBUG" ) ; command . command . put ( "-scope" , "local" ) ; command . command . put ( "-stopfirst" , "custom" 5 ) ; command . command . put ( "custom" 0 , "0.01" ) ; command . command . put ( "custom" 6 , ( ( ( ( ( ( ( "clustercollectedvalues:true:disablelog:false:maxnumbersolutions:" + maxSolutions ) + "custom" 3 ) + ( filef . getAbsolutePath ( ) ) ) + "custom" 8 ) + ( fr . inria . astor . approaches . tos . core . evalTos . navigation . UpdateParentDiffOrderFromJSON . class . getName ( ) ) ) + "custom" 4 ) + ( fr . inria . astor . approaches . tos . core . evalTos . EvalTOSClusterApproach . class . getCanonicalName ( ) ) ) ) ; this . executeAndAssert ( command , 144 , "custom" 7 , null ) ; } }
public class aTest{ @Test public void testReadConsistencyLevel ( ) { int levelsChecked = 0 ; for ( com . thinkaurelius . titan . diskstorage . cassandra . CLevel writeLevel : com . thinkaurelius . titan . diskstorage . cassandra . CLevel . values ( ) ) { com . thinkaurelius . titan . diskstorage . util . StandardBaseTransactionConfig . Builder b = new com . thinkaurelius . titan . diskstorage . util . StandardBaseTransactionConfig . Builder ( ) ; com . thinkaurelius . titan . diskstorage . configuration . ModifiableConfiguration mc = com . thinkaurelius . titan . graphdb . configuration . GraphDatabaseConfiguration . buildGraphConfiguration ( ) ; mc . set ( com . thinkaurelius . titan . diskstorage . cassandra . AbstractCassandraStoreManager . CASSANDRA_READ_CONSISTENCY , writeLevel . name ( ) ) ; b . timestampProvider ( TimestampProviders . MICRO ) ; b . customOptions ( mc ) ; com . thinkaurelius . titan . diskstorage . cassandra . CassandraTransaction ct = new com . thinkaurelius . titan . diskstorage . cassandra . CassandraTransaction ( b . build ( ) ) ; org . junit . Assert . assertEquals ( writeLevel , ct . getReadConsistencyLevel ( ) ) ; levelsChecked ++ ; } }
public class aTest{ @Test public void testReadWriteAttributeListExpressionsStandalone ( ) { javax . management . MBeanServerConnection connection = setupAndGetConnection ( new org . jboss . as . jmx . ModelControllerMBeanTestCase . MBeanInfoAdditionalInitialization ( org . jboss . as . controller . ProcessType . STANDALONE_SERVER , new org . jboss . as . jmx . TestExtension ( true ) ) ) ; javax . management . ObjectName name = org . jboss . as . jmx . ModelControllerMBeanTestCase . createObjectName ( ( ( org . jboss . as . jmx . ModelControllerMBeanTestCase . EXPR_DOMAIN ) + "roInt" 7 ) ) ; java . lang . String [ ] attrNames = new java . lang . String [ ] { "roInt" , "bigdec" 0 , "${should.not.exist!!!!!:true}" 8 , "bigdec" , "${should.not.exist!!!!!:true}" 1 , "${should.not.exist!!!!!:true}" 6 , "${should.not.exist!!!!!:true}" 4 , "string" , "roInt" 0 , "long" , "type" } ; javax . management . AttributeList list = connection . getAttributes ( name , attrNames ) ; org . junit . Assert . assertEquals ( list . size ( ) , attrNames . length ) ; checkAttributeList ( attrNames , list , "roInt" 1 , "2" , "${should.not.exist!!!!!:true}" 9 , "roInt" 4 , "${should.not.exist!!!!!:true}" 0 , new byte [ ] { 5 , 6 } , "${should.not.exist!!!!!:true}" 7 , "8" , java . util . Collections . singletonList ( "9" ) , "roInt" 2 , "roInt" 6 ) ; list = new javax . management . AttributeList ( ) ; list . add ( new javax . management . Attribute ( "bigdec" 0 , "roInt" 8 ) ) ; list . add ( new javax . management . Attribute ( "${should.not.exist!!!!!:true}" 8 , "${should.not.exist!!!!!:true}" 3 ) ) ; list . add ( new javax . management . Attribute ( "bigdec" , "roInt" 9 ) ) ; list . add ( new javax . management . Attribute ( "${should.not.exist!!!!!:true}" 1 , "${should.not.exist!!!!!:true}" ) ) ; list . add ( new javax . management . Attribute ( "${should.not.exist!!!!!:true}" 6 , new byte [ ] { 105 , 106 } ) ) ; list . add ( new javax . management . Attribute ( "${should.not.exist!!!!!:true}" 4 , "roInt" 3 ) ) ; list . add ( new javax . management . Attribute ( "string" , "${should.not.exist!!!!!:true}" 5 ) ) ; list . add ( new javax . management . Attribute ( "roInt" 0 , new java . lang . String [ ] { "roInt" 5 } ) ) ; list . add ( new javax . management . Attribute ( "long" , "${should.not.exist!!!!!:110L}" ) ) ; list . add ( new javax . management . Attribute ( "type" , "${should.not.exist!!!!!:true}" 2 ) ) ; connection . setAttributes ( name , list ) ; list = connection . getAttributes ( name , attrNames ) ; checkAttributeList ( attrNames , list , "roInt" 1 , "roInt" 8 , "${should.not.exist!!!!!:true}" 3 , "roInt" 9 , "${should.not.exist!!!!!:true}" , new byte [ ] { 105 , 106 } , "roInt" 3 , "${should.not.exist!!!!!:true}" 5 , java . util . Collections . singletonList ( "roInt" 5 ) , "${should.not.exist!!!!!:110L}" , "${should.not.exist!!!!!:true}" 2 ) ; } }
public class aTest{ @Test public void mockClassExtendingAGenericBaseClassHavingTypeArgumentOfArrayType ( mockit . GenericMockedTypesTest$DerivedClass ) { java . lang . Number [ ] result = mock . doSomething ( ) ; org . junit . Assert . assertEquals ( 0 , result . length ) ; } }
public class aTest{ @Test public void testSkipCommitItem ( ) { final org . tmatesoft . svn . test . TestOptions options = org . tmatesoft . svn . test . TestOptions . getInstance ( ) ; final org . tmatesoft . svn . core . wc2 . SvnOperationFactory svnOperationFactory = new org . tmatesoft . svn . core . wc2 . SvnOperationFactory ( ) ; final org . tmatesoft . svn . test . Sandbox sandbox = org . tmatesoft . svn . test . Sandbox . createWithCleanup ( ( ( getTestName ( ) ) + ".testSkipCommitItem" ) , options ) ; try { final org . tmatesoft . svn . core . SVNURL url = sandbox . createSvnRepository ( ) ; final org . tmatesoft . svn . test . WorkingCopy workingCopy = sandbox . checkoutNewWorkingCopy ( url ) ; final java . io . File directory = workingCopy . getFile ( "directory" ) ; final java . io . File file1 = new java . io . File ( directory , "file1" ) ; final java . io . File file2 = new java . io . File ( directory , "file2" ) ; org . tmatesoft . svn . core . internal . wc . SVNFileUtil . ensureDirectoryExists ( directory ) ; org . tmatesoft . svn . test . TestUtil . writeFileContentsString ( file1 , "contents" ) ; org . tmatesoft . svn . test . TestUtil . writeFileContentsString ( file2 , "contents" ) ; final org . tmatesoft . svn . core . wc2 . SvnScheduleForAddition scheduleForAddition = svnOperationFactory . createScheduleForAddition ( ) ; scheduleForAddition . addTarget ( org . tmatesoft . svn . core . wc2 . SvnTarget . fromFile ( directory ) ) ; scheduleForAddition . addTarget ( org . tmatesoft . svn . core . wc2 . SvnTarget . fromFile ( file1 ) ) ; scheduleForAddition . addTarget ( org . tmatesoft . svn . core . wc2 . SvnTarget . fromFile ( file2 ) ) ; scheduleForAddition . run ( ) ; final org . tmatesoft . svn . core . wc . SVNClientManager clientManager = org . tmatesoft . svn . core . wc . SVNClientManager . newInstance ( ) ; try { final org . tmatesoft . svn . core . wc . SVNCommitClient commitClient = clientManager . getCommitClient ( ) ; commitClient . setCommitHandler ( new org . tmatesoft . svn . core . wc . DefaultSVNCommitHandler ( ) ) ; final org . tmatesoft . svn . core . wc . SVNCommitPacket commitPacket = commitClient . doCollectCommitItems ( new java . io . File [ ] { workingCopy . getWorkingCopyDirectory ( ) } , false , true , SVNDepth . INFINITY , null ) ; for ( org . tmatesoft . svn . core . wc . SVNCommitItem commitItem : commitPacket . getCommitItems ( ) ) { if ( commitItem . getFile ( ) . equals ( file2 ) ) { commitPacket . setCommitItemSkipped ( commitItem , true ) ; } } commitClient . doCommit ( commitPacket , true , "" ) ; } finally { clientManager . dispose ( ) ; } final org . tmatesoft . svn . core . wc2 . SvnLog log = svnOperationFactory . createLog ( ) ; log . addRange ( org . tmatesoft . svn . core . wc2 . SvnRevisionRange . create ( org . tmatesoft . svn . core . wc . SVNRevision . create ( 1 ) , SVNRevision . HEAD ) ) ; log . setSingleTarget ( org . tmatesoft . svn . core . wc2 . SvnTarget . fromURL ( url ) ) ; log . setDiscoverChangedPaths ( true ) ; final org . tmatesoft . svn . core . SVNLogEntry logEntry = log . run ( ) ; final java . util . Map < java . lang . String , org . tmatesoft . svn . core . SVNLogEntryPath > changedPaths = logEntry . getChangedPaths ( ) ; final org . tmatesoft . svn . core . SVNLogEntryPath logEntryPath = changedPaths . get ( "/directory/file2" ) ; org . junit . Assert . assertNull ( logEntryPath ) ; } }
public class aTest{ @Test public void test0Byte ( ) { final int N = 10 ; final long X = 123L ; com . geophile . erdo . map . diskmap . CompressibleLongArray a = new com . geophile . erdo . map . diskmap . CompressibleLongArray ( N ) ; for ( int i = 0 ; i < N ; i ++ ) { a . append ( X ) ; } a . close ( ) ; org . junit . Assert . assertEquals ( 0 , a . deltaBytes ( ) ) ; try { a . deltas1Byte ( ) ; } }
public class aTest{ @Test public void testCreateDispatchBadPort ( ) { java . net . URL wsdl1 = getClass ( ) . getResource ( "/wsdl/calculator.wsdl" ) ; org . junit . Assert . assertNotNull ( wsdl1 ) ; org . apache . cxf . jaxws . ServiceImpl service = new org . apache . cxf . jaxws . ServiceImpl ( getBus ( ) , wsdl1 , org . apache . cxf . jaxws . ServiceImplTest . SERVICE_1 , org . apache . cxf . jaxws . ServiceImpl . class ) ; javax . xml . namespace . QName badPort = new javax . xml . namespace . QName ( "http://apache.org/cxf/calculator" , "PortDoesNotExist" ) ; try { service . createDispatch ( badPort , javax . xml . transform . Source . class , Service . Mode . PAYLOAD ) ; } }
public class aTest{ @Test public void shouldUseDocumentationTitleAsDocTitleAttribute ( ) { java . util . List < com . github . cukedoctor . api . model . Feature > features = new java . util . ArrayList ( ) ; features . add ( com . github . cukedoctor . util . builder . FeatureBuilder . instance ( ) . id ( ":toc:<sp>right" 1 ) . name ( ":toc:<sp>right" 2 ) . build ( ) ) ; com . github . cukedoctor . api . DocumentAttributes attrs = new com . github . cukedoctor . api . DocumentAttributes ( ) ; attrs . toc ( ":toc:<sp>right" 4 ) . backend ( "html5" ) . docType ( "book" ) . linkCss ( true ) . icons ( ":chapter-label:<sp>Chapter" 0 ) . numbered ( false ) . sectAnchors ( true ) . sectLink ( true ) . chapterLabel ( ":toc:<sp>right" 5 ) . versionLabel ( "Version" ) ; java . lang . String expected = ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ":toc:<sp>right" + ( newLine ( ) ) ) + ":backend:<sp>html5" ) + ( newLine ( ) ) ) + ":toc:<sp>right" 3 ) + ( newLine ( ) ) ) + ":doctype:<sp>book" ) + ( newLine ( ) ) ) + ":toc:<sp>right" 0 ) + ( newLine ( ) ) ) + ":!numbered:" ) + ( newLine ( ) ) ) + ":linkcss:" ) + ( newLine ( ) ) ) + ":sectanchors:" ) + ( newLine ( ) ) ) + ":toc:<sp>right" 7 ) + ( newLine ( ) ) ) + ":toc:<sp>right" 9 ) + ( newLine ( ) ) ) + ":chapter-label:<sp>Chapter" 1 ) + ( newLine ( ) ) ) + ":chapter-label:<sp>Chapter" ) + ( newLine ( ) ) ) + ":toc:<sp>right" 8 ) + ( newLine ( ) ) ; attrs . docTitle ( ":toc:<sp>right" 6 ) ; java . lang . String document = com . github . cukedoctor . Cukedoctor . instance ( features , attrs ) . renderAttributes ( ) . getDocumentation ( ) . toString ( ) ; org . junit . Assert . assertEquals ( document , expected ) ; } }
public class aTest{ @Test public void testGetFailed ( ) { final java . lang . String topic = "topic1" ; final int maxSize = 1024 ; final com . taobao . metamorphosis . cluster . Partition partition = new com . taobao . metamorphosis . cluster . Partition ( "0-0" ) ; final long offset = 12 ; final byte [ ] data = "hello" . getBytes ( ) ; final java . lang . String url = "meta://localhost:0" ; final com . taobao . metamorphosis . consumer . MessageIterator messageIterator = new com . taobao . metamorphosis . consumer . MessageIterator ( topic , data ) ; this . producerZooKeeper . publishTopic ( topic , this . consumer ) ; org . easymock . classextension . EasyMock . expectLastCall ( ) ; org . easymock . classextension . EasyMock . expect ( this . remotingClient . isConnected ( url ) ) . andReturn ( true ) ; org . easymock . classextension . EasyMock . expect ( this . producerZooKeeper . selectBroker ( topic , partition ) ) . andReturn ( url ) ; org . easymock . classextension . EasyMock . expect ( this . remotingClient . invokeToGroup ( url , new com . taobao . metamorphosis . network . GetCommand ( topic , this . consumerConfig . getGroup ( ) , partition . getPartition ( ) , offset , maxSize , Integer . MIN_VALUE ) , 10000 , TimeUnit . MILLISECONDS ) ) . andReturn ( new com . taobao . metamorphosis . network . BooleanCommand ( 500 , "test<sp>error" , Integer . MIN_VALUE ) ) ; this . mocksControl . replay ( ) ; try { com . taobao . gecko . core . util . OpaqueGenerator . resetOpaque ( ) ; org . junit . Assert . assertEquals ( messageIterator , this . consumer . get ( topic , partition , offset , maxSize ) ) ; org . junit . Assert . fail ( ) ; } }
public class aTest{ @Test public void testMergeType2SCD01 ( ) { runStatementOnDriver ( "select<sp>s.*,<sp>case<sp>when<sp>t.cur<sp>is<sp>null<sp>then<sp>0<sp>else<sp>1<sp>end<sp>m<sp>from<sp>source<sp>s<sp>left<sp>outer<sp>join<sp>(select<sp>*<sp>from<sp>target<sp>where<sp>target.cur=1)<sp>t<sp>on<sp>s.key=t.key" 6 ) ; runStatementOnDriver ( "select<sp>s.*,<sp>case<sp>when<sp>t.cur<sp>is<sp>null<sp>then<sp>0<sp>else<sp>1<sp>end<sp>m<sp>from<sp>source<sp>s<sp>left<sp>outer<sp>join<sp>(select<sp>*<sp>from<sp>target<sp>where<sp>target.cur=1)<sp>t<sp>on<sp>s.key=t.key" 1 ) ; runStatementOnDriver ( "drop<sp>table<sp>if<sp>exists<sp>splitTable" ) ; runStatementOnDriver ( "select<sp>s.*,<sp>case<sp>when<sp>t.cur<sp>is<sp>null<sp>then<sp>0<sp>else<sp>1<sp>end<sp>m<sp>from<sp>source<sp>s<sp>left<sp>outer<sp>join<sp>(select<sp>*<sp>from<sp>target<sp>where<sp>target.cur=1)<sp>t<sp>on<sp>s.key=t.key" 7 ) ; runStatementOnDriver ( "select<sp>s.*,<sp>case<sp>when<sp>t.cur<sp>is<sp>null<sp>then<sp>0<sp>else<sp>1<sp>end<sp>m<sp>from<sp>source<sp>s<sp>left<sp>outer<sp>join<sp>(select<sp>*<sp>from<sp>target<sp>where<sp>target.cur=1)<sp>t<sp>on<sp>s.key=t.key" 0 ) ; runStatementOnDriver ( "select<sp>s.*,<sp>case<sp>when<sp>t.cur<sp>is<sp>null<sp>then<sp>0<sp>else<sp>1<sp>end<sp>m<sp>from<sp>source<sp>s<sp>left<sp>outer<sp>join<sp>(select<sp>*<sp>from<sp>target<sp>where<sp>target.cur=1)<sp>t<sp>on<sp>s.key=t.key" 2 ) ; runStatementOnDriver ( ( ( "create<sp>table<sp>target<sp>(key<sp>int,<sp>data<sp>int,<sp>cur<sp>int)<sp>clustered<sp>by<sp>(key)<sp>into<sp>" + ( BUCKET_COUNT ) ) + "<sp>buckets<sp>stored<sp>as<sp>orc<sp>TBLPROPERTIES<sp>('transactional'='true')" ) ) ; int [ ] [ ] targetVals = new int [ ] [ ] { new int [ ] { 1 , 5 , 1 } , new int [ ] { 2 , 6 , 1 } , new int [ ] { 1 , 18 , 0 } } ; runStatementOnDriver ( ( "insert<sp>into<sp>target<sp>" + ( makeValuesClause ( targetVals ) ) ) ) ; int [ ] [ ] sourceVals = new int [ ] [ ] { new int [ ] { 1 , 7 } , new int [ ] { 3 , 8 } } ; runStatementOnDriver ( ( "insert<sp>into<sp>source<sp>" + ( makeValuesClause ( sourceVals ) ) ) ) ; java . lang . String curMatch = "select<sp>s.*,<sp>case<sp>when<sp>t.cur<sp>is<sp>null<sp>then<sp>0<sp>else<sp>1<sp>end<sp>m<sp>from<sp>source<sp>s<sp>left<sp>outer<sp>join<sp>(select<sp>*<sp>from<sp>target<sp>where<sp>target.cur=1)<sp>t<sp>on<sp>s.key=t.key" ; java . lang . String teeCurMatch = ( "select<sp>curMatch.*,<sp>case<sp>when<sp>splitTable.op<sp>is<sp>null<sp>or<sp>splitTable.op<sp>=<sp>0<sp>then<sp>0<sp>else<sp>1<sp>end<sp>`o/p\\n`<sp>from<sp>(" + curMatch ) + "select<sp>s.*,<sp>case<sp>when<sp>t.cur<sp>is<sp>null<sp>then<sp>0<sp>else<sp>1<sp>end<sp>m<sp>from<sp>source<sp>s<sp>left<sp>outer<sp>join<sp>(select<sp>*<sp>from<sp>target<sp>where<sp>target.cur=1)<sp>t<sp>on<sp>s.key=t.key" 4 ; if ( false ) { java . util . List < java . lang . String > r1 = runStatementOnDriver ( curMatch ) ; java . util . List < java . lang . String > r2 = runStatementOnDriver ( teeCurMatch ) ; } java . lang . String stmt = ( ( ( "select<sp>s.*,<sp>case<sp>when<sp>t.cur<sp>is<sp>null<sp>then<sp>0<sp>else<sp>1<sp>end<sp>m<sp>from<sp>source<sp>s<sp>left<sp>outer<sp>join<sp>(select<sp>*<sp>from<sp>target<sp>where<sp>target.cur=1)<sp>t<sp>on<sp>s.key=t.key" 5 + teeCurMatch ) + ")<sp>s<sp>on<sp>t.key=s.key<sp>and<sp>t.cur=1<sp>and<sp>s.`o/p\\n`=1<sp>" ) + "select<sp>s.*,<sp>case<sp>when<sp>t.cur<sp>is<sp>null<sp>then<sp>0<sp>else<sp>1<sp>end<sp>m<sp>from<sp>source<sp>s<sp>left<sp>outer<sp>join<sp>(select<sp>*<sp>from<sp>target<sp>where<sp>target.cur=1)<sp>t<sp>on<sp>s.key=t.key" 3 ) + "when<sp>not<sp>matched<sp>then<sp>insert<sp>values(s.key,s.data,1)" ; hiveConf . setBoolVar ( HiveConf . ConfVars . HIVE_STRICT_CHECKS_CARTESIAN , false ) ; runStatementOnDriver ( stmt ) ; int [ ] [ ] resultVals = new int [ ] [ ] { new int [ ] { 1 , 5 , 0 } , new int [ ] { 1 , 7 , 1 } , new int [ ] { 1 , 18 , 0 } , new int [ ] { 2 , 6 , 1 } , new int [ ] { 3 , 8 , 1 } } ; java . util . List < java . lang . String > r = runStatementOnDriver ( "select<sp>*<sp>from<sp>target<sp>order<sp>by<sp>key,data,cur" ) ; org . junit . Assert . assertEquals ( stringifyValues ( resultVals ) , r ) ; } }
public class aTest{ @Test public void testRingBuffer ( ) { final org . csstudio . apputil . ringbuffer . RingBuffer < java . lang . Integer > queue = new org . csstudio . apputil . ringbuffer . RingBuffer < java . lang . Integer > ( 100 ) ; final long start = java . lang . System . currentTimeMillis ( ) ; final long run = start + ( org . csstudio . archive . engine . model . QueueDemo . RUNTIME ) ; int i = 0 ; while ( run > ( java . lang . System . currentTimeMillis ( ) ) ) { synchronized ( queue ) { queue . add ( java . lang . Integer . valueOf ( i ) ) ; } synchronized ( queue ) { final java . lang . Integer val = queue . remove ( ) ; final int number = ( val == null ) ? - 1 : val . intValue ( ) ; org . junit . Assert . assertEquals ( i , number ) ; } }
public class aTest{ @Test public void testAsyncBulkWritePartialFailureBufferFailure ( ) { java . lang . String name = "distrlog-testAsyncBulkWritePartialFailure" ; org . apache . distributedlog . DistributedLogConfiguration confLocal = new org . apache . distributedlog . DistributedLogConfiguration ( ) ; confLocal . loadConf ( testConf ) ; confLocal . setOutputBufferSize ( 1024 ) ; org . apache . distributedlog . api . DistributedLogManager dlm = createNewDLM ( confLocal , name ) ; org . apache . distributedlog . BKAsyncLogWriter writer = ( ( org . apache . distributedlog . BKAsyncLogWriter ) ( dlm . startAsyncLogSegmentNonPartitioned ( ) ) ) ; final int goodRecs = 10 ; final java . util . List < org . apache . distributedlog . LogRecord > records = org . apache . distributedlog . DLMTestUtil . getLargeLogRecordInstanceList ( 1 , goodRecs ) ; records . add ( org . apache . distributedlog . DLMTestUtil . getLogRecordInstance ( goodRecs , ( ( org . apache . distributedlog . LogRecord . MAX_LOGRECORD_SIZE ) + 1 ) ) ) ; records . addAll ( org . apache . distributedlog . DLMTestUtil . getLargeLogRecordInstanceList ( 1 , goodRecs ) ) ; java . util . concurrent . CompletableFuture < java . util . List < java . util . concurrent . CompletableFuture < org . apache . distributedlog . DLSN > > > futureResults = writer . writeBulk ( records ) ; java . util . List < java . util . concurrent . CompletableFuture < org . apache . distributedlog . DLSN > > results = org . apache . distributedlog . DLMTestUtil . validateFutureSucceededAndGetResult ( futureResults ) ; org . junit . Assert . assertEquals ( ( ( 2 * goodRecs ) + 1 ) , results . size ( ) ) ; for ( int i = 0 ; i < goodRecs ; i ++ ) { org . apache . distributedlog . DLSN dlsn = org . apache . distributedlog . DLMTestUtil . validateFutureSucceededAndGetResult ( results . get ( i ) ) ; } }
public class aTest{ @Test public void testConfigureEnableRawOnly ( ) { System . out . println ( ( ( getTestTraceHead ( "[NGSICartoDBSink.configure]" ) ) + "--------<sp>Only<sp>enable_raw<sp>configuration<sp>works" ) ) ; org . apache . flume . Context context = new org . apache . flume . Context ( ) ; context . put ( "enable_raw" , "false" ) ; context . put ( "keys_conf_file" , "" ) ; com . telefonica . iot . cygnus . sinks . NGSICartoDBSink sink = new com . telefonica . iot . cygnus . sinks . NGSICartoDBSink ( ) ; sink . configure ( context ) ; try { org . junit . Assert . assertTrue ( ( ! ( sink . getEnableRawHistoric ( ) ) ) ) ; System . out . println ( ( ( getTestTraceHead ( "[NGSICartoDBSink.configure]" ) ) + "-<sp>OK<sp>-<sp>Only<sp>'enable_raw'<sp>was<sp>configured<sp>and<sp>worked" ) ) ; } }
public class aTest{ @Test public void shouldGetAllQuestionGroups ( ) { org . mifos . platform . questionnaire . service . QuestionGroupDetail questionGroupDetail1 = getQuestionGroupDetail ( 1 , ( ( org . mifos . platform . questionnaire . ui . controller . QuestionGroupControllerTest . TITLE ) + "sectionName2" 0 ) , "View" , "Loan" , true , true , "title1" , "sectionName1" ) ; org . mifos . platform . questionnaire . service . QuestionGroupDetail questionGroupDetail2 = getQuestionGroupDetail ( 2 , ( ( org . mifos . platform . questionnaire . ui . controller . QuestionGroupControllerTest . TITLE ) + "sectionName2" 1 ) , "View" , "Loan" , true , true , "sectionName2" 5 , "sectionName2" ) ; org . mifos . platform . questionnaire . service . QuestionGroupDetail questionGroupDetail3 = getQuestionGroupDetail ( 3 , ( ( org . mifos . platform . questionnaire . ui . controller . QuestionGroupControllerTest . TITLE ) + "3" ) , "sectionName2" 2 , "Loan" , true , true , "title3" , "sectionName3" ) ; java . util . List < org . mifos . platform . questionnaire . service . QuestionGroupDetail > questionGroupDetails = asList ( questionGroupDetail1 , questionGroupDetail2 , questionGroupDetail3 ) ; java . util . Map < java . lang . String , java . util . List < org . mifos . platform . questionnaire . service . QuestionGroupDetail > > questionGroupsCategoriesSplit = new java . util . HashMap < java . lang . String , java . util . List < org . mifos . platform . questionnaire . service . QuestionGroupDetail > > ( ) ; questionGroupsCategoriesSplit . put ( "sectionName2" 3 , asList ( questionGroupDetail1 , questionGroupDetail2 ) ) ; questionGroupsCategoriesSplit . put ( "sectionName2" 4 , asList ( questionGroupDetail3 ) ) ; when ( questionnaireServiceFacade . getAllQuestionGroups ( ) ) . thenReturn ( questionGroupDetails ) ; when ( questionnaireServiceFacade . getAllEventSources ( ) ) . thenReturn ( asList ( questionGroupDetail2 . getEventSources ( ) . get ( 0 ) , questionGroupDetail3 . getEventSources ( ) . get ( 0 ) ) ) ; java . lang . String view = questionGroupController . getAllQuestionGroups ( model , httpServletRequest ) ; org . junit . Assert . assertThat ( view , org . hamcrest . core . Is . is ( "viewQuestionGroups" ) ) ; verify ( questionnaireServiceFacade ) . getAllQuestionGroups ( ) ; verify ( questionnaireServiceFacade ) . getAllEventSources ( ) ; verify ( model ) . addAttribute ( org . mockito . Matchers . eq ( "questionGroups" ) , argThat ( new org . mifos . platform . questionnaire . matchers . QuestionGroupsGroupByEventSourceMatcher ( questionGroupsCategoriesSplit ) ) ) ; } }
public class aTest{ @Test public void testExpire ( ) { java . net . InetSocketAddress resolved1 = new java . net . InetSocketAddress ( java . net . InetAddress . getByAddress ( "ns1" , new byte [ ] { 10 , 0 , 0 , 1 } ) , 53 ) ; java . net . InetSocketAddress resolved2 = new java . net . InetSocketAddress ( java . net . InetAddress . getByAddress ( "ns2" , new byte [ ] { 10 , 0 , 0 , 2 } ) , 53 ) ; io . netty . channel . EventLoopGroup group = new io . netty . channel . DefaultEventLoopGroup ( 1 ) ; try { io . netty . channel . EventLoop loop = group . next ( ) ; final io . netty . resolver . dns . DefaultAuthoritativeDnsServerCache cache = new io . netty . resolver . dns . DefaultAuthoritativeDnsServerCache ( ) ; cache . cache ( "netty.io" , resolved1 , 1 , loop ) ; cache . cache ( "netty.io" , resolved2 , 10000 , loop ) ; java . lang . Throwable error = loop . schedule ( new java . util . concurrent . Callable < java . lang . Throwable > ( ) { @ io . netty . resolver . dns . Override public java . lang . Throwable call ( ) { try { org . junit . Assert . assertNull ( cache . get ( "netty.io" ) ) ; return null ; } }
public class aTest{ @Test public void testDeleteAllTenantTableData ( ) { java . util . Properties props = org . apache . phoenix . util . PropertiesUtil . deepCopy ( org . apache . phoenix . end2end . TEST_PROPERTIES ) ; java . sql . Connection conn = java . sql . DriverManager . getConnection ( getUrl ( ) , props ) ; java . sql . Connection tsConn = java . sql . DriverManager . getConnection ( org . apache . phoenix . end2end . PHOENIX_JDBC_TENANT_SPECIFIC_URL , props ) ; try { conn . setAutoCommit ( true ) ; conn . createStatement ( ) . executeUpdate ( ( "delete<sp>from<sp>" + ( PARENT_TABLE_NAME ) ) ) ; conn . createStatement ( ) . executeUpdate ( ( ( "upsert<sp>into<sp>" + ( PARENT_TABLE_NAME ) ) + "<sp>(tenant_id,<sp>tenant_type_id,<sp>id,<sp>\"user\")<sp>values<sp>(\'AC/DC\',<sp>\'abc\',<sp>1,<sp>\'Bon<sp>Scott\')" ) ) ; conn . createStatement ( ) . executeUpdate ( ( ( ( ( ( ( "upsert<sp>into<sp>" + ( PARENT_TABLE_NAME ) ) + "<sp>(tenant_id,<sp>tenant_type_id,<sp>id,<sp>\"user\")<sp>values<sp>(\'" ) + ( TENANT_ID ) ) + "',<sp>'" ) + ( TENANT_TYPE_ID ) ) + "',<sp>1,<sp>'Billy<sp>Gibbons')" ) ) ; conn . createStatement ( ) . executeUpdate ( ( ( ( ( "upsert<sp>into<sp>" + ( PARENT_TABLE_NAME ) ) + "<sp>(tenant_id,<sp>tenant_type_id,<sp>id,<sp>\"user\")<sp>values<sp>(\'" ) + ( TENANT_ID ) ) + "',<sp>'def',<sp>1,<sp>'Billy<sp>Gibbons')" ) ) ; analyzeTable ( tsConn , org . apache . phoenix . end2end . PARENT_TABLE_NAME ) ; tsConn . createStatement ( ) . execute ( ( "delete<sp>from<sp>" + ( TENANT_TABLE_NAME ) ) ) ; tsConn . commit ( ) ; java . sql . ResultSet rs = conn . createStatement ( ) . executeQuery ( ( "select<sp>count(*)<sp>from<sp>" + ( PARENT_TABLE_NAME ) ) ) ; rs . next ( ) ; org . junit . Assert . assertEquals ( 2 , rs . getInt ( 1 ) ) ; } }
public class aTest{ @Test public void testSelectDateMultiFunction ( ) { java . lang . String sqlText = java . lang . String . format ( ( "2014-03-04<sp>|<sp>1<sp>|<sp>7<sp>|<sp>7<sp>|" 3 + ( ( "DENSE_RANK()<sp>OVER<sp>(PARTITION<sp>BY<sp>dept<sp>ORDER<sp>BY<sp>hiredate,<sp>salary)<sp>AS<sp>DenseRank_HireDate_Salary_By_Dept,<sp>" + "ROW_NUMBER()<sp>OVER<sp>(PARTITION<sp>BY<sp>dept<sp>ORDER<sp>BY<sp>hiredate,<sp>dept)<sp>AS<sp>RowNumber_HireDate_Salary_By_Dept<sp>" ) + "2014-03-04<sp>|<sp>1<sp>|<sp>7<sp>|<sp>7<sp>|" 4 ) ) , this . getTableReference ( com . splicemachine . derby . impl . sql . execute . operations . WindowFunctionIT . EMPTAB_HIRE_DATE ) , useSpark ) ; java . sql . ResultSet rs = com . splicemachine . derby . impl . sql . execute . operations . WindowFunctionIT . methodWatcher . executeQuery ( sqlText ) ; java . lang . String expected = "2014-03-04<sp>|<sp>1<sp>|<sp>7<sp>|<sp>7<sp>|" 2 + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "------------------------------------------------------------------------------------------\n" + "2010-03-20<sp>|<sp>1<sp>|<sp>1<sp>|<sp>1<sp>|\n" ) + "2014-03-04<sp>|<sp>1<sp>|<sp>7<sp>|<sp>7<sp>|" 6 ) + "2010-04-12<sp>|<sp>3<sp>|<sp>1<sp>|<sp>1<sp>|\n" ) + "2010-08-09<sp>|<sp>3<sp>|<sp>2<sp>|<sp>2<sp>|\n" ) + "2014-03-04<sp>|<sp>1<sp>|<sp>7<sp>|<sp>7<sp>|" 7 ) + "2014-03-04<sp>|<sp>1<sp>|<sp>7<sp>|<sp>7<sp>|" 0 ) + "2012-04-03<sp>|<sp>2<sp>|<sp>1<sp>|<sp>1<sp>|\n" ) + "2010-04-12<sp>|<sp>3<sp>|<sp>1<sp>|<sp>1<sp>|\n" 0 ) + "2012-04-03<sp>|<sp>3<sp>|<sp>3<sp>|<sp>3<sp>|\n" ) + "2014-03-04<sp>|<sp>1<sp>|<sp>7<sp>|<sp>7<sp>|" 8 ) + "2014-03-04<sp>|<sp>1<sp>|<sp>7<sp>|<sp>7<sp>|" 1 ) + "2014-03-04<sp>|<sp>1<sp>|<sp>7<sp>|<sp>7<sp>|" 5 ) + "2010-04-12<sp>|<sp>3<sp>|<sp>1<sp>|<sp>1<sp>|\n" 1 ) + "2013-12-20<sp>|<sp>2<sp>|<sp>4<sp>|<sp>4<sp>|\n" ) + "2014-03-04<sp>|<sp>1<sp>|<sp>7<sp>|<sp>7<sp>|" ) ; org . junit . Assert . assertEquals ( ( ( "2014-03-04<sp>|<sp>1<sp>|<sp>7<sp>|<sp>7<sp>|" 9 + sqlText ) + "2014-03-04<sp>|<sp>1<sp>|<sp>7<sp>|<sp>7<sp>|" 9 ) , expected , TestUtils . FormattedResult . ResultFactory . toStringUnsorted ( rs ) ) ; rs . close ( ) ; } }
public class aTest{ @Test public void testExpression10 ( ) { java . lang . String expression = oldExpressions [ 10 ] ; try { java . util . List list = org . eclipse . birt . data . engine . expression . ExpressionCompilerUtilTest . extractColumnExpression ( new org . eclipse . birt . data . engine . api . querydefn . ScriptExpression ( expression ) ) ; org . junit . Assert . assertTrue ( ( ( list . size ( ) ) == 2 ) ) ; } }
public class aTest{ @Test public void readPartialFile ( ) { long length = ( ( alluxio . client . block . stream . GrpcDataReaderTest . CHUNK_SIZE ) * 1024 ) + ( ( alluxio . client . block . stream . GrpcDataReaderTest . CHUNK_SIZE ) / 3 ) ; long offset = 10 ; long checksumStart = 100 ; long bytesToRead = length / 3 ; try ( alluxio . client . block . stream . DataReader reader = create ( offset , length ) ) { long checksum = setReadResponses ( mClient , length , checksumStart , ( bytesToRead - 1 ) ) ; long checksumActual = checkChunks ( reader , checksumStart , bytesToRead ) ; org . junit . Assert . assertEquals ( checksum , checksumActual ) ; } }
public class aTest{ @Test public void testExitFlag ( ) { org . finra . datagenerator . engine . scxml . SCXMLEngine e = new org . finra . datagenerator . engine . scxml . SCXMLEngine ( ) ; java . io . InputStream is = org . finra . datagenerator . engine . scxml . SCXMLEngineTest . class . getResourceAsStream ( "/bigtest.xml" ) ; e . setModelByInputFileStream ( is ) ; java . util . List < org . finra . datagenerator . engine . scxml . tags . CustomTagExtension > tagExtensionList = customTagExtensionList ( ) ; try { java . util . List < org . finra . datagenerator . engine . scxml . PossibleState > bfs = e . bfs ( 1 ) ; org . finra . datagenerator . engine . scxml . PossibleState p = bfs . get ( 0 ) ; try { is = org . finra . datagenerator . engine . scxml . SCXMLEngineTest . class . getResourceAsStream ( "/bigtest.xml" ) ; org . apache . commons . scxml . model . SCXML model = org . apache . commons . scxml . io . SCXMLParser . parse ( new org . xml . sax . InputSource ( is ) , null , customActionsFromTagExtensions ( tagExtensionList ) ) ; org . finra . datagenerator . engine . scxml . SCXMLFrontier frontier = new org . finra . datagenerator . engine . scxml . SCXMLFrontier ( p , model , tagExtensionList ) ; java . util . Queue < java . util . Map < java . lang . String , java . lang . String > > queue = new java . util . LinkedList ( ) ; java . util . concurrent . atomic . AtomicBoolean flag = new java . util . concurrent . atomic . AtomicBoolean ( true ) ; frontier . searchForScenarios ( new org . finra . datagenerator . distributor . multithreaded . QueueResultsProcessing ( queue ) , flag ) ; org . junit . Assert . assertEquals ( queue . isEmpty ( ) , true ) ; } }
public class aTest{ @Test public void testSimpleFileUpload ( ) { java . io . File tmpFile = java . io . File . createTempFile ( "mechanize" , "#form" 0 ) ; addPageRequest ( "http://test.com" , newHtml ( "#form" 1 , newForm ( "form" ) . method ( "#form" 2 ) . id ( "form" ) . enctype ( "multipart/form-data" ) . addFileInput ( "fileUpload" , "" ) ) ) ; addPageRequest ( "POST" , "http://test.com/form" , newHtml ( "OK" , "" ) ) ; com . gistlabs . mechanize . document . AbstractDocument page = agent ( ) . get ( "http://test.com" ) ; com . gistlabs . mechanize . document . html . form . Form form = page . forms ( ) . find ( "#form" ) ; form . findUpload ( byIdOrName ( "fileUpload" ) ) . setValue ( tmpFile ) ; com . gistlabs . mechanize . document . AbstractDocument response = form . submit ( ) ; org . junit . Assert . assertEquals ( "OK" , response . getTitle ( ) ) ; } }
public class aTest{ @Test public void testChopOffIndexOfC ( ) { java . lang . String var1value = "D<Exqaa:saksajij1§n" ; org . evosuite . symbolic . expr . str . StringVariable var1 = new org . evosuite . symbolic . expr . str . StringVariable ( "var1" , var1value ) ; org . evosuite . symbolic . expr . bv . IntegerConstant colon_code = new org . evosuite . symbolic . expr . bv . IntegerConstant ( 58 ) ; org . evosuite . symbolic . expr . bv . IntegerConstant minus_one = new org . evosuite . symbolic . expr . bv . IntegerConstant ( ( - 1 ) ) ; int colon_int_code = ( ( int ) ( ':' ) ) ; int concrete_value = var1value . indexOf ( colon_int_code ) ; org . evosuite . symbolic . expr . bv . StringBinaryToIntegerExpression index_of_colon = new org . evosuite . symbolic . expr . bv . StringBinaryToIntegerExpression ( var1 , org . evosuite . symbolic . expr . Operator . INDEXOFC , colon_code , ( ( long ) ( concrete_value ) ) ) ; org . evosuite . symbolic . expr . IntegerConstraint constr1 = new org . evosuite . symbolic . expr . IntegerConstraint ( index_of_colon , org . evosuite . symbolic . expr . Comparator . EQ , minus_one ) ; java . util . List < org . evosuite . symbolic . expr . Constraint < ? > > constraints = new java . util . ArrayList < org . evosuite . symbolic . expr . Constraint < ? > > ( ) ; constraints . add ( constr1 ) ; org . evosuite . symbolic . solver . avm . EvoSuiteSolver solver = new org . evosuite . symbolic . solver . avm . EvoSuiteSolver ( ) ; java . util . Map < java . lang . String , java . lang . Object > solution ; try { solution = org . evosuite . symbolic . solver . TestSolver . solve ( solver , constraints ) ; org . junit . Assert . assertNotNull ( solution ) ; } }
public class aTest{ @Test public void testAddPropertyWithSameName_shouldLeaveListUnchanged ( ) { tester . startPage ( new org . openengsb . ui . admin . connectorEditorPage . ConnectorEditorPage ( "testdomain" , "testconnector" ) ) ; org . apache . wicket . util . tester . FormTester formTester = tester . newFormTester ( "bar" 2 ) ; formTester . setValue ( "bar" 4 , "bar" 5 ) ; org . apache . wicket . ajax . markup . html . form . AjaxButton newPropertyButton = ( ( org . apache . wicket . ajax . markup . html . form . AjaxButton ) ( tester . getComponentFromLastRenderedPage ( "editor:form:addProperty" ) ) ) ; formTester . setValue ( "newPropertyKey" , "bar" 6 ) ; tester . executeAjaxEvent ( newPropertyButton , "bar" 0 ) ; tester . executeAjaxEvent ( "bar" 3 , "bar" 0 ) ; formTester . setValue ( "attributesPanel:properties:0:values:1:value:editor" , "foo" ) ; tester . executeAjaxEvent ( "editor:form:attributesPanel:properties:0:newArrayEntry" , "bar" 0 ) ; tester . executeAjaxEvent ( "editor:form:attributesPanel:properties:0:values:2:value:label" , "bar" 0 ) ; formTester . setValue ( "attributesPanel:properties:0:values:2:value:editor" , "bar" ) ; formTester . setValue ( "newPropertyKey" , "bar" 6 ) ; tester . executeAjaxEvent ( newPropertyButton , "bar" 0 ) ; org . apache . wicket . markup . repeater . AbstractRepeater list = ( ( org . apache . wicket . markup . repeater . AbstractRepeater ) ( tester . getComponentFromLastRenderedPage ( "bar" 1 ) ) ) ; org . junit . Assert . assertThat ( list . size ( ) , org . hamcrest . CoreMatchers . is ( 2 ) ) ; } }
public class aTest{ @Test public void testSubscribeToProductIntNoUsers ( ) { org . junit . Assert . assertNotNull ( subMgmtLocal ) ; runTX ( new java . util . concurrent . Callable < java . lang . Void > ( ) { @ org . oscm . subscriptionservice . bean . Override public org . oscm . subscriptionservice . bean . Void call ( ) throws org . oscm . subscriptionservice . bean . Exception { subMgmtLocal . subscribeToServiceInt ( createTriggerProcessForSubscription ( "testSubscribeToProduct" ) ) ; return null ; } } }
public class aTest{ @Test public void testDeprecatesCacheWhenConnected_failure ( ) { handler . addRequest ( false , "GET" , "/qcbin/rest/domains/domain/projects/project/customization/entities/non-existing-type/fields" , 404 ) ; handler . async ( ) ; metadataService . loadEntityMetadataAsync ( "non-existing-type" , new com . hp . alm . ali . idea . services . MetadataService . MetadataCallback ( ) { @ com . hp . alm . ali . idea . services . Override public void metadataLoaded ( com . hp . alm . ali . idea . model . Metadata metadata ) { handler . fail ( "Should<sp>have<sp>failed" ) ; } @ com . hp . alm . ali . idea . services . Override public void metadataFailed ( ) { handler . done ( new java . lang . Runnable ( ) { @ com . hp . alm . ali . idea . services . Override public void run ( ) { org . junit . Assert . assertNull ( metadataService . getCachedFailure ( "non-existing-type" ) ) ; } } }
public class aTest{ @Test public void testQueryStringMissingOperator ( ) { try { java . lang . String queryString = "key4" ; java . net . URI requestUri = new java . net . URI ( ( ( ( org . slc . sli . api . service . query . ApiQueryTest . URI_STRING ) + "?" ) + queryString ) ) ; when ( uriInfo . getRequestUri ( ) ) . thenReturn ( requestUri ) ; } catch ( java . net . URISyntaxException urise ) { org . junit . Assert . assertTrue ( false ) ; } }
public class aTest{ @Test public void testExtFun8 ( ) { java . lang . String init = "prefix<sp>ex:<sp><http://example.org/><sp>" + ( ( "filter<sp>exists<sp>{<sp>select<sp>{<sp>?y<sp>?p<sp>?n<sp>filter<sp>(<sp>xt:fun(?n))<sp>}<sp>}<sp>" 0 + "ex:John<sp>rdf:value<sp>1,<sp>2<sp>" ) + "}" ) ; java . lang . String q = "prefix<sp>ex:<sp><http://example.org/><sp>" + ( ( ( ( ( ( ( "select<sp>*<sp>" + "where<sp>{" ) + "?x<sp>rdf:value<sp>?n<sp>" ) + "filter<sp>exists<sp>{<sp>select<sp>{<sp>?y<sp>?p<sp>?n<sp>filter<sp>(<sp>xt:fun(?n))<sp>}<sp>}<sp>" ) + "}" ) + "function<sp>xt:fun(?n)<sp>{<sp>" ) + "exists<sp>{<sp>?x<sp>?q<sp>?n<sp>}<sp>" ) + "}<sp>" ) ; fr . inria . corese . core . Graph g = createGraph ( ) ; fr . inria . corese . core . query . QueryProcess exec = fr . inria . corese . core . query . QueryProcess . create ( g ) ; exec . query ( init ) ; fr . inria . corese . kgram . core . Mappings map = exec . query ( q ) ; org . junit . Assert . assertEquals ( 2 , map . size ( ) ) ; } }
public class aTest{ @Test public void testProcessInjectionPointNotFiredMultipleTimes ( org . jboss . weld . tests . extensions . lifecycle . processInjectionPoint . subclass . Foo , org . jboss . weld . tests . extensions . lifecycle . processInjectionPoint . subclass . VerifyingExtension ) { foo . ping ( ) ; org . junit . Assert . assertEquals ( 1 , extension . getInjectionPoints ( ) . size ( ) ) ; } }
public class aTest{ @Test public void test_getResourceRelationContextWithResourceActivations ( ) { ch . puzzle . itc . mobiliar . business . resourcegroup . entity . ResourceEntity master = ch . puzzle . itc . mobiliar . business . resourcegroup . entity . ResourceFactory . createNewResource ( "master" ) ; ch . puzzle . itc . mobiliar . business . resourcegroup . entity . ResourceEntity slave = ch . puzzle . itc . mobiliar . business . resourcegroup . entity . ResourceFactory . createNewResource ( "slave" ) ; ch . puzzle . itc . mobiliar . business . resourcerelation . entity . ConsumedResourceRelationEntity r = new ch . puzzle . itc . mobiliar . business . resourcerelation . entity . ConsumedResourceRelationEntity ( ) ; ch . puzzle . itc . mobiliar . business . environment . entity . ContextEntity someContext = new ch . puzzle . itc . mobiliar . business . environment . entity . ContextEntity ( ) ; try { r . setMasterResource ( master ) ; r . setSlaveResource ( slave ) ; entityManager . persist ( master ) ; someContext . setName ( "bla" ) ; entityManager . persist ( someContext ) ; ch . puzzle . itc . mobiliar . business . resourcerelation . entity . ResourceRelationContextEntity resRelCtxEntity = resourceRelationContextRepository . createResourceRelationContext ( r , someContext ) ; entityManager . flush ( ) ; ch . puzzle . itc . mobiliar . business . resourcerelation . entity . ResourceRelationContextEntity resRelCtxFromDB = resourceRelationContextRepository . getResourceRelationContextWithResourceActivations ( r , someContext ) ; org . junit . Assert . assertEquals ( resRelCtxEntity . getId ( ) , resRelCtxFromDB . getId ( ) ) ; } }
public class aTest{ @Test public void test9 ( ) { java . lang . String addr = "cd2a3d9f938e13cd947ec05abc7fe734df8dd826" ; byte [ ] keyBytes = org . spongycastle . util . encoders . Hex . decode ( "c5d2460186f7233c927e7db2dcc703c0e500b653ca82273b7bfad8045d85a470" ) ; org . ethereum . vm . DataWord key = new org . ethereum . vm . DataWord ( keyBytes ) ; org . ethereum . facade . Repository repository = new org . ethereum . db . RepositoryImpl ( ) ; try { org . ethereum . vm . DataWord value = repository . getStorageValue ( org . spongycastle . util . encoders . Hex . decode ( addr ) , key ) ; org . junit . Assert . assertNull ( value ) ; } }
public class aTest{ @Test public void verifySpiLoadedGeoFunctions ( ) { final java . lang . String [ ] functions = new java . lang . String [ ] { "intersection" 3 , "rcc8tpp" 2 , "boundary" , "rcc8tpp" 6 , "sfOverlaps" 6 , "intersection" , "intersection" 2 , "sfOverlaps" 9 , "intersection" 0 , "sfDisjoint" , "sfOverlaps" 3 , "rcc8tpp" 0 , "intersection" 1 , "sfOverlaps" 2 , "sfContains" , "sfOverlaps" , "ehDisjoint" , "rcc8tpp" 1 , "sfOverlaps" 4 , "sfOverlaps" 7 , "rcc8tpp" 5 , "sfOverlaps" 0 , "rcc8tpp" 3 , "rcc8dc" , "rcc8tpp" 9 , "intersection" 4 , "rcc8tppi" , "rcc8tpp" , "sfOverlaps" 1 , "rcc8tpp" 4 } ; java . util . HashSet < java . lang . String > functionsCheckList = new java . util . HashSet < java . lang . String > ( ) ; functionsCheckList . addAll ( java . util . Arrays . asList ( functions ) ) ; for ( java . lang . String f : org . eclipse . rdf4j . query . algebra . evaluation . function . FunctionRegistry . getInstance ( ) . getKeys ( ) ) { java . lang . String functionShortName = f . replaceFirst ( "sfOverlaps" 5 , "$1" ) ; functionsCheckList . remove ( functionShortName ) ; } org . junit . Assert . assertTrue ( ( "rcc8tpp" 7 + functionsCheckList ) , functionsCheckList . isEmpty ( ) ) ; } }
public class aTest{ @Test public void testMVELClassReferences ( ) { final java . lang . String str = "package<sp>org.drools.compiler\n" + ( ( ( ( ( ( ( ( ( ( ( ( ( "org.drools.compiler" 3 + "org.drools.compiler" 1 ) + "org.drools.compiler" 0 ) + "org.drools.compiler" 4 ) + "org.drools.compiler" 5 ) + "when\n" ) + "<sp>Assignment(<sp>$t:<sp>target<sp>==<sp>java.lang.Object.class<sp>||<sp>target<sp>==<sp>source<sp>)\n" ) + "then\n" ) + "org.drools.compiler" 4 ) + "rule<sp>ObjectIsAssignable2\n" ) + "when\n" ) + "<sp>Assignment(<sp>$t:<sp>target<sp>==<sp>source<sp>||<sp>target<sp>==<sp>java.lang.Object.class<sp>)\n" ) + "then\n" ) + "end" ) ; final org . kie . api . KieBase kbase = loadKnowledgeBaseFromString ( str ) ; final org . kie . api . runtime . KieSession ksession = createKnowledgeSession ( kbase ) ; final org . kie . api . definition . type . FactType asgType = kbase . getFactType ( "org.drools.compiler" , "Assignment" ) ; final java . lang . Object asg = asgType . newInstance ( ) ; asgType . set ( asg , "source" , java . lang . Object . class ) ; asgType . set ( asg , "org.drools.compiler" 2 , java . lang . Object . class ) ; ksession . insert ( asg ) ; final int rules = ksession . fireAllRules ( ) ; ksession . dispose ( ) ; org . junit . Assert . assertEquals ( 2 , rules ) ; } }
public class aTest{ @Test public void testMultivariateFactorization23 ( ) { cc . redberry . rings . poly . multivar . IntegersZp64 domain = new cc . redberry . rings . poly . multivar . IntegersZp64 ( 3 ) ; java . lang . String [ ] vars = new java . lang . String [ ] { "a" , "b" , "c" , "d" , "e" } ; cc . redberry . rings . poly . multivar . MultivariatePolynomialZp64 [ ] factors = new cc . redberry . rings . poly . multivar . MultivariatePolynomialZp64 [ ] { cc . redberry . rings . poly . multivar . MultivariatePolynomialZp64 . parse ( "1+2*a*b*c*e^2+a^2*b^2*c^3*e+a^3*d^3*e^3+a^3*b^2*c^2*e^3" , domain , vars ) , cc . redberry . rings . poly . multivar . MultivariatePolynomialZp64 . parse ( "1+d*e^3+a^3*b*e^2+a^3*b*d^3*e^3" , domain , vars ) , cc . redberry . rings . poly . multivar . MultivariatePolynomialZp64 . parse ( "1+b^2*c^2*d*e+2*b^2*c^3*d*e^2+a^3*c^3*d*e^2+a^3*b^3*c^3*d*e^3" , domain , vars ) , cc . redberry . rings . poly . multivar . MultivariatePolynomialZp64 . parse ( "2*b^3*c*d^3*e+2*a*b^2*c^2*e^3+2*a^2*c^3*e+2*a^2*b*c^2*d^3*e^2+a^3*b^3*d^2" , domain , vars ) } ; cc . redberry . rings . poly . multivar . MultivariatePolynomialZp64 base = factors [ 0 ] . createOne ( ) . multiply ( factors ) ; assert cc . redberry . rings . poly . multivar . MultivariateSquareFreeFactorization . isSquareFree ( base ) ; for ( int i = 0 ; i < ( its ( 20 , 20 ) ) ; i ++ ) { cc . redberry . rings . poly . multivar . PrivateRandom . getRandom ( ) . setSeed ( 3 ) ; long start = java . lang . System . nanoTime ( ) ; cc . redberry . rings . poly . multivar . PolynomialFactorDecomposition < cc . redberry . rings . poly . multivar . MultivariatePolynomialZp64 > decomposition = cc . redberry . rings . poly . multivar . MultivariateFactorization . MultivariateFactorization . FactorInGF ( base ) ; org . junit . Assert . assertEquals ( 4 , decomposition . size ( ) ) ; System . out . println ( ( ( ( "a" 0 + i ) + "<sp>" ) + ( cc . redberry . rings . util . TimeUnits . nanosecondsToString ( ( ( java . lang . System . nanoTime ( ) ) - start ) ) ) ) ) ; } } }
public class aTest{ @Test public void testCustomDateTimeFormatReflection ( ) { java . lang . String input = "Tue<sp>Jan<sp>17<sp>21:21:46<sp>Z<sp>2012" ; java . lang . Long outputMillis = 1326835306000L ; com . fasterxml . jackson . databind . ObjectMapper mapper = org . apache . streams . jackson . StreamsJacksonMapper . getInstance ( ) ; try { java . lang . String json = ( "{\"published\":\"" + input ) + "\"}" ; org . apache . streams . pojo . json . Activity activity = mapper . readValue ( json , org . apache . streams . pojo . json . Activity . class ) ; java . lang . Long result = activity . getPublished ( ) . getMillis ( ) ; org . junit . Assert . assertEquals ( result , outputMillis ) ; } }
public class aTest{ @Test public void testParallelMap ( ) { org . stringtemplate . v4 . STGroup group = new org . stringtemplate . v4 . org . stringtemplate . v4 . STGroup ( '$' , '$' ) ; group . defineTemplate ( "test" , "names,phones" , "hi<sp>$names,phones:{n,p<sp>|<sp>$n$:$p$;}$" ) ; org . stringtemplate . v4 . ST st = group . getInstanceOf ( "test" ) ; st . add ( "Tom" 0 , "Ter" ) ; st . add ( "Tom" 0 , "Tom" ) ; st . add ( "Tom" 0 , "Tom" 1 ) ; st . add ( "phones" , "x5001" ) ; st . add ( "phones" , "x5002" ) ; st . add ( "phones" , "x5003" ) ; java . lang . String expected = "hi<sp>Ter:x5001;Tom:x5002;Sumana:x5003;" ; java . lang . String result = st . render ( ) ; org . junit . Assert . assertEquals ( expected , result ) ; } }
public class aTest{ @Test public void addMonitorCalledAfterInstall ( ) { org . jboss . msc . service . ServiceBuilder sb = serviceContainer . addService ( org . jboss . msc . multi_value_services . WrongUsageOfNewServicesAPITestCase . ID ) ; sb . provides ( org . jboss . msc . multi_value_services . WrongUsageOfNewServicesAPITestCase . FOO ) ; org . junit . Assert . assertNotNull ( sb . install ( ) ) ; try { sb . addMonitor ( new org . jboss . msc . service . StabilityMonitor ( ) ) ; org . junit . Assert . fail ( "IllegalStateException<sp>expected" ) ; } }
public class aTest{ @Test public void testPreventModifyingSdk_canRead ( ) { fixture . preventModifyingSdk ( ) ; try { java . util . concurrent . locks . Lock readLock = modifyLock . readLock ( ) ; org . junit . Assert . assertTrue ( readLock . tryLock ( ) ) ; readLock . unlock ( ) ; } }
public class aTest{ @Test public void testWeaksRemovedOnlyUsingNext ( ) { org . glassfish . hk2 . utilities . general . WeakHashClock < java . lang . String , java . lang . String > clock = org . glassfish . hk2 . utilities . general . GeneralUtilities . getWeakHashClock ( true ) ; java . lang . String key = new java . lang . String ( org . glassfish . hk2 . utilities . general . test . WeakHashClockTest . KEY ) ; clock . put ( key , org . glassfish . hk2 . utilities . general . test . WeakHashClockTest . VALUE ) ; org . junit . Assert . assertEquals ( 1 , clock . size ( ) ) ; key = null ; for ( int lcv = 0 ; lcv < ( org . glassfish . hk2 . utilities . general . test . WeakHashClockTest . ITERATIONS ) ; lcv ++ ) { java . lang . System . gc ( ) ; if ( ( clock . next ( ) ) == null ) { return ; } }
public class aTest{ @Test public void fakeJREMethodAndConstructorWithFakeClass ( ) { thrown . expect ( mockit . LoginException . class ) ; new mockit . MockUp < mockit . LoginContext > ( ) { @ mockit . Mock void $init ( java . lang . String name ) { org . junit . Assert . assertEquals ( "test" , name ) ; } }
public class aTest{ @Test public void testWriteField ( ) { java . lang . reflect . Field field = parentClass . getDeclaredField ( "s" ) ; org . apache . commons . lang3 . reflect . FieldUtils . writeField ( field , publicChild , "S" ) ; org . junit . Assert . assertEquals ( "S" , field . get ( publicChild ) ) ; field = parentClass . getDeclaredField ( "b" ) ; try { org . apache . commons . lang3 . reflect . FieldUtils . writeField ( field , publicChild , Boolean . TRUE ) ; org . junit . Assert . fail ( "Expected<sp>IllegalAccessException" ) ; } }
public class aTest{ @Test public void testImportMixedChannelsIntoLexerGrammar ( ) { org . antlr . v4 . test . runtime . BaseRuntimeTest . mkdir ( tmpdir ) ; java . lang . String master = "lexer<sp>grammar<sp>M;\n" + ( ( ( "import<sp>S;\n" + "channels<sp>{CH_A,<sp>CH_B}\n" ) + "A<sp>:<sp>\'a\'<sp>-><sp>channel(CH_A);\n" ) + "channels<sp>{CH_A,<sp>CH_B}\n" 0 ) ; writeFile ( tmpdir , "M.g4" , master ) ; java . lang . String slave = "channels<sp>{CH_A,<sp>CH_B}\n" 1 + ( "channels<sp>{CH_C}\n" + "C<sp>:<sp>\'c\'<sp>-><sp>channel(CH_C);\n" ) ; writeFile ( tmpdir , "S.g4" , slave ) ; org . antlr . v4 . test . runtime . ErrorQueue equeue = org . antlr . v4 . test . runtime . BaseRuntimeTest . antlrOnString ( tmpdir , "Java" , "M.g4" , false , "-lib" , tmpdir ) ; org . junit . Assert . assertEquals ( 0 , equeue . errors . size ( ) ) ; } }
public class aTest{ @Test public void testOperationTypeSets ( ) { java . lang . String code = "" ; try { code = _setupTestOperationType ( true ) ; _checkOperationTypeIntoDb ( code ) ; } catch ( java . lang . Exception e ) { e . printStackTrace ( ) ; org . junit . Assert . assertEquals ( true , false ) ; } }
public class aTest{ @Test public void testWriteConsistencyLevel ( ) { int levelsChecked = 0 ; for ( com . thinkaurelius . titan . diskstorage . cassandra . CLevel writeLevel : com . thinkaurelius . titan . diskstorage . cassandra . CLevel . values ( ) ) { com . thinkaurelius . titan . diskstorage . util . StandardBaseTransactionConfig . Builder b = new com . thinkaurelius . titan . diskstorage . util . StandardBaseTransactionConfig . Builder ( ) ; com . thinkaurelius . titan . diskstorage . configuration . ModifiableConfiguration mc = com . thinkaurelius . titan . graphdb . configuration . GraphDatabaseConfiguration . buildGraphConfiguration ( ) ; mc . set ( com . thinkaurelius . titan . diskstorage . cassandra . AbstractCassandraStoreManager . CASSANDRA_WRITE_CONSISTENCY , writeLevel . name ( ) ) ; b . customOptions ( mc ) ; b . timestampProvider ( TimestampProviders . MICRO ) ; com . thinkaurelius . titan . diskstorage . cassandra . CassandraTransaction ct = new com . thinkaurelius . titan . diskstorage . cassandra . CassandraTransaction ( b . build ( ) ) ; org . junit . Assert . assertEquals ( writeLevel , ct . getWriteConsistencyLevel ( ) ) ; levelsChecked ++ ; } }
public class aTest{ @Test public void testAddSameCnameForSameHostname ( ) { io . netty . channel . EventLoopGroup group = new io . netty . channel . DefaultEventLoopGroup ( 1 ) ; try { io . netty . channel . EventLoop loop = group . next ( ) ; final io . netty . resolver . dns . DefaultDnsCnameCache cache = new io . netty . resolver . dns . DefaultDnsCnameCache ( ) ; cache . cache ( "netty.io" , "mapping.netty.io" , 10 , loop ) ; cache . cache ( "netty.io" , "mapping.netty.io" , 10000 , loop ) ; org . junit . Assert . assertEquals ( "mapping.netty.io" , cache . get ( "netty.io" ) ) ; } }
public class aTest{ @Test public void testParentQueueUpdateInQueueMappingFailsAfterAutoCreation ( ) { org . apache . hadoop . yarn . server . resourcemanager . MockRM newMockRM = setupSchedulerInstance ( ) ; org . apache . hadoop . yarn . server . resourcemanager . scheduler . capacity . CapacityScheduler newCS = ( ( org . apache . hadoop . yarn . server . resourcemanager . scheduler . capacity . CapacityScheduler ) ( newMockRM . getResourceScheduler ( ) ) ) ; try { submitApp ( newCS , org . apache . hadoop . yarn . server . resourcemanager . scheduler . capacity . USER0 , org . apache . hadoop . yarn . server . resourcemanager . scheduler . capacity . USER0 , org . apache . hadoop . yarn . server . resourcemanager . scheduler . capacity . PARENT_QUEUE ) ; org . junit . Assert . assertNotNull ( newCS . getQueue ( org . apache . hadoop . yarn . server . resourcemanager . scheduler . capacity . USER0 ) ) ; setupQueueMapping ( newCS , org . apache . hadoop . yarn . server . resourcemanager . scheduler . capacity . USER0 , "d" , org . apache . hadoop . yarn . server . resourcemanager . scheduler . capacity . USER0 ) ; newCS . updatePlacementRules ( ) ; org . apache . hadoop . yarn . server . resourcemanager . RMContext rmContext = mock ( org . apache . hadoop . yarn . server . resourcemanager . RMContext . class ) ; when ( rmContext . getDispatcher ( ) ) . thenReturn ( dispatcher ) ; newCS . setRMContext ( rmContext ) ; org . apache . hadoop . yarn . api . records . ApplicationId appId = org . apache . hadoop . yarn . server . utils . BuilderUtils . newApplicationId ( 1 , 1 ) ; org . apache . hadoop . yarn . server . resourcemanager . scheduler . event . SchedulerEvent addAppEvent = new org . apache . hadoop . yarn . server . resourcemanager . scheduler . event . AppAddedSchedulerEvent ( appId , USER0 , USER0 , new org . apache . hadoop . yarn . server . resourcemanager . placement . ApplicationPlacementContext ( USER0 , "d" ) ) ; newCS . handle ( addAppEvent ) ; org . apache . hadoop . yarn . server . resourcemanager . rmapp . RMAppEvent event = new org . apache . hadoop . yarn . server . resourcemanager . rmapp . RMAppEvent ( appId , org . apache . hadoop . yarn . server . resourcemanager . rmapp . RMAppEventType . APP_REJECTED , "error" ) ; dispatcher . spyOnNextEvent ( event , 10000 ) ; } }
public class aTest{ @Test public void testDiffBetweenDiffs ( ) { com . google . common . collect . ImmutableList < java . lang . String > expectedValue = com . google . common . collect . ImmutableList . of ( "#<sp>HG<sp>changeset<sp>patch" , "#<sp>User<sp>Joe<sp>Blogs<sp><joe.blogs@fb.com>" , "#<sp>Date<sp>1440589545<sp>-3600" , "#<sp>Wed<sp>Aug<sp>26<sp>12:45:45<sp>2015<sp>+0100" , "#<sp>Node<sp>ID<sp>2911b3cab6b24374a3649ebb96b0e53324e9c02e" , "#<sp>Parent<sp>b1fd7e5896af8aa30e3e797ef1445605eec6d055" , "diverge<sp>from<sp>master_2" , "" , "#<sp>Wed<sp>Aug<sp>26<sp>12:45:45<sp>2015<sp>+0100" 1 , "#<sp>Wed<sp>Aug<sp>26<sp>12:45:45<sp>2015<sp>+0100" 2 , "" ) ; try ( java . io . InputStream diffFileStream = com . facebook . buck . util . versioncontrol . HgCmdLineInterfaceIntegrationTest . repoThreeCmdLine . diffBetweenRevisions ( "b1fd7e" , "#<sp>Wed<sp>Aug<sp>26<sp>12:45:45<sp>2015<sp>+0100" 0 ) . get ( ) ) { java . io . InputStreamReader diffFileReader = new java . io . InputStreamReader ( diffFileStream , com . google . common . base . Charsets . UTF_8 ) ; java . lang . String actualDiff = com . google . common . io . CharStreams . toString ( diffFileReader ) ; org . junit . Assert . assertEquals ( java . lang . String . join ( "\n" , expectedValue ) , actualDiff ) ; } } }
public class aTest{ @Test public void testCompareScreen_fourRetriesImagesDiffer_retriesFourTimes ( ) { com . vaadin . testbench . Parameters . setMaxScreenshotRetries ( 4 ) ; try { org . openqa . selenium . WebDriver driver = mockScreenshotDriver ( 4 , true ) ; com . vaadin . testbench . screenshot . ReferenceNameGenerator rngMock = mockReferenceNameGenerator ( "foo" , "foo_bar_11" ) ; com . vaadin . testbench . screenshot . ImageComparison icMock = mockImageComparison ( 4 , "foo_bar_11" , false ) ; replay ( driver , icMock , rngMock ) ; com . vaadin . testbench . commands . TestBenchCommandExecutor tbce = new com . vaadin . testbench . commands . TestBenchCommandExecutor ( icMock , rngMock ) ; tbce . setDriver ( com . vaadin . testbench . TestBench . createDriver ( driver , tbce ) ) ; org . junit . Assert . assertFalse ( tbce . compareScreen ( "foo" ) ) ; verify ( driver , icMock , rngMock ) ; } }
public class aTest{ @Test public void testCustomAgg ( ) { fr . inria . corese . core . Graph g = fr . inria . corese . core . Graph . create ( ) ; fr . inria . corese . core . query . QueryProcess exec = fr . inria . corese . core . query . QueryProcess . create ( g ) ; java . lang . String init = "<sp>xt:get(?l,<sp>xsd:integer((xt:size(?l)<sp>-<sp>1)<sp>/<sp>2))" 1 + ( ( "[]<sp>rdf:value<sp>4,<sp>5,<sp>6,<sp>7" + "<sp>xt:get(?l,<sp>xsd:integer((xt:size(?l)<sp>-<sp>1)<sp>/<sp>2))" 0 ) + "}" ) ; java . lang . String q = "<sp>xt:get(?l,<sp>xsd:integer((xt:size(?l)<sp>-<sp>1)<sp>/<sp>2))" 2 + ( ( ( ( ( ( ( ( ( "where<sp>{" + "<sp>?x<sp>rdf:value<sp>?v<sp>" ) + "}" ) + "" ) + "function<sp>us:mediane(?list){" ) + "<sp>let<sp>(?l<sp>=<sp>xt:sort(?list)){" ) + "<sp>xt:get(?l,<sp>xsd:integer((xt:size(?l)<sp>-<sp>1)<sp>/<sp>2))" ) + "<sp>}" ) + "}" ) + "" ) ; exec . query ( init ) ; fr . inria . corese . kgram . core . Mappings map = exec . query ( q ) ; fr . inria . corese . sparql . api . IDatatype dt = ( ( fr . inria . corese . sparql . api . IDatatype ) ( map . getValue ( "?res" ) ) ) ; org . junit . Assert . assertEquals ( 4 , dt . intValue ( ) ) ; } }
public class aTest{ @Test public void testImportHashes ( ) { org . neo4j . io . fs . FileUtils . deleteRecursively ( new java . io . File ( org . neo4j . batchimport . ImporterIntegrationTest . DB_DIRECTORY ) ) ; java . io . FileWriter writer = new java . io . FileWriter ( "target/hashes.csv" ) ; writer . write ( "a\n000000F8BE951D6DE6480F4AFDFB670C553E47C0\r\n0000021449360C1A398ED9A18800B2B13AA098A4\r\n00000DABDE4C555FC82F7D534835247B94873C2C\r\n00001BE4128DB41729365A41D3AC1D019E5ED8A6\r\n" ) ; writer . close ( ) ; org . neo4j . batchimport . Importer . main ( org . neo4j . batchimport . ImporterIntegrationTest . DB_DIRECTORY , "target/hashes.csv" ) ; org . neo4j . consistency . ConsistencyCheckTool . main ( new java . lang . String [ ] { org . neo4j . batchimport . ImporterIntegrationTest . DB_DIRECTORY } ) ; org . neo4j . graphdb . GraphDatabaseService db = new org . neo4j . graphdb . factory . GraphDatabaseFactory ( ) . newEmbeddedDatabase ( new java . io . File ( org . neo4j . batchimport . ImporterIntegrationTest . DB_DIRECTORY ) ) ; try ( org . neo4j . graphdb . Transaction tx = db . beginTx ( ) ) { for ( org . neo4j . graphdb . Node node : db . getAllNodes ( ) ) { java . lang . Object value = node . getProperty ( "a" , null ) ; System . out . println ( ( "value<sp>=<sp>" + value ) ) ; org . junit . Assert . assertTrue ( ( value != null ) ) ; } }
public class aTest{ @Test public void receiveStringTest ( ) { org . eclipse . smarthome . binding . mqtt . generic . internal . generic . ChannelState c = spy ( new org . eclipse . smarthome . binding . mqtt . generic . internal . generic . ChannelState ( config , channelUID , textValue , channelStateUpdateListener ) ) ; java . util . concurrent . CompletableFuture < java . lang . @ org . eclipse . jdt . annotation . Nullable Void > future = c . start ( connection , scheduler , 100 ) ; c . processMessage ( "state" , "A<sp>TEST" . getBytes ( ) ) ; future . get ( 300 , TimeUnit . MILLISECONDS ) ; org . junit . Assert . assertThat ( textValue . getChannelState ( ) . toString ( ) , org . hamcrest . CoreMatchers . is ( "A<sp>TEST" ) ) ; verify ( channelStateUpdateListener ) . updateChannelState ( eq ( channelUID ) , org . hamcrest . CoreMatchers . any ( ) ) ; } }
public class aTest{ @Test public void validate_shouldPassValidationForLocationClassIfFieldLengthsAreCorrect ( ) { org . openmrs . Location location = new org . openmrs . Location ( ) ; location . setName ( "name" ) ; location . setDescription ( "location" 5 ) ; location . setAddress1 ( "location" 6 ) ; location . setAddress2 ( "address2" ) ; location . setAddress3 ( "address3" ) ; location . setAddress4 ( "location" 0 ) ; location . setAddress5 ( "address5" ) ; location . setAddress6 ( "address6" ) ; location . setCityVillage ( "cityVillage" ) ; location . setStateProvince ( "stateProvince" ) ; location . setCountry ( "location" 2 ) ; location . setPostalCode ( "location" 3 ) ; location . setLatitude ( "latitude" ) ; location . setLongitude ( "location" 4 ) ; location . setCountyDistrict ( "countyDistrict" ) ; location . setRetireReason ( "location" 1 ) ; org . springframework . validation . Errors errors = new org . springframework . validation . BindException ( location , "location" ) ; dao . validate ( location , errors ) ; org . junit . Assert . assertFalse ( errors . hasErrors ( ) ) ; } }
public class aTest{ @Test public void testCreateSearchIndexHelperInvalidSearchIndexType ( ) { try { searchIndexServiceImpl . createSearchIndexHelper ( new org . finra . herd . model . api . xml . SearchIndexKey ( SEARCH_INDEX_NAME ) , org . finra . herd . service . impl . SEARCH_INDEX_TYPE ) ; org . junit . Assert . fail ( ) ; } catch ( java . lang . IllegalArgumentException e ) { org . junit . Assert . assertEquals ( java . lang . String . format ( "Search<sp>index<sp>type<sp>with<sp>code<sp>\"%s\"<sp>is<sp>not<sp>supported." , org . finra . herd . service . impl . SEARCH_INDEX_TYPE ) , e . getMessage ( ) ) ; } }
public class aTest{ @Test public void testMultipleLabels ( ) { org . apache . tinkerpop . gremlin . structure . Vertex a1 = this . sqlgGraph . addVertex ( T . label , "A" , "name" , "a1" ) ; org . apache . tinkerpop . gremlin . structure . Vertex b1 = this . sqlgGraph . addVertex ( T . label , "B" , "name" , "b1" ) ; org . apache . tinkerpop . gremlin . structure . Vertex b2 = this . sqlgGraph . addVertex ( T . label , "B" , "name" , "a" 0 ) ; org . apache . tinkerpop . gremlin . structure . Vertex b3 = this . sqlgGraph . addVertex ( T . label , "B" , "name" , "a" 0 ) ; org . apache . tinkerpop . gremlin . structure . Vertex c1 = this . sqlgGraph . addVertex ( T . label , "C" , "name" , "c1" ) ; a1 . addEdge ( "ab" , b1 ) ; a1 . addEdge ( "ab" , b2 ) ; a1 . addEdge ( "ab" , b3 ) ; b1 . addEdge ( "bc" , c1 ) ; this . sqlgGraph . tx ( ) . commit ( ) ; java . util . List < org . apache . tinkerpop . gremlin . structure . Vertex > vertices = this . sqlgGraph . traversal ( ) . V ( a1 . id ( ) ) . as ( "a" ) . out ( "ab" ) . as ( "a" ) . out ( "bc" ) . as ( "a" ) . toList ( ) ; org . junit . Assert . assertEquals ( 1 , vertices . size ( ) ) ; } }
public class aTest{ @Test public void testVerificationWithWycheproofVectors ( ) { org . json . JSONObject json = com . google . crypto . tink . WycheproofTestUtil . readJson ( "testGroups" 3 ) ; int errors = 0 ; org . json . JSONArray testGroups = json . getJSONArray ( "testGroups" ) ; for ( int i = 0 ; i < ( testGroups . length ( ) ) ; i ++ ) { org . json . JSONObject group = testGroups . getJSONObject ( i ) ; org . json . JSONObject key = group . getJSONObject ( "key" ) ; byte [ ] publicKey = com . google . crypto . tink . subtle . Hex . decode ( key . getString ( "pk" ) ) ; org . json . JSONArray tests = group . getJSONArray ( "tests" ) ; for ( int j = 0 ; j < ( tests . length ( ) ) ; j ++ ) { org . json . JSONObject testcase = tests . getJSONObject ( j ) ; java . lang . String tcId = java . lang . String . format ( "testcase<sp>%d<sp>(%s)" , testcase . getInt ( "tcId" ) , testcase . getString ( "testGroups" 2 ) ) ; byte [ ] msg = getMessage ( testcase ) ; byte [ ] sig = com . google . crypto . tink . subtle . Hex . decode ( testcase . getString ( "sig" ) ) ; java . lang . String result = testcase . getString ( "result" ) ; com . google . crypto . tink . subtle . Ed25519Verify verifier = new com . google . crypto . tink . subtle . Ed25519Verify ( publicKey ) ; try { verifier . verify ( sig , msg ) ; if ( result . equals ( "testGroups" 0 ) ) { System . out . printf ( "testGroups" 1 , tcId ) ; errors ++ ; } } catch ( java . security . GeneralSecurityException ex ) { if ( result . equals ( "valid" ) ) { System . out . printf ( "FAIL<sp>%s:<sp>rejecting<sp>valid<sp>signature,<sp>exception:<sp>%s%n" , tcId , ex ) ; errors ++ ; } } } } org . junit . Assert . assertEquals ( 0 , errors ) ; } }
public class aTest{ @Test public void testInsertWithDeferredIdAllocation_LongId ( ) { com . jmethods . catatumbo . DatastoreTransaction transaction = com . jmethods . catatumbo . DatastoreTransactionTest . em . newTransaction ( ) ; try { com . jmethods . catatumbo . entities . LongId entity = new com . jmethods . catatumbo . entities . LongId ( ) ; entity . setField1 ( "Transaction<sp>Insert<sp>Test<sp>with<sp>Deferred<sp>ID<sp>Allocation" ) ; transaction . insertWithDeferredIdAllocation ( entity ) ; com . jmethods . catatumbo . DatastoreTransaction . Response response = transaction . commit ( ) ; org . junit . Assert . assertTrue ( ( ( response . getGeneratedKeys ( ) . size ( ) ) == 1 ) ) ; } }
public class aTest{ @Test public void testGetServicesLocalNoneAvailable ( ) { java . util . concurrent . Future < java . util . List < org . societies . api . schema . servicelifecycle . model . Service > > asyncResult = null ; try { stub ( mockedHost . getJid ( ) ) . toReturn ( hostJid ) ; stub ( mockedServiceReg . retrieveServicesInCSSNode ( mockedHost . getJid ( ) ) ) . toReturn ( null ) ; stub ( mockedHost . getType ( ) ) . toReturn ( IdentityType . CSS ) ; stub ( mockedCommManager . getIdManager ( ) ) . toReturn ( mockedIdentityManager ) ; stub ( mockedCommManager . getIdManager ( ) . getThisNetworkNode ( ) ) . toReturn ( mockedHost ) ; asyncResult = classUnderTest . getLocalServices ( ) ; org . junit . Assert . assertNull ( asyncResult . get ( ) ) ; } }
public class aTest{ @Test public void testGetUserByCompanyIdByDepartmentId_Test1 ( ) { org . mockito . Mockito . reset ( mockUserCompanyMapper ) ; org . mockito . Mockito . doReturn ( new java . util . ArrayList < com . onboard . domain . model . User > ( ) ) . when ( mockUserCompanyMapper ) . selectByExample ( org . mockito . Mockito . argThat ( new com . onboard . test . exampleutils . ExampleMatcher < com . onboard . domain . mapper . model . UserCompanyExample > ( ) { @ com . onboard . service . account . impl . test . Override public boolean matches ( com . onboard . domain . mapper . model . common . BaseExample example ) { return ( com . onboard . test . exampleutils . CriterionVerifier . verifyEqualTo ( example , "companyId" , ModuleHelper . companyId ) ) && ( com . onboard . test . exampleutils . CriterionVerifier . verifyEqualTo ( example , "groupId" , ModuleHelper . groupId ) ) ; } } ) ) ; java . util . List < com . onboard . domain . model . User > list = testedUserServiceImpl . getUserByCompanyIdByDepartmentId ( ModuleHelper . groupId , ModuleHelper . companyId ) ; org . junit . Assert . assertEquals ( 0 , list . size ( ) ) ; verify ( mockUserCompanyMapper , times ( 1 ) ) . selectByExample ( org . mockito . Mockito . argThat ( new com . onboard . test . exampleutils . ExampleMatcher < com . onboard . domain . mapper . model . UserCompanyExample > ( ) { @ com . onboard . service . account . impl . test . Override public boolean matches ( com . onboard . domain . mapper . model . common . BaseExample example ) { return ( com . onboard . test . exampleutils . CriterionVerifier . verifyEqualTo ( example , "companyId" , ModuleHelper . companyId ) ) && ( com . onboard . test . exampleutils . CriterionVerifier . verifyEqualTo ( example , "groupId" , ModuleHelper . groupId ) ) ; } } }
public class aTest{ @Test public void testAddHandlerBeforeRegisteredThenReplace ( ) { final io . netty . channel . EventLoop loop = io . netty . channel . DefaultChannelPipelineTest . group . next ( ) ; final java . util . concurrent . CountDownLatch latch = new java . util . concurrent . CountDownLatch ( 1 ) ; io . netty . channel . DefaultChannelPipelineTest . CheckEventExecutorHandler handler = new io . netty . channel . DefaultChannelPipelineTest . CheckEventExecutorHandler ( loop ) ; io . netty . channel . ChannelPipeline pipeline = new io . netty . channel . local . LocalChannel ( ) . pipeline ( ) ; pipeline . addFirst ( handler ) ; org . junit . Assert . assertFalse ( handler . addedPromise . isDone ( ) ) ; io . netty . channel . DefaultChannelPipelineTest . group . register ( pipeline . channel ( ) ) ; handler . addedPromise . syncUninterruptibly ( ) ; pipeline . replace ( handler , null , new io . netty . channel . ChannelHandlerAdapter ( ) { @ io . netty . channel . Override public void handlerAdded ( io . netty . channel . ChannelHandlerContext ctx ) throws io . netty . channel . Exception { latch . countDown ( ) ; } } }
public class aTest{ @Test public void testLexerUnicodeEscapedSMPSetWithRange ( ) { org . antlr . v4 . tool . LexerGrammar lg = new org . antlr . v4 . tool . LexerGrammar ( ( "2:RULE_STOP<sp>0\n" 2 + "2:RULE_STOP<sp>0\n" 3 ) ) ; java . lang . String expecting = "max<sp>type<sp>1\n" + ( ( ( ( ( ( ( ( ( ( ( ( "2:RULE_STOP<sp>0\n" 5 + "1:RULE_START<sp>0\n" ) + "2:RULE_STOP<sp>0\n" ) + "2:RULE_STOP<sp>0\n" 1 ) + "4:BASIC<sp>0\n" ) + "2:RULE_STOP<sp>0\n" 0 ) + "mode<sp>0:0\n" ) + "0:119823..119827,<sp>128065..128065,<sp>128169..128170\n" ) + "0->1<sp>EPSILON<sp>0,0,0\n" ) + "1->3<sp>EPSILON<sp>0,0,0\n" ) + "3->4<sp>SET<sp>0,0,0\n" ) + "2:RULE_STOP<sp>0\n" 4 ) + "0:0\n" ) ; org . antlr . v4 . runtime . atn . ATN atn = createATN ( lg , true ) ; java . lang . String result = org . antlr . v4 . runtime . atn . ATNSerializer . getDecoded ( atn , java . util . Arrays . asList ( lg . getTokenNames ( ) ) ) ; org . junit . Assert . assertEquals ( expecting , result ) ; } }
public class aTest{ @Test public void testDescendantNodeQueries ( ) { org . jahia . services . content . JCRSessionWrapper session = org . jahia . services . content . JCRSessionFactory . getInstance ( ) . getCurrentUserSession ( Constants . EDIT_WORKSPACE , org . jahia . utils . LanguageCodeConverters . languageCodeToLocale ( org . jahia . services . query . QueryResultIT . DEFAULT_LANGUAGE ) ) ; org . jahia . services . query . QueryResultWrapper res ; res = doQuery ( session , ( ( "/contents/events])<sp>ORDER<sp>BY<sp>content.[jcr:title]" 0 + ( org . jahia . services . query . QueryResultIT . SITECONTENT_ROOT_NODE ) ) + "/contents/news])<sp>ORDER<sp>BY<sp>news.[jcr:title]" ) , Query . JCR_SQL2 ) ; checkResultSize ( res , 23 ) ; res = doQuery ( session , ( ( ( ( "/contents/events])<sp>ORDER<sp>BY<sp>content.[jcr:title]" 6 + ( org . jahia . services . query . QueryResultIT . SITECONTENT_ROOT_NODE ) ) + "/contents/events])<sp>ORDER<sp>BY<sp>content.[jcr:title]" 9 ) + ( org . jahia . services . query . QueryResultIT . SITECONTENT_ROOT_NODE ) ) + "/contents/events])<sp>ORDER<sp>BY<sp>content.[jcr:title]" ) , Query . JCR_SQL2 ) ; checkResultSize ( res , 50 ) ; res = doQuery ( session , ( ( "SELECT<sp>*<sp>FROM<sp>[jmix:editorialContent]<sp>as<sp>content<sp>WHERE<sp>NOT<sp>ISDESCENDANTNODE(content,<sp>[" + ( org . jahia . services . query . QueryResultIT . SITECONTENT_ROOT_NODE ) ) + "/contents/news])<sp>ORDER<sp>BY<sp>content.[jcr:title]" ) , Query . JCR_SQL2 ) ; int size = checkHierarchy ( res , org . jahia . services . query . QueryResultIT . NOT_DESCENDANT_CHECK , ( ( org . jahia . services . query . QueryResultIT . SITECONTENT_ROOT_NODE ) + "/contents/news" ) ) ; res = doQuery ( session , ( ( ( ( "/contents/events])<sp>ORDER<sp>BY<sp>content.[jcr:title]" 2 + ( org . jahia . services . query . QueryResultIT . SITECONTENT_ROOT_NODE ) ) + "/contents/events])<sp>ORDER<sp>BY<sp>content.[jcr:title]" 8 ) + ( org . jahia . services . query . QueryResultIT . SITECONTENT_ROOT_NODE ) ) + "/contents/news]))<sp>ORDER<sp>BY<sp>content.[jcr:title]" ) , Query . JCR_SQL2 ) ; int size2 = checkHierarchy ( res , org . jahia . services . query . QueryResultIT . NOT_DESCENDANT_CHECK , ( ( org . jahia . services . query . QueryResultIT . SITECONTENT_ROOT_NODE ) + "/contents/events" ) , ( ( org . jahia . services . query . QueryResultIT . SITECONTENT_ROOT_NODE ) + "/contents/news" ) ) ; org . junit . Assert . assertEquals ( "/contents/events])<sp>ORDER<sp>BY<sp>content.[jcr:title]" 4 , 27 , ( size - size2 ) ) ; res = doQuery ( session , ( ( ( ( "/contents/events])<sp>ORDER<sp>BY<sp>content.[jcr:title]" 6 + ( org . jahia . services . query . QueryResultIT . SITECONTENT_ROOT_NODE ) ) + "/contents/events])<sp>ORDER<sp>BY<sp>content.[jcr:title]" 1 ) + ( org . jahia . services . query . QueryResultIT . SITECONTENT_ROOT_NODE ) ) + "/contents/news])<sp>ORDER<sp>BY<sp>content.[jcr:title]" ) , Query . JCR_SQL2 ) ; checkResultSize ( res , 27 ) ; res = doQuery ( session , ( ( "/contents/events])<sp>ORDER<sp>BY<sp>content.[jcr:title]" 6 + ( org . jahia . services . query . QueryResultIT . SITECONTENT_ROOT_NODE ) ) + "/contents/events])<sp>ORDER<sp>BY<sp>content.[jcr:title]" 3 ) , Query . JCR_SQL2 ) ; checkResultSize ( res , 11 ) ; res = doQuery ( session , ( ( "/contents/events])<sp>ORDER<sp>BY<sp>content.[jcr:title]" 6 + ( org . jahia . services . query . QueryResultIT . SITECONTENT_ROOT_NODE ) ) + "/contents/news])<sp>AND<sp>contains(content.*,<sp>'cucumber')" ) , Query . JCR_SQL2 ) ; checkResultSize ( res , 13 ) ; res = doQuery ( session , ( ( ( ( ( ( "SELECT<sp>*<sp>FROM<sp>[jmix:editorialContent]<sp>as<sp>content<sp>WHERE<sp>(ISCHILDNODE(content,<sp>[" + ( org . jahia . services . query . QueryResultIT . SITECONTENT_ROOT_NODE ) ) + "/contents/news])<sp>AND<sp>content.[jcr:title]<sp>LIKE<sp>'news_1%')<sp>OR<sp>(ISDESCENDANTNODE(content,<sp>[" ) + ( org . jahia . services . query . QueryResultIT . SITECONTENT_ROOT_NODE ) ) + "/contents/events])<sp>ORDER<sp>BY<sp>content.[jcr:title]" 5 ) + ( org . jahia . services . query . QueryResultIT . GENEVA ) ) + "/contents/events])<sp>ORDER<sp>BY<sp>content.[jcr:title]" 7 ) , Query . JCR_SQL2 ) ; checkResultSize ( res , ( 7 + 12 ) ) ; } }
public class aTest{ @Test public void testTopic ( ) { try { mq . createTopicProducer ( "tmpIn" ) ; mq . createTopicConsumer ( "tmpOut" ) ; javax . jms . MessageListener listener = new javax . jms . MessageListener ( ) { at . ac . tuwien . infosys . jcloudscale . messaging . IMQWrapper responder = at . ac . tuwien . infosys . jcloudscale . configuration . JCloudScaleConfiguration . createMQWrapper ( ) ; @ at . ac . tuwien . infosys . jcloudscale . test . unit . Override public void onMessage ( javax . jms . Message msg ) { try { javax . jms . Destination dest ; dest = msg . getJMSReplyTo ( ) ; responder . respond ( new at . ac . tuwien . infosys . jcloudscale . messaging . objects . ui . ListServersResponse ( ) , dest ) ; } catch ( javax . jms . JMSException e ) { e . printStackTrace ( ) ; } finally { responder . close ( ) ; } } } ; at . ac . tuwien . infosys . jcloudscale . messaging . IMQWrapper serverSim = at . ac . tuwien . infosys . jcloudscale . configuration . JCloudScaleConfiguration . createMQWrapper ( ) ; try { serverSim . createTopicConsumer ( "tmpIn" ) ; serverSim . registerListener ( listener ) ; at . ac . tuwien . infosys . jcloudscale . messaging . objects . MessageObject resp = mq . requestResponse ( new at . ac . tuwien . infosys . jcloudscale . messaging . objects . ui . ListServersRequest ( ) ) ; org . junit . Assert . assertNotNull ( resp ) ; } }
public class aTest{ @Test public void testEnableLocalStaging ( ) { com . liferay . portal . kernel . model . Group group = com . liferay . portal . kernel . test . util . GroupTestUtil . addGroup ( ) ; try { com . liferay . exportimport . kernel . service . StagingLocalServiceUtil . enableLocalStaging ( _user . getUserId ( ) , group , false , false , new com . liferay . portal . kernel . service . ServiceContext ( ) ) ; group = com . liferay . portal . kernel . service . GroupLocalServiceUtil . getGroup ( group . getGroupId ( ) ) ; org . junit . Assert . assertTrue ( group . hasStagingGroup ( ) ) ; } }
public class aTest{ @Test public void testGetScreenWidth ( ) { try { java . awt . Dimension screenSize = java . awt . Toolkit . getDefaultToolkit ( ) . getScreenSize ( ) ; org . junit . Assert . assertEquals ( org . esa . beam . framework . ui . UIUtils . getScreenWidth ( ) , screenSize . width ) ; } }
public class aTest{ @Test public void testInnerJoin ( ) { java . sql . ResultSet rs = conn . createStatement ( ) . executeQuery ( ( ( "<sp>0<sp>|<sp>0<sp>|\n" 1 + ( joinStrategy ) ) + "inner<sp>join<sp>a<sp>b<sp>on<sp>a.ia[0]<sp>=<sp>b.ia[0]" ) ) ; java . lang . String s = TestUtils . FormattedResult . ResultFactory . toString ( rs ) ; rs . close ( ) ; java . lang . String expected = "<sp>0<sp>|<sp>0<sp>|\n" 2 + ( ( ( ( ( ( ( ( ( ( "--------\n" + "<sp>0<sp>|<sp>0<sp>|\n" ) + "<sp>1<sp>|<sp>1<sp>|\n" ) + "<sp>2<sp>|<sp>2<sp>|\n" ) + "<sp>3<sp>|<sp>3<sp>|\n" ) + "<sp>0<sp>|<sp>0<sp>|\n" 0 ) + "<sp>5<sp>|<sp>5<sp>|\n" ) + "<sp>0<sp>|<sp>0<sp>|\n" 3 ) + "<sp>7<sp>|<sp>7<sp>|\n" ) + "<sp>8<sp>|<sp>8<sp>|\n" ) + "<sp>9<sp>|<sp>9<sp>|" ) ; org . junit . Assert . assertEquals ( s , expected , s ) ; } }
public class aTest{ @Test public void testRegisterServiceErrors ( ) { java . lang . String OBJCLASS = org . osgi . framework . BundleContext . class . getName ( ) ; java . lang . String [ ] OBJCLASSES = new java . lang . String [ ] { OBJCLASS } ; org . osgi . framework . Bundle bundle = installBundle ( getBundleArchiveA ( ) ) ; try { bundle . start ( ) ; org . osgi . framework . BundleContext bundleContext = bundle . getBundleContext ( ) ; org . junit . Assert . assertNotNull ( bundleContext ) ; try { bundleContext . registerService ( ( ( java . lang . String ) ( null ) ) , new java . lang . Object ( ) , null ) ; org . junit . Assert . fail ( "Should<sp>not<sp>be<sp>here!" ) ; } }
public class aTest{ @Test public void testRemoveAssertionWithSwitchCase ( ) { final spoon . reflect . declaration . CtClass < ? > testClass = eu . stamp_project . Utils . findClass ( "fr.inria.assertionremover.TestClassWithAssertToBeRemoved" ) ; final eu . stamp_project . dspot . assertgenerator . AssertionRemover assertionRemover = new eu . stamp_project . dspot . assertgenerator . AssertionRemover ( ) ; testClass . getMethodsByName ( "<sp>cl.getTrue();" 2 ) . get ( 0 ) . getElements ( new spoon . reflect . visitor . filter . TypeFilter < spoon . reflect . code . CtInvocation > ( spoon . reflect . code . CtInvocation . class ) { @ eu . stamp_project . dspot . assertgenerator . Override public boolean matches ( spoon . reflect . code . CtInvocation element ) { return eu . stamp_project . test_framework . TestFramework . get ( ) . isAssert ( element ) ; } } ) . forEach ( assertionRemover :: removeAssertion ) ; final java . lang . String expectedMethod = ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "@org.junit.Test" + ( eu . stamp_project . utils . AmplificationHelper . LINE_SEPARATOR ) ) + "<sp>cl.getTrue();" 0 ) + ( eu . stamp_project . utils . AmplificationHelper . LINE_SEPARATOR ) ) + "<sp>fr.inria.sample.ClassWithBoolean<sp>cl<sp>=<sp>new<sp>fr.inria.sample.ClassWithBoolean();" ) + ( eu . stamp_project . utils . AmplificationHelper . LINE_SEPARATOR ) ) + "<sp>cl.getTrue();" ) + ( eu . stamp_project . utils . AmplificationHelper . LINE_SEPARATOR ) ) + "<sp>int<sp>one<sp>=<sp>1;" ) + ( eu . stamp_project . utils . AmplificationHelper . LINE_SEPARATOR ) ) + "<sp>switch<sp>(one)<sp>{" ) + ( eu . stamp_project . utils . AmplificationHelper . LINE_SEPARATOR ) ) + "<sp>case<sp>1<sp>:" ) + ( eu . stamp_project . utils . AmplificationHelper . LINE_SEPARATOR ) ) + "<sp>fr.inria.assertionremover.TestClassWithAssertToBeRemoved.getNegation(cl.getFalse());" ) + ( eu . stamp_project . utils . AmplificationHelper . LINE_SEPARATOR ) ) + "<sp>cl.getTrue();" 1 ) + ( eu . stamp_project . utils . AmplificationHelper . LINE_SEPARATOR ) ) + "<sp>}" ) + ( eu . stamp_project . utils . AmplificationHelper . LINE_SEPARATOR ) ) + "}" ; org . junit . Assert . assertEquals ( expectedMethod , testClass . getMethodsByName ( "<sp>cl.getTrue();" 2 ) . get ( 0 ) . toString ( ) ) ; } }
public class aTest{ @Test public void testNames ( ) { org . apache . avro . Schema r = org . apache . avro . SchemaBuilder . record ( "org.test.long" 7 ) . fields ( ) . name ( "org.test.long" 6 ) . type ( ) . fixed ( "org.foo.MyFixed" ) . size ( 1 ) . noDefault ( ) . name ( "org.test.long" 0 ) . type ( "org.foo.MyFixed" ) . noDefault ( ) . name ( "org.test.long" 3 ) . type ( "org.foo.MyFixed" , "" ) . noDefault ( ) . name ( "f3" ) . type ( "org.foo.MyFixed" , null ) . noDefault ( ) . name ( "org.test.long" 4 ) . type ( "org.foo.MyFixed" , "org.test.long" 5 ) . noDefault ( ) . name ( "f5" ) . type ( "MyFixed" , "org.foo" ) . noDefault ( ) . endRecord ( ) ; org . apache . avro . Schema expected = org . apache . avro . Schema . createFixed ( "org.foo.MyFixed" , null , null , 1 ) ; checkField ( r , expected , "org.test.long" 6 ) ; checkField ( r , expected , "org.test.long" 0 ) ; checkField ( r , expected , "org.test.long" 3 ) ; checkField ( r , expected , "f3" ) ; checkField ( r , expected , "org.test.long" 4 ) ; checkField ( r , expected , "f5" ) ; org . apache . avro . Schema f = org . apache . avro . SchemaBuilder . builder ( "" ) . fixed ( "org.test.long" 2 ) . size ( 1 ) ; org . junit . Assert . assertEquals ( org . apache . avro . Schema . createFixed ( "org.test.long" 2 , null , null , 1 ) , f ) ; r = org . apache . avro . SchemaBuilder . record ( "org.test.long" 7 ) . namespace ( "org.foo" ) . fields ( ) . name ( "org.test.long" 6 ) . type ( ) . fixed ( "MyFixed" ) . size ( 1 ) . noDefault ( ) . name ( "org.test.long" 0 ) . type ( "org.foo.MyFixed" ) . noDefault ( ) . name ( "org.test.long" 3 ) . type ( "org.foo.MyFixed" , "" ) . noDefault ( ) . name ( "f3" ) . type ( "org.foo.MyFixed" , null ) . noDefault ( ) . name ( "org.test.long" 4 ) . type ( "org.foo.MyFixed" , "org.test.long" 5 ) . noDefault ( ) . name ( "f5" ) . type ( "MyFixed" , "org.foo" ) . noDefault ( ) . name ( "f6" ) . type ( "MyFixed" , null ) . noDefault ( ) . name ( "f7" ) . type ( "MyFixed" ) . noDefault ( ) . endRecord ( ) ; checkField ( r , expected , "org.test.long" 6 ) ; checkField ( r , expected , "org.test.long" 0 ) ; checkField ( r , expected , "org.test.long" 3 ) ; checkField ( r , expected , "f3" ) ; checkField ( r , expected , "org.test.long" 4 ) ; checkField ( r , expected , "f5" ) ; checkField ( r , expected , "f6" ) ; checkField ( r , expected , "f7" ) ; r = org . apache . avro . SchemaBuilder . record ( "org.test.long" 7 ) . namespace ( "org.test.long" 8 ) . fields ( ) . name ( "org.test.long" 6 ) . type ( ) . fixed ( "MyFixed" ) . namespace ( "org.foo" ) . size ( 1 ) . noDefault ( ) . name ( "org.test.long" 0 ) . type ( "org.foo.MyFixed" ) . noDefault ( ) . name ( "org.test.long" 3 ) . type ( "org.foo.MyFixed" , "" ) . noDefault ( ) . name ( "f3" ) . type ( "org.foo.MyFixed" , null ) . noDefault ( ) . name ( "org.test.long" 4 ) . type ( "org.foo.MyFixed" , "org.test.long" 5 ) . noDefault ( ) . name ( "f5" ) . type ( "MyFixed" , "org.foo" ) . noDefault ( ) . endRecord ( ) ; checkField ( r , expected , "org.test.long" 6 ) ; checkField ( r , expected , "org.test.long" 0 ) ; checkField ( r , expected , "org.test.long" 3 ) ; checkField ( r , expected , "f3" ) ; checkField ( r , expected , "org.test.long" 4 ) ; checkField ( r , expected , "f5" ) ; expected = org . apache . avro . Schema . createFixed ( "MyFixed" , null , null , 1 ) ; r = org . apache . avro . SchemaBuilder . record ( "org.test.long" 7 ) . namespace ( "org.test.long" 8 ) . fields ( ) . name ( "org.test.long" 6 ) . type ( ) . fixed ( "MyFixed" ) . namespace ( "" ) . size ( 1 ) . noDefault ( ) . name ( "org.test.long" 0 ) . type ( "MyFixed" , "" ) . noDefault ( ) . endRecord ( ) ; checkField ( r , expected , "org.test.long" 6 ) ; checkField ( r , expected , "org.test.long" 0 ) ; org . apache . avro . SchemaBuilder . fixed ( "org.test.long" ) . size ( 1 ) ; org . apache . avro . SchemaBuilder . fixed ( "long" ) . namespace ( "org.test.long" 1 ) . size ( 1 ) ; org . apache . avro . SchemaBuilder . builder ( "org.test.long" 1 ) . fixed ( "long" ) . size ( 1 ) ; } }
public class aTest{ @Test public void testCommandLine ( ) { try { org . apache . hadoop . fs . FileUtil . fullyDelete ( OUTPUT_DIR . getAbsoluteFile ( ) ) ; } catch ( java . lang . Exception e ) { } try { createInput ( ) ; boolean mayExit = false ; job = new org . apache . hadoop . streaming . StreamJob ( genArgs ( ) , mayExit ) ; job . go ( ) ; org . apache . hadoop . streaming . File outFile = new org . apache . hadoop . streaming . File ( OUTPUT_DIR , "part-00000" ) . getAbsoluteFile ( ) ; java . lang . String output = org . apache . hadoop . streaming . StreamUtil . slurp ( outFile ) ; outFile . delete ( ) ; System . err . println ( ( "outEx1=" + ( outputExpect ) ) ) ; System . err . println ( ( "<sp>out1=" + output ) ) ; System . err . println ( ( "<sp>equals=" + ( outputExpect . compareTo ( output ) ) ) ) ; org . junit . Assert . assertEquals ( outputExpect , output ) ; } }
public class aTest{ @Test public void testPlusDouble ( ) { try ( final com . questdb . store . JournalWriter w = getFactory ( ) . writer ( new com . questdb . store . factory . configuration . JournalStructure ( "XUNYQXTGNJ\t6.359375000000\t18.859375000000\n" 1 ) { { $str ( "ccy" ) ; $double ( "PVQFULMERT\t0.000005817310\t12.500005817310\n" 3 ) ; } } ) ) { com . questdb . std . Rnd rnd = new com . questdb . std . Rnd ( ) ; for ( int i = 0 ; i < 100 ; i ++ ) { com . questdb . store . JournalEntryWriter ew = w . entryWriter ( ) ; ew . putStr ( 0 , rnd . nextString ( 10 ) ) ; ew . putDouble ( 1 , rnd . nextDouble ( ) ) ; ew . append ( ) ; } w . commit ( ) ; com . questdb . std . str . StringSink sink = new com . questdb . std . str . StringSink ( ) ; com . questdb . ql . RecordSourcePrinter p = new com . questdb . ql . RecordSourcePrinter ( sink ) ; com . questdb . BootstrapEnv env = new com . questdb . BootstrapEnv ( ) ; env . configuration = new com . questdb . ServerConfiguration ( ) ; final com . questdb . ql . ops . plus . AddDoubleOperator plus = ( ( com . questdb . ql . ops . plus . AddDoubleOperator ) ( AddDoubleOperator . FACTORY . newInstance ( 0 , env ) ) ) ; plus . setName ( "plus" ) ; plus . setLhs ( new com . questdb . ql . ops . col . DoubleRecordSourceColumn ( w . getMetadata ( ) . getColumnIndex ( "PVQFULMERT\t0.000005817310\t12.500005817310\n" 3 ) , 0 ) ) ; plus . setRhs ( new com . questdb . ql . ops . constant . DoubleConstant ( 12.5 , 0 ) ) ; try ( com . questdb . ql . virtual . VirtualColumnRecordSource src = new com . questdb . ql . virtual . VirtualColumnRecordSource ( compile ( "XUNYQXTGNJ\t6.359375000000\t18.859375000000\n" 1 ) , new com . questdb . std . ObjList < com . questdb . ql . ops . VirtualColumn > ( ) { { add ( plus ) ; } } ) ) { p . print ( src , getFactory ( ) ) ; final java . lang . String expected = "XUNYQXTGNJ\t6.359375000000\t18.859375000000\n" 5 + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "ccy" 8 + "PVQFULMERT\t0.000005817310\t12.500005817310\n" 02 ) + "DEYYQEHBHF\t0.000000401164\t12.500000401164\n" ) + "QHGJBFQBBK\t103.908081054688\t116.408081054688\n" 0 ) + "PVQFULMERT\t0.000005817310\t12.500005817310\n" 5 ) + "plus" 2 ) + "RFBVTMHGOO\t172.796875000000\t185.296875000000\n" ) + "QHGJBFQBBK\t103.908081054688\t116.408081054688\n" 1 ) + "ZGHWVDKFLO\t0.000102697388\t12.500102697388\n" 6 ) + "plus" 6 ) + "PVQFULMERT\t0.000005817310\t12.500005817310\n" 03 ) + "DEYYQEHBHF\t0.000000401164\t12.500000401164\n" 3 ) + "PVQFULMERT\t0.000005817310\t12.500005817310\n" 8 ) + "QHGJBFQBBK\t103.908081054688\t116.408081054688\n" 8 ) + "XUNYQXTGNJ\t6.359375000000\t18.859375000000\n" 9 ) + "DEYYQEHBHF\t0.000000401164\t12.500000401164\n" 4 ) + "ccy" 4 ) + "PVQFULMERT\t0.000005817310\t12.500005817310\n" 9 ) + "ZGHWVDKFLO\t0.000102697388\t12.500102697388\n" ) + "PVQFULMERT\t0.000005817310\t12.500005817310\n" 1 ) + "XUNYQXTGNJ\t6.359375000000\t18.859375000000\n" 8 ) + "ccy" 1 ) + "DEYYQEHBHF\t0.000000401164\t12.500000401164\n" 0 ) + "DEYYQEHBHF\t0.000000401164\t12.500000401164\n" 9 ) + "ZULIGYVFZF\t-327.250000000000\t-314.750000000000\n" 6 ) + "QHGJBFQBBK\t103.908081054688\t116.408081054688\n" 4 ) + "ZULIGYVFZF\t-327.250000000000\t-314.750000000000\n" ) + "ZGHWVDKFLO\t0.000102697388\t12.500102697388\n" 0 ) + "DEYYQEHBHF\t0.000000401164\t12.500000401164\n" 8 ) + "plus" 0 ) + "DEYYQEHBHF\t0.000000401164\t12.500000401164\n" 5 ) + "PVQFULMERT\t0.000005817310\t12.500005817310\n" 00 ) + "plus" 5 ) + "RFBVTMHGOO\t172.796875000000\t185.296875000000\n" 2 ) + "ccy" 9 ) + "ccy" 0 ) + "ZGHWVDKFLO\t0.000102697388\t12.500102697388\n" 1 ) + "ZULIGYVFZF\t-327.250000000000\t-314.750000000000\n" 4 ) + "plus" 4 ) + "ZGHWVDKFLO\t0.000102697388\t12.500102697388\n" 2 ) + "RFBVTMHGOO\t172.796875000000\t185.296875000000\n" 5 ) + "PVQFULMERT\t0.000005817310\t12.500005817310\n" 2 ) + "DEYYQEHBHF\t0.000000401164\t12.500000401164\n" 2 ) + "ZULIGYVFZF\t-327.250000000000\t-314.750000000000\n" 0 ) + "RFBVTMHGOO\t172.796875000000\t185.296875000000\n" 6 ) + "RFBVTMHGOO\t172.796875000000\t185.296875000000\n" 0 ) + "ZULIGYVFZF\t-327.250000000000\t-314.750000000000\n" 5 ) + "QHGJBFQBBK\t103.908081054688\t116.408081054688\n" 9 ) + "QHGJBFQBBK\t103.908081054688\t116.408081054688\n" 7 ) + "XUNYQXTGNJ\t6.359375000000\t18.859375000000\n" 0 ) + "plus" 1 ) + "QHGJBFQBBK\t103.908081054688\t116.408081054688\n" 3 ) + "plus" 9 ) + "ZGHWVDKFLO\t0.000102697388\t12.500102697388\n" 3 ) + "PVQFULMERT\t0.000005817310\t12.500005817310\n" 4 ) + "RFBVTMHGOO\t172.796875000000\t185.296875000000\n" 8 ) + "XUNYQXTGNJ\t6.359375000000\t18.859375000000\n" 6 ) + "PVQFULMERT\t0.000005817310\t12.500005817310\n" 01 ) + "PVQFULMERT\t0.000005817310\t12.500005817310\n" 7 ) + "ZULIGYVFZF\t-327.250000000000\t-314.750000000000\n" 2 ) + "QHGJBFQBBK\t103.908081054688\t116.408081054688\n" 6 ) + "RFBVTMHGOO\t172.796875000000\t185.296875000000\n" 1 ) + "RFBVTMHGOO\t172.796875000000\t185.296875000000\n" 4 ) + "ccy" 2 ) + "ZGHWVDKFLO\t0.000102697388\t12.500102697388\n" 7 ) + "XBBZVRLPTY\t70.810325622559\t83.310325622559\n" ) + "ccy" 5 ) + "ZULIGYVFZF\t-327.250000000000\t-314.750000000000\n" 7 ) + "XUNYQXTGNJ\t6.359375000000\t18.859375000000\n" ) + "ZULIGYVFZF\t-327.250000000000\t-314.750000000000\n" 3 ) + "ZGHWVDKFLO\t0.000102697388\t12.500102697388\n" 4 ) + "XUNYQXTGNJ\t6.359375000000\t18.859375000000\n" 4 ) + "XUNYQXTGNJ\t6.359375000000\t18.859375000000\n" 3 ) + "ccy" 6 ) + "RFBVTMHGOO\t172.796875000000\t185.296875000000\n" 9 ) + "DEYYQEHBHF\t0.000000401164\t12.500000401164\n" 6 ) + "ZULIGYVFZF\t-327.250000000000\t-314.750000000000\n" 9 ) + "RFBVTMHGOO\t172.796875000000\t185.296875000000\n" 3 ) + "PVQFULMERT\t0.000005817310\t12.500005817310\n" ) + "ZULIGYVFZF\t-327.250000000000\t-314.750000000000\n" 1 ) + "plus" 7 ) + "plus" 8 ) + "XUNYQXTGNJ\t6.359375000000\t18.859375000000\n" 7 ) + "PVQFULMERT\t0.000005817310\t12.500005817310\n" 0 ) + "ccy" 7 ) + "QHGJBFQBBK\t103.908081054688\t116.408081054688\n" 5 ) + "ZGHWVDKFLO\t0.000102697388\t12.500102697388\n" 9 ) + "QHGJBFQBBK\t103.908081054688\t116.408081054688\n" ) + "DEYYQEHBHF\t0.000000401164\t12.500000401164\n" 1 ) + "DEYYQEHBHF\t0.000000401164\t12.500000401164\n" 7 ) + "RFBVTMHGOO\t172.796875000000\t185.296875000000\n" 7 ) + "XUNYQXTGNJ\t6.359375000000\t18.859375000000\n" 2 ) + "ZULIGYVFZF\t-327.250000000000\t-314.750000000000\n" 8 ) + "ZGHWVDKFLO\t0.000102697388\t12.500102697388\n" 5 ) + "plus" 3 ) + "PVQFULMERT\t0.000005817310\t12.500005817310\n" 6 ) + "QHGJBFQBBK\t103.908081054688\t116.408081054688\n" 2 ) + "ccy" 3 ) + "ZGHWVDKFLO\t0.000102697388\t12.500102697388\n" 8 ) ; org . junit . Assert . assertEquals ( expected , sink . toString ( ) ) ; } } } }
public class aTest{ @Test public void testPongServerNoData ( ) { org . glassfish . tyrus . server . Server server = startServer ( org . glassfish . tyrus . test . standard_config . PingPongTest . PingPongEndpointNoData . class ) ; try { final java . util . concurrent . CountDownLatch messageLatch = new java . util . concurrent . CountDownLatch ( 1 ) ; org . glassfish . tyrus . client . ClientManager client = createClient ( ) ; client . connectToServer ( new javax . websocket . Endpoint ( ) { @ org . glassfish . tyrus . test . standard_config . Override public void onOpen ( javax . websocket . Session session , javax . websocket . EndpointConfig config ) { try { session . addMessageHandler ( new javax . websocket . MessageHandler . Whole < java . lang . String > ( ) { @ java . lang . Override public void onMessage ( java . lang . String message ) { if ( message . equals ( org . glassfish . tyrus . test . standard_config . PingPongTest . PONG_RECEIVED ) ) { messageLatch . countDown ( ) ; } } } ) ; session . getBasicRemote ( ) . sendText ( "ping-initiator" ) ; } catch ( java . io . IOException e ) { } } } , ClientEndpointConfig . Builder . create ( ) . build ( ) , getURI ( org . glassfish . tyrus . test . standard_config . PingPongTest . PingPongEndpointNoData . class ) ) ; messageLatch . await ( 2 , TimeUnit . SECONDS ) ; org . junit . Assert . assertEquals ( 0 , messageLatch . getCount ( ) ) ; } }
public class aTest{ @Test public void testErrorFlow2 ( ) { org . glassfish . tyrus . spi . ClientEngine engine = getClientEngine ( java . util . Collections . < java . lang . String , java . lang . Object > emptyMap ( ) ) ; org . glassfish . tyrus . spi . UpgradeRequest upgradeRequest = engine . createUpgradeRequest ( null ) ; org . junit . Assert . assertNotNull ( "" , upgradeRequest ) ; engine . processError ( new java . lang . Exception ( ) ) ; try { engine . processResponse ( null , null , null ) ; org . junit . Assert . fail ( "processResponse<sp>after<sp>processError<sp>must<sp>fail." ) ; } }
public class aTest{ @Test public void testResourceNameNotSet ( ) { java . io . InputStream is ; try { is = new java . io . FileInputStream ( content ) ; ddf . catalog . resource . impl . ResourceImpl ri = new ddf . catalog . resource . impl . ResourceImpl ( is , mimeType , null ) ; org . junit . Assert . assertEquals ( null , ri . getName ( ) ) ; } }
public class aTest{ @Test public void findModule ( ) { final org . erlide . engine . model . root . IErlModule module2 = org . erlide . engine . util . ErlideTestUtils . createModule ( project , "zz.erl" , "-module(zz).\n" ) ; final org . eclipse . core . resources . IFile file = ( ( org . eclipse . core . resources . IFile ) ( module2 . getResource ( ) ) ) ; final org . erlide . engine . model . root . IErlModule findModule = model . findModule ( file ) ; final org . erlide . engine . model . IErlElement parent = ( ( org . erlide . engine . model . IErlElement ) ( module2 . getParent ( ) ) ) ; final org . eclipse . core . resources . IFolder folder = ( ( org . eclipse . core . resources . IFolder ) ( parent . getResource ( ) ) ) ; org . eclipse . core . resources . IFile createFile = null ; try { createFile = org . erlide . engine . util . ErlideTestUtils . createFile ( folder . getProject ( ) , ( ( folder . getName ( ) ) + "/tt.erl" ) , "-module(tt).\n" ) ; model . findModule ( createFile ) ; org . junit . Assert . assertEquals ( module2 , findModule ) ; } }
public class aTest{ @Test public void readRequest_nullHeaders_expectSuccess ( ) { com . amazonaws . serverless . proxy . model . AwsProxyRequest req = new com . amazonaws . serverless . proxy . internal . testutils . AwsProxyRequestBuilder ( "/path" , "GET" ) . build ( ) ; req . setMultiValueHeaders ( null ) ; try { com . amazonaws . serverless . proxy . internal . servlet . AwsProxyHttpServletRequest servletReq = reader . readRequest ( req , null , null , com . amazonaws . serverless . proxy . model . ContainerConfig . defaultConfig ( ) ) ; java . lang . String headerValue = servletReq . getHeader ( HttpHeaders . CONTENT_TYPE ) ; org . junit . Assert . assertNull ( headerValue ) ; } }
public class aTest{ @Test public void testLexerUnicodeEscapedBMPSetWithRange ( ) { org . antlr . v4 . tool . LexerGrammar lg = new org . antlr . v4 . tool . LexerGrammar ( ( "2:RULE_STOP<sp>0\n" 2 + "2:RULE_STOP<sp>0\n" 3 ) ) ; java . lang . String expecting = "max<sp>type<sp>1\n" + ( ( ( ( ( ( ( ( ( ( ( ( "2:RULE_STOP<sp>0\n" 4 + "1:RULE_START<sp>0\n" ) + "2:RULE_STOP<sp>0\n" ) + "2:RULE_STOP<sp>0\n" 1 ) + "4:BASIC<sp>0\n" ) + "2:RULE_STOP<sp>0\n" 0 ) + "mode<sp>0:0\n" ) + "2:RULE_STOP<sp>0\n" 5 ) + "0->1<sp>EPSILON<sp>0,0,0\n" ) + "1->3<sp>EPSILON<sp>0,0,0\n" ) + "3->4<sp>SET<sp>0,0,0\n" ) + "4->2<sp>EPSILON<sp>0,0,0\n" ) + "0:0\n" ) ; org . antlr . v4 . runtime . atn . ATN atn = createATN ( lg , true ) ; java . lang . String result = org . antlr . v4 . runtime . atn . ATNSerializer . getDecoded ( atn , java . util . Arrays . asList ( lg . getTokenNames ( ) ) ) ; org . junit . Assert . assertEquals ( expecting , result ) ; } }
public class aTest{ @Test public void whenSearchingWithIncompleteKeyword_thenKeywordSuggestionsShouldBeReturned ( ) { com . baeldung . solr . fulltext . search . service . ItemSearchServiceLiveTest . itemSearchService . index ( "hm0001" , "/suggest" 6 , "Home<sp>Appliances" , 100.0F ) ; com . baeldung . solr . fulltext . search . service . ItemSearchServiceLiveTest . itemSearchService . index ( "hm0002" , "/suggest" 8 , "Home<sp>Appliances" , 300.0F ) ; com . baeldung . solr . fulltext . search . service . ItemSearchServiceLiveTest . itemSearchService . index ( "/suggest" 7 , "Brand2<sp>Ceiling<sp>Fan" , "Home<sp>Appliances" , 200.0F ) ; com . baeldung . solr . fulltext . search . service . ItemSearchServiceLiveTest . itemSearchService . index ( "/suggest" 1 , "Brand2<sp>Dishwasher" , "/suggest" 2 , 250.0F ) ; org . apache . solr . client . solrj . SolrQuery query = new org . apache . solr . client . solrj . SolrQuery ( ) ; query . setRequestHandler ( "/suggest" ) ; query . set ( "/suggest" 0 , "/suggest" 3 ) ; query . set ( "suggest.build" , "/suggest" 3 ) ; query . set ( "/suggest" 4 , "mySuggester" ) ; query . set ( "suggest.q" , "Hom" ) ; org . apache . solr . client . solrj . response . QueryResponse response = com . baeldung . solr . fulltext . search . service . ItemSearchServiceLiveTest . solrClient . query ( query ) ; org . apache . solr . client . solrj . response . SuggesterResponse suggesterResponse = response . getSuggesterResponse ( ) ; java . util . Map < java . lang . String , java . util . List < java . lang . String > > suggestedTerms = suggesterResponse . getSuggestedTerms ( ) ; java . util . List < java . lang . String > suggestions = suggestedTerms . get ( "mySuggester" ) ; org . junit . Assert . assertEquals ( 2 , suggestions . size ( ) ) ; for ( java . lang . String term : suggestions ) { if ( ( ! ( "Home<sp>Appliances" . equals ( term ) ) ) && ( ! ( "/suggest" 2.e quals ( term ) ) ) ) { org . junit . Assert . fail ( "/suggest" 5 ) ; } } } }
public class aTest{ @Test public void test8 ( ) { final com . persistit . Key key1 = new com . persistit . Key ( _persistit ) ; System . out . print ( "test8<sp>" ) ; final int start = Integer . MIN_VALUE ; final int end = ( Integer . MAX_VALUE ) - ( com . persistit . unit . KeyTest1 . INCREMENT ) ; for ( int u = start ; u < end ; u += com . persistit . unit . KeyTest1 . INCREMENT ) { if ( ( u % 100000000 ) == 0 ) { System . out . print ( ( "<sp>" + u ) ) ; } key1 . clear ( ) . append ( u ) ; final int v = key1 . reset ( ) . decodeInt ( ) ; if ( u != v ) { org . junit . Assert . assertEquals ( u , v ) ; } } }
public class aTest{ @Test public void convert ( org . apache . servicecomb . swagger . invocation . SwaggerInvocation , org . apache . servicecomb . swagger . invocation . exception . InvocationException ) { org . apache . servicecomb . swagger . invocation . Response response = converter . convert ( swaggerInvocation , e ) ; org . junit . Assert . assertSame ( e , response . getResult ( ) ) ; } }
public class aTest{ @Test public void testEncodePostgreSQLUnderscore ( ) { System . out . println ( ( ( com . telefonica . iot . cygnus . utils . CommonUtilsForTests . getTestTraceHead ( "[NGSICharsets.encodePostgreSQL]" ) ) + "--------<sp>Underscore<sp>is<sp>not<sp>encoded" ) ) ; java . lang . String in = "_" ; java . lang . String expected = "_" ; java . lang . String out = com . telefonica . iot . cygnus . utils . NGSICharsets . encodePostgreSQL ( in ) ; try { org . junit . Assert . assertEquals ( expected , out ) ; System . out . println ( ( ( ( ( com . telefonica . iot . cygnus . utils . CommonUtilsForTests . getTestTraceHead ( "[NGSICharsets.encodePostgreSQL]" ) ) + "-<sp>OK<sp>-<sp>'" ) + in ) + "'<sp>has<sp>not<sp>been<sp>encoded" ) ) ; } }
public class aTest{ @Test public void writeWithDefaultConfiguration ( ) { com . paritytrading . parity . file . taq . TAQ . Quote quote = new com . paritytrading . parity . file . taq . TAQ . Quote ( ) ; quote . date = "Trade<sp>Side\n" 6 ; quote . timestampMillis = ( ( 8 * 60 ) * 60 ) * 1000 ; quote . instrument = "Trade<sp>Side\n" 5 ; quote . bidPrice = 100.5 ; quote . bidSize = 1000 ; quote . askPrice = 100.75 ; quote . askSize = 250 ; try ( java . io . ByteArrayOutputStream out = new java . io . ByteArrayOutputStream ( ) ; com . paritytrading . parity . file . taq . TAQWriter writer = new com . paritytrading . parity . file . taq . TAQWriter ( out ) ) { writer . write ( quote ) ; writer . flush ( ) ; java . lang . String output = "Trade<sp>Side\n" 1 + ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( "Date\t" + "Timestamp\t" ) + "Instrument\t" ) + "Trade<sp>Side\n" 4 ) + "Trade<sp>Side\n" 7 ) + "Bid<sp>Size\t" ) + "Ask<sp>Price\t" ) + "Trade<sp>Side\n" 8 ) + "100.50\t" 4 ) + "Trade<sp>Size\t" ) + "Trade<sp>Side\n" ) + "Trade<sp>Side\n" 3 ) + "08:00:00.000\t" ) + "100.50\t" 3 ) + "100.50\t" 0 ) + "100.50\t" ) + "Trade<sp>Side\n" 9 ) + "100.75\t" ) + "100.50\t" 1 ) + "Trade<sp>Side\n" 2 ) + "Trade<sp>Side\n" 2 ) + "100.50\t" 2 ) ; org . junit . Assert . assertEquals ( output , out . toString ( "Trade<sp>Side\n" 0 ) ) ; } } }
public class aTest{ @Test public void testGetRelationshipByUuid_SLAVE ( ) { ga . uuid . GraphDatabaseService db = getMasterDatabase ( ) ; createPerson ( db , "AlessandroSlave1" ) ; createPerson ( db , "AlessandroSlave2" ) ; java . lang . Long idRel = createRelation ( db , "AlessandroSlave1" , "AlessandroSlave2" ) ; java . lang . String relUuid = getUuidForRelation ( db , idRel ) ; try ( ga . uuid . Transaction tx = db . beginTx ( ) ) { ga . uuid . Result result = db . execute ( ( ( "RETURN<sp>ga.uuid.findRelationship('" + relUuid ) + "')<sp>as<sp>n" ) ) ; while ( result . hasNext ( ) ) { java . util . Map < java . lang . String , java . lang . Object > row = result . next ( ) ; ga . uuid . Relationship rel = ( ( ga . uuid . Relationship ) ( row . get ( "n" ) ) ) ; org . junit . Assert . assertEquals ( idRel , rel . getId ( ) , 0L ) ; } }
public class aTest{ @Test public void syncGetAttributeWithProviderRuntimeException ( ) { io . joynr . proxy . ProxyBuilder < joynr . tests . testProxy > proxyBuilder = consumerRuntime . getProxyBuilder ( domain , joynr . tests . testProxy . class ) ; joynr . tests . testProxy proxy = proxyBuilder . setMessagingQos ( messagingQos ) . setDiscoveryQos ( discoveryQos ) . build ( ) ; try { proxy . getAttributeWithProviderRuntimeException ( ) ; org . junit . Assert . fail ( "Should<sp>throw<sp>ProviderRuntimeException" ) ; } catch ( joynr . exceptions . ProviderRuntimeException e ) { joynr . exceptions . ProviderRuntimeException expected = new joynr . exceptions . ProviderRuntimeException ( "ProviderRuntimeException" ) ; org . junit . Assert . assertEquals ( expected , e ) ; } }
public class aTest{ @Test public void testNoArgs ( ) { org . eclipse . ceylon . common . tool . ToolModel < org . eclipse . ceylon . tools . info . CeylonInfoTool > model = pluginLoader . loadToolModel ( "info" ) ; org . junit . Assert . assertNotNull ( model ) ; try { org . eclipse . ceylon . tools . info . CeylonInfoTool tool = pluginFactory . bindArguments ( model , getMainTool ( ) , java . util . Collections . < java . lang . String > emptyList ( ) ) ; org . junit . Assert . fail ( ) ; } }
