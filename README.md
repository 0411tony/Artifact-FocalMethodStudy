# Artifact-FocalMethodStudy
Artifact of "An Empirical Study on Focal Methods in Deep-Learning-Based Approaches for Assertion Generation".

The datasets and our evaluation results can be found at:
https://zenodo.org/record/7802160#.ZC1rGy-9HJM


## Datasets
- TCTracer: Used to evaluate adapted test-to-code traceability techniques(RQ1)
- Variant Datasets: Used to evaluate DL-based assertion generation approaches(RQ2 & RQ3)

### TCTracer
Original TCTracer is available at https://zenodo.org/record/4608587#.ZClJly-9FAc. We split test methods in TCTracer using ```Scripts/split_tctracer.py``` and check the synatx of splitted test methods manually. Splited TCTracer is shown in ```Datasets/TCTracer```.

There are four subjects in TCTracer: commons-io, commons-lang, gson, and jfreechart, and their source code can be found in ```Datasets/TCTracer/subjects```. In each directory (named after the project name, e.g. ```Datasets/TCTracer/subjects/commons-io```), splitted test methods are put in the directories, and unsplit test methods are put in the txt files.

### Variant datasets
Generated by ```Scripts/dataset_variation.py``` based on the ATLAS dataset(https://sites.google.com/view/atlas-nmt/home). All entries in ATLAS with syntax errors are removed.

- (origianl) ATLAS: focal methods with implementation identified by atlas
- DS-atlas: focal methods without implementation identified by atlas
- DS-null: without focal methods 
- DS-lcba: focal methods without implementation identified by LCBA
- DS-combined: focal methods without implementation identified by combined score

Variant datasets for TOGA (ATLAS star) are generated by ```Scripts/atlas_star_datagen.py```.


## Evaluation
### Test-to-code traceability techniques
Run  ```Scripts/eval_traceability_tech.py```.

### ATLAS
Run a docker container provided by nvidia for tensorflow. More details in https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow .
```
docker pull nvcr.io/nvidia/tensorflow:xx.xx-tf1-py3
docker run -i -d -t --gpus all nvcr.io/nvidia/tensorflow:xx.xx-tf1-py3
```

Training and inference: https://sites.google.com/view/atlas-nmt/home


### T5
Use our script```Scripts/generate_results_T5.py```.

Datasets: ```Variant_ATLAS_for_T5```

Models and other detials: https://github.com/antonio-mastropaolo/T5-learning-ICSE_2021

### IR
Use script ```Retrieval/IR.py``` in https://github.com/yh1105/Artifact-of-Assertion-ICSE22

### TOGA
Datasets: ```Variant_ATLAS_for_TOGA```

We use the docker comtainer: ```docker pull edinella/toga-artifact```. More detials at https://github.com/microsoft/toga

Fine-tuning script: ```toga/model/assertions/run_train.sh```

### Results
Get correct number: ```Scripts/inference_accuracy.py```, 
e.g.:
```
python3 inference_accuracy.py -atlas xxxx/assertLines.txt xxxx/prediction.txt
python3 inference_accuracy.py -t5 xxxx/test.tsv xxxx/prediction.txt
python3 inference_accuracy.py -toga xxxx/test.csv xxxx/prediction.csv
```

Get BLEU: ```multi-bleu.perl```, 
e.g.:
```
./multi-bleu.perl xxxx/assertLines.txt < xxxx/prediction.txt
```
